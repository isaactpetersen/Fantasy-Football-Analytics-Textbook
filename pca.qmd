# Data Reduction: Principal Component Analysis {#sec-pca}

## Getting Started {#sec-pcaGettingStarted}

### Load Packages {#sec-pcaLoadPackages}

```{r}

```

## Overview of Principal Component Analysis {#sec-pcaOverview}

Principal component analysis (PCA) is used if you want to reduce your data matrix.\index{principal component analysis}
PCA composites represent the variances of an observed measure in as economical a fashion as possible, with no latent underlying variables.\index{principal component analysis}
The goal of PCA is to identify a smaller number of components that explain as much variance in a set of variables as possible.\index{principal component analysis}\index{data!reduction}
It is an atheoretical way to decompose a matrix.\index{principal component analysis}
PCA involves decomposition of a data matrix into a set of eigenvectors, which are transformations of the old variables.\index{principal component analysis}\index{eigenvector}

The eigenvectors attempt to simplify the data in the matrix.\index{parsimony}\index{principal component analysis}\index{eigenvector}
PCA takes the data matrix and identifies the weighted sum of all variables that does the best job at explaining variance: these are the principal components, also called eigenvectors.\index{principal component analysis}\index{eigenvector}
Principal components reflect optimally weighted sums.\index{principal component analysis}

PCA decomposes the data matrix into any number of components—as many as the number of variables, which will always account for all variance.\index{principal component analysis}
After the PCA is performed, you can look at the results and discard the components which likely reflect error variance.\index{principal component analysis}
Judgments about which components to retain are based on empirical criteria in conjunction with theory to select a parsimonious number of components that account for the majority of variance.\index{principal component analysis}

The eigenvalue reflects the amount of variance explained by the component (eigenvector).\index{principal component analysis}\index{eigenvalue}
When using a varimax (orthogonal) rotation, an eigenvalue for a component is calculated as the sum of squared standardized component loadings on that component.\index{principal component analysis}\index{orthogonal rotation}\index{eigenvalue}
When using oblique rotation, however, the items explain more variance than is attributable to their factor loadings because the factors are correlated.\index{principal component analysis}\index{oblique rotation}

PCA pulls the first principal component out (i.e., the eigenvector that explains the most variance) and makes a new data matrix: i.e., new correlation matrix.\index{principal component analysis}\index{eigenvector}
Then the PCA pulls out the component that explains the next most variance—i.e., the eigenvector with the next largest eigenvalue, and it does this for all components, equal to the same number of variables.\index{principal component analysis}\index{eigenvector}\index{eigenvalue}
For instance, if there are six variables, it will iteratively extract an additional component up to six components.\index{principal component analysis}
You can extract as many eigenvectors as there are variables.\index{principal component analysis}\index{eigenvector}
If you extract all six components, the data matrix left over will be the same as the correlation matrix in Figure \@ref(fig:correlationMatrix2).\index{principal component analysis}
That is, the remaining variables (as part of the leftover data matrix) will be entirely uncorrelated with each other, because six components explain 100% of the variance from six variables.\index{principal component analysis}
In other words, you can explain (6) variables with (6) new things!\index{principal component analysis}

However, it does no good if you have to use all (6) components because there is no data reduction from the original number of variables.\index{principal component analysis}\index{data!reduction}
When the goal is data reducation (as in PCA), the hope is that the first few components will explain most of the variance, so we can explain the variability in the data with fewer components than there are variables.\index{principal component analysis}\index{data!reduction}

The sum of all eigenvalues is equal to the number of variables in the analysis.\index{principal component analysis}\index{eigenvalue}
PCA does not have the same assumptions as [factor analysis](#sec-factorAnalysis), which assumes that measures are partly from common variance and error.\index{principal component analysis}\index{factor analysis}\index{measurement error}
But if you estimate (6) eigenvectors and only keep (2), the model is a two-component model and whatever left becomes error.\index{principal component analysis}\index{measurement error}\index{eigenvector}
Therefore, PCA does not have the same assumptions as [factor analysis](#sec-factorAnalysis), but it often ends up in the same place.\index{principal component analysis}\index{factor analysis}

## Ensuring the Variables are on the Same Scale {#sec-pcaStandardize}

Before performing a PCA, it is important to ensure that the variables included in the PCA are on the same scale.
PCA seeks to identify components that explain variance in the data, so if the variables are not on the same scale, some variables may contribute considerably more variance than others.
A common way of ensuring that variables are on the same scale is to standardize them using, for example,  [*z*-scores](#sec-zScores).

## Component Rotation {#sec-pcaRotation}

Similar considerations as in [factor analysis](#sec-factorAnalysis) can be used to determine whether and how to rotate components in PCA.
The considerations for determining whether and how to rotate factors in [factor analysis](#sec-factorAnalysis) are described in @sec-factorAnalysisRotation.

## Determining the Number of Components to Retain {#sec-pcaNumComponents}

Similar criteria as in [factor analysis](#sec-factorAnalysis) can be used to determine the number of components to retain in PCA.
The criteria for determining the number of factors to retain in [factor analysis](#sec-factorAnalysis) are described in @sec-factorAnalysisNumFactors.

## Interpreting and Using PCA Components {#sec-interpretingAndUsingComponentsPCA}

The next step is interpreting the PCA components.\index{principal component analysis}
Use theory to interpret and label the components.\index{principal component analysis}

## PCA Versus Factor Analysis {#sec-pcaVsFactorAnalysis}

Both [factor analysis](#sec-factorAnalysis) and PCA can be used for data reducation.
The key distinction between [factor analysis](#sec-factorAnalysis) and PCA is depicted in @fig-pcaVsFactorAnalysis.\index{factor analysis}\index{principal component analysis}

::: {#fig-pcaVsFactorAnalysis}
![](images/pcaVsFactorAnalysis.png){fig-alt="Distinction Between Factor Analysis and Principal Component Analysis."}

Distinction Between Factor Analysis and Principal Component Analysis.
:::

There are several differences between [factor analysis](#sec-factorAnalysis) and PCA.\index{factor analysis}\index{principal component analysis}
[Factor analysis](#sec-factorAnalysis) has greater sophistication than PCA, but greater sophistication often results in greater assumptions.\index{factor analysis}\index{principal component analysis}
[Factor analysis](#sec-factorAnalysis) does not always work; the data may not always fit to a [factor analysis](#sec-factorAnalysis) model.\index{factor analysis}\index{principal component analysis}
However, PCA can decompose any data matrix; it always works.\index{principal component analysis}
PCA is okay if you are not interested in the factor structure.\index{principal component analysis}
PCA uses all variance of variables and assumes variables have no error, so it does not account for measurement error.\index{principal component analysis}\index{measurement error}
PCA is good if you just want to form a linear composite and perform data reduction.\index{principal component analysis}\index{linear composite}\index{construct!formative}\index{construct!reflective}
However, if you are interested in the factor structure, use [factor analysis](#sec-factorAnalysis), which estimates a latent variable that accounts for the common variance and discards error variance.\index{factor analysis}
[Factor analysis](#sec-factorAnalysis) better handles error than PCA—[factor analysis](#sec-factorAnalysis) assumes that what is in the variable is the combination of common construct variance and error.\index{principal component analysis}\index{factor analysis}
By contrast, PCA assumes that the measures have no measurement error.\index{principal component analysis}\index{measurement error}
[Factor analysis](#sec-factorAnalysis) is useful for the identification of latent constructs—i.e., underlying dimensions or factors that explain (cause) observed scores.\index{factor analysis}

## Conclusion {#sec-pcaConclusion}

::: {.content-visible when-format="html"}

## Session Info {#sec-pcaSessionInfo}

```{r}
sessionInfo()
```

:::
