# Basic Statistics {#sec-basicStats}

## Getting Started {#sec-basicStatsGettingStarted}

### Load Packages {#sec-basicStatsLoadPackages}

```{r}
library("petersenlab")
library("tidyverse")
```

## Descriptive Statistics {#sec-descriptiveStatistics}

Descriptive statistics are used to describe data.
For instance, they may be used to describe the center, spread, or shape of the data.
There are various indices of each.

### Center {#sec-descriptiveStatisticsCenter}

Indices to describe the *center* (central tendency) of a variable's data include:

- mean
- median
- mode

The mean of $X$ ($\bar{X}$) is calculated as:

$$
\bar{X} = \frac{\sum X_i}{n} = \frac{X_1 + X_2 + ... + X_n}{n}
$$ {#eq-IQR}

That is, to compute the mean, sum all of the values and divide by the number of values ($n$).

The median is determined as the value at the 50th percentile (i.e., the value that is higher than 50% of the values and is lower than the other 50% of values).

The mode is the most common value.

Below is R code to estimate each:

```{r}

```

### Spread {#sec-descriptiveStatisticsSpread}

Indices to describe the *spread* (variability) of a variable's data include:

- standard deviation
- variance
- range
- minimum and maximum
- interquartile range (IQR)

The (sample) variance of $X$ ($s^2$) is calculated as:

$$
s^2 = \frac{\sum (X_i - \bar{X})^2}{n-1}
$$ {#eq-variance}

The (sample) standard deviation of $X$ ($s$) is calculated as:

$$
s = \sqrt{\frac{\sum (X_i - \bar{X})^2}{n-1}}
$$ {#eq-sd}

The range is calculated of $X$ is calculated as:

$$
\text{range} = \text{maximum} - \text{minimum}
$$ {#eq-range}

The interquartile range (IQR) is calculated as:

$$
\text{IQR} = Q_3 - Q_1
$$ {#eq-IQR}

where $Q_3$ is the score at the third quartile (i.e., 75th percentile), and $Q_1$ is the score at the first quartile (i.e., 25th percentile).

Below is R code to estimate each:

```{r}

```

### Shape {#sec-descriptiveStatisticsShape}

Indices to describe the *shape* of a variable's data include:

- skewness
- kurtosis

Below is R code to estimate each:

```{r}

```

### Combination {#sec-descriptiveStatisticsCombination}

To estimate multiple indices of center, spread, and shape of the data, you can use the following code:

```{r}
#psych::describe(mydata)

#mydata %>% 
#  summarise(across(
#      everything(),
#      .fns = list(
#        n = ~ length(na.omit(.)),
#        missingness = ~ mean(is.na(.)) * 100,
#        M = ~ mean(., na.rm = TRUE),
#        SD = ~ sd(., na.rm = TRUE),
#        min = ~ min(., na.rm = TRUE),
#        max = ~ max(., na.rm = TRUE),
#        range = ~ max(., na.rm = TRUE) - min(., na.rm = TRUE),
#        IQR = ~ IQR(., na.rm = TRUE),
#        median = ~ median(., na.rm = TRUE),
#        mode = ~ petersenlab::Mode(., multipleModes = "mean"),
#        skewness = ~ psych::skew(., na.rm = TRUE),
#        kurtosis = ~ psych::kurtosi(., na.rm = TRUE)),
#      .names = "{.col}.{.fn}")) %>%
#    pivot_longer(
#      cols = everything(),
#      names_to = c("variable","index"),
#      names_sep = "\\.") %>% 
#    pivot_wider(
#      names_from = index,
#      values_from = value)
```

## Inferential Statistics {#sec-inferentialStatistics}

Inferential statistics are used to draw inferences regarding whether there is (a) a difference in level on variable across groups or (b) an association between variables.
For instance, inferential statistics may be used to evaluate whether Quarterbacks tend to have longer careers compared to Running Backs.
Or, they could be used to evaluate whether number of carries is associated with injury likelihood.
To apply inferential statistics, we make use of the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$).

### Null Hypothesis ($H_0$)

When testing whether there are differences in level across groups on a variable of interest, the null hypothesis ($H_0$) is that there is <u>no difference</u> in level across groups.
For instance, when testing whether Quarterbacks tend to have longer careers compared to Running Backs, the null hypothesis ($H_0$) is that Quarterbacks do not systematically differ from Running Backs in the length of their career.

When testing whether there is an association between variables, the null hypothesis ($H_0$) is that there is <u>no association</u> between the variables.
For instance, when testing whether number of carries is associated with injury likelihood, the null hypothesis ($H_0$) is that there is no association between number of carries and injury likelihood.

### Alternative Hypothesis ($H_1$)

The alternative hypothesis ($H_1$) is the researcher's hypothesis that they want to evaluate.
An alternative hypothesis ($H_1$) might be directional (i.e., one-sided) or non-directional (i.e., two-sided).

Directional hypotheses specify a particular direction, such as which group will have larger scores or which direction (positive or negative) two variables will be associated.
Examples of directional hypotheses include:

- Quarterbacks have <u>longer</u> careers compared to Running Backs
- Number of carries is <u>positively</u> associated with injury likelihood

Non-directional hypotheses do not specify a particular direction.
For instance, non-directional hypotheses may state that two groups differ but do not specify which group will have larger scores.
Or, non-directional hypotheses may state that two variables are associated but do not state what the sign is of the association—i.e., positive or negative.
Examples of non-directional hypotheses include:

- Quarterbacks <u>differ</u> in the length of their careers compared to Running Backs
- Number of carries is <u>associated</u> with injury likelihood

### Statistical Significance {#sec-statisticalSignificance}

In science, statistical significance is evaluated with the *p*-value.
The *p*-value does not represent the probability that you observed the result by chance.
The *p*-value represents a conditional probability—it examines the probability of one event given another event.
In particular, the *p*-value evaluates the likelihood that you would detect a result as at least as extreme as the one observed (in terms of the magnitude of the difference or of the association) given that the null hypothesis ($H_0$) is true.

This can be expressed in conditional probability notation, $Pr(A | B)$, which is the probability (likelihood) of event A occurring given that event B occurred (or given condition B).

The conditional probability notation for a left-tailed directional test (i.e., Quarterbacks have <u>shorter</u> careers than Running Backs; or number of carries is <u>negatively</u> associated with injury likelihood) is in @eq-pvalueLeftTailed.

$$
p\text{-value} = Pr(T \le t | H_0)
$$ {#eq-pvalueLeftTailed}

where $T$ is the test statistic of interest (e.g., the distribution of $t$-, $r-$, or $F$ values, depending on the test) and $t$ is the observed test statistic (e.g., $t$-, $r-$, or $F$-coefficient, depending on the test).

The conditional probability notation for a right-tailed directional test (i.e., Quarterbacks have <u>longer</u> careers than Running Backs; or number of carries is <u>positively</u> associated with injury likelihood) is in @eq-pvalueRightTailed.

$$
p\text{-value} = Pr(T \ge t | H_0)
$$ {#eq-pvalueRightTailed}

The conditional probability notation for a two-tailed non-directional test (i.e., Quarterbacks <u>differ</u> in the length of their careers compared to Running Backs; or number of carries is <u>associated</u> with injury likelihood) is in @eq-pvalueTwoTailed.

$$
p\text{-value} = 2 \times \text{min}(Pr(T \le t | H_0), Pr(T \ge t | H_0))
$$ {#eq-pvalueTwoTailed}

where `min(a, b)` is the smaller number of `a` and `b`.

If the distribution of the test statistic is symmetric around zero, the *p*-value for the two-tailed non-directional test simplifies to @eq-pvalueTwoTailedSimple.

$$
p\text{-value} = 2 \times Pr(T \ge |t| | H_0)
$$ {#eq-pvalueTwoTailedSimple}

Nevertheless, to be conservative (i.e., to avoid false positive/Type I errors), many researchers use two-tailed *p*-values regardless whether their hypothesis is one- or two-tailed.

For a test of group differences, the *p*-value evaluates the likelihood that you would observe a difference as large or larger than the one you observed between the groups if there were no systematic difference between the groups.
For instance, when evaluating whether Quarterbacks have <u>longer</u> careers than Running Backs, and you observed a mean difference of 0.03 years, the *p*-value evaluates the likelihood that you would observe a difference as larger or larger than 0.03 years between the groups if Quarterbacks do not differ from Running Backs in terms of the length of their career.

For a test of whether two variables are associated, the *p*-value evaluates the likelihood that you would observe an association as strong or stronger than the one you observed between the groups if there were no association between the variables.
For instance, when evaluating whether number of carries is <u>positively</u> associated with injury likelihood, and you observed a correlation coefficient of $r = .25$ between number of carries and injury likelihood, the *p*-value evaluates the likelihood that you would observe a correlation as strong or stronger than $r = .25$ between the variables if number of carries is not associated with injury likelihood.

In science, using what is called null-hypothesis significance testing (NHST), we typically consider an effect to be *statistically significant* if the *p*-value is less than .05.
That is, there is a small chance (5%; or 1 in 20 times) that we would observe an effect at least as extreme as the effect observed, if the null hypothesis were true.
So, you might expect around 5% of tests where the null hypothesis is true to be statistically significant just by chance.

If the *p*-value is less than .05, we reject the null hypothesis ($H_0$) that there was no difference or association.
Thus, we conclude that there was a statistically significant (non-zero) difference or association.
If the *p*-value is greater than .05, we fail to reject the null hypothesis; the difference/association was not statistically significant.
Thus, we do not have confidence that there was a difference or association.
However, we do not accept the null hypothesis; it could be there we did not observe an effect because we did not have adequate power to detect the effect—e.g., if the effect size was small, the data were noisy, and the sample size was small and/or unrepresentative.

A two-by-two confusion matrix for null-hypothesis significance testing is in @fig-nhstConfusionMatrix.

::: {#fig-nhstConfusionMatrix}
![](images/nhstConfusionMatrix.png)

A Two-by-Two Confusion Matrix for Null-Hypothesis Significance Testing.
:::

Interactive visualizations by Kristoffer Magnusson on *p*-values and null-hypothesis significance testing are below:

- <https://rpsychologist.com/pvalue/> (archived at <https://perma.cc/JP9F-9ZVY>)
- <https://rpsychologist.com/d3/pdist/> (archived at <https://perma.cc/BE96-8LSJ>)
- <https://rpsychologist.com/d3/nhst/> (archived at <https://perma.cc/ZU9A-37F3>)

Twelve misconceptions about *p*-values [@Goodman2008] are in @tbl-pValueMisconceptions.

| Number | Misconception                                                                                                                              |
|:-------|:-------------------------------------------------------------------------------------------------------------------------------------------|
| 1      | If $p = .05$, the null hypothesis has only a 5% chance of being true.                                                                      |
| 2      | A nonsignificant difference (eg, $p > .05$) means there is no difference between groups.                                                   |
| 3      | A statistically significant finding is clinically important.                                                                               |
| 4      | Studies with $p$-values on opposite sides of .05 are conflicting.                                                                          |
| 5      | Studies with the same $p$-value provide the same evidence against the null hypothesis.                                                     |
| 6      | $p = .05$ means that we have observed data that would occur only 5% of the time under the null hypothesis.                                 |
| 7      | $p = .05$ and $p < .05$ mean the same thing.                                                                                               |
| 8      | $p$-values are properly written as inequalities (e.g., "$p \le .05$" when $p = .015$).                                                     |
| 9      | $p = .05$ means that if you reject the null hypothesis, the probability of a Type I error is only 5%.                                      |
| 10     | With a $p = .05$ threshold for significance, the chance of a Type I error will be 5%.                                                      |
| 11     | You should use a one-sided $p$-value when you don't care about a result in one direction, or a difference in that direction is impossible. |
| 12     | A scientific conclusion or treatment policy should be based on whether or not the $p$-value is significant.                                |

: Twelve Misconceptions About *p*-Values from @Goodman2008. Goodman also provides a discussion about why each is false. {#tbl-pValueMisconceptions}

Statistical significance involves the *consistency* of an effect/association/difference; it suggests that the association/difference is reliably non-zero.
However, just because something is statistically significant does not mean that it is important.
In addition to statistical significance, it is also important to consider practical significance.

### Practical Significance {#sec-practicalSignificance}

*Practical significance* deals with how important the effect/association/difference is.
It is based on the magnitude of the effect, called the *effect size*.
Effect size can be quantified in various ways including:

- Cohen's $d$
- Standardized regression coefficient (beta; $\beta$)
- Correlation coefficient ($r$)
- Coefficient of determination ($R^2$)
- Eta squared ($\eta^2$)
- Partial eta squared ($\eta_p^2$)

Effect size thresholds [@McGrath2006] for small, medium, and large effect sizes are in @tbl-effectSizeThresholds.

| Effect Size Index                                   | Small       | Medium      | Large       |
|:----------------------------------------------------|:------------|:------------|:------------|
| Cohen's $d$                                         | $\ge |.20|$ | $\ge |.50|$ | $\ge |.80|$ |
| Standardized regression coefficient (beta; $\beta$) | $\ge |.10|$ | $\ge |.24|$ | $\ge |.37|$ |
| Correlation coefficient ($r$)                       | $\ge |.10|$ | $\ge |.24|$ | $\ge |.37|$ |
| Coefficient of determination ($R^2$)                | $\ge .01$   | $\ge .06$   | $\ge .14$   |
| Eta squared ($\eta^2$)                              | $\ge .01$   | $\ge .06$   | $\ge .14$   |
| Partial eta squared ($\eta_p^2$)                    | $\ge .01$   | $\ge .06$   | $\ge .14$   |

: Effect Size Thresholds for Small, Medium, and Large Effect Sizes. {#tbl-effectSizeThresholds}

## Statistical Decision Tree {#sec-statisticalDecisionTree}

An example statistical decision tree is depicted in @fig-statisticalDecisionTree.

::: {#fig-statisticalDecisionTree}
![](images/statisticalDecisionTree.png)

A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure. Adapted from: <https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf>. The original source is: Corston, R. & Colman, A. M. (2000). *A crash course in SPSS for Windows*. Wiley-Blackwell. Changes were made to the original, including the addition of several statistical tests.
:::

For an online, interactive statistical decision tree to help you decide which statistical analysis to use, see here: <https://www.statsflowchart.co.uk>
