# Basic Statistics {#sec-basicStats}

## Getting Started {#sec-basicStatsGettingStarted}

### Load Packages {#sec-basicStatsLoadPackages}

```{r}

```

## Descriptive Statistics {#sec-descriptiveStatistics}

Descriptive statistics are used to describe data.
For instance, they may be used to describe the center, spread, or shape of the data.

There are various indices of each.
Indices to describe the center of a variable's data include:

- mean
- median
- mode

Indices to describe the center of a variable's data include:

- standard deviation
- variance

Indices to describe the shape of a variable's data include:

- skewness
- kurtosis

## Inferential Statistics {#sec-inferentialStatistics}

Inferential statistics are used to draw inferences regarding whether there is (a) a difference in level on variable across groups or (b) an association between variables.
For instance, inferential statistics may be used to evaluate whether Quarterbacks tend to have longer careers compared to Running Backs.
Or, they could be used to evaluate whether number of carries is associated with injury likelihood.
To apply inferential statistics, we make use of the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$).

### Null Hypothesis ($H_0$)

When testing whether there are differences in level across groups on a variable of interest, the null hypothesis ($H_0$) is that there is <u>no difference</u> in level across groups.
For instance, when testing whether Quarterbacks tend to have longer careers compared to Running Backs, the null hypothesis ($H_0$) is that Quarterbacks do not systematically differ from Running Backs in the length of their career.

When testing whether there is an association between variables, the null hypothesis ($H_0$) is that there is <u>no association</u> between the variables.
For instance, when testing whether number of carries is associated with injury likelihood, the null hypothesis ($H_0$) is that there is no association between number of carries and injury likelihood.

### Alternative Hypothesis ($H_1$)

The alternative hypothesis ($H_1$) is the researcher's hypothesis that they want to evaluate.
An alternative hypothesis ($H_1$) might directional or non-directional.

Directional hypotheses specify a particular direction, such as which group will have larger scores or which direction (positive or negative) two variables will be associated.
Examples of directional hypotheses include:

- Quarterbacks have <u>longer</u> careers compared to Running Backs
- Number of carries is <u>positively</u> associated with injury likelihood

Non-directional hypotheses do not specify a particular direction.
For instance, non-directional hypotheses may state that two groups differ but do not specify which group will have larger scores.
Or, non-directional hypotheses may state that two variables are associated but do not state what the sign is of the association—i.e., positive or negative.
Examples of non-directional hypotheses include:

- Quarterbacks <u>differ</u> in the length of their careers compared to Running Backs
- Number of carries is <u>associated</u> with injury likelihood

### Statistical Significance {#sec-statisticalSignificance}

In science, statistical significance is evaluated with the *p*-value.
The *p*-value represents a conditional probability—it examines the probability of one event given another event.
In particular, the *p*-value evaluates the likelihood that you would detect a result as extreme as (or more extreme than) the one observed (in terms of the magnitude of the difference or of the association) given that the null hypothesis ($H_0$) is true.

This can be expressed as:

Left-tailed directional test (i.e., Quarterbacks have <u>shorter</u> careers than Running Backs; or number of carries is <u>negatively</u> associated with injury likelihood):
$$
p\text{-value} = Pr(T \le t | H_0)
$$

Right-tailed directional test (i.e., Quarterbacks have <u>longer</u> careers than Running Backs; or number of carries is <u>positively</u> associated with injury likelihood):

$$
p\text{-value} = Pr(T \ge t | H_0)
$$

Two-tailed non-directional test (i.e., Quarterbacks <u>differ</u> in the length of their careers compared to Running Backs; or number of carries is <u>associated</u> with injury likelihood):

$$
p\text{-value} = 2 \times Pr(T \ge |t| | H_0)
$$

where $T$ is the test statistic of interest (e.g., $t$-, $r-$, $F$-, or $B$-coefficient, depending on the test), $t$ is the observed test statistic, and $Pr$ is the probability of the event.

That is, for a test of group differences, the *p*-value evaluates the likelihood that you would observe a difference as large or larger than the one you observed between the groups if there were no systematic difference between the groups.
For instance, when evaluating whether Quarterbacks have <u>longer</u> careers than Running Backs, and you observed a mean difference of 0.03 years, the *p*-value evaluates the likelihood that you would observe a difference as larger or larger than 0.03 years between the groups if Quarterbacks do not differ from Running Backs in terms of the length of their career.

That is, for a test of whether two variables are associated, the *p*-value evaluates the likelihood that you would observe an association as strong or stronger than the one you observed between the groups if there were no association between the variables.
For instance, when evaluating whether number of carries is <u>positively</u> associated with injury likelihood, and you observed a correlation coefficient of $r = .25$ between number of carries and injury likelihood, the *p*-value evaluates the likelihood that you would observe a correlation as strong or stronger than $r = .25$ between the variables if number of carries is not associated with injury likelihood.

In science, we typically consider an effect to be *statistically significant* if the *p*-value is less than .05.
That is, there is a small chance (5%; or 1 in 20 times) that we would observe an effect as extreme as (or more extreme than) the effect observed, if the null hypothesis were true.
Statistical significance involves the *consistency* of an effect/association/difference; it suggests that the association/difference is reliably non-zero.
However, just because something is statistically significant does not mean that it is important.
In addition to statistical significance, it is also important to consider practical significance.

### Practical Significance {#sec-practicalSignificance}

*Practical significance* deals with how important the effect/association/difference is.
It is based on the magnitude of the effect, called the *effect size*.
Effect size can be quantified in various ways including:

- Cohen's $d$
- Standardized regression coefficient (beta; $\beta$)
- Correlation coefficient ($r$)
- Coefficient of determination ($R^2$)
- Eta squared ($\eta^2$)
- Partial eta squared ($\eta_p^2$)

| Effect Size Index                             | Small       | Medium      | Large       |
|:----------------------------------------------|:------------|:------------|:------------|
| Cohen's d                                     | $\ge |.20|$ | $\ge |.50|$ | $\ge |.80|$ |
| Standardized regression coefficient ($\beta$) | $\ge |.10|$ | $\ge |.24|$ | $\ge |.37|$ |
| Correlation coefficient ($r$)                 | $\ge |.10|$ | $\ge |.24|$ | $\ge |.37|$ |
| Coefficient of determination ($R^2$)          | $\ge .01$   | $\ge .06$   | $\ge .14$   |
| Eta squared ($\eta^2$)                        | $\ge .01$   | $\ge .06$   | $\ge .14$   |
| Partial eta squared ($\eta_p^2$)              | $\ge .01$   | $\ge .06$   | $\ge .14$   |

: Effect Size Thresholds for Small, Medium, and Large Effect Sizes. {#tbl-effectSize Thresholds}

## Statistical Decision Tree {#sec-statisticalDecisionTree}

An example statistical decision tree is depicted in @fig-statisticalDecisionTree.

::: {#fig-statisticalDecisionTree}
![](images/statisticalDecisionTree.png)

A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure. Adapted from: <https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf>. The original source is: Corston, R., & Colman, A. M. (2000). *A crash course in SPSS for Windows*. Wiley-Blackwell. Changes were made to the original, including the addition of several statistical tests.
:::

For an online, interactive statistical decision tree to help you decide which statistical analysis to use, see here: <https://www.statsflowchart.co.uk>
