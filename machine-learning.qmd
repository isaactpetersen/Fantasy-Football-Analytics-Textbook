{{< include _chunk-timing.qmd >}}

# Machine Learning {#sec-machineLearning}

This chapter provides an overview of machine learning.

## Getting Started {#sec-machineLearningGettingStarted}

### Load Packages {#sec-machineLearningLoadPackages}

```{r}
library("petersenlab")
library("future")
library("missRanger")
library("powerjoin")
library("tidymodels")
library("LongituRF")
library("gpboost")
library("effectsize")
library("tidyverse")
library("knitr")
```

### Load Data {#sec-machineLearningLoadData}

```{r}
#| eval: false
#| include: false

# Downloaded Data - Processed
load(file = "./data/nfl_players.RData")
load(file = "./data/nfl_teams.RData")
load(file = "./data/nfl_rosters.RData")
load(file = "./data/nfl_rosters_weekly.RData")
load(file = "./data/nfl_schedules.RData")
load(file = "./data/nfl_combine.RData")
load(file = "./data/nfl_draftPicks.RData")
load(file = "./data/nfl_depthCharts.RData")
#load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/nfl_pbp.RData", fsep = ""))
#load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/nfl_4thdown.RData", fsep = ""))
#load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/nfl_participation.RData", fsep = ""))
#load(file = "./data/nfl_actualFantasyPoints_weekly.RData")
load(file = "./data/nfl_injuries.RData")
load(file = "./data/nfl_snapCounts.RData")
load(file = "./data/nfl_espnQBR_seasonal.RData")
load(file = "./data/nfl_espnQBR_weekly.RData")
load(file = "./data/nfl_nextGenStats_weekly.RData")
load(file = "./data/nfl_advancedStatsPFR_seasonal.RData")
load(file = "./data/nfl_advancedStatsPFR_weekly.RData")
load(file = "./data/nfl_playerContracts.RData")
load(file = "./data/nfl_ftnCharting.RData")
load(file = "./data/nfl_playerIDs.RData")
load(file = "./data/nfl_rankings_draft.RData")
load(file = "./data/nfl_rankings_weekly.RData")
load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/nfl_expectedFantasyPoints_weekly.RData", fsep = ""))
#load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/nfl_expectedFantasyPoints_pbp.RData", fsep = ""))

# Calculated Data - Processed
load(file = "./data/nfl_actualStats_player_career.RData")
load(file = "./data/nfl_actualStats_seasonal.RData")
load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/player_stats_weekly.RData", fsep = ""))
load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/player_stats_seasonal.RData", fsep = ""))
```

```{r}
# Downloaded Data - Processed
load(file = "./data/nfl_players.RData")
load(file = "./data/nfl_teams.RData")
load(file = "./data/nfl_rosters.RData")
load(file = "./data/nfl_rosters_weekly.RData")
load(file = "./data/nfl_schedules.RData")
load(file = "./data/nfl_combine.RData")
load(file = "./data/nfl_draftPicks.RData")
load(file = "./data/nfl_depthCharts.RData")
#load(file = "./data/nfl_pbp.RData")
#load(file = "./data/nfl_4thdown.RData")
#load(file = "./data/nfl_participation.RData")
#load(file = "./data/nfl_actualFantasyPoints_weekly.RData")
load(file = "./data/nfl_injuries.RData")
load(file = "./data/nfl_snapCounts.RData")
load(file = "./data/nfl_espnQBR_seasonal.RData")
load(file = "./data/nfl_espnQBR_weekly.RData")
load(file = "./data/nfl_nextGenStats_weekly.RData")
load(file = "./data/nfl_advancedStatsPFR_seasonal.RData")
load(file = "./data/nfl_advancedStatsPFR_weekly.RData")
load(file = "./data/nfl_playerContracts.RData")
load(file = "./data/nfl_ftnCharting.RData")
load(file = "./data/nfl_playerIDs.RData")
load(file = "./data/nfl_rankings_draft.RData")
load(file = "./data/nfl_rankings_weekly.RData")
load(file = "./data/nfl_expectedFantasyPoints_weekly.RData")
#load(file = "./data/nfl_expectedFantasyPoints_pbp.RData")

# Calculated Data - Processed
load(file = "./data/nfl_actualStats_player_career.RData")
load(file = "./data/nfl_actualStats_seasonal.RData")
load(file = "./data/player_stats_weekly.RData")
load(file = "./data/player_stats_seasonal.RData")
```

```{r}
#| label: free-up-memory1-ml
#| include: false

gc()
```

We created the `player_stats_weekly.RData` and `player_stats_seasonal.RData` objects in @sec-calculatePlayerAge.

### Specify Options {#machineLearningSpecifyOptions}

```{r}
options(scipen = 999) # prevent scientific notation
```

## Overview of Machine Learning {#sec-machineLearningOverview}

Machine learning takes us away from focusing on [causal inference](#sec-causalInference).
Machine learning does not care about which processes are causal—i.e., which processes influence the outcome.
Instead, machine learning cares about prediction—it cares about a predictor variable to the extent that it increases predictive accuracy regardless of whether it is causally related to the outcome.

Machine learning can be useful for leveraging big data and lots of predictor variable to develop predictive models with greater accuracy.
However, many machine learning techniques are black boxes—it is often unclear how or why certain predictions are made, which can make it difficult to interpret the model's decisions and understand the underlying relationships between variables.
Machine learning tends to be a data-driven, atheoretical technique.
This can result in [overfitting](#sec-overfitting).
Thus, when estimating machine learning models, it is common to keep a hold-out sample for use in [cross-validation](#sec-crossValidation) to evaluate the extent of shrinkage of model coefficients.
The data that the model is trained on is known as the "training data".
The data that the model was not trained on but is then is independently tested on—i.e., the hold-out sample—is the "test data".
Shrinkage occurs when predictor variables explain some random error variance in the original model.
When the model is applied to an independent sample (i.e., the test data), the predictive model will likely not perform quite as well, and the regressions coefficients will tend to get smaller (i.e., shrink).

If the test data were collected as part of the same processes as the original data and were merely held out for purposes of analysis, this is called internal [cross-validation](#sec-crossValidation).
If the test data were collected separately from the original data used to train the model, this is called external [cross-validation](#sec-crossValidation).

Most machine learning methods were developed with cross-sectional data in mind.
That is, they assume that each person has only one observation on the outcome variable.
However, with longitudinal data, each person has multiple observations on the outcome variable.

When performing machine learning, various approaches may help address this:

- transform data from [long to wide](#sec-longToWide) form, so that each person has only one row
- when designing the training and test sets, keep all measurements from the same person in the same data object (either the training or test set); do not have some measurements from a given person in the training set and other measurements from the same person in the test set
- use a machine learning approach that accounts for the clustered/nested nature of the data

## Types of Machine Learning {#sec-machineLearningTypes}

There are many approaches to machine learning.
This chapter discusses several key ones:

- supervised learning
    - continuous outcome (i.e., regression)
      - [linear regression](#sec-fittingModels-regression)
      - [least absolute shrinkage and selection option (LASSO) regression](#sec-lasso)
      - [ridge regression](#sec-ridgeRegression)
      - [elastic net regression](#sec-elasticNet)
      - [random forest](#sec-randomForest)
    - categorical outcome (i.e., classification)
      - [logistic regression](#sec-multipleRegressionTypes)
      - support vector machine
      - [random forest](#sec-randomForest)
      - boosting
        - [tree boosting](#sec-treeBoosting)
- unsupervised learning
    - [cluster analysis](#sec-clusterAnalysis)
    - [principal component analysis](#sec-pca)
    - [factor analysis](#sec-factorAnalysis)
- semi-supervised learning
- reinforcement learning
    - deep learning
- ensemble

*Ensemble* machine learning methods combine multiple machine learning approaches with the goal that combining multiple approaches might lead to more accurate predictions than any one method might be able to achieve on its own.

### Supervised Learning {#sec-machineLearningTypesSupervised}

Supervised learning involves learning from data where the correct classification or outcome is known.
For instance, predicting how many points a player will score is a supervised learning task, because there is a ground truth—the actual number of points scored—that can be used to train and evaluate the model.

Unlike linear and logistic regression, various machine learning techniques can handle [multicollinearity](#sec-multipleRegressionMulticollinearity), including [LASSO regression](#sec-lasso), [ridge regression](#sec-ridgeRegression), and [elastic net regression](#sec-elasticNet).
[Least absolute shrinkage and selection option (LASSO) regression](#sec-lasso) performs selection of which predictor variables to keep in the model by shrinking some coefficients to zero, effectively removing them from the model.
[Ridge regression](#sec-ridgeRegression) shrinks the coefficients of predictor variables toward zero, but not to zero, so it does not perform selection of which predictor variables to retain; this allows it to yield stable estimates for multiple correlated predictor variables in the context of [multicollinearity](#sec-multipleRegressionMulticollinearity).
[Elastic net](#sec-elasticNet) involves a combination of [LASSO](#sec-lasso) and [ridge](#sec-ridgeRegression) regression; it performs selection of which predictor variables to keep by shrinking the coefficients of some predictor variables to zero (like [LASSO](#sec-lasso), for variable selection), and it shrinks the coefficients of some predictor variables toward zero (like [ridge](#sec-ridgeRegression), for handling [multicollinearity](#sec-multipleRegressionMulticollinearity) among correlated predictors).

Unless interactions or nonlinear terms are specified, linear, logistic, [LASSO](#sec-lasso), [ridge](#sec-ridgeRegression), and [elastic net](#sec-elasticNet) regression assume additive and linear associations between the predictors and outcome.
That is, they do not automatically account for interactions among the predictor variables or for nonlinear associations between the predictor variables and the outcome variable (unless interaction terms or nonlinear transformations are explicitly included).
By contrast, random forests and tree boosting methods automatically account for interactions and nonlinear associations between predictors and the outcome variable.
These models recursively partition the data in ways that capture complex patterns without the need to manually specify interaction or polynomial terms.

### Unsupervised Learning {#sec-machineLearningTypesUnsupervised}

Unsupervised learning involves learning from data without known classifications.
Unsupervised learning is used to discover hidden patterns, groupings, or structures in the data.
For instance, if we want to identify different subtypes of Wide Receivers based on their playing style or performance metrics, or uncover underlying dimensions in a large dataset, we would use an unsupervised learning approach.

We describe [cluster analysis](#sec-clusterAnalysis) in @sec-clusterAnalysis.
We describe [principal component analysis](#sec-pca) in @sec-pca.

### Semi-supervised Learning {#sec-machineLearningTypesSemisupervised}

Semi-supervised learning combines supervised learning and unsupervised learning by training the model on some data for which the classification is known and some data for which the classification is not known.

### Reinforcement Learning {#sec-machineLearningTypesReinforcement}

Reinforcement learning involves an agent learning to make decisions by interacting with the environment.
Through trial and error, the agent receives feedback in the form of rewards or penalties and learns a strategy that maximizes the cumulative reward over time.

## Data Processing {#sec-machineLearningDataProcessing}

Several data processing steps are necessary to get the data in the form necessary for machine learning.

```{r}
#| eval: false
#| include: false

varNames <- c(
  names(nfl_players),
  names(nfl_teams),
  names(nfl_rosters),
  names(nfl_rosters_weekly),
  names(nfl_schedules),
  names(nfl_combine),
  names(nfl_draftPicks),
  names(nfl_depthCharts),
  #names(nfl_pbp),
  #names(nfl_4thdown),
  #names(nfl_participation),
  #names(nfl_actualFantasyPoints_player_weekly),
  names(nfl_injuries),
  names(nfl_snapCounts),
  names(nfl_espnQBR_seasonal),
  names(nfl_espnQBR_weekly),
  names(nfl_nextGenStats_weekly),
  names(nfl_advancedStatsPFR_seasonal),
  names(nfl_advancedStatsPFR_weekly),
  names(nfl_playerContracts),
  names(nfl_ftnCharting),
  names(nfl_playerIDs),
  names(nfl_rankings_draft),
  names(nfl_rankings_weekly),
  names(nfl_expectedFantasyPoints_weekly)#,
  #names(nfl_expectedFantasyPoints_pbp)
)

varNames <- unique(varNames)

write.csv(
  varNames,
  file = "./data/varNames.csv",
  row.names = FALSE
)

nfl_players$gsis_id
nfl_rosters$gsis_id
nfl_rosters_weekly$gsis_id
nfl_draftPicks$gsis_id
nfl_depthCharts$gsis_id
nfl_advancedStatsPFR_seasonal$gsis_id

#nfl_actualStats_offense_weekly$player_id
nfl_expectedFantasyPoints_weekly$player_id

nfl_rankings_draft$id
nfl_rankings_weekly$fantasypros_id

nfl_combine$pfr_id
nfl_advancedStatsPFR_seasonal$pfr_id
#nfl_advancedStatsPFR_seasonal$pfr_player_id

nfl_playerIDs$gsis_id
nfl_playerIDs$pfr_id
```

### Prepare Data for Merging {#sec-machineLearningPrepareDataForMerging}

First, we apply several steps.
We subset to the positions and variables of interest.
We also rename columns and change variable types to make sure they match the column names and types across objects, which will be important later when we merge the data.

```{r}
#| include: false

#-todo:
#-calculate years_of_experience
#-use common name for the same (gsis_id) ID variable
```

```{r}
#| label: prepare-data-merging-machine-learning

# Prepare data for merging

#nfl_actualFantasyPoints_player_weekly <- nfl_actualFantasyPoints_player_weekly %>% 
#  rename(gsis_id = player_id)
#
#nfl_actualFantasyPoints_player_seasonal <- nfl_actualFantasyPoints_player_seasonal %>% 
#  rename(gsis_id = player_id)

player_stats_seasonal_offense <- player_stats_seasonal %>% 
  filter(position_group %in% c("QB","RB","WR","TE")) %>% 
  rename(gsis_id = player_id)

player_stats_weekly_offense <- player_stats_weekly %>% 
  filter(position_group %in% c("QB","RB","WR","TE")) %>% 
  rename(gsis_id = player_id)

## Rename other variables to ensure common names

## Ensure variables with the same name have the same type
nfl_players <- nfl_players %>% 
  mutate(
    birth_date = as.Date(birth_date),
    jersey_number = as.character(jersey_number),
    nfl_id = as.character(nfl_id),
    years_of_experience = as.integer(years_of_experience))

player_stats_seasonal_offense <- player_stats_seasonal_offense %>% 
  mutate(
    birth_date = as.Date(birth_date),
    jersey_number = as.character(jersey_number))

nfl_rosters <- nfl_rosters %>% 
  mutate(
    draft_number = as.integer(draft_number))

nfl_rosters_weekly <- nfl_rosters_weekly %>% 
  mutate(
    draft_number = as.integer(draft_number))

nfl_depthCharts <- nfl_depthCharts %>% 
  mutate(
    season = as.integer(season))

nfl_expectedFantasyPoints_weekly <- nfl_expectedFantasyPoints_weekly %>% 
  rename(gsis_id = player_id) %>% 
  mutate(
    season = as.integer(season),
    receptions = as.integer(receptions)) %>% 
  distinct(gsis_id, season, week, .keep_all = TRUE) # drop duplicated rows

## Rename variables
nfl_draftPicks <- nfl_draftPicks %>%
  rename(
    games_career = games,
    pass_completions_career = pass_completions,
    pass_attempts_career = pass_attempts,
    pass_yards_career = pass_yards,
    pass_tds_career = pass_tds,
    pass_ints_career = pass_ints,
    rush_atts_career = rush_atts,
    rush_yards_career = rush_yards,
    rush_tds_career = rush_tds,
    receptions_career = receptions,
    rec_yards_career = rec_yards,
    rec_tds_career = rec_tds,
    def_solo_tackles_career = def_solo_tackles,
    def_ints_career = def_ints,
    def_sacks_career = def_sacks
  )

## Subset variables
nfl_expectedFantasyPoints_weekly <- nfl_expectedFantasyPoints_weekly %>% 
  select(gsis_id:position, contains("_exp"), contains("_diff"), contains("_team")) #drop "raw stats" variables (e.g., rec_yards_gained) so they don't get coalesced with actual stats

# Check duplicate ids
player_stats_seasonal_offense %>% 
  group_by(gsis_id, season) %>% 
  filter(n() > 1) %>% 
  head()

nfl_advancedStatsPFR_seasonal %>% 
  group_by(gsis_id, season) %>% 
  filter(n() > 1, !is.na(gsis_id)) %>% 
  select(gsis_id, pfr_id, season, team, everything()) %>% 
  head()
```

```{r}
#| label: free-up-memory2-ml
#| include: false

gc()
```

Below, we identify shared variable names across objects to be merged to make sure we account for them in merging:

```{r}
dplyr::intersect(
  names(nfl_players),
  names(nfl_draftPicks))

length(na.omit(nfl_players$position)) # use by default (more cases)
length(na.omit(nfl_draftPicks$position))

dplyr::intersect(
  names(player_stats_seasonal_offense),
  names(nfl_advancedStatsPFR_seasonal))

length(na.omit(player_stats_seasonal_offense$season)) # use by default (more cases)
length(na.omit(nfl_advancedStatsPFR_seasonal$season))

length(na.omit(player_stats_seasonal_offense$team)) # use by default (more cases)
length(na.omit(nfl_advancedStatsPFR_seasonal$team))

length(na.omit(player_stats_seasonal_offense$age)) # use by default (more cases)
length(na.omit(nfl_advancedStatsPFR_seasonal$age))

dplyr::intersect(
  names(nfl_rosters_weekly),
  names(nfl_expectedFantasyPoints_weekly))

length(na.omit(nfl_rosters_weekly$season)) # use by default (more cases)
length(na.omit(nfl_expectedFantasyPoints_weekly$season))

length(na.omit(nfl_rosters_weekly$week)) # use by default (more cases)
length(na.omit(nfl_expectedFantasyPoints_weekly$week))

length(na.omit(nfl_rosters_weekly$position)) # use by default (more cases)
length(na.omit(nfl_expectedFantasyPoints_weekly$position))

length(na.omit(nfl_rosters_weekly$full_name)) # use by default (more cases)
length(na.omit(nfl_expectedFantasyPoints_weekly$full_name))
```

### Merge Data {#sec-machineLearningMergeData}

To perform machine learning, we need all of the [predictor variables](#sec-correlationalStudy) and the [outcome variable](#sec-correlationalStudy) in the same data file.
Thus, we must merge data files.
To merge data, we use the `powerjoin` package [@R-powerjoin], which allows coalescing variables with the same name from two different objects.
We specify `coalesce_xy`, which means that—for variables that have the same name across both objects—it keeps the value from object 1 (if present); if not, it keeps the value from object 2.
We first merge variables from objects that have the same structure—player data (i.e., `id` form), seasonal data (i.e., `id`-`season` form), or weekly data (i.e., `id`-`season`-`week` form).

```{r}
#| include: false

#-todo:
#-remove redundant variables
```

```{r}
#| label: merge-data1-machine-learning

# Create lists of objects to merge, depending on data structure: id; or id-season; or id-season-week
playerListToMerge <- list(
  nfl_players %>% filter(!is.na(gsis_id)),
  nfl_draftPicks %>% filter(!is.na(gsis_id)) %>% select(-season)
)

playerSeasonListToMerge <- list(
  player_stats_seasonal_offense %>% filter(!is.na(gsis_id), !is.na(season)),
  nfl_advancedStatsPFR_seasonal %>% filter(!is.na(gsis_id), !is.na(season))
)

playerSeasonWeekListToMerge <- list(
  nfl_rosters_weekly %>% filter(!is.na(gsis_id), !is.na(season), !is.na(week)),
  #nfl_actualStats_offense_weekly,
  nfl_expectedFantasyPoints_weekly %>% filter(!is.na(gsis_id), !is.na(season), !is.na(week))
  #nfl_advancedStatsPFR_weekly,
)

playerSeasonWeekPositionListToMerge <- list(
  nfl_depthCharts %>% filter(!is.na(gsis_id), !is.na(season), !is.na(week))
)

# Merge data
playerMerged <- playerListToMerge %>% 
  reduce(
    powerjoin::power_full_join,
    by = c("gsis_id"),
    conflict = powerjoin::coalesce_xy) # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2

playerSeasonMerged <- playerSeasonListToMerge %>% 
  reduce(
    powerjoin::power_full_join,
    by = c("gsis_id","season"),
    conflict = powerjoin::coalesce_xy) # where the objects have the same variable name (e.g., team), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2

playerSeasonWeekMerged <- playerSeasonWeekListToMerge %>% 
  reduce(
    powerjoin::power_full_join,
    by = c("gsis_id","season","week"),
    conflict = powerjoin::coalesce_xy) # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2
```

```{r}
#| label: free-up-memory3-ml
#| include: false

gc()
```

To prepare for merging player data with seasonal data, we identify shared variable names across the objects:

```{r}
dplyr::intersect(
  names(playerSeasonMerged),
  names(playerMerged))
```

Then we merge the player data with the seasonal data:

```{r}
#| label: merge-data2-machine-learning

seasonalData <- powerjoin::power_full_join(
  playerSeasonMerged,
  playerMerged %>% select(-age, -years_of_experience, -team, -latest_team, -last_season, -pff_status), # drop variables from id objects that change from year to year (and thus are not necessarily accurate for a given season)
  by = "gsis_id",
  conflict = powerjoin::coalesce_xy # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2
) %>% 
  filter(!is.na(season)) %>% 
  select(gsis_id, season, player_display_name, position, team, games, everything())
```

```{r}
#| label: free-up-memory4-ml
#| include: false

gc()
```

To prepare for merging player and seasonal data with weekly data, we identify shared variable names across the objects:

```{r}
dplyr::intersect(
  names(playerSeasonWeekMerged),
  names(seasonalData))
```

Then we merge the player and seasonal data with the weekly data:

```{r}
#| label: merge-data3-machine-learning

seasonalAndWeeklyData <- powerjoin::power_full_join(
  playerSeasonWeekMerged,
  seasonalData,
  by = c("gsis_id","season"),
  conflict = powerjoin::coalesce_xy # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2
) %>% 
  filter(!is.na(week)) %>% 
  select(gsis_id, season, week, full_name, position, team, everything())
```

```{r}
#| label: free-up-memory5-ml
#| include: false

gc()
```

```{r}
# Duplicate cases
seasonalData %>% 
  group_by(gsis_id, season) %>% 
  filter(n() > 1) %>% 
  head()

seasonalAndWeeklyData %>% 
  group_by(gsis_id, season, week) %>% 
  filter(n() > 1) %>% 
  head()
```

### Additional Processing {#sec-mlAdditionalProcessing}

For purposes of machine learning, we set all character and logical columns to factors.

```{r}
#| label: additional-processing-machine-learning

# Convert character and logical variables to factors
seasonalData <- seasonalData %>% 
  mutate(
    across(
      where(is.character),
      as.factor
    ),
    across(
      where(is.logical),
      as.factor
    )
  )
```

```{r}
#| label: free-up-memory6-ml
#| include: false

gc()
```

### Fill in Missing Data for Static Variables {#sec-fillMissingData}

For variables that are not expected to change, such as a player's name and position, we fill in missing values by using a player's value on those variables from other rows in the data.

```{r}
#| label: fill-in-missing-data-machine-learning

seasonalData <- seasonalData %>% 
  arrange(gsis_id, season) %>% 
  group_by(gsis_id) %>% 
  fill(
    player_name, player_display_name, pos, position, position_group,
    .direction = "downup") %>% 
  ungroup()
```

```{r}
#| label: free-up-memory7-ml
#| include: false

gc()
```

### Create New Data Object for Merging with Later Predictions {#sec-newDataObject}

We create a new data object that contains the latest seasonal data, for merging with later predictions.

```{r}
#| label: create-new-data-machine-learning

newData_seasonal <- seasonalData %>% 
  filter(season == max(season, na.rm = TRUE))
```

### Lag Fantasy Points {#sec-lagFantasyPoints}

To develop a machine learning model that uses a player's performance metrics in a given season for predicting the player's fantasy points in the subsequent season, we need to include the player's fantasy points from the subsequent season in the same row as the previous season's performance metrics.
Thus, we need to create a lagged variable for fantasy points.
That way, 2024 fantasy points are in the same row as 2023 performance metrics, 2023 fantasy points are in the same row as 2023 performance metrics, and so on.
We call this the lagged fantasy points variable (`fantasyPoints_lag`).
We also retain the original same-year fantasy points variable (`fantasyPoints`) so it can be used as predictor of their subsequent-year fantasy points.

```{r}
#| label: lag-fantasy-points-machine-learning

seasonalData_lag <- seasonalData %>% 
  arrange(gsis_id, season) %>% 
  group_by(gsis_id) %>% 
  mutate(
    fantasyPoints_lag = lead(fantasyPoints)
  ) %>% 
  ungroup()

seasonalData_lag %>% 
  select(gsis_id, player_display_name, season, fantasyPoints, fantasyPoints_lag) # verify that lagging worked as expected
```

```{r}
#| label: free-up-memory8-ml
#| include: false

gc()
```

### Subset to Predictor Variables and Outcome Variable {#sec-subsetToPredictorsAndOutcome}

Then, we drop variables that we do not want to include in the model as our [predictor](#sec-correlationalStudy) or [outcome](#sec-correlationalStudy) variable.
Thus, all of the variables in the object are our [predictor](#sec-correlationalStudy) and [outcome](#sec-correlationalStudy) variables.

```{r}
#| label: subset-data-machine-learning

seasonalData_lag %>% select_if(~class(.) == "Date")
seasonalData_lag %>% select_if(is.character)
seasonalData_lag %>% select_if(is.factor)
seasonalData_lag %>% select_if(is.logical)

dropVars <- c(
  "birth_date", "player_display_name", "team", "player_name", "headshot_url", "season_type", "fg_made_list", "fg_missed_list", "fg_blocked_list", "gwfg_distance_list", "pff_status", "startdate", "pos", "merge_name", "pfr_player_id", "cfb_player_id", "hof", "category", "side", "college", "car_av", "display_name", "common_first_name", "first_name", "last_name", "short_name", "football_name", "suffix", "esb_id", "nfl_id", "pff_id", "otc_id", "espn_id", "smart_id", "ngs_position_group", "ngs_position", "headshot", "college_name", "college_conference", "jersey_number", "status", "ngs_status", "ngs_status_short_description", "pff_position", "draft_team", "pfr_player_name", "pfr_id")

seasonalData_lag_subset <- seasonalData_lag %>% 
  dplyr::select(-any_of(dropVars))
```

```{r}
#| label: free-up-memory9-ml
#| include: false

gc()
```

### Separate by Position {#sec-separateByPosition}

Then, we separate the objects by position, so we can develop different machine learning models for each position.

```{r}
#| label: separate-by-position-machine-learning

seasonalData_lag_subsetQB <- seasonalData_lag_subset %>% 
  filter(position == "QB") %>% 
  select(
    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,
    height, weight, rookie_season, draft_pick,
    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,
    completions:rushing_2pt_conversions, special_teams_tds, contains(".pass"), contains(".rush"))

seasonalData_lag_subsetRB <- seasonalData_lag_subset %>% 
  filter(position == "RB") %>% 
  select(
    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,
    height, weight, rookie_season, draft_pick,
    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,
    carries:special_teams_tds, contains(".rush"), contains(".rec"))

seasonalData_lag_subsetWR <- seasonalData_lag_subset %>% 
  filter(position == "WR") %>% 
  select(
    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,
    height, weight, rookie_season, draft_pick,
    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,
    carries:special_teams_tds, contains(".rush"), contains(".rec"))

seasonalData_lag_subsetTE <- seasonalData_lag_subset %>% 
  filter(position == "TE") %>% 
  select(
    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,
    height, weight, rookie_season, draft_pick,
    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,
    carries:special_teams_tds, contains(".rush"), contains(".rec"))
```

```{r}
#| label: free-up-memory10-ml
#| include: false

gc()
```

### Split into Test and Training Data {#sec-splitTestTraining}

Because machine learning can leverage many predictors, it is at high risk of [overfitting](#sec-overfitting)—explaining error variance that would not generalize to new data, such as data for new players or future seasons.
Thus, it is important to develop and tune the machine learning model so as not to [overfit](sec-overfitting) the model.
In machine learning, it is common to use [cross-validation](#sec-crossValidation) where we train the model on a subset of the observations, and we evaluate how well the model generalizes to unseen (e.g., "hold-out") observations.
Then, we select the model parameters by how well the model generalizes to the hold-out data, so we are selecting  a model that maximizes accuracy and generalizability (i.e., parsimony).

For internal [cross-validation](#sec-crossValidation), it is common to divide the data into three subsets:

1. training data
1. validation data
1. test data

The training set is used to fit the model.
It is usually the largest portion of the data.
We fit various models to the training set based on which parameters we want to evaluate (e.g., how many trees to use in a tree-boosting model).

The models fit with the training set are then evaluated using the unseen observations in the validation set.
The validation set is used to tune the model parameters and prevent [overfitting](#sec-overfitting).
We select the model parameters that yield the greatest accuracy in the validation set.
In [*k*-fold cross-validation](#sec-kFoldCrossValidation), the validation set rotates across folds, thus replacing the need for a separate validation set.

The test set is used after model training and tuning to evaluate the model's generalizability to unseen data.

Below, we split the data into test and training data.
Our ultimate goal is to predict next year's fantasy points.
However, to do that effectively, we must first develop a model for which we can evaluate its accuracy against historical fantasy points (because we do not yet know players will score in the future).
We want to include all current/active players in our training data, so that our predictions of their future performance can be accounted for by including their prior data in the model.
Thus, we use retired players as our hold-out (test) data.
We split our data into 80% training data and 20% testing data.
The 20% testing data thus includes all retired players, but not all retired players are in the testing data.

Then, for the analysis, we can either a) use rotating folds (as the case for [*k*-fold](#sec-kFoldCrossValidation) and [leave-one-out [LOO]](#sec-looCrossValidation) cross-validation) for which a separate validation set (from the training set) is not needed, as we do in @sec-folds, or we can b) subdivide the training set into an inner training set and validation set, as we do in @sec-treeBoostingOptimalTuningParameters.

```{r}
#| eval: false
#| include: false

# Notes to self:
# - Using only retired players as the holdout sample, so that we can include all current/active players in the training model

# CURRENTLY (WILL CHANGE):
# 
# - seasonalData: 1999-2024: i.e. all data currently available
# - seasonalData_lag: 1999-2023 (predicting fantasy points in 2024)
# - newData_seasonal: 2024 (to be used for predicting fantasy points in 2025)

# to create:
# 
# - seasonalData_lag_all: 1999-2023 (predicting fantasy points in 2024)
# - seasonalData_lag_train: 1999-2023 (predicting fantasy points in 2024), most players
# - seasonalData_lag_test: 1999-2022 (predicting fantasy points in 2023), (some of the) retired players; 2023 rather than 2024 because don't have fantasy points for retired players in most recent year
# - (eventually, newData_seasonal, which is derived from the imputed version of seasonalData_lag_all, and then keeps only data from 2024, thus predicting fantasy points in 2025, and removing the fantasy_points_lag variable)

# To impute:
# 
# - seasonalData_lag_all: 1999-2023 (predicting fantasy points in 2024)
# - seasonalData_lag_train: 1999-2023 (predicting fantasy points in 2024), most players
# - seasonalData_lag_test: 1999-2022 (predicting fantasy points in 2023), (some of the) retired players

# IMPUTATION AND MODEL PROCESS:
# 
# IMPUTATION:
# - training data
# - test data
# - all data (training and test data), used for generating next year predictions
# 
# MODEL:
# - training data (imputed version of seasonalData_lag_all): all seasons (except 2024 predicting 2025), most players
# 
# EVALUATE MODEL PREDICTIONS:
# - test data (imputed version of seasonalData_lag_test): all seasons (except 2024 predicting 2025), (some of the) retired players
# 
# GENERATE MODEL PREDICTIONS
# - next year data (newData_seasonal): 2024 (predicting 2025)
```

```{r}
#| label: split-test-training-machine-learning

seasonalData_lag_qb_all <- seasonalData_lag_subsetQB
seasonalData_lag_rb_all <- seasonalData_lag_subsetRB
seasonalData_lag_wr_all <- seasonalData_lag_subsetWR
seasonalData_lag_te_all <- seasonalData_lag_subsetTE

set.seed(52242) # for reproducibility (to keep the same train/holdout players)

activeQBs <- unique(seasonalData_lag_qb_all$gsis_id[which(seasonalData_lag_qb_all$season == max(seasonalData_lag_qb_all$season, na.rm = TRUE))])
retiredQBs <- unique(seasonalData_lag_qb_all$gsis_id[which(seasonalData_lag_qb_all$gsis_id %ni% activeQBs)])
numQBs <- length(unique(seasonalData_lag_qb_all$gsis_id))
qbHoldoutIDs <- sample(retiredQBs, size = ceiling(.2 * numQBs)) # holdout 20% of players

activeRBs <- unique(seasonalData_lag_rb_all$gsis_id[which(seasonalData_lag_rb_all$season == max(seasonalData_lag_rb_all$season, na.rm = TRUE))])
retiredRBs <- unique(seasonalData_lag_rb_all$gsis_id[which(seasonalData_lag_rb_all$gsis_id %ni% activeRBs)])
numRBs <- length(unique(seasonalData_lag_rb_all$gsis_id))
rbHoldoutIDs <- sample(retiredRBs, size = ceiling(.2 * numRBs)) # holdout 20% of players

set.seed(52242) # for reproducibility (to keep the same train/holdout players); added here to prevent a downstream error with predict.missRanger() due to missingness; this suggests that an error can arise from including a player in the holdout sample who has missingness in particular variables; would be good to identify which player(s) in the holdout sample evoke that error to identify the kinds of missingness that yield the error

activeWRs <- unique(seasonalData_lag_wr_all$gsis_id[which(seasonalData_lag_wr_all$season == max(seasonalData_lag_wr_all$season, na.rm = TRUE))])
retiredWRs <- unique(seasonalData_lag_wr_all$gsis_id[which(seasonalData_lag_wr_all$gsis_id %ni% activeWRs)])
numWRs <- length(unique(seasonalData_lag_wr_all$gsis_id))
wrHoldoutIDs <- sample(retiredWRs, size = ceiling(.2 * numWRs)) # holdout 20% of players

activeTEs <- unique(seasonalData_lag_te_all$gsis_id[which(seasonalData_lag_te_all$season == max(seasonalData_lag_te_all$season, na.rm = TRUE))])
retiredTEs <- unique(seasonalData_lag_te_all$gsis_id[which(seasonalData_lag_te_all$gsis_id %ni% activeTEs)])
numTEs <- length(unique(seasonalData_lag_te_all$gsis_id))
teHoldoutIDs <- sample(retiredTEs, size = ceiling(.2 * numTEs)) # holdout 20% of players
  
seasonalData_lag_qb_train <- seasonalData_lag_qb_all %>% 
  filter(gsis_id %ni% qbHoldoutIDs)
seasonalData_lag_qb_test <- seasonalData_lag_qb_all %>% 
  filter(gsis_id %in% qbHoldoutIDs)

seasonalData_lag_rb_train <- seasonalData_lag_rb_all %>% 
  filter(gsis_id %ni% rbHoldoutIDs)
seasonalData_lag_rb_test <- seasonalData_lag_rb_all %>% 
  filter(gsis_id %in% rbHoldoutIDs)

seasonalData_lag_wr_train <- seasonalData_lag_wr_all %>% 
  filter(gsis_id %ni% wrHoldoutIDs)
seasonalData_lag_wr_test <- seasonalData_lag_wr_all %>% 
  filter(gsis_id %in% wrHoldoutIDs)

seasonalData_lag_te_train <- seasonalData_lag_te_all %>% 
  filter(gsis_id %ni% teHoldoutIDs)
seasonalData_lag_te_test <- seasonalData_lag_te_all %>% 
  filter(gsis_id %in% teHoldoutIDs)
```

```{r}
#| label: free-up-memory11-ml
#| include: false

rm(nfl_players)
rm(nfl_teams)
rm(nfl_rosters)
rm(nfl_rosters_weekly)
rm(nfl_schedules)
rm(nfl_combine)
rm(nfl_draftPicks)
rm(nfl_depthCharts)
rm(nfl_pbp)
rm(nfl_4thdown)
rm(nfl_participation)
rm(nfl_actualFantasyPoints_weekly)
rm(nfl_injuries)
rm(nfl_snapCounts)
rm(nfl_espnQBR_seasonal)
rm(nfl_espnQBR_weekly)
rm(nfl_nextGenStats_weekly)
rm(nfl_advancedStatsPFR_seasonal)
rm(nfl_advancedStatsPFR_weekly)
rm(nfl_playerContracts)
rm(nfl_ftnCharting)
#rm(nfl_playerIDs)
rm(nfl_rankings_draft)
rm(nfl_rankings_weekly)
rm(nfl_expectedFantasyPoints_weekly)
rm(nfl_expectedFantasyPoints_pbp)
rm(nfl_actualStats_career)
rm(nfl_actualStats_career_player)
rm(nfl_actualStats_career_player_inclPost)
rm(nfl_actualStats_seasonal)
rm(nfl_actualStats_seasonal_player)
rm(nfl_actualStats_seasonal_player_inclPost)
rm(nfl_actualStats_seasonal_team)
rm(nfl_actualStats_seasonal_team_inclPost)
rm(player_stats_weekly)
rm(player_stats_weekly_offense)
rm(player_stats_seasonal)
rm(player_stats_seasonal_offense)

rm(playerListToMerge)
rm(playerMerged)
rm(playerSeasonListToMerge)
rm(playerSeasonMerged)
rm(playerSeasonWeekListToMerge)
rm(playerSeasonWeekMerged)
rm(playerSeasonWeekPositionListToMerge)
rm(seasonalAndWeeklyData)

gc()
```

### Impute the Missing Data {#sec-missingDataImputation}

```{r}
#| eval: false
#| include: false

# Notes to self

# - CONSIDER WIDENING THE DATA TO AVOID MULTILEVEL IMPUTATION
# - CONSIDER IMPUTING THE TRAINING AND TEST DATA SEPARATELY BY POSITION
```

Many of the machine learning approaches described in this chapter require no missing observations in order for a case to be included in the analysis.
In this section, we demonstrate one approach to imputing missing data.
Here is a vignette demonstrating how to impute missing data using `missForest()`: <https://rpubs.com/lmorgan95/MissForest> (archived at: <https://perma.cc/6GB4-2E22>).
Below, we impute the training data (and all data) separately by position.
We then use the imputed training data to make out-of-sample predictions to fill in the missing data for the testing data.
We do not want to impute the training and testing data together so that we can keep them separate for the purposes of [cross-validation](#sec-crossValidation).
However, we impute all data (training and test data together) for purposes of making out-of-sample predictions from the machine learning models to predict players' performance next season (when actuals are not yet available for evaluating their accuracy).
To impute data, we use the `missRanger` package [@R-missRanger].

::: {#nte-machineLearningImputeMissingData .callout-note title="Impute missing data for machine learning"}
Note: the following code takes a while to run.
:::

```{r}
#| label: impute-qb-machine-learning

# QBs
seasonalData_lag_qb_all_imp <- missRanger::missRanger(
  seasonalData_lag_qb_all,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_qb_all_imp

data_all_qb <- seasonalData_lag_qb_all_imp$data
data_all_qb$fantasyPointsMC_lag <- scale(data_all_qb$fantasyPoints_lag, scale = FALSE) # mean-centered
data_all_qb_matrix <- data_all_qb %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
newData_qb <- data_all_qb %>% 
  filter(season == max(season, na.rm = TRUE)) %>% 
  select(-fantasyPoints_lag, -fantasyPointsMC_lag)
newData_qb_matrix <- data_all_qb_matrix[
  data_all_qb_matrix[, "season"] == max(data_all_qb_matrix[, "season"], na.rm = TRUE), # keep only rows with the most recent season
  , # all columns
  drop = FALSE]

dropCol_qb <- which(colnames(newData_qb_matrix) %in% c("fantasyPoints_lag","fantasyPointsMC_lag"))
newData_qb_matrix <- newData_qb_matrix[, -dropCol_qb, drop = FALSE]

seasonalData_lag_qb_train_imp <- missRanger::missRanger(
  seasonalData_lag_qb_train,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_qb_train_imp

data_train_qb <- seasonalData_lag_qb_train_imp$data
data_train_qb$fantasyPointsMC_lag <- scale(data_train_qb$fantasyPoints_lag, scale = FALSE) # mean-centered
data_train_qb_matrix <- data_train_qb %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()

seasonalData_lag_qb_test_imp <- predict(
  object = seasonalData_lag_qb_train_imp,
  newdata = seasonalData_lag_qb_test,
  seed = 52242)

data_test_qb <- seasonalData_lag_qb_test_imp
data_test_qb_matrix <- data_test_qb %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
```

```{r}
#| label: free-up-memory12-ml
#| include: false

gc()
```

```{r}
#| label: impute-rb-machine-learning
#| eval: false

# RBs
seasonalData_lag_rb_all_imp <- missRanger::missRanger(
  seasonalData_lag_rb_all,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_rb_all_imp

data_all_rb <- seasonalData_lag_rb_all_imp$data
data_all_rb$fantasyPointsMC_lag <- scale(data_all_rb$fantasyPoints_lag, scale = FALSE) # mean-centered
data_all_rb_matrix <- data_all_rb %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
newData_rb <- data_all_rb %>% 
  filter(season == max(season, na.rm = TRUE)) %>% 
  select(-fantasyPoints_lag, -fantasyPointsMC_lag)
newData_rb_matrix <- data_all_rb_matrix[
  data_all_rb_matrix[, "season"] == max(data_all_rb_matrix[, "season"], na.rm = TRUE), # keep only rows with the most recent season
  , # all columns
  drop = FALSE]

dropCol_rb <- which(colnames(newData_rb_matrix) %in% c("fantasyPoints_lag","fantasyPointsMC_lag"))
newData_rb_matrix <- newData_rb_matrix[, -dropCol_rb, drop = FALSE]

seasonalData_lag_rb_train_imp <- missRanger::missRanger(
  seasonalData_lag_rb_train,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_rb_train_imp

data_train_rb <- seasonalData_lag_rb_train_imp$data
data_train_rb$fantasyPointsMC_lag <- scale(data_train_rb$fantasyPoints_lag, scale = FALSE) # mean-centered
data_train_rb_matrix <- data_train_rb %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()

seasonalData_lag_rb_test_imp <- predict(
  object = seasonalData_lag_rb_train_imp,
  newdata = seasonalData_lag_rb_test,
  seed = 52242)

data_test_rb <- seasonalData_lag_rb_test_imp
data_test_rb_matrix <- data_test_rb %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
```

```{r}
#| label: free-up-memory13-ml
#| include: false

gc()
```

```{r}
#| label: impute-wr-machine-learning
#| eval: false

# WRs
seasonalData_lag_wr_all_imp <- missRanger::missRanger(
  seasonalData_lag_wr_all,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_wr_all_imp

data_all_wr <- seasonalData_lag_wr_all_imp$data
data_all_wr$fantasyPointsMC_lag <- scale(data_all_wr$fantasyPoints_lag, scale = FALSE) # mean-centered
data_all_wr_matrix <- data_all_wr %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
newData_wr <- data_all_wr %>% 
  filter(season == max(season, na.rm = TRUE)) %>% 
  select(-fantasyPoints_lag, -fantasyPointsMC_lag)
newData_wr_matrix <- data_all_wr_matrix[
  data_all_wr_matrix[, "season"] == max(data_all_wr_matrix[, "season"], na.rm = TRUE), # keep only rows with the most recent season
  , # all columns
  drop = FALSE]

dropCol_wr <- which(colnames(newData_wr_matrix) %in% c("fantasyPoints_lag","fantasyPointsMC_lag"))
newData_wr_matrix <- newData_wr_matrix[, -dropCol_wr, drop = FALSE]

seasonalData_lag_wr_train_imp <- missRanger::missRanger(
  seasonalData_lag_wr_train,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_wr_train_imp

data_train_wr <- seasonalData_lag_wr_train_imp$data
data_train_wr$fantasyPointsMC_lag <- scale(data_train_wr$fantasyPoints_lag, scale = FALSE) # mean-centered
data_train_wr_matrix <- data_train_wr %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()

seasonalData_lag_wr_test_imp <- predict(
  object = seasonalData_lag_wr_train_imp,
  newdata = seasonalData_lag_wr_test,
  seed = 52242)

data_test_wr <- seasonalData_lag_wr_test_imp
data_test_wr_matrix <- data_test_wr %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
```

```{r}
#| label: free-up-memory14-ml
#| include: false

gc()
```

```{r}
#| label: impute-te-machine-learning
#| eval: false

# TEs
seasonalData_lag_te_all_imp <- missRanger::missRanger(
  seasonalData_lag_te_all,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_te_all_imp

data_all_te <- seasonalData_lag_te_all_imp$data
data_all_te$fantasyPointsMC_lag <- scale(data_all_te$fantasyPoints_lag, scale = FALSE) # mean-centered
data_all_te_matrix <- data_all_te %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
newData_te <- data_all_te %>% 
  filter(season == max(season, na.rm = TRUE)) %>% 
  select(-fantasyPoints_lag, -fantasyPointsMC_lag)
newData_te_matrix <- data_all_te_matrix[
  data_all_te_matrix[, "season"] == max(data_all_te_matrix[, "season"], na.rm = TRUE), # keep only rows with the most recent season
  , # all columns
  drop = FALSE]

dropCol_te <- which(colnames(newData_te_matrix) %in% c("fantasyPoints_lag","fantasyPointsMC_lag"))
newData_te_matrix <- newData_te_matrix[, -dropCol_te, drop = FALSE]

seasonalData_lag_te_train_imp <- missRanger::missRanger(
  seasonalData_lag_te_train,
  pmm.k = 5,
  verbose = 2,
  seed = 52242,
  keep_forests = TRUE)

seasonalData_lag_te_train_imp

data_train_te <- seasonalData_lag_te_train_imp$data
data_train_te$fantasyPointsMC_lag <- scale(data_train_te$fantasyPoints_lag, scale = FALSE) # mean-centered
data_train_te_matrix <- data_train_te %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()

seasonalData_lag_te_test_imp <- predict(
  object = seasonalData_lag_te_train_imp,
  newdata = seasonalData_lag_te_test,
  seed = 52242)

data_test_te <- seasonalData_lag_te_test_imp
data_test_te_matrix <- data_test_te %>%
  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %>% 
  as.matrix()
```

```{r}
#| label: free-up-memory15-ml
#| include: false

gc()
```

## Identify Cores for Parallel Processing {#sec-coresParallel}

```{r}
#| label: parallel-cores-machine-learning
#| include: false

num_cores <- parallelly::availableCores()
num_true_cores <- parallelly::availableCores(logical = FALSE)
```

```{r}
#| eval: false

num_cores <- parallelly::availableCores() - 1
num_true_cores <- parallelly::availableCores(logical = FALSE) - 1
```

```{r}
num_cores
```

We use the `future` package [@R-future] for parallel (faster) processing.

```{r}
future::plan(future::multisession, workers = num_cores)
```

## Set up the Cross-Validation Folds {#sec-folds}

In the examples below, we predict the future fantasy points of Quarterbacks.
However, the examples could be applied to any of the positions.
There are various approaches to [cross-validation](#sec-crossValidation).
In the examples below, we use [*k*-fold cross-validation](#sec-kFoldCrossValidation).
However, we also provide the code to apply [leave-one-out (LOO) cross-validation](#sec-looCrossValidation).

### *k*-Fold Cross-Validation {#sec-kFoldCrossValidation}

*k*-fold cross-validation partitions the data into *k* folds (subsets).
In each of the *k* iterations, the model is trained on $k - 1$ folds and is evaluated on the remaining fold.
For example, in a 10-fold cross-validation (i.e., $k = 10$), as used below, the model is trained 10 times, each time leaving out a different 10% of the data for validation.
*k*-fold cross-validation is widely used because it tends to yield stable estimates of model performance, by balancing bias and variance.
It is also computationally efficient, requiring only *k* model fits to evaluate model performance.

We set up the *k* folds using the `rsample::group_vfold_cv()` function of the `rsample` package [@R-rsample].

```{r}
set.seed(52242) # for reproducibility

folds_kFold <- rsample::group_vfold_cv(
  data_train_qb,
  group = gsis_id, # ensures all rows for a player are in the training set or all in the validation set for each fold
  v = 10) # 10-fold cross-validation
```

### Leave-One-Out (LOO) Cross-Validation {#sec-looCrossValidation}

Leave-one-out (LOO) cross-validation partitions the data into *n* folds, where *n* is the sample size.
In each of the *n* iterations, the model is trained on $n - 1$ observations and is evaluated on the one left out.
For example, in a LOO cross-validation with 100 players, the model is trained 100 times, each time leaving out a different player for validation.
LOO cross-validation is especially useful when the dataset is small—too small to form reliable training sets in [*k*-fold cross-validation](#sec-kFoldCrossValidation) (e.g., with $k = 5$ or $k = 10$, which divide the sample into 5 or 10 folds, respectively).
However, LOO tends to be less computationally efficient because it requires more model fits than [*k*-fold cross-validation](#sec-kFoldCrossValidation).
LOO tends to have low bias, producing performance estimates closer to those obtained when fitting the model to the full dataset, because each model is trained on nearly all the data.
However, LOO also tends to have high variance in its error estimates, because each validation fold contains only a single observation, making those estimates more sensitive to individual data points.

We set up the LOO folds using the `rsample::loo_cv()` function of the `rsample` package [@R-rsample].

```{r}
set.seed(52242) # for reproducibility

folds_loo <- rsample::loo_cv(data_train_qb)
```

## Fitting the Traditional Linear Regression Models {#sec-fittingModels-regression}

We describe linear regression in @sec-multipleRegression.

### Regression with One Predictor {#sec-regressionOnePredictor}

Below, we fit a linear regression model with one predictor and evaluate it with [cross-validation](#sec-crossValidation).
We also evaluate its accuracy on the hold-out (test) data.
For each of the models, we fit and evaluate the models using the `tidymodels` ecosystem of packages [@R-tidymodels; @Kuhn2020_packages].
Modeling using `tidymodels` is described in @Kuhn2023: <https://www.tmwr.org>.
We specify our model formula using the `recipes::recipe()` function of the `recipes` package [@R-recipes].
We define the model using the `parsnip::linear_reg()`, `parsnip::set_engine()`, and `parsnip::set_mode()` functions of the `parsnip` package [@R-parsnip].
We specify the workflow using the `workflows::workflow()`, `workflows::add_recipe()`, and `workflows::add_model()` functions of the `workflows` package [@R-workflows].
We fit the cross-validation model using the `tune::fit_resamples()` function of the `tune` package [@R-tune].
We specify the accuracy metrics to evaluate using the `yardstick::metric_set()` function of the `yardstick` package [@R-yardstick].
We fit the final model using the `workflows::fit()` function of the `workflows` package [@R-workflows].
We evaluate the accuracy of the model's predictions on the test data using the `petersenlab::accuracyOverall()` of the [`petersenlab`](https://cran.r-project.org/web/packages/petersenlab/index.html) package [@R-petersenlab].

```{r}
# Set seed for reproducibility
set.seed(52242)

# Set up Cross-Validation
folds <- folds_kFold

# Define Recipe (Formula)
rec <- recipes::recipe(
  fantasyPoints_lag ~ fantasyPoints,
  data = data_train_qb)

# Define Model
lm_spec <- parsnip::linear_reg() %>%
  parsnip::set_engine("lm") %>%
  parsnip::set_mode("regression")

# Workflow
lm_wf <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(lm_spec)

# Fit Model with Cross-Validation
cv_results <- tune::fit_resamples(
  lm_wf,
  resamples = folds,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_resamples(save_pred = TRUE)
)

# View Cross-Validation metrics
tune::collect_metrics(cv_results)

# Fit Final Model on Training Data
final_model <- workflows::fit(
  lm_wf,
  data = data_train_qb)

# View Coefficients
final_model %>% 
  workflows::extract_fit_parsnip() %>% 
  broom::tidy()

final_model %>%
  workflows::extract_fit_parsnip() %>% 
  effectsize::standardize_parameters()

# Predict on Test Data
df <- data_test_qb %>%
  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)

# Evaluate Accuracy of Predictions
petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)
```

```{r}
#| include: false

accuracyIndices_onePredictor_train <- tune::collect_metrics(cv_results)

rsquare_onePredictor_train <- round(as.numeric(accuracyIndices_onePredictor_train[accuracyIndices_onePredictor_train$.metric == "rsq", c("mean")]), digits = 2)

accuracyIndices_onePredictor <- petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)

rsquare_onePredictor <- round(accuracyIndices_onePredictor$rsquared, digits = 2)
me_onePredictor <- round(accuracyIndices_onePredictor$ME, digits = 2)
mae_onePredictor <- round(accuracyIndices_onePredictor$MAE, digits = 2)
```

There was modest [shrinkage](#sec-shrinkage) from the training model to the test model: the $R^2$ for the model on the training data was `{r} rsquare_onePredictor_train`; the $R^2$ for the same model applied to the test data was `{r} rsquare_onePredictor`.

@fig-regressionOnePredictor depicts the predicted versus actual fantasy points for the model on the test data.

```{r}
#| label: fig-regressionOnePredictor
#| fig-cap: "Predicted Versus Actual Fantasy Points for Regression Model with One Predictor (Player Age)."
#| fig-alt: "Predicted Versus Actual Fantasy Points for Regression Model with One Predictor (Player Age)."

# Calculate combined range for axes
axis_limits <- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)

ggplot(
  df,
  aes(
    x = pred,
    y = fantasyPoints_lag)) +
  geom_point(
    size = 2,
    alpha = 0.6) +
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "dashed") +
  coord_equal(
    xlim = axis_limits,
    ylim = axis_limits) +
  labs(
    title = "Predicted vs Actual Fantasy Points (Test Data)",
    x = "Predicted Fantasy Points",
    y = "Actual Fantasy Points"
  ) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

Below are the model predictions for next year's fantasy points:

```{r}
newData_qb %>%
  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %>% 
  left_join(
    .,
    nfl_playerIDs %>% select(gsis_id, name),
    by = "gsis_id"
  ) %>% 
  select(name, fantasyPoints_lag) %>% 
  arrange(-fantasyPoints_lag)
```

### Regression with Multiple Predictors {#sec-regressionMultiplePredictors}

Below, we fit a linear regression model with multiple predictors and evaluate it with [cross-validation](#sec-crossValidation).
We also evaluate its accuracy on the hold-out (test) data.

```{r}
# Set seed for reproducibility
set.seed(52242)

# Set up Cross-Validation
folds <- folds_kFold

# Define Recipe (Formula)
rec <- recipes::recipe(
  fantasyPoints_lag ~ ., # use all predictors
  data = data_train_qb %>% select(-gsis_id, -fantasyPointsMC_lag))

# Define Model
lm_spec <- parsnip::linear_reg() %>%
  parsnip::set_engine("lm") %>%
  parsnip::set_mode("regression")

# Workflow
lm_wf <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(lm_spec)

# Fit Model with Cross-Validation
cv_results <- tune::fit_resamples(
  lm_wf,
  resamples = folds,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_resamples(save_pred = TRUE)
)

# View Cross-Validation metrics
tune::collect_metrics(cv_results)

# Fit Final Model on Training Data
final_model <- workflows::fit(
  lm_wf,
  data = data_train_qb)

# View Coefficients
final_model %>% 
  workflows::extract_fit_parsnip() %>% 
  broom::tidy()

final_model %>%
  workflows::extract_fit_parsnip() %>% 
  effectsize::standardize_parameters()

# Predict on Test Data
df <- data_test_qb %>%
  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)

# Evaluate Accuracy of Predictions
petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)
```

```{r}
#| include: false

accuracyIndices_multiplePredictors_train <- tune::collect_metrics(cv_results)

rsquare_multiplePredictors_train <- round(as.numeric(accuracyIndices_multiplePredictors_train[accuracyIndices_multiplePredictors_train$.metric == "rsq", c("mean")]), digits = 2)

accuracyIndices_multiplePredictors <- petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)

rsquare_multiplePredictors <- round(accuracyIndices_multiplePredictors$rsquared, digits = 2)
me_multiplePredictors <- round(accuracyIndices_multiplePredictors$ME, digits = 2)
mae_multiplePredictors <- round(accuracyIndices_multiplePredictors$MAE, digits = 2)
```

There was modest [shrinkage](#sec-shrinkage) from the training model to the test model: the $R^2$ for the model on the training data was `{r} rsquare_multiplePredictors_train`; the $R^2$ for the same model applied to the test data was `{r} rsquare_multiplePredictors`.

@fig-regressionMultiplePredictors depicts the predicted versus actual fantasy points for the model on the test data.

```{r}
#| label: fig-regressionMultiplePredictors
#| fig-cap: "Predicted Versus Actual Fantasy Points for Regression Model with Multiple Predictors."
#| fig-alt: "Predicted Versus Actual Fantasy Points for Regression Model with Multiple Predictors."

# Calculate combined range for axes
axis_limits <- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)

ggplot(
  df,
  aes(
    x = pred,
    y = fantasyPoints_lag)) +
  geom_point(
    size = 2,
    alpha = 0.6) +
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "dashed") +
  coord_equal(
    xlim = axis_limits,
    ylim = axis_limits) +
  labs(
    title = "Predicted vs Actual Fantasy Points (Test Data)",
    x = "Predicted Fantasy Points",
    y = "Actual Fantasy Points"
  ) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

Below are the model predictions for next year's fantasy points:

```{r}
newData_qb %>%
  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %>% 
  left_join(
    .,
    nfl_playerIDs %>% select(gsis_id, name),
    by = "gsis_id"
  ) %>% 
  select(name, fantasyPoints_lag) %>% 
  arrange(-fantasyPoints_lag)
```

## Fitting the Machine Learning Models {#sec-fittingModels-machineLearning}

### Least Absolute Shrinkage and Selection Option (LASSO) {#sec-lasso}

Below, we fit a LASSO model.
We evaluate it and tune its penalty parameter with [cross-validation](#sec-crossValidation).
The penalty parameter in a LASSO model controls the strength of regularization applied to the model coefficients.
Smaller penalty values result in less regularization, allowing the model to retain more predictors with larger (nonzero) coefficients.
This typically reduces bias but increases variance of the model's predictions, as the model may [overfit](#sec-overfitting) to the training data by including irrelevant or weak predictors.
Larger penalty values apply stronger regularization, shrinking more coefficients exactly to zero.
This encourages a sparser model that may increase bias (by excluding useful predictors) but reduces variance and improves generalizability by simplifying the model and reducing [overfitting](#sec-overfitting).

After tuning the model, we evaluate its accuracy on the hold-out (test) data.

The LASSO models were fit using the `glmnet` package [@R-glmnet; @Friedman2010_packages; @Tay2023_packages].

For the machine learning models, we perform the parameter tuning using the `tune::tune()`, `tune::tune_grid()`, `tune::select_best()`, and `tune::finalize_workflow()` functions of the `tune` package [@R-tune].
We specify the grid of possible parameter values using the `dials::grid_regular()` function of the `dials` package [@R-dials].

```{r}
# Set seed for reproducibility
set.seed(52242)

# Set up Cross-Validation
folds <- folds_kFold

# Define Recipe (Formula)
rec <- recipes::recipe(
  fantasyPoints_lag ~ ., # use all predictors
  data = data_train_qb %>% select(-gsis_id, -fantasyPointsMC_lag))

# Define Model
lasso_spec <- 
  parsnip::linear_reg(
    penalty = tune::tune(),
    mixture = 1) %>%
  parsnip::set_engine("glmnet")

# Workflow
lasso_wf <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(lasso_spec)

# Define grid of penalties to try (log scale is typical)
penalty_grid <- dials::grid_regular(
  dials::penalty(range = c(-4, -1)),
  levels = 20)

# Tune the Penalty Parameter
cv_results <- tune::tune_grid(
  lasso_wf,
  resamples = folds,
  grid = penalty_grid,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_grid(save_pred = TRUE)
)

# View Cross-Validation metrics
tune::collect_metrics(cv_results)

# Identify best penalty
tune::select_best(cv_results, metric = "rmse")
tune::select_best(cv_results, metric = "mae")
tune::select_best(cv_results, metric = "rsq")

best_penalty <- tune::select_best(cv_results, metric = "mae")

# Finalize Workflow with Best Penalty
final_wf <- tune::finalize_workflow(
  lasso_wf,
  best_penalty)

# Fit Final Model on Training Data
final_model <- workflows::fit(
  final_wf,
  data = data_train_qb)

# View Coefficients
final_model %>% 
  workflows::extract_fit_parsnip() %>% 
  broom::tidy()

# Predict on Test Data
df <- data_test_qb %>%
  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)

# Evaluate Accuracy of Predictions
petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)
```

```{r}
#| include: false

accuracyIndices_lasso_train <- tune::collect_metrics(cv_results)

rsquare_lasso_train <- accuracyIndices_lasso_train %>% 
  filter(.metric == "rsq" & penalty == best_penalty$penalty) %>% 
  select(mean) %>% 
  pull() %>% 
  as.numeric() %>% 
  round(., 2)

accuracyIndices_lasso <- petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)

rsquare_lasso <- round(accuracyIndices_lasso$rsquared, digits = 2)
me_lasso <- round(accuracyIndices_lasso$ME, digits = 2)
mae_lasso <- round(accuracyIndices_lasso$MAE, digits = 2)
```

There was modest [shrinkage](#sec-shrinkage) from the training model to the test model: the $R^2$ for the model on the training data was `{r} rsquare_lasso_train`; the $R^2$ for the same model applied to the test data was `{r} rsquare_lasso`.

@fig-lassoModel depicts the predicted versus actual fantasy points for the model on the test data.

```{r}
#| label: fig-lassoModel
#| fig-cap: "Predicted Versus Actual Fantasy Points for Least Absolute Shrinkage and Selection Option (Lasso) Model."
#| fig-alt: "Predicted Versus Actual Fantasy Points for Least Absolute Shrinkage and Selection Option (Lasso) Model."

# Calculate combined range for axes
axis_limits <- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)

ggplot(
  df,
  aes(
    x = pred,
    y = fantasyPoints_lag)) +
  geom_point(
    size = 2,
    alpha = 0.6) +
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "dashed") +
  coord_equal(
    xlim = axis_limits,
    ylim = axis_limits) +
  labs(
    title = "Predicted vs Actual Fantasy Points (Test Data)",
    x = "Predicted Fantasy Points",
    y = "Actual Fantasy Points"
  ) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

Below are the model predictions for next year's fantasy points:

```{r}
newData_qb %>%
  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %>% 
  left_join(
    .,
    nfl_playerIDs %>% select(gsis_id, name),
    by = "gsis_id"
  ) %>% 
  select(name, fantasyPoints_lag) %>% 
  arrange(-fantasyPoints_lag)
```

### Ridge Regression {#sec-ridgeRegression}

Below, we fit a ridge regression model.
We evaluate it and tune its penalty parameter with [cross-validation](#sec-crossValidation).
The penalty parameter in a ridge regression model controls the amount of regularization applied to the model's coefficients.
Smaller penalty values allow coefficients to remain large, resulting in a model that closely fits the training data.
This may reduce bias but increases the risk of [overfitting](#sec-overfitting), especially in the presence of [multicollinearity](#sec-multipleRegressionMulticollinearity) or many weak predictors.
Larger penalty values shrink coefficients toward zero (though not exactly to zero), which reduces model complexity.
This typically increases bias slightly but reduces variance of the model's predictions, making the model more stable and better suited for generalization to new data.

After tuning the model, we also evaluate its accuracy on the hold-out (test) data.

The ridge regression models were fit using the `glmnet` package [@R-glmnet; @Friedman2010_packages; @Tay2023_packages].

```{r}
# Set seed for reproducibility
set.seed(52242)

# Set up Cross-Validation
folds <- folds_kFold

# Define Recipe (Formula)
rec <- recipes::recipe(
  fantasyPoints_lag ~ ., # use all predictors
  data = data_train_qb %>% select(-gsis_id, -fantasyPointsMC_lag))

# Define Model
ridge_spec <- 
  parsnip::linear_reg(
    penalty = tune::tune(),
    mixture = 0) %>%
  parsnip::set_engine("glmnet")

# Workflow
ridge_wf <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(ridge_spec)

# Define grid of penalties to try (log scale is typical)
penalty_grid <- dials::grid_regular(
  dials::penalty(range = c(-4, -1)),
  levels = 20)

# Tune the Penalty Parameter
cv_results <- tune::tune_grid(
  ridge_wf,
  resamples = folds,
  grid = penalty_grid,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_grid(save_pred = TRUE)
)

# View Cross-Validation metrics
tune::collect_metrics(cv_results)

# Identify best penalty
tune::select_best(cv_results, metric = "rmse")
tune::select_best(cv_results, metric = "mae")
tune::select_best(cv_results, metric = "rsq")

best_penalty <- tune::select_best(cv_results, metric = "mae")

# Finalize Workflow with Best Penalty
final_wf <- tune::finalize_workflow(
  ridge_wf,
  best_penalty)

# Fit Final Model on Training Data
final_model <- workflows::fit(
  final_wf,
  data = data_train_qb)

# View Coefficients
final_model %>% 
  workflows::extract_fit_parsnip() %>% 
  broom::tidy()

# Predict on Test Data
df <- data_test_qb %>%
  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)

# Evaluate Accuracy of Predictions
petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)
```

```{r}
#| include: false

accuracyIndices_ridge_train <- tune::collect_metrics(cv_results)

rsquare_ridge_train <- accuracyIndices_ridge_train %>% 
  filter(.metric == "rsq" & penalty == best_penalty$penalty) %>% 
  select(mean) %>% 
  pull() %>% 
  as.numeric() %>% 
  round(., 2)

accuracyIndices_ridge <- petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)

rsquare_ridge <- round(accuracyIndices_ridge$rsquared, digits = 2)
me_ridge <- round(accuracyIndices_ridge$ME, digits = 2)
mae_ridge <- round(accuracyIndices_ridge$MAE, digits = 2)
```

There was modest [shrinkage](#sec-shrinkage) from the training model to the test model: the $R^2$ for the model on the training data was `{r} rsquare_ridge_train`; the $R^2$ for the same model applied to the test data was `{r} rsquare_ridge`.

@fig-ridgeRegressionModel depicts the predicted versus actual fantasy points for the model on the test data.

```{r}
#| label: fig-ridgeRegressionModel
#| fig-cap: "Predicted Versus Actual Fantasy Points for Ridge Regression Model."
#| fig-alt: "Predicted Versus Actual Fantasy Points for Ridge Regression Model."

# Calculate combined range for axes
axis_limits <- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)

ggplot(
  df,
  aes(
    x = pred,
    y = fantasyPoints_lag)) +
  geom_point(
    size = 2,
    alpha = 0.6) +
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "dashed") +
  coord_equal(
    xlim = axis_limits,
    ylim = axis_limits) +
  labs(
    title = "Predicted vs Actual Fantasy Points (Test Data)",
    x = "Predicted Fantasy Points",
    y = "Actual Fantasy Points"
  ) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

Below are the model predictions for next year's fantasy points:

```{r}
newData_qb %>%
  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %>% 
  left_join(
    .,
    nfl_playerIDs %>% select(gsis_id, name),
    by = "gsis_id"
  ) %>% 
  select(name, fantasyPoints_lag) %>% 
  arrange(-fantasyPoints_lag)
```

### Elastic Net {#sec-elasticNet}

Below, we fit an elastic net model.
We evaluate it and tune its penalty and mixture parameters with [cross-validation](#sec-crossValidation).

The penalty parameter controls the overall strength of regularization applied to the model’s coefficients.
Smaller penalty values allow coefficients to remain large, which can reduce bias but increase variance of the model's predictions and can increase the risk of [overfitting](#sec-overfitting).
Larger penalty values shrink coefficients more aggressively, leading to simpler models with potentially higher bias but lower variance of predictions.
This regularization helps prevent [overfitting](#sec-overfitting), especially when the model includes many predictors or [multicollinearity](#sec-multipleRegressionMulticollinearity).

The mixture parameter controls the balance between [LASSO](#sec-lasso) and [ridge](#sec-ridgeRegression) regularization.
It ranges from 0 to 1:
A value of 0 applies pure [ridge regression](#sec-ridgeRegression), which shrinks all coefficients but keeps them in the model.
A value of 1 applies pure [LASSO](#sec-lasso), which can shrink some coefficients exactly to zero, effectively performing variable selection.
Values between 0 and 1 apply a combination: [ridge](#sec-ridgeRegression)-like smoothing and [LASSO](#sec-lasso)-like sparsity.
Smaller mixture values favor shrinkage without variable elimination, whereas larger values favor sparser solutions by excluding weak predictors.

After tuning the model, we also evaluate its accuracy on the hold-out (test) data.

The elastic net models were fit using the `glmnet` package [@R-glmnet; @Friedman2010_packages; @Tay2023_packages].

```{r}
# Set seed for reproducibility
set.seed(52242)

# Set up Cross-Validation
folds <- folds_kFold

# Define Recipe (Formula)
rec <- recipes::recipe(
  fantasyPoints_lag ~ ., # use all predictors
  data = data_train_qb %>% select(-gsis_id, -fantasyPointsMC_lag))

# Define Model
enet_spec <- 
  parsnip::linear_reg(
    penalty = tune::tune(),
    mixture = tune::tune()) %>%
  parsnip::set_engine("glmnet")

# Workflow
enet_wf <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(enet_spec)

# Define a regular grid for both penalty and mixture
grid_enet <- dials::grid_regular(
  dials::penalty(range = c(-4, -1)),
  dials::mixture(range = c(0, 1)),
  levels = c(20, 5) # 20 penalty values × 5 mixture values
)

# Tune the Grid
cv_results <- tune::tune_grid(
  enet_wf,
  resamples = folds,
  grid = grid_enet,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_grid(save_pred = TRUE)
)

# View Cross-Validation metrics
tune::collect_metrics(cv_results)

# Identify best penalty
tune::select_best(cv_results, metric = "rmse")
tune::select_best(cv_results, metric = "mae")
tune::select_best(cv_results, metric = "rsq")

best_penalty <- tune::select_best(cv_results, metric = "mae")

# Finalize Workflow with Best Penalty
final_wf <- tune::finalize_workflow(
  enet_wf,
  best_penalty)

# Fit Final Model on Training Data
final_model <- workflows::fit(
  final_wf,
  data = data_train_qb)

# View Coefficients
final_model %>% 
  workflows::extract_fit_parsnip() %>% 
  broom::tidy()

# Predict on Test Data
df <- data_test_qb %>%
  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)

# Evaluate Accuracy of Predictions
petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)
```

```{r}
#| include: false

accuracyIndices_elasticNet_train <- tune::collect_metrics(cv_results)

rsquare_elasticNet_train <- accuracyIndices_elasticNet_train %>% 
  filter(.metric == "rsq" & penalty == best_penalty$penalty & mixture == best_penalty$mixture) %>% 
  select(mean) %>% 
  pull() %>% 
  as.numeric() %>% 
  round(., 2)

accuracyIndices_elasticNet <- petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)

rsquare_elasticNet <- round(accuracyIndices_elasticNet$rsquared, digits = 2)
me_elasticNet <- round(accuracyIndices_elasticNet$ME, digits = 2)
mae_elasticNet <- round(accuracyIndices_elasticNet$MAE, digits = 2)
```

There was modest [shrinkage](#sec-shrinkage) from the training model to the test model: the $R^2$ for the model on the training data was `{r} rsquare_elasticNet_train`; the $R^2$ for the same model applied to the test data was `{r} rsquare_elasticNet`.

@fig-elasticNetModel depicts the predicted versus actual fantasy points for the model on the test data.

```{r}
#| label: fig-elasticNetModel
#| fig-cap: "Predicted Versus Actual Fantasy Points for Elastic Net Model."
#| fig-alt: "Predicted Versus Actual Fantasy Points for Elastic Net Model."

# Calculate combined range for axes
axis_limits <- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)

ggplot(
  df,
  aes(
    x = pred,
    y = fantasyPoints_lag)) +
  geom_point(
    size = 2,
    alpha = 0.6) +
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "dashed") +
  coord_equal(
    xlim = axis_limits,
    ylim = axis_limits) +
  labs(
    title = "Predicted vs Actual Fantasy Points (Test Data)",
    x = "Predicted Fantasy Points",
    y = "Actual Fantasy Points"
  ) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

Below are the model predictions for next year's fantasy points:

```{r}
newData_qb %>%
  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %>% 
  left_join(
    .,
    nfl_playerIDs %>% select(gsis_id, name),
    by = "gsis_id"
  ) %>% 
  select(name, fantasyPoints_lag) %>% 
  arrange(-fantasyPoints_lag)
```

### Random Forest Machine Learning {#sec-randomForest}

#### Assuming Independent Observations {#sec-randomForestIndependent}

Below, we fit a random forest model.
We evaluate it and tune two parameters with [cross-validation](#sec-crossValidation).
The first parameter is `mtry`, which controls the number of predictors randomly sampled at each split in a decision tree.
Smaller `mtry` values increase tree diversity by forcing trees to consider different subsets of predictors.
This typically reduces the variance of the overall model's predictions (because the final prediction is averaged over more diverse trees) but may increase bias if strong predictors are frequently excluded.
Larger `mtry` allow more predictors to be considered at each split, making trees more similar to each other.
This can reduce bias but often increases variance of the model's predictions, because the trees are more correlated and less effective at error cancellation when averaged.

The second parameter is `min_n`, which controls the minimum number of observations that must be present in a node for a split to be attempted.
Smaller `min_n` values allow trees to grow deeper and capture more fine-grained patterns in the training data.
This typically reduces bias but increases variance of the overall model's predictions, because deeper trees are more likely to [overfit](#sec-overfitting) to noise in the training data.
Larger `min_n` values restrict the depth of the trees by requiring more data to justify a split.
This can reduce variance by producing simpler, more generalizable trees—but may increase bias if the trees are unable to capture important structure in the data.

After tuning the model, we evaluate its accuracy on the hold-out (test) data.

The random forest models were fit using the `ranger` package [@R-ranger; @Wright2017_packages].
We specify the grid of possible parameter values using the `dials::grid_random()`, `dials::update.parameters()`, `dials::mtry()`, and `dials::min_n()` functions of the `dials` package [@R-dials] and the `hardhat::extract_parameter_set_dials()` function of the `hardhat` package [@R-hardhat].

```{r}
# Set seed for reproducibility
set.seed(52242)

# Set up Cross-Validation
folds <- folds_kFold

# Define Recipe (Formula)
rec <- recipes::recipe(
  fantasyPoints_lag ~ ., # use all predictors
  data = data_train_qb %>% select(-gsis_id, -fantasyPointsMC_lag))

# Define Model
rf_spec <- 
  parsnip::rand_forest(
    mtry = tune::tune(),
    min_n = tune::tune(),
    trees = 500) %>%
  parsnip::set_mode("regression") %>%
  parsnip::set_engine(
    "ranger",
    importance = "impurity")

# Workflow
rf_wf <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(rf_spec)

# Create Grid
n_predictors <- recipes::prep(rec) %>%
  recipes::juice() %>%
  dplyr::select(-fantasyPoints_lag) %>%
  ncol()

# Dynamically define ranges based on data
rf_params <- hardhat::extract_parameter_set_dials(rf_spec) %>%
  dials:::update.parameters(
    mtry = dials::mtry(range = c(1L, n_predictors)),
    min_n = dials::min_n(range = c(2L, 10L))
  )

rf_grid <- dials::grid_random(rf_params, size = 15) #dials::grid_regular(rf_params, levels = 5)

# Tune the Grid
cv_results <- tune::tune_grid(
  rf_wf,
  resamples = folds,
  grid = rf_grid,
  metrics = yardstick::metric_set(rmse, mae, rsq),
  control = tune::control_grid(save_pred = TRUE)
)

# View Cross-Validation metrics
tune::collect_metrics(cv_results)

# Identify best penalty
tune::select_best(cv_results, metric = "rmse")
tune::select_best(cv_results, metric = "mae")
tune::select_best(cv_results, metric = "rsq")

best_penalty <- tune::select_best(cv_results, metric = "mae")

# Finalize Workflow with Best Penalty
final_wf <- tune::finalize_workflow(
  rf_wf,
  best_penalty)

# Fit Final Model on Training Data
final_model <- workflows::fit(
  final_wf,
  data = data_train_qb)

# View Feature Importance
rf_fit <- final_model %>% 
  workflows::extract_fit_parsnip()

rf_fit

ranger_obj <- rf_fit$fit

ranger_obj$variable.importance

# Predict on Test Data
df <- data_test_qb %>%
  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)

# Evaluate Accuracy of Predictions
petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)
```

```{r}
#| include: false

accuracyIndices_randomForest_train <- tune::collect_metrics(cv_results)

rsquare_randomForest_train <- accuracyIndices_randomForest_train %>% 
  filter(.metric == "rsq" & mtry == best_penalty$mtry & min_n == best_penalty$min_n) %>% 
  select(mean) %>% 
  pull() %>% 
  as.numeric() %>% 
  round(., 2)

accuracyIndices_randomForest <- petersenlab::accuracyOverall(
  predicted = df$pred,
  actual = df$fantasyPoints_lag,
  dropUndefined = TRUE
)

rsquare_randomForest <- round(accuracyIndices_randomForest$rsquared, digits = 2)
me_randomForest <- round(accuracyIndices_randomForest$ME, digits = 2)
mae_randomForest <- round(accuracyIndices_randomForest$MAE, digits = 2)
```

There was modest [shrinkage](#sec-shrinkage) from the training model to the test model: the $R^2$ for the model on the training data was `{r} rsquare_randomForest_train`; the $R^2$ for the same model applied to the test data was `{r} rsquare_randomForest`.

@fig-randomForestModel depicts the predicted versus actual fantasy points for the model on the test data.

```{r}
#| label: fig-randomForestModel
#| fig-cap: "Predicted Versus Actual Fantasy Points for Random Forest Model."
#| fig-alt: "Predicted Versus Actual Fantasy Points for Random Forest Model."

# Calculate combined range for axes
axis_limits <- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)

ggplot(
  df,
  aes(
    x = pred,
    y = fantasyPoints_lag)) +
  geom_point(
    size = 2,
    alpha = 0.6) +
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "dashed") +
  coord_equal(
    xlim = axis_limits,
    ylim = axis_limits) +
  labs(
    title = "Predicted vs Actual Fantasy Points (Test Data)",
    x = "Predicted Fantasy Points",
    y = "Actual Fantasy Points"
  ) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

Below are the model predictions for next year's fantasy points:

```{r}
newData_qb %>%
  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %>% 
  left_join(
    .,
    nfl_playerIDs %>% select(gsis_id, name),
    by = "gsis_id"
  ) %>% 
  select(name, fantasyPoints_lag) %>% 
  arrange(-fantasyPoints_lag)
```

Now we can stop the parallel backend:

```{r}
future::plan(future::sequential)
```

```{r}
#| include: false

gc()
```

#### Accounting for Longitudinal Data {#sec-randomForestLongitudinalData}

The above approaches to machine learning assume that the observations are independent across rows.
However, in our case, this assumption does not hold because the data are longitudinal—each player has multiple seasons, and each row corresponds to a unique player–season combination.
In the approaches below, using a random forest model (this section) and a [tree-boosting model](#sec-treeBoosting) (in @sec-treeBoosting), we address this by explicitly accounting for the nesting of longitudinal observations within players.

Approaches to estimating random forest models with longitudinal data are described by @Hu2023.
Below, we fit longitudinal random forest models using the `LongituRF::MERF()` function of the `LongituRF` package [@R-LongituRF].

```{r}
smerf <- LongituRF::MERF(
  X = data_train_qb %>% dplyr::select(season:att_br.rush) %>% as.matrix(), # predictors of the fixed effects
  Y = data_train_qb[,c("fantasyPoints_lag")] %>% as.matrix(), # outcome variable
  Z = data_train_qb %>% dplyr::mutate(constant = 1) %>% dplyr::select(constant, passing_yards, passing_tds, passing_interceptions, passing_epa, pacr) %>% as.matrix(), # predictors of the random effects
  id = data_train_qb[,c("gsis_id")] %>% as.matrix(), # player ID (for nesting)
  time = data_train_qb[,c("ageCentered20")] %>% as.matrix(), # time variable
  ntree = 500,
  sto = "BM")

smerf$forest # the fitted random forest (obtained at the last iteration)
smerf$random_effects %>% data.frame() # the predicted random effects for each player
smerf$omega %>% data.frame() # the predicted stochastic processes
smerf$OOB # OOB error at each iteration
```

```{r}
#| label: fig-randomForestLongitudinalLogLikelihood
#| fig-cap: "Evolution of the Log-Likelihood."
#| fig-alt: "Evolution of the Log-Likelihood."

plot(smerf$Vraisemblance)
```

The following code generates an error, for some reason.
This issue has been posted on the GitHub repository: <https://github.com/sistm/LongituRF/issues/5>.
Hopefully the package maintainer will help address the issue.

```{r}
# Predict on Test Data
predict(
  smerf,
  X = data_test_qb %>% dplyr::select(season:att_br.rush) %>% as.matrix(),
  Z = data_train_qb %>% dplyr::mutate(constant = 1) %>% dplyr::select(constant, passing_yards, passing_tds, passing_interceptions, passing_epa, pacr) %>% as.matrix(),
  id = data_test_qb[,c("gsis_id")] %>% as.matrix(),
  time = data_test_qb[,c("ageCentered20")] %>% as.matrix())
```

### Combining Tree-Boosting with Mixed Models {#sec-treeBoosting}

To combine tree-boosting with mixed models, we use the `gpboost` package [@R-gpboost].

Adapted from here:
<https://towardsdatascience.com/mixed-effects-machine-learning-for-longitudinal-panel-data-with-gpboost-part-iii-523bb38effc>

#### Process Data {#sec-treeBoostingProcessData}

If using a gamma distribution, it requires positive-only values:

```{r}
data_train_qb_matrix[,"fantasyPoints_lag"][data_train_qb_matrix[,"fantasyPoints_lag"] <= 0] <- 0.01
```

#### Specify Predictor Variables {#sec-treeBoostingSpecifyPredictors}

```{r}
#| label: specify-predictors-machine-learning

pred_vars_qb <- data_train_qb_matrix %>% 
  as_tibble() %>% 
  select(-fantasyPoints_lag, -fantasyPointsMC_lag, -ageCentered20, ageCentered20Quadratic) %>% # -gsis_id
  names()

pred_vars_qb_categorical <- "gsis_id" # to specify categorical predictors
```

#### Specify General Model Options {#sec-treeBoostingSpecifyGeneralModelOptions}

```{r}
#| label: specify-general-model-options-machine-learning

model_likelihood <- "gamma" # gaussian
nrounds <- 2000 # maximum number of boosting iterations (i.e., number of trees built sequentially); more rounds = potentially better learning, but also greater risk of overfitting
```

#### Identify Optimal Tuning Parameters {#sec-treeBoostingOptimalTuningParameters}

For identifying the optimal tuning parameters for boosting, we partition the training data into inner training data and validation data.
We randomly split the training data into 80% inner training data and 20% held-out validation data.
We then use the [mean absolute error](#sec-meanAbsoluteError) as our index of prediction accuracy on the held-out validation data.
We use the `gpboost::gpb.Dataset()` function to specify the data, the `gpboost::GPModel()` function to specify the model, and the `gpboost::gpb.grid.search.tune.parameters()` function to identify the optimal tuning parameters for that model.

```{r}
#| label: tuning-parameters-machine-learning

# Partition training data into inner training data and validation data
ntrain_qb <- dim(data_train_qb_matrix)[1]

set.seed(52242)
valid_tune_idx_qb <- sample.int(ntrain_qb, as.integer(0.2*ntrain_qb)) # validation set

folds_qb <- list(valid_tune_idx_qb)

# Specify parameter grid, gp_model, and gpb.Dataset
param_grid_qb <- list(
  "learning_rate" = c(0.2, 0.1, 0.05, 0.01), # the step size used when updating predictions after each boosting round (high values make big updates, which can speed up learning but risk overshooting; low values are usually more accurate but require more rounds)
  "max_depth" = c(3, 5, 7), # maximum depth (levels) of each decision tree; deeper trees capture more complex patterns and interactions but risk overfitting; shallower trees tend to generalize better
  "min_data_in_leaf" = c(10, 50, 100), # minimum number of training examples in a leaf node; higher values = more regularization (simpler trees)
  "lambda_l2" = c(0, 1, 5)) # L2 regularization penalty for large weights in tree splits; adds a "cost" for complexity; helps prevent overfitting by shrinking the contribution of each tree

other_params_qb <- list(
  num_leaves = 2^6) # maximum number of leaves per tree; controls the maximum complexity of each tree (along with max_depth); more leaves = more expressive models, but can overfit if min_data_in_leaf is too small; num_leaves must be consistent with max_depth, because deeper trees naturally support more leaves; max is: 2^n, where n is the largest max_depth

gp_model_qb <- gpboost::GPModel(
  group_data = data_train_qb_matrix[,"gsis_id"],
  likelihood = model_likelihood,
  group_rand_coef_data = cbind(
    data_train_qb_matrix[,"ageCentered20"],
    data_train_qb_matrix[,"ageCentered20Quadratic"]),
  ind_effect_group_rand_coef = c(1,1))

gp_data_qb <- gpboost::gpb.Dataset(
  data = data_train_qb_matrix[,pred_vars_qb],
  categorical_feature = pred_vars_qb_categorical,
  label = data_train_qb_matrix[,"fantasyPoints_lag"]) # could instead use mean-centered variable (fantasyPointsMC_lag) and add mean back afterward

# Find optimal tuning parameters
opt_params_qb <- gpboost::gpb.grid.search.tune.parameters(
  param_grid = param_grid_qb,
  params = other_params_qb,
  num_try_random = NULL,
  folds = folds_qb,
  data = gp_data_qb,
  gp_model = gp_model_qb,
  nrounds = nrounds,
  early_stopping_rounds = 50, # stops training early if the model hasn’t improved on the validation set in 50 rounds; prevents overfitting and saves time
  verbose_eval = 1,
  metric = "mae")

opt_params_qb
```

```{r}
#| label: free-up-memory16-ml
#| include: false

gc()
```

We use the optimal parameters identified during tuning.
We use a low learning rate (0.1) to avoid [overfitting](#sec-overfitting).
We set some light regularization (`lambda_l2`) for better generalization.
We also set the maximum tree depth (`max_depth`) at 5 to capture complex (up to 5-way) interactions, and set the maximum number of terminal nodes (`num_leaves`) per tree at $2^6$ (64)—though the number will not reach greater than $2^{\text{max depth}} = 2^5 = 32$.
We set the minimum number of samples in any leaf (`min_data_in_leaf`) to be 100.

#### Specify Model and Tuning Parameters {#sec-treeBoostingSpecifyModel}

```{r}
#| label: model-parameters-machine-learning

gp_model_qb <- gpboost::GPModel(
  group_data = data_train_qb_matrix[,"gsis_id"],
  likelihood = model_likelihood,
  group_rand_coef_data = cbind(
    data_train_qb_matrix[,"ageCentered20"],
    data_train_qb_matrix[,"ageCentered20Quadratic"]),
  ind_effect_group_rand_coef = c(1,1))

gp_data_qb <- gpboost::gpb.Dataset(
  data = data_train_qb_matrix[,pred_vars_qb],
  categorical_feature = pred_vars_qb_categorical,
  label = data_train_qb_matrix[,"fantasyPoints_lag"])

params_qb <- list(
  learning_rate = 0.1,
  max_depth = 5,
  min_data_in_leaf = 100,
  lambda_l2 = 5,
  num_leaves = 2^6,
  num_threads = num_cores)

nrounds_qb <- 84 # identify optimal number of trees through iteration and cross-validation

#gp_model_qb$set_optim_params(params = list(optimizer_cov = "nelder_mead")) # to speed up model estimation
```

#### Fit Model {#sec-treeBoostingFitModel}

We use the `gpboost::gpb.train()` function to fit the specified model with the specified tuning parameters and dataset.

```{r}
#| label: fit-model-machine-learning

gp_model_fit_qb <- gpboost::gpb.train(
  data = gp_data_qb,
  gp_model = gp_model_qb,
  nrounds = nrounds_qb,
  params = params_qb) # verbose = 0
```

```{r}
#| label: free-up-memory17-ml
#| include: false

gc()
```

#### Model Results {#sec-treeBoostingModelResults}

```{r}
summary(gp_model_qb) # estimated random effects model
```

We evaluate the importance of features using the `gpboost::gpb.importance()` and `gpboost::gpb.plot.importance()` functions:

```{r}
gp_model_qb_importance <- gpboost::gpb.importance(gp_model_fit_qb)
gp_model_qb_importance
```

```{r}
#| label: fig-featureImportanceTreeBoosting
#| fig-cap: "Importance of Features (Predictors) in Tree Boosting Machine Learning Model."
#| fig-alt: "Importance of Features (Predictors) in Tree Boosting Machine Learning Model."

gpboost::gpb.plot.importance(gp_model_qb_importance)
```

#### Evaluate Accuracy of Model on Training Data {#sec-treeBoostingModelAccuracyTraining}

```{r}
# Model-implied predictions
pred_train_qb <- predict(
  gp_model_fit_qb,
  data = data_train_qb_matrix[, pred_vars_qb],
  group_data_pred = data_train_qb_matrix[, "gsis_id"],
  group_rand_coef_data_pred = cbind(
    data_train_qb_matrix[, "ageCentered20"],
    data_train_qb_matrix[, "ageCentered20Quadratic"]),
  predict_var = FALSE,
  pred_latent = FALSE)

petersenlab::accuracyOverall(
  predicted = pred_train_qb[["response_mean"]],
  actual = data_train_qb_matrix[, "fantasyPoints_lag"],
  dropUndefined = TRUE
)
```

#### Evaluate Accuracy of Model on Test Data {#sec-treeBoostingModelAccuracy}

```{r}
#| label: evaluate-model-machine-learning

# Test Model on Test Data
pred_test_qb <- predict(
  gp_model_fit_qb,
  data = data_test_qb_matrix[,pred_vars_qb],
  group_data_pred = data_test_qb_matrix[,"gsis_id"],
  group_rand_coef_data_pred = cbind(
    data_test_qb_matrix[,"ageCentered20"],
    data_test_qb_matrix[,"ageCentered20Quadratic"]),
  predict_var = FALSE,
  pred_latent = FALSE)

y_pred_test_qb <- pred_test_qb[["response_mean"]] # if outcome is mean-centered, add mean(data_train_qb_matrix[,"fantasyPoints_lag"])

predictedVsActual <- data.frame(
  predictedPoints = y_pred_test_qb,
  actualPoints = data_test_qb_matrix[,"fantasyPoints_lag"]
)

predictedVsActual

petersenlab::accuracyOverall(
  predicted = predictedVsActual$predictedPoints,
  actual = predictedVsActual$actualPoints,
  dropUndefined = TRUE
)
```

```{r}
#| include: false

accuracyIndices_treeBoosting_train <- petersenlab::accuracyOverall(
  predicted = pred_train_qb[["response_mean"]],
  actual = data_train_qb_matrix[, "fantasyPoints_lag"],
  dropUndefined = TRUE
)

rsquare_treeBoosting_train <- round(accuracyIndices_treeBoosting_train$rsquared, digits = 2)

accuracyIndices_treeBoosting <- petersenlab::accuracyOverall(
  predicted = predictedVsActual$predictedPoints,
  actual = predictedVsActual$actualPoints,
  dropUndefined = TRUE
)

rsquare_treeBoosting <- round(accuracyIndices_treeBoosting$rsquared, digits = 2)
me_treeBoosting <- round(accuracyIndices_treeBoosting$ME, digits = 2)
mae_treeBoosting <- round(accuracyIndices_treeBoosting$MAE, digits = 2)
```

There was moderate [shrinkage](#sec-shrinkage) from the training model to the test model: the $R^2$ for the model on the training data was `{r} rsquare_treeBoosting_train`; the $R^2$ for the same model applied to the test data was `{r} rsquare_treeBoosting`.

@fig-treeBoostingModel depicts the predicted versus actual fantasy points for the model on the test data.

```{r}
#| label: fig-treeBoostingModel
#| fig-cap: "Predicted Versus Actual Fantasy Points for Tree-Boosting Model."
#| fig-alt: "Predicted Versus Actual Fantasy Points for Tree-Boosting Model."

# Calculate combined range for axes
axis_limits <- range(c(predictedVsActual$predictedPoints, predictedVsActual$actualPoints), na.rm = TRUE)

ggplot(
  predictedVsActual,
  aes(
    x = predictedPoints,
    y = actualPoints)) +
  geom_point(
    size = 2,
    alpha = 0.6) +
  geom_abline(
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "dashed") +
  coord_equal(
    xlim = axis_limits,
    ylim = axis_limits) +
  labs(
    title = "Predicted vs Actual Fantasy Points (Test Data)",
    x = "Predicted Fantasy Points",
    y = "Actual Fantasy Points"
  ) +
  theme_classic() +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

#### Generate Predictions for Next Season {#sec-treeBoostingModelPredictions}

```{r}
#| label: next-season-predictions-machine-learning

# Generate model predictions for next season
pred_nextYear_qb <- predict(
  gp_model_fit_qb,
  data = newData_qb_matrix[,pred_vars_qb],
  group_data_pred = newData_qb_matrix[,"gsis_id"],
  group_rand_coef_data_pred = cbind(
    newData_qb_matrix[,"ageCentered20"],
    newData_qb_matrix[,"ageCentered20Quadratic"]),
  predict_var = FALSE,
  pred_latent = FALSE)

newData_qb$fantasyPoints_lag <- pred_nextYear_qb$response_mean

# Merge with player names
newData_qb <- left_join(
  newData_qb,
  nfl_playerIDs %>% select(gsis_id, name),
  by = "gsis_id"
)

newData_qb %>% 
  arrange(-fantasyPoints_lag) %>% 
  select(name, fantasyPoints_lag, fantasyPoints)
```

## Summarizing the Machine Learning Models {#sec-summarizingModels-machineLearning}

```{r}
#| include: false

machineLearningTable <- data.frame(
  model = c(
    "Regression with One Predictor",
    "Regressions with Multiple Predictors",
    "LASSO",
    "Ridge Regression",
    "Elastic Net",
    "Random Forest",
    "Tree-Boosting"
  ), 
  rsquared = c(
    rsquare_onePredictor,
    rsquare_multiplePredictors,
    rsquare_lasso,
    rsquare_ridge,
    rsquare_elasticNet,
    rsquare_randomForest,
    rsquare_treeBoosting
  ),
  me = c(
    me_onePredictor,
    me_multiplePredictors,
    me_lasso,
    me_ridge,
    me_elasticNet,
    me_randomForest,
    me_treeBoosting
  ),
  mae = c(
    mae_onePredictor,
    mae_multiplePredictors,
    mae_lasso,
    mae_ridge,
    mae_elasticNet,
    mae_randomForest,
    mae_treeBoosting
  )
)

rsquare_maximum <- max(
  c(rsquare_onePredictor, rsquare_multiplePredictors,
    rsquare_lasso, rsquare_ridge, rsquare_elasticNet,
    rsquare_randomForest, rsquare_treeBoosting),
  na.rm = TRUE
)

mae_minimum <- min(
  c(mae_onePredictor, mae_multiplePredictors,
    mae_lasso, mae_ridge, mae_elasticNet,
    mae_randomForest, mae_treeBoosting),
  na.rm = TRUE
)
```

We examined multiple machine learning models that included many predictor variables to identify the model that generates the most accurate prediction of Quarterbacks' future fantasy points.
We also compared them to simpler models, including a [regression model with one predictor](#sec-regressionOnePredictor) and a [regression model with multiple predictors](#sec-regressionMultiplePredictors) to deteremine the [incremental validity](#sec-incrementalValidity) of the machine learning models above and beyond the simpler models.
A summary of the accuracy of the machine learning models is in @tbl-machineLearningModelSummary.

```{r}
#| label: tbl-machineLearningModelSummary
#| tbl-cap: "Summary of Machine Learning Models."
#| echo: false

knitr::kable(
  machineLearningTable,
  col.names = c("Model","$R^2$","Mean Error","Mean Absolute Error"),
  booktabs = TRUE)
```

The most accurate models, in terms of [discrimination](#sec-discrimination) ($R^2$), had an $R^2$ of `{r} rsquare_maximum`, and it was achieved using one of two approaches—a [tree-boosting model](#sec-treeBoosting) with many predictors or a [regression model with one predictor](#sec-regressionOnePredictor): the player's fantasy points in the previous season.
The [regression model with one predictor](#sec-regressionOnePredictor) was more accurate (in terms of $R^2$) than a [regression model with multiple predictors](#sec-regressionMultiplePredictors) and the other [machine learning models](#sec-fittingModels-machineLearning) that leveraged multiple predictors.
This suggests that the other predictors examined did not have [incremental validity](#sec-incrementalValidity)—they did not show utility above and beyond a player's fantasy points in predicting their future fantasy points.
If anything, considering the additional predictor variables showed *decremental* validity—the inclusion of the additional variables worsened prediction, possibly by weakening the predictive association of the previous season's fantasy points and misattributing its predictive effect to other, related predictors in the model.
This is consistent with the common finding that simple models (and parsimony) are often preferable.
As an example, @Youngstrom2018 found that simple models did just as well and in some cases better than [LASSO models](#sec-lasso) in classifying bipolar disorder.

<!--
Among the machine learning models, the [random forest model](#sec-randomForest) was most accurate according to $R^2$.
However, it was still less accurate than a [simple regression model with one predictor](#sec-regressionOnePredictor).
-->

The most accurate model in terms of [calibration](#sec-calibration) ([mean absolute error](#sec-meanAbsoluteError)) was the [tree-boosting model](#sec-treeBoosting), which had a [mean absolute error](#sec-meanAbsoluteError) of `{r} mae_minimum`, and was just slightly more accurate than the [regression model with one predictor](#sec-regressionOnePredictor).
However, even in the most accurate model, the predicted values were over `{r} petersenlab::apa(mae_minimum, decimals = 0)` points away from the actual values, suggesting that the model-predicted values were not particularly accurate.
In sum, the [regression model with one predictor](#sec-regressionOnePredictor) was the most accurate in terms of differentiating between players ($R^2$), whereas the [tree-boosting model](#sec-treeBoosting) was the most accurate in terms of how close the predicted values were to the actual values.

All models explained less than half of the variance in players' future fantasy points.
Thus, there remains considerable variance to be explained.
The considerable unexplained variance suggests that we are missing key variables that are important predictors of future fantasy performance.
It is also consistent with the conclusion from many other domains in psychology that human behavior is challenging to predict [@PetersenPrinciplesPsychAssessment].
We could likely improve our predictive accuracy by including projections, but it is unclear what information projections are or are not based on.
For instance, if projections already account for the prior season's statistics and fantasy points, then including both may not buy you much in terms of increased predictive accuracy. 

## Conclusion {#sec-machineLearningConclusion}

Machine learning attempts to maximize prediction accuracy.
There are various types of machine learning models.
[Supervised learning](#sec-machineLearningTypesSupervised) involves learning from data where the correct classification or outcome is known.
[Unsupervised learning](#sec-machineLearningTypesUnsupervised) involves learning from data without known classifications.
Machine learning can incorporate many predictor variables, so it is important to perform [cross-validation](#sec-crossValidation) to avoid [overfitting](#sec-overfitting).
We examined multiple machine learning models to identify the model that generates the most accurate prediction of Quarterbacks' future fantasy points.
All models explained less than half of the variance in players' fantasy points.
The most accurate model, in terms of [discrimination](#sec-discrimination) ($R^2$), was achieved using regression with one predictor: the player's fantasy points in the previous season.
The most accurate model in terms of [calibration](#sec-calibration) ([mean absolute error](#sec-meanAbsoluteError)) was the [tree-boosting model](#sec-treeBoosting), which accounted for the longitudinal nature of the data.
The considerable unexplained variance suggests that we are missing key variables that are important predictors of future fantasy performance.

::: {.content-visible when-format="html"}

## Session Info {#sec-machineLearningSessionInfo}

```{r}
sessionInfo()
```

:::
