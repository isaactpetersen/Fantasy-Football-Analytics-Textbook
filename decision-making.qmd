# Decision Making in the Context of Uncertainty {#sec-decisionMaking}

## Getting Started {#sec-decisionMakingGettingStarted}

### Load Packages {#sec-decisionMakingLoadPackages}

```{r}

```

## Wisdom of the Crowd {#sec-wisdomOfCrowd}

In many domains, the average of forecasters' predictions is more accurate than the accuracy of the constituent individuals.
In some domains, the average of non-expert forecasts is more accurate than the forecasts by individual experts.
This phenomenon is called "collective intelligence", the "wisdom of the crowd", or the "wisdom of crowds" [@Larrick2024; @Rader2017; @Simoiu2019; @Surowiecki2005; @Wagner2010].

Aggregation of predictions from multiple people leverages several important features, including cognitive diversity and error cancellation.

Cognitive diversity refers to the representation of individuals with different perspectives because of their "differences in knowledge, training, experience, or thinking styles" [@Rader2017, p. 8].
Cognitive diversity is important because judgments from a cognitively homogeneous group will tend to err *systematically*.
That is, they tend to err in the same direction—either consistently above or below the truth.
By contrast, a cognitively diverse group will not tend to err systematically.
For people of a cognitively diverse group, their judgments will tend to err *randomly*—where some people's predictions fall above the truth and some people's predictions fall below the truth—i.e., individual judgments "bracket" the truth [@Mannes2014].
Error cancellation deals with the idea that, when individuals' judgments bracket the truth and show random rather than systematic error, the average of the predictions will "cancel out" some of the errors so that the predictions average out to more closely approximate the truth.
However, when individuals' judgments do not bracket the truth, the average of the predictions will not cancel out the errors.

Averaging projections from individuals tends to yield predictions that are more accurate than the accuracy of most forecasters in the group [@Mannes2014].
Indeed, averaged predictions must be as (if not more) accurate than the average forecaster (i.e., the forecaster at the 50th percentile in terms of accuracy), and averaged predictions are often much more accurate [@Larrick2024].
Moreover, averaged projections tends to be more accurate than consensus-based judgments from groups of people that interact and discuss, due to cognitive biases associated with the social interaction among groups, such as herding in which people align their behavior with others [@Mannes2014; @Simoiu2019].

Crowd-averaged projections tend to be most accurate when:

- the crowd consists of individuals who hold expertise in the domain such that they will make predictions that fall close to the truth
- there is relatively low variability in the expertise of the individual forecasters in terms of their ability to make accurate forecasts
- there is cognitive diversity among the forecasters
- the projections are made independently—i.e., the forecasters are not aware of others' forecasts and do not discuss or interact with the other forecasters
- the bracketing rate—i.e., the frequency with which any two forecasters' predictions fall on opposite sides of the truth—is high

When forecasters' predictions perfect bracket the truth (i.e., 50% of predictions are above the truth and 50% of predictive are below the truth), the average predictions must be more accurate than the accuracy of the average forecaster [@Larrick2024].

However, the crowd is not more accurate than the expert or best forecaster in all situations or domains.
For instance, the crowd tends to be less accurate than the (prospectively identified) best forecaster when there is great variability in forecasters' expertise (in terms of the forecasters' ability to forecast accurately) and when the bracketing rate is low  [@Mannes2014].
Some forecasters may provide terrible projections; thus, including them in an average may make the average projections substantially less accurate.
Thus, it may be necessary to examine the average of a "select crowd", by aggregating the projections of the most consistently accurate forecasters [@Mannes2014].

## Accuracy of Fantasy Football Crowd Projections {#sec-accuracyOfCrowd}

Even though the crowd tends to be more accurate than individual forecasters, crowd-averaged projections (at least among experts) are not necessarily highly accurate.
In fantasy football, [DESCRIBE THEIR ACCURACY--BOTH FOR ALL PLAYERS AND WHEN SUBSETTING TO THE TOP X PLAYERS].
Nevertheless, individual sources tend to be even less accurate. [DESCRIBE THEIR ACCURACY].

Even though some sources are more accurate than the average in a given year, they are not *consistently* more accurate than the average.
Prediction involves a combination of luck and skill.
In some years, a prediction will invariably do better than others, in party, based on luck.
However, luck is unlikely to continue systematically into future years, so a source that got lucky in a given year is unlikely to continue to be lucky in subsequent years.
That is, determinining the most accurate source in a given year, after the fact, is not necessarily the same as identifying the most skilled forecaster.
It requires a large sample of predictions to determine whether a given forecaster is reliably (i.e., consistently) more accurate than other forecasters and to identify the most accurate forecaster [@Larrick2024].
Thus, it can be challenging to know, in advance, who the most accurate forecasters will be.
Because average projections are as or more accurate than the average forecaster's prediction, averaging projections across all forecasters is superior to choosing individual forecasters when the forecasters are roughly similar in forecasting ability or when it is hard to distinguish their ability in advance [@Larrick2024].

The relatively modest accuracy of the projections by so-called fantasy "experts'" and of their average of their projections could occur for a number of reasons.
One possibility is that the level of expertise of the "expert" forecasters in terms of being able to provide accurate forecasts is not strong.
That is, because football performance and injuries are so challenging to predict, individual forecasters' projections may not be particularly close to the truth.

A second possibility is that the bracketing rate of the predictions is not particularly high [@Mannes2014].
Even if the individual forecasters' projections are not close to the truth, if ~50% of them overestimate the truth and the other 50% of the underestimate the truth, the average will more closely approximate the truth.
However, if most forecasters overestimate the truth, averaging the projections will not necessarily lead to more accurate projections.

A third possibility is that the forecasts of the different experts are not independent.

Each of these possibilities is likely true to some degree.
First, individuals' predictions are unlikely to be highly accurate consistently.
Second, there are many players who are systematically *over*predicted (e.g., due to their injury) or *under*predicted (e.g., due to their becoming the starter after a teammate becomes injured, is traded, etc.)—an example of [overextremity miscalibration](#sec-calibration).
In general, it is likely for players who are projected to score more points to be overpredicted and for players who are projected to score fewer points to be underpredicted [PRESENT CALIBRATION STATS].
Third, the experts may interact and discuss with one another.
Interaction and discussion among experts may lead them to follow the herd and conform their projections to what each other predict.
In any case, they are able to see each other's projections and make change their projections, accordingly.

## Conclusion {#sec-decisionMakingConclusion}

::: {.content-visible when-format="html"}

## Session Info {#sec-decisionMakingSessionInfo}

```{r}
sessionInfo()
```

:::
