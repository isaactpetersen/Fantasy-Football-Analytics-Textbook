# Decision Making in the Context of Uncertainty {#sec-decisionMaking}

## Getting Started {#sec-decisionMakingGettingStarted}

### Load Packages {#sec-decisionMakingLoadPackages}

```{r}

```

## Wisdom of the Crowd {#sec-wisdomOfCrowd}

In many domains, the average of forecasters' predictions is more accurate than the accuracy of the constituent individuals.
In some domains, the average of non-expert forecasts is more accurate than the forecasts by individual experts.
This phenomenon is called "collective intelligence", the "wisdom of the crowd", or the "wisdom of crowds" [@Rader2017; @Simoiu2019; @Surowiecki2005; @Wagner2010].

Aggregation of predictions from multiple people leverages several important features, including cognitive diversity, error cancellation, INSERT.

Cognitive diversity refers to the representation of individuals with different perspectives because of their "differences in knowledge, training, experience, or thinking styles" [@Rader2017, p. 8].
Cognitive diversity is important because judgments from a cognitively homogeneous group will tend to err *systematically*.
That is, they tend to err in the same direction—either consistently above or below the truth.
By contrast, a cognitively diverse group will not tend to err systematically.
For people of a cognitively diverse group, their judgments will tend to err *randomly*—where some people's predictions fall above the truth and some people's predictions fall below the truth—i.e., individual judgments "bracket" the truth [@Mannes2014].
Error cancellation deals with the idea that, when individuals' judgments bracket the truth and show random rather than systematic error, the average of the predictions will "cancel out" some of the errors so that the predictions average out to more closely approximate the truth.

Averaging projections from individuals tends to yield predictions that are more accurate than the accuracy of most forecasters in the group [@Mannes2014].
Moreover, averaged projections tends to be more accurate than consensus-based judgments from groups of people that interact and discuss, due to cognitive biases associated with the social interaction among groups, such as herding in which people align their behavior with others [@Mannes2014; @Simoiu2019].

Crowd-averaged projections tend to be most accurate when:

- the crowd consists of individuals who hold expertise in the domain such that they will make predictions that fall close to the truth
- there is relatively low variability in the expertise of the individual forecasters in terms of their ability to make accurate forecasts
- there is cognitive diversity among the forecasters
- the projections are made independently—i.e., the forecasters are not aware of others' forecasts and do not discuss or interact with the other forecasters
- the bracketing rate—i.e., the frequency with which any two forecasters' predictions fall on opposite sides of the truth—is high

However, the crowd is not more accurate than the expert or best forecaster in all situations or domains.
For instance, the crowd tends to be less accurate than the (prospectively identified) best forecaster when there is great variability in forecasters' expertise (in terms of the forecasters' ability to forecast accurately) and when the bracketing rate is low  [@Mannes2014].
Some forecasters may provide terrible projections; thus, including them in an average may make the average projections substantially less accurate.
Thus, it may be necessary to examine the average of a "select crowd", by aggregating the projections of the most consistently accurate forecasters [@Mannes2014].

## Accuracy of Fantasy Football Crowd Projections {#sec-accuracyOfCrowd}

Even though the crowd tends to be more accurate than individual forecasters, crowd-averaged projections (at least among experts) are not necessarily highly accurate.
In fantasy football, [DESCRIBE THEIR ACCURACY].
The relatively modest accuracy of the projections by so-called fantasy "experts'" and of their average of their projections could occur for a number of reasons.
One possibility is that the level of expertise of the "expert" forecasters in terms of being able to provide accurate forecasts is not strong.
That is, because football performance and injuries are so challenging to predict, individual forecasters' projections may not be particularly close to the truth.

A second possibility is that the bracketing rate of the predictions is not particularly high [@Mannes2014].
Even if the individual forecasters' projections are not close to the truth, if ~50% of them overestimate the truth and the other 50% of the underestimate the truth, the average will more closely approximate the truth.
However, if most forecasters overestimate the truth, averaging the projections will not necessarily lead to more accurate projections.

A third possibility is that the forecasts of the different experts are not independent.

Each of these possibilities is likely true to some degree.
First, individuals' predictions are unlikely to be highly accurate consistently.
Second, there are many players who are systematically *over*predicted (e.g., due to their injury) or *under*predicted (e.g., due to their becoming the starter after a teammate becomes injured, is traded, etc.)—an example of [overextremity miscalibration](#sec-calibration).
In general, it is likely for players who are projected to score more points to be overpredicted and for players who are projected to score fewer points to be underpredicted [PRESENT CALIBRATION STATS].
Third, the experts may interact and discuss with one another.
In any case, they are able to see each other's projections and make change their projections, accordingly.

## Conclusion {#sec-decisionMakingConclusion}

::: {.content-visible when-format="html"}

## Session Info {#sec-decisionMakingSessionInfo}

```{r}
sessionInfo()
```

:::
