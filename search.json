[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "",
    "text": "Preface\nThis is a book in progress—it is incomplete. I will continue to add to and update it as I am able.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-contribute",
    "href": "index.html#sec-contribute",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "How to Contribute",
    "text": "How to Contribute\nThis is an open-access textbook. My goal is to share data analysis strategies for free! Anyone is welcome to contribute to the project. If you would like to contribute, please consider one of the following:\n\n\nopen an issue or create a pull request on the book’s GitHub repository.\n\nbuy me a coffee—Support me in developing this (free!) resource for fantasy football analytics… Even a cup of coffee helps me stay awake!\n\n(or use PayPal)\n\nThe GitHub repository for the book is located here: https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook. If you have data or analysis examples that are you willing to share and include in the book, feel free to contact me.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-openAccess",
    "href": "index.html#sec-openAccess",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Open Access",
    "text": "Open Access\nThis is an open-access book. This means that it is freely available for anyone to access.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-license",
    "href": "index.html#sec-license",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "License",
    "text": "License\n\nThe online version of this book is licensed under the Creative Commons Attribution License. In short, you can use my work as long as you cite it.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-citation",
    "href": "index.html#sec-citation",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Citation",
    "text": "Citation\nThe APA-style citation for the book is:\nPetersen, I. T. (2025). Fantasy football analytics: Statistics, prediction, and empiricism using R. Version 0.0.1. University of Iowa Libraries. https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook. [INSERT DOI LINK]\n\nThe BibTeX citation for the book is:\n\nCode@book{petersenFantasyFootballAnalytics,\n  title = {Fantasy football analytics: Statistics, prediction, and empiricism using {R}},\n  author = {Petersen, Isaac T.},\n  year = {2025},\n  publisher = {{University of Iowa Libraries}},\n  note = {Version 0.0.1},\n  doi = {INSERT},\n  isbn = {INSERT},\n  url = {https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-author",
    "href": "index.html#sec-author",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "About the Author",
    "text": "About the Author\nI am an Associate Professor in the Department of Psychological and Brain Sciences at the University of Iowa. I am a licensed psychologist with expertise in child clinical psychology. Why am I writing about fantasy football and data analysis? Because fantasy football involves the intersection of two things I love: sports and statistics.\nThrough my training, I have learned the value of statistics for answering important questions that I find interesting. In graduate training, I came to the realization that statistics are relevant not only for psychology and science, but also for domains that I enjoy as hobbies, including sports and fantasy sports. I have played in a longstanding fantasy football league for over 20 years (since my junior year of high school) with old friends from high school. I wanted to apply what I was learning about statistics to help others improve their performance in fantasy football and to help people—including those who might not otherwise be interested—to learn statistics. So I began blogging online about the value of applying statistics to improve decision making in fantasy football. Apparently, many people were interested in learning statistics when they could apply them to a domain that they find interesting like fantasy football. My blog eventually became FantasyFootballAnalytics.net, a website that uses advanced statistics to help people win their fantasy football leagues.\nIn terms of my R and statistics background, I have published many peer-reviewed publications that employ advanced statistical methods, have published a book on psychological assessment (Petersen, 2024, 2025b) that includes applied examples in R, and have published the petersenlab R package (Petersen, 2025a) on the Comprehensive R Archive Network (CRAN). Several sections in this book come from Petersen (2025b). I am also a co-author of the ffanalytics R package (Andersen et al., 2025) that provides free utilities for downloading fantasy football projections and additional fantasy-relevant data, and for calculating projected points given your league settings.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-accessibility",
    "href": "index.html#sec-accessibility",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Accessibility",
    "text": "Accessibility\nI strive to follow principles of accessibility [Xie et al. (2024); Xie et al. (2020); archived at https://perma.cc/8XJ9-Q6QJ] to make the book content accessible to people with visual impairments and physical disabilities. If there are additional ways I can make the content more accessible, please let me know.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-acknowledgments",
    "href": "index.html#sec-acknowledgments",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI thank Dr. Benjamin Motz, who provided consultation and many helpful resources based on his fantasy football statistics class. I also thank key members of FantasyFootballAnalytics.net, including Val Pinskiy, Andrew Tungate, Dennis Andersen, and Adam Peterson, who helped develop and provide fantasy football-related resources and who helped sharpen my thinking about the topic. I also thank Professor Patrick Carroll, who taught me the value of statistics for answering important questions.\n\n\n\n\nAndersen, D., Petersen, I. T., & Tungate, A. (2025). ffanalytics: Scrape data for fantasy football. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R Markdown cookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2024). R markdown cookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 About this Book\nHow can we use information to make predictions about uncertain events? This book is about empiricism (basing theories on observed data) and judgment, prediction, and decision making in the context of uncertainty. The book provides an introduction to modern analytical techniques used to make informed predictions, test theories, and draw conclusions from a given dataset. The book leverages the software R for providing applied data analysis examples.\nThis book was originally written for a undergraduate-level course entitled, “Fantasy Football: Predictive Analytics and Empiricism”. The chapters provide an overview of topics that each could have its own class and textbook, such as causal inference, factor analysis, cluster analysis, principal component analysis, machine learning, cognitive biases, modern portfolio theory, data visualization, simulation, etc. The book gives readers an overview of the breadth of the approaches to prediction and empiricism. As a consequence, the book does not cover any one technique or approach in great depth.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whatIsFantasyFootball",
    "href": "intro.html#sec-whatIsFantasyFootball",
    "title": "1  Introduction",
    "section": "1.2 What is Fantasy Football?",
    "text": "1.2 What is Fantasy Football?\nFantasy football is an online game where participants assemble (i.e., “draft”) imaginary teams composed of real-life National Football League (NFL) players. In this game, participants compete against their opponents (e.g., friends/coworkers/classmates), accumulating points based on players’ actual statistical performances in games. The goal is to outscore one’s opponent each week to win matches and ultimately claim victory in the league.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whyFantasyFootball",
    "href": "intro.html#sec-whyFantasyFootball",
    "title": "1  Introduction",
    "section": "1.3 Why Focus on Fantasy Football?",
    "text": "1.3 Why Focus on Fantasy Football?\nI was fortunate to have an excellent instructor who taught me the value of learning statistics to answer interesting and important questions. That is, I do not find statistics intrinsically interesting; rather, I find them interesting because of what they allow me to do. Many students find statistics intimidating in part because of how it is typically taught—with examples like dice rolls and coin flips that are (seemingly irrelevant and) boring to students. My contention is that applied examples are a more effective lens to teach many concepts in psychology and data analysis. It can be more engaging and relatable to learn statistics in the applied context of sports, a domain that is more intuitive to many. Many people play fantasy sports. This book involves applying statistics to a particular domain (football). People actually want to learn statistical principles and methods when they can apply them to interesting questions (e.g., sports). In my opinion [and supported by evidence; Motz (2013)], this is a much more effective way of engaging people and teaching statistics than in the context of abstract coin flips and dice rolls. Fantasy football relies heavily on prediction—trying to predict which players will perform best and selecting them accordingly. In this way, fantasy football provides a plethora of decision making opportunities in the face of uncertainty, and a wealth of data for analyzing these decisions. However, unlike many other applied domains in psychology, fantasy football (1) allows a person to see the accuracy of their predictions on a timely basis and (2) provides a safe environment for friendly competition. Thus, it provides a unique domain to evaluate—and improve—the accuracy of various prediction models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whyR",
    "href": "intro.html#sec-whyR",
    "title": "1  Introduction",
    "section": "1.4 Why R?",
    "text": "1.4 Why R?\nThe book provides data analysis examples using the statistical analysis software, R (R Core Team, 2024). Why R?\n\nR is free! Anyone can use it.\nR is open source—it is not a black box. You can see what is going on “under the hood” and can examine the code for any function or computation you perform. You can even modify and improve these functions by changing the code, and you can create your own functions.\nR is open platform—you can use it on multiple platforms, including Windows, MacOS, and Linux.\nR has advanced statistics capabilities. It was designed for statistical analysis and has strong capabilities for data wrangling.\nR has capabilities for state-of-the-art graphics. It has advanced capabilities for creating statistical graphics.\nR is widely used—there is a large community of people who use R for data analysis that you can draw upon for help from others.\nR analyses are based on code (rather than a graphical user interface), which allows reproducibility—with the same data, code, and setup (platform, R version, package versions, etc.), you should get the same answer every time. There are strong resources available for ensuring your analyses in R are reproducible by others (Gandrud, 2020).\nAnyone (including you) can contribute R packages to the community to improve its functionality. Statistical experts from all over the world have contributed open source packages to R for specialized tasks. In the chance there is not an R package that does what you need to do, you can write a function to perform the task and can contribute it as a package to the community for others to use and improve. The number of R packages contributed to the community is growing at a rapid rate. As of this writing, over 20,000 packages have been contributed to the Comprehensive R Archive Network (CRAN). And many more are stored on publicly available version control repositories like GitHub and GitLab. Chances are, if there is an analysis you need to do, an R package exists to do it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-educationalValue",
    "href": "intro.html#sec-educationalValue",
    "title": "1  Introduction",
    "section": "1.5 Educational Value",
    "text": "1.5 Educational Value\nSkills in statistics, statistical programming, and data analysis are highly valuable. This book includes practical and conceptual tools that build a foundation for critical thinking. The book aims to help readers evaluate theory in the light of evidence (and vice versa) and to refine decision making in the context of uncertainty. Readers will learn about the ways that psychological science (and related disciplines) poses questions, formulates hypotheses, designs studies to test those questions, and interprets the findings, collectively with the aim to answer questions, improve decision making, and solve problems.\nOf course, this is not a traditional psychology textbook. However, the book incorporates important psychological concepts, such as cognitive biases in judgment and prediction, etc. In the modern world of big data, research and society need people who know how to make sense of the information around us. Psychology is in a prime position to teach applied statistics to a wide variety of students, most of whom will not have careers as psychologists. Psychology can teach the importance of statistics given humans’ cognitive biases. It can also teach about how these biases can influence how people interpret statistics. This book will teach readers the applications of statistics (prediction) and research methods (empiricism) to answer questions they find interesting, while applying scientific and psychological rigor.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-objectives",
    "href": "intro.html#sec-objectives",
    "title": "1  Introduction",
    "section": "1.6 Learning Objectives",
    "text": "1.6 Learning Objectives\nThis book aims to help readers accomplish the following learning objectives:\n\nApply empirical inference and appreciate the value it provides over speculative supposition.\nAsk educated questions when confronted with decisions in the face of uncertainty.\nUnderstand human decision making, including common heuristics and cognitive biases and how to mitigate them analytically.\nEngage in critical thinking about causality, including devising plausible alternative explanations for observed effects.\nUnderstand causal inference including confounding, causal pathways, and counterfactuals.\nThink empirically about human behavior and performance.\nDescribe the strengths and weaknesses of humans versus computers in prediction scenarios.\nApply basic skills in statistical programming using R to manipulate and summarize datasets and to conduct data analysis.\nCritically evaluate the strengths and limitations of different statistical models and methodologies used in predicting uncertain events, enhancing their understanding of statistical inference and model selection.\nUse various analytical techniques for predicting the outcome of uncertain events, and for uncovering latent causes of patterns in observed data.\nInterpret findings from various statistical approaches and evaluate the accuracy of predictions.\nEngage in iterative problem-solving processes, refining analytical approaches based on feedback and outcomes, and adapting strategies accordingly.\nCommunicate statistical findings and analyses in both written and oral formats, demonstrating proficiency in presenting complex information to diverse audiences.\nMake sense of big data.\nUse practical analytical skills that can be applied in future research and job settings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-disclosures",
    "href": "intro.html#sec-disclosures",
    "title": "1  Introduction",
    "section": "1.7 Disclosures",
    "text": "1.7 Disclosures\nI am the Owner of Fantasy Football Analytics, LLC, which operates https://fantasyfootballanalytics.net.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-disclaimer",
    "href": "intro.html#sec-disclaimer",
    "title": "1  Introduction",
    "section": "1.8 Disclaimer",
    "text": "1.8 Disclaimer\n“This material probably won’t win you fantasy football championships. You could take what we learn and apply it to fantasy football and you might become 5 percent more likely to win. Or… Consider the broader relevance of this. You could learn data analysis and figure out ways to apply it to other systems. And you could be making a six-figure salary within the next five years.” – Benjamin Motz, Ph.D.\nHere is a video of Professor Benjamin Motz that describes the value of teaching statistics through the lens of fantasy football (NFL Films Presents, 2014):\nVideo of Prof. Benjamin Motz Discussing Statistics and His Fantasy Football Course. From: https://www.facebook.com/watch/?v=10155572257183615.\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGandrud, C. (2020). Reproducible research with R and R studio (3rd ed.). CRC Press. https://www.routledge.com/Reproducible-Research-with-R-and-RStudio/Gandrud/p/book/9780367143985\n\n\nMotz, B. (2013). Fantasy football: A touchdown for undergraduate statistics education. Proceedings of the Games, Learning, and Society Conference, 9.0, 222–228. https://doi.org/10.1184/R1/6686804.v1\n\n\nNFL Films Presents. (2014). Playing fantasy football for college credit?? Welcome to C105 - Prediction, Probability, & Pigskin. https://www.facebook.com/watch/?v=10155572257183615\n\n\nR Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html",
    "href": "fantasy-football.html",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "2.1 Football\nThis chapter provides a brief primer on (American) football and fantasy football. If you are already familiar with fantasy football, feel free to skip this chapter.\nFootball is the most widely watched sport in the United States [Jones (2024); archived at https://perma.cc/X2UG-RAAK; statistica (2023b); archived at https://perma.cc/JNU6-S96A].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#sec-football",
    "href": "fantasy-football.html#sec-football",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "2.1.1 The Objective\nThe goal in football is for a team to score more points than their opponent. A game lasts 60 minutes, and it is separated into four 15-minute quarters. The team with the most points when the time runs out wins.\nHere is a video that provides a brief introduction to American football (Ly, 2015):\nVideo that Provides An Introduction to American Football. From: https://www.youtube.com/watch?v=Ddwp1HyEFRE.\n\n\n\n2.1.2 The Roster\n\n2.1.2.1 Overview\nEach team has 11 players on the field at a time. The particular players who are on the field will depend on the situation, but usually includes one of the three subsets of players:\n\nOffense\nDefense\nSpecial Teams\n\nAn example formation is depicted in Figure 2.1.\n\n\n\n\n\n\nFigure 2.1: An Example Football Formation for the Offense and Defense. The solid line indicates the line of scrimmage. The arrow indicates the direction the offense tries to advance the ball.\n\n\n\n\n\n2.1.2.2 Offense\nThe offense is on the field when the team has the ball.\nPlayers on offense include:\n\nQuarterback (QB)\nRunning Back (RB)\n\nHalfback (HB) or Tailback (TB)\nFullback (FB)\n\nWide Receiver (WR)\nTight End (TE)\nOffensive Linemen (OL), part of the “Offensive Line”\n\nCenter (C)\nOffensive Guard (OG)\nOffensive Tackle (OT)\n\n\nThe quarterback is the most important player on the offense. They help lead the team down the field. The quarterback receives the ball from the Center at the beginning of the play, and they can either hand the ball off (typically to a Running Back or Fullback), pass the ball (typically to a Wide Receiver or Tight End), or run the ball. Quarterbacks tend to have a strong arm for throwing the ball far and accurately. Some quarterbacks are fast and are considered “dual threats” to pass or run.\nRunning Backs take a hand-off from the Quarterback to execute a running play (i.e., a rush). They may also catch short passes from the Quarterback or help protect (i.e., block for) the Quarterback from the defensive players who are trying to tackle the Quarterback. Halfbacks and Tailbacks tend to be quick and agile. Fullbacks tend to be strong and powerful.\nWide Receivers catch passes from the Quarterback to execute a passing play. On running plays, they provide protection for the player running the ball (e.g., the Running Back) so the ball carrier can get as far as possible without being tackled. Wide receivers tend to be tall, fast, have good hands (can catch the ball well), and can jump high.\nTight Ends block for running and passing plays, and they catch passes from the Quarterback. Tight ends tend to be strong and have good hands.\nOffensive Linemen block for running and passing plays. On passing plays, they provide protection for the Quarterback so the Quarterback has time to pass the ball without being tackled. On running plays, they provide protection for the player running the ball (e.g., the Running Back) so the ball carrier can get as far as possible without being tackled. Offensive Linemen tend to be large so they can provide adequate protection for the Quarterback and Running Back.\n\n\n2.1.2.3 Defense\nThe defense is on the field when the team does not have the ball (i.e., when the opposing team has the ball).\nPlayers on defense include:\n\nDefensive Linemen (DL), part of the “Defensive Line”\n\nDefensive End (DE)\nDefensive Tackle (DT)\n\nLinebacker (LB)\n\nMiddle (or Inside) Linebacker (MLB)\nOutside Linebacker (OLB)\n\nDefensive Back (DB), part of the “Secondary”\n\nCornerback (CB)\nSafety (S)\n\nFree Safety (FS)\nStrong Safety (SS)\n\n\n\nThe players on the defense attempt to tackle the offensive players for as short of gains as possible and attempt to prevent completed passes.\nOn passing plays, Defensive Linemen try to apply pressure to the Quarterback and try to tackle the Quarterback behind the line of scrimmage before the Quarterback can throw the ball (i.e., a sack). On rushing plays, Defensive Linemen try to tackle the ball carrier to prevent the ball carrier from advancing the ball (i.e., gaining yards). Defensive Linemen tend to be large yet quick so they can apply pressure to the Quarterback.\nLinebackers are versatile in that, on a given play, they may attempt to a) “blitz” to sack the Quarterback, b) stop the Running Back, or c) prevent a completed pass. Linebackers tend to be strong yet agile.\nDefensive Backs are specialist pass defenders. The main role of Cornerbacks is to cover the Wide Receivers. Safeties serve as the last line of defense for longer passes. Defensive Backs tend to be quick and agile.\n\n\n2.1.2.4 Special Teams\nThe special teams involves specialist players who are on the field during all kicking plays including kickoffs, field goals, and punts.\nPlayers on special teams include:\n\nKicker (K)\nPunter (P)\nHolder\nLong Snapper\nPunt Returner\nKick Returner\nand other players intended to block for or to tackle the ball carrier\n\nOn a field goal attempt, the Long Snapper snaps the ball to the Holder, who holds the ball for the Kicker. The Kicker attempts field goals and, during kickoffs, kicks the ball to the opposing team. During kickoffs, the Kick Returner catches the kicked ball and returns it for as many yards as possible. During a punt play, the Long Snapper snaps the ball to the Punter who kicks (i.e., punts) the ball to the opposing team. The Punt Returner catches the punted ball and returns it for as many yards as possible.\n\n\n\n2.1.3 The Field\nThe football field is rectangular and is 120 yards long and 53 1/3 yards wide (109.73 m x 48.77 m).1 At each end of the 120-yard field is a team’s end zone. Each end zone is 10 yards long (9.14 m). Thus, the distance from one end zone to the other end zone is 100 yards (91.44 m). Behind each end zone is a field goal post. A diagram of a football field is depicted in Figure 2.2.\n\n\n\n\n\n\nFigure 2.2: A Diagram of a Football Field. The yard markers depict the distance from the nearest end zone. The orange shaded area is called the “red zone”, where chances of scoring points are highest. The original figure was modified to depict field goal posts. (Figure retrieved from https://commons.wikimedia.org/wiki/File:American_football_field.svg)\n\n\n\n\n\n2.1.4 The Gameplay\nAt the beginning of the game, there is a coin flip to determine which teams receives the ball first and which team takes which side of the field. During the kickoff, the kicking team kicks the ball to the receiving team, who has the option to return the kick. The offense starts their possession at the 25 yard line—if there is no return (i.e., a touchback)—or wherever the kick returner is tackled or goes out of bounds.\nThe team with the ball (i.e., the offense) has four opportunities (“downs”) to advance the ball (i.e., gain) 10 yards. A team can advance the ball either by running it or by throwing (i.e., passing) and catching it. At the end of a rushing play, the ball advances to wherever the ball carrier is tackled or goes out of bounds (i.e., wherever the player is “down”). At the end of a passing play, if the thrown ball is caught (i.e., a completed pass), the ball advances to wherever the ball carrier is tackled or goes out of bounds. If the thrown ball is not caught in bounds before the ball hits the ground (i.e., an incomplete pass), the ball does not advance. Wherever the ball is advanced to dictates where the next play begins. The yard position on the field where the next play takes place from is known as the “line of scrimmage”. Neither team can cross the line the line of scrimmage until the next play begins. To begin the play, the ball is placed on the line of scrimmage and the Center gives (or “snaps”) the ball to the Quarterback.\nIf the team advances the ball 10 or more yards within four downs, the team receives a “first down” and is awarded a new set of downs—four more downs to advance the ball 10 more yards. If the team advances the ball all the way to the other team’s end zone, they score a touchdown. If the team fails to advance the ball 10 or more yards within four downs, the team loses the ball, and the other team takes possession at that spot on the field. There are risks of giving the other team the ball with a short distance to score. Thus, on fourth down, instead of trying to advance the ball for a first down, a team may choose to kick a field goal—to get points—or to punt.\nA field goal involves a kicker kicking the ball with an intent to kick the ball through the field goal posts (“uprights”). To score points by making a field goal, the kicked ball must go between the uprights (extended vertically) and over the cross bar.\nPunting involves a punter kicking the ball to the other team with an intent to give their opponent worse field position, thus making it harder for the other team to score. The punting team tries to pin the opponent as close as possible to the opponent’s end zone (i.e., as far as possible from the own team’s end zone), so they have a longer distance to go to score a touchdown.\nThere are multiple ways that ball possession can switch from the offense to the other team. After scoring a touchdown, field goal, or safety, there is a kickoff, in which the scoring team kicks the ball to the opponent. Another way that the ball switches possession to the other team is if the team commits a turnover. The defense can force a turnover by an interception, fumble recovery, or turnover on downs. A turnover due to an interception occurs when a defensive player catches the quarterback’s pass. A turnover due to a fumble recovery occurs when an offensive player, who had possession of the ball, loses the ball before being down or scoring a touchdown and the ball is recovered by the opponent. A turnover on downs occurs when the team attempts on fourth down to achieve the remainder of the needed 10 yards to go but fails.\nOther football-related situations include tackles for loss and sacks. A tackle for loss occurs when a ball carrier is tackled behind the line of scrimmage. A sack occurs when a Quarterback is tackled with the ball behind the line of scrimmage. A pass defended occurs when a defensive player knocks down the ball in the air so that the indended receiver cannot catch the ball.\n\n\n2.1.5 The Scoring\nThe goal of the team with the ball (i.e., the offense) is to score points. It can do this by either advancing the ball into the other team’s end zone (6 points) or by kicking a field goal (3 points). Advancing the ball in the other team’s end zone is called a touchdown. After a touchdown, the offense chooses to attempt either a point-after-touchdown (PAT) or a two-point conversion. A PAT is a short kick attempt from the 15-yard line (i.e., 15 yards away from the end zone) that, if it goes through the goal posts (“uprights”) and over the cross bar, is worth 1 point. A two-point conversion is a single-scoring opportunity from the 3-yard line (i.e., 3 yards away from the end zone). If the offense scores (i.e., advances the ball into the end zone) from the 3-yard line, the team is awarded 2 points.\nA team can kick a field goal from any distance as long as the kick goes through the goal posts. The current record for the longest field goal is 66 yards (by Justin Tucker in 2021).\nA safety occurs when the offense is tackled with the ball in their own end zone. When a safety occurs, the opposing team (i.e., defense) is awarded two points and the ball.\n\n\n2.1.6 Glossary of Terms\n\nrunning play (“run”) or rushing play (or “rush”)—the attempt by an offensive player, typically the Running Back or Quarterback, to advance the ball “on the ground” by running it—not by passing it forward\npassing play (or “pass”)—the attempt by an offensive player, typically the Quarterback, to advance the ball by throwing it forward to an offensive player\npassing attempt—the attempt to advance the ball by passing it (i.e., a thrown pass)\nrushing attempt—the attempt to advance the ball by running it\npassing completion—a thrown pass that is succesfully caught by an offensive player\npassing incompletion—a thrown pass that is not caught by an offensive player\npassing yards—the distance (in yards) the player advanced the ball by throwing it\nrushing yards—the distance (in yards) the player advanced the ball by running it\nreceving yards—the distance (in yards) the player advanced the ball by catching thrown passes and then running with it further upfield\nkick/punt return yards—the distance (in yards) the player advanced the ball by returning kicks or punts\nturnover return yards—the distance (in yards) the player advanced the ball by returning turnovers\nreception—a pass that is caught by the offensive player\ntouchdown—advancing the ball into the opponent’s end zone either by a) throwing a completed pass that ends up in the end zone, b) running it into the end zone, c) catching it in the end zone, or d) catching it and then running it into the end zone\npassing touchdown—advancing the ball into the opponent’s end zone either by throwing a completed pass that ends up in the end zone\nrushing touchdown—advancing the ball into the opponent’s end zone either by running it into the end zone\nreceiving touchdown—advancing the ball into the opponent’s end zone either by catching it in the end zone or by catching it and then running it into the end zone\nkick/punt return touchdown—advancing the ball into the opponent’s end zone when returning a kick or punt\nturnover return touchdown—advancing the ball into the opponent’s end zone when returning a turnover (i.e., interception or fumble)\ntwo-point conversion—a single-scoring opportunity from the 3-yard line (i.e., 3 yards away from the end zone) that is an option given to a team that scores a touchdown; if the offense scores (i.e., advances the ball into the end zone) from the 3-yard line, the team is awarded 2 points\nblock—when the defense/special teams blocks a kick or field goal by hitting the ball just after it is kicked to prevent the ball from going far\nkickoff—the kicking team kicks the ball to the receiving team, who has the option to return the kick\nfield goal—a kicker kicks the ball with an intent to kick the ball through the field goal posts (“uprights”). To score points by making a field goal, the kicked ball must go between the uprights (extended vertically) and over the cross bar. If the field goal attempt is successful, the team gains 3 points.\npoint after touchdown (PAT)—a short kick attempt from the 15-yard line (i.e., 15 yards away from the end zone) that, if it goes through the goal posts (“uprights”) and over the cross bar, is worth 1 point\nextra point returned—if the defense/special teams returns the ball into the opponent’s end zone during a point after touchdown (PAT) attempt, it is worth 2 points\npunt—a punter kicks the ball to the other team with an intent to give their opponent worse field position, thus making it harder for the other team to score\nfumble lost—when an offensive player, who had possession of the ball, loses the ball before being down or scoring a touchdown and the ball is recovered by the opponent\nfumble forced—when a defensive player knocks the ball out of the hands of an offensive player, who had possession of the ball\nfumble recovery—when a defensive player recovers a fumble by the opponent\ninterception—when a defensive player catches a pass from an offensive player\ntackle—when a player brings down the ball carrier\ntackle solo—when a player is the main tackler (i.e., the primary player to bring down the ball carrier)\ntackle assist—when a player is one of two or more players who, together, bring down the ball carrier\ntackle for loss—when an offensive player is tackled with the ball behind the line of scrimmage\nsack—when a Quarterback is tackled with the ball behind the line of scrimmage\npass defended—when a defensive player knocks down the ball in the air so that the indended receiver cannot catch the ball\nsafety—when the offense is tackled with the ball in their own end zone",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#sec-fantasyFootball",
    "href": "fantasy-football.html#sec-fantasyFootball",
    "title": "2  Intro to Football and Fantasy",
    "section": "2.2 Fantasy Football",
    "text": "2.2 Fantasy Football\n\n2.2.1 Overview of Fantasy Football\nFantasy football is one of the most widely played games in the history of games. It is estimated that around 62 million people play fantasy sports [Fantasy Sports & Gaming Association (2023); archived at https://perma.cc/9PB8-ZDJJ], of whom around 29 million play fantasy football [statistica (2023a); archived at https://perma.cc/8YSN-UUNT]. As noted in the Introduction, fantasy football is an online game where participants assemble (i.e., “draft”) imaginary teams composed of real-life National Football League (NFL) players.2 The participants are in charge of managing and making strategic decisions for their imaginary team to have the best possible team that will score the most points. Thus, the participants are called “managers”. Managers make decisions such as selecting which players to draft, selecting which players to play (i.e., “start”) on a weekly basis, identifying players to pick up from the remaining pool of available players (i.e., waiver wire), and making trades with other teams.\nThere are variety of types of fantasy football leagues. In standard re-draft leagues, managers re-draft players each season. In keeper leagues, managers are allowed to keep one or more players from one season to the next, possibly for some cost (e.g., toward a keeper cap, loss of future draft pick). In dynasty leagues, managers act like a general manager and keep most of their roster from year to year. They may involve player contracts, salary caps, and free agent drafts. In best ball leagues, the manager’s best possible lineup (in terms of the highest-scoring players for the necessary roster positions) are automatically selected for that week’s lineup.\nFantasy football relies heavily on prediction—trying to predict which players will perform best and selecting them accordingly.\nHere is a video that provides a brief introduction to fantasy football (NFL Fantasy Football, 2020):\nVideo that Provides an Introduction to Fantasy Football. From: https://www.youtube.com/watch?v=XhrBapdhLEc.\n\n\n\n2.2.2 The Fantasy League\nA fantasy football “league” is composed of various imaginary (i.e., “fantasy”) teams—and their associated manager. In the fantasy league, the managers’ fantasy teams play against each other. A fantasy league is commonly composed of 8, 10, or 12 fantasy teams, but leagues can have more or fewer teams.\n\n\n2.2.3 The Roster of a Fantasy Team\nOn a given roster, a manager has a “starting lineup” and a “bench”. Each week, the manager decides which players on their roster to put in the starting lineup, and which to keep on the bench. In many leagues, a starting lineup is composed of offensive players, a kicker, and defense/special teams:\nOffensive players:\n\n\n\nTable 2.1: Offensive Players in the Starting Lineup\n\n\n\n\n\n\n\n\n\nPosition\nTypical Number of Players in Starting Lineup\n\n\n\n\nQuarterback (QB)\n1\n\n\nRunning Back (RB)\n2\n\n\nWide Receiver (WR)\n2\n\n\nTight End (TE)\n1\n\n\nFlex Position\n1\n\n\n\n\n\n\nA “flex position” is a flexible position that can involve a player from various positions: e.g., a Running Back, Wide Receiver, or Tight End.\nKickers:\n\none Kicker (K)\n\nDefense/Special Teams:\n\none Team Defense (DST/D/DEF) or multiple Individual Defensive Players (IDP)\n\n\n\n2.2.4 Scoring\n\n2.2.4.1 Scoring Overview\nIn the game of fantasy football, managers accumulate points on a weekly basis based on players’ actual statistical performances in NFL games. Managers receive points for only those players who are on their starting lineup (not players on their bench). In a standard league, a manager’s goal is to outscore their opponent each week to win matches and ultimately claim victory in the league. At the end of the regular season, many leagues have a playoffs to determine the league champion. In total points leagues, the league champion is determined by how many points they score throughout the entire season, rather than based on weekly matchups and playoffs.\nScoring settings can differ from league to league. Below are common scoring settings for fantasy leagues.\n\n\n2.2.4.2 Offensive Players\n\n\n\nTable 2.2: Common Scoring Settings for Offensive Players\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nRushing or receiving TD\n6\n\n\nReturning a kick or punt for a TD\n6\n\n\nReturning or recovering a fumble for a TD\n6\n\n\nPassing TD\n4\n\n\nPassing INT\n−2\n\n\nFumble lost\n−2\n\n\nRushing, passing, or receiving 2-point conversion\n2\n\n\nRushing or receiving yards\n1 point per 10 yards\n\n\nPassing yards\n1 point per 25 yards\n\n\n\n\n\n\nNote: “TD” = touchdown; “INT” = interception\nOther common (but not necessarily standard) statistical categories include:\n\nreceptions (called “point per reception” [PPR] leagues)\nreturn yards\npassing attempts\nrushing attempts\n\n\n\n2.2.4.3 Kickers\n\n\n\nTable 2.3: Common Scoring Settings for Kickers\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nFG made: 50+ yards\n5\n\n\nFG made: 40–49 yards\n4\n\n\nFG made: 39 yards or less\n3\n\n\nRushing, passing, or receiving 2-point conversion\n2\n\n\nPoint after touchdown attempt made\n1\n\n\nPoint after touchdown attempt missed\n−1\n\n\nMissed FG: 0–39 yards\n−2\n\n\nMissed FG: 40–49 yards\n−1\n\n\n\n\n\n\nNote: “FG” = field goal\n\n\n2.2.4.4 Team Defense/Special Teams\n\n\n\nTable 2.4: Common Scoring Settings for Team Defense/Special Teams\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nDefensive or special teams TD\n6\n\n\nInterception\n2\n\n\nFumble recovery\n2\n\n\nBlocked punt, PAT, or FG\n2\n\n\nSafety\n2\n\n\nSack\n1\n\n\n\n\n\n\nNote: “TD” = touchdown; “PAT” = point after touchdown; “FG” = field goal\n\n\n2.2.4.5 Individual Defensive Players\n\n\n\nTable 2.5: Common Scoring Settings for Individual Defensive Players\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nTackle solo\n1\n\n\nTackle assist\n0.5\n\n\nTackle for loss\n1\n\n\nSack\n2\n\n\nInterception\n4\n\n\nFumble forced\n2\n\n\nFumble recovery\n2\n\n\nTD\n6\n\n\nSafety\n2\n\n\nPass defended\n1\n\n\nBlocked kick\n2\n\n\nExtra point returned\n2\n\n\n\n\n\n\nNote: “TD” = touchdown\nOther common (but not necessarily standard) statistical categories include:\n\nturnover return yards\n\n\n\n2.2.4.6 Common Scoring Abbreviations\n\n“TD” = touchdown\n“INT” = interception\n“yds” = yards\n“ATT” = attempts\n“2-pt conversion” = two-point conversion\n“FG” = field goal\n“PAT” = point after touchdown (i.e., extra point/point after attempt)\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFantasy Sports & Gaming Association. (2023). Industry demographics. https://thefsga.org/industry-demographics/\n\n\nJones, J. M. (2024). Football retains dominant position as favorite U.S. sport. https://news.gallup.com/poll/610046/football-retains-dominant-position-favorite-sport.aspx\n\n\nLy, N. (2015). The rules of American football - EXPLAINED! (NFL). https://www.youtube.com/watch?v=Ddwp1HyEFRE\n\n\nNFL Fantasy Football. (2020). How to play fantasy football for BEGINNERS. https://www.youtube.com/watch?v=XhrBapdhLEc\n\n\nstatistica. (2023a). Fantasy sports in the U.S.- statistics & facts. https://www.statista.com/topics/10895/fantasy-sports-in-the-us/\n\n\nstatistica. (2023b). Most watched sports leagues in the United States in 2023, by minutes watched. https://www.statista.com/statistics/1430289/most-watched-sports-leagues-usa/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#footnotes",
    "href": "fantasy-football.html#footnotes",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "One yard is equal to three feet. A yard is just smaller than a meter (0.9144 meters).↩︎\nFantasy leagues are also available for baseball, basketball, and many other sports.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "",
    "text": "3.1 Learning R\nThe book uses the software R (R Core Team, 2024) for statistical analyses (http://www.r-project.org). R is a free software environment; you can download it at no charge here: https://cran.r-project.org.\nHere are a various resources for learning R:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-learningR",
    "href": "getting-started.html#sec-learningR",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "",
    "text": "Intro to R: https://www.statmethods.net\n\nVideo training courses in R skills: https://www.pluralsight.com/search?q=R\n\nBrowse the Cookbook for R to find solutions to common tasks and problems: http://www.cookbook-r.com\n\nBrowse the R Graph Gallery to find examples of various graphs: https://r-graph-gallery.com\n\nFree Codeacademy course on R: https://www.codecademy.com/learn/learn-r\n\nFree Coursera courses on R: https://www.coursera.org/search?query=R\n\nWatch these videos from Coursera: https://blog.revolutionanalytics.com/2012/12/coursera-videos.html\n\n\nPosit/Rstudio Webinars: https://posit.co/resources/videos/\n\nUCLA Stats Website: https://stats.idre.ucla.edu/r/\n\nIntroduction to R course on Datacamp: https://www.datacamp.com/courses/free-introduction-to-r\n\nTeaching R in a Kinder, Gentler, More Effective Manner: https://github.com/matloff/TidyverseSkeptic\n\nLearn R interactively with swirl: https://swirlstats.com\n\nUse the learnr package (Aden-Buie et al., 2023): https://rstudio.github.io/learnr/\n\nResources for learning tidyverse (Wickham et al., 2019; Wickham, 2023), which is a collection of R packages for data management: https://www.tidyverse.org/learn/\n\nYou will sometimes find relevant articles on R-bloggers: https://www.r-bloggers.com",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingHelpR",
    "href": "getting-started.html#sec-gettingHelpR",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.2 Getting Help with R\n",
    "text": "3.2 Getting Help with R\n\nIf you have R questions, you can ask them in a number of places:\n\nForums:\n\n\nPosit: https://forum.posit.co\n\n\nStackOverflow: https://stackoverflow.com/questions/tagged/r\n\n\nReddit: https://www.reddit.com/r/rstats/\n\n\n\nThe R mailing list: https://stat.ethz.ch/mailman/listinfo/r-help\n\n\nSalmon (2018) provides additional resources and good guidance for getting help with R: https://masalmon.eu/2018/07/22/wheretogethelp/ (archived at https://perma.cc/4RRE-KL33).\nWhen posting a question on forums or mailing lists, keep a few things in mind:\n\nRead the posting guidelines before posting!\nBe respectful of other people and their time. R is free software. People are offering their free time to help. They are under no obligation to help you. If you are disrespectful or act like they owe you anything, you will rub people the wrong way and will be less likely to get help.\nProvide a minimal, reproducible example. Providing a minimal, reproducible example can be crucial for getting a helpful response. By going to the trouble of creating a minimal, reproducible example and identifying the minimum conditions necessary to reproduce the issue, you will often figure out how to resolve it. Stack Overflow (2025) offers guidelines on providing a minimal, reproducible example: https://stackoverflow.com/help/minimal-reproducible-example (archived at https://perma.cc/6NUB-UTYF). Stack Overflow (2018) offers a good example and guidelines for providing a minimal, reproducible example in R: https://stackoverflow.com/a/5963610 (archived at https://perma.cc/PC9L-DQZG). My strong recommendation is to provide a reprex whenever possible: https://reprex.tidyverse.org. Bryan et al. (2025) provide Reprex do’s and don’ts: https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-initialSetup",
    "href": "getting-started.html#sec-initialSetup",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.3 Initial Setup",
    "text": "3.3 Initial Setup\nTo get started, follow the following steps:\n\nInstall R: https://cran.r-project.org\nInstall RStudio Desktop: https://posit.co/download/rstudio-desktop\n\nAfter installing RStudio, open RStudio and run the following code in the console to install several key R packages:\n\nCodeinstall.packages(\n  c(\"petersenlab\",\"remotes\",\"nflreadr\",\"nflfastR\",\"nfl4th\",\"nflplotR\",\n  \"gsisdecoder\",\"progressr\",\"lubridate\",\"tidyverse\",\"psych\"))\n\n\n\n\nSome necessary packages, including the ffanalytics package (Andersen et al., 2025), are hosted in GitHub and need to be installed using the following code (after installing the remotes package (Csárdi et al., 2024) above):\n\nCoderemotes::install_github(\"FantasyFootballAnalytics/ffanalytics\")\n\n\n\n\n\n\n\n\n\n\nNote 3.1: If you are in Dr. Petersen’s class\n\n\n\nIf you are in Dr. Petersen’s class, also perform the following steps:\n\nDownload and install git: https://git-scm.com/downloads\n\nSet up a free account on GitHub.com.\nDownload and install GitHub Desktop: https://desktop.github.com\n\nMake sure you are logged into your GitHub account on GitHub.com.\nGo to the following GitHub repository: https://github.com/isaactpetersen/QuartoBlogFantasyFootball and complete the following steps:\n\nClick “Use this Template” (in the top right of the screen) &gt; “Create a new repository”\nMake sure the checkbox is selected for the following option: “Include all branches”\nMake sure your Owner account is selected\nSpecify the repository name to whatever you want, such as FantasyFootballBlog\n\nType a brief description, such as Files for my fantasy football blog\n\nKeep the repository public (this is necessary for generating your blog)\nSelect “Create repository”\n\n\nAfter creating the new repository, make sure you are on the page of of your new repository and complete the following steps:\n\nClick “Settings” (in the top of the screen)\nClick “Actions” (in the left sidebar) &gt; “General”\nMake sure the following are selected:\n\n“Read and write permissions” (under “Workflow permissions”)\n“Allow GitHub Actions to create and approve pull requests”\nthen click “Save”\n\n\nClick “Pages” (in the left sidebar)\nMake sure the following are selected:\n\n“Deploy from a branch” (under “Source”)\n“gh-pages/(root)” (under “Branch”)\nthen click “Save”\n\n\n\n\nClone the repository to your local computer by clicking “Code” &gt; “Open with GitHub Desktop”, select the folder where you want the repository to be saved on your local computer, and click “Clone”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-installingPackages",
    "href": "getting-started.html#sec-installingPackages",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.4 Installing Packages",
    "text": "3.4 Installing Packages\nYou can install R packages using the following syntax:\n\nCodeinstall.packages(\"INSERT_PACKAGE_NAME_HERE\")\n\n\nFor instance, you can use the following code to install the tidyverse package (Wickham, 2023):\n\nCodeinstall.packages(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-loadPackages",
    "href": "getting-started.html#sec-loadPackages",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.5 Load Packages",
    "text": "3.5 Load Packages\n\nCodelibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-functionsArguments",
    "href": "getting-started.html#sec-functionsArguments",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.6 Using Functions and Arguments",
    "text": "3.6 Using Functions and Arguments\nYou can learn about a particular function and its arguments by entering a question mark before the name of the function:\n\nCode?NAME_OF_FUNCTION()\n\n\nBelow, we provide examples for how to learn about and use functions and arguments, by using the seq() function as an example. The seq() function creates a sequence of numbers. To learn about the seq() function, which creates a sequence of numbers, you can execute the following command:\n\nCode?seq()\n\n\nThis is what the documentation shows for the seq() function in the Usage section:\n\nCodeseq(\n  from = 1,\n  to = 1,\n  by = ((to - from)/(length.out - 1)),\n  length.out = NULL,\n  along.with = NULL,\n  ...)\n\n\nBased on this information, we know that the seq() function takes the following arguments:\n\nfrom\nto\nby\nlength.out\nalong.with\n...\n\nThe arguments have default values that are used if the user does not specify values for the arguments. The default values are provided in the Usage section and are in Table 3.1:\n\n\nTable 3.1: Arguments and defaults for the seq() function. Arguments with a default of NULL are not used unless a value is provided by the user.\n\n\n\nArgument\nDefault Value for Argument\n\n\n\nfrom\n1\n\n\nto\n1\n\n\nby\n((to - from)/(length.out - 1))\n\n\nlength.out\nNULL\n\n\nalong.with\nNULL\n\n\n\n\n\n\nWhat each argument represents (i.e., the meaning of from, to, by, etc.) is provided in the Arguments section of the documentation. You can specify a function and its arguments either by providing values for each argument in the order indicated by the function, or by naming its arguments.\nHere is an example of providing values to the arguments in the order indicated by the function, to create a sequence of numbers from 1 to 9:\n\nCodeseq(1, 9)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nHere is an example of providing values to the arguments by naming its arguments:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 1)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nIf you provide values to arguments by naming the arguments, you can reorder the arguments and get the same answer:\n\nCodeseq(\n  by = 1,\n  to = 9,\n  from = 1)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nThere are various combinations of arguments that one could use to obtain the same result. For instance, here is code to generate a sequence from 1 to 9 by 2:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 2)\n\n[1] 1 3 5 7 9\n\n\nOr, alternatively, you could specify the length of the desired sequence (5 values):\n\nCodeseq(\n  from = 1,\n  to = 9,\n  length.out = 5)\n\n[1] 1 3 5 7 9\n\n\nIf you want to generate a series with decimal values, you could specify a long desired sequence of 81 values:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  length.out = 81)\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n[20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n[39] 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6\n[58] 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 8.5\n[77] 8.6 8.7 8.8 8.9 9.0\n\n\nThis is equivalent to specifying a sequence from 1 to 9 by 0.1:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 0.1)\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n[20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n[39] 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6\n[58] 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 8.5\n[77] 8.6 8.7 8.8 8.9 9.0\n\n\nHopefully, that provides an example for how to learn about a particular function, its arguments, and how to use them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createVector",
    "href": "getting-started.html#sec-createVector",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.7 Create a Vector",
    "text": "3.7 Create a Vector\nA vector is a series of elements that can be numeric or character. It has one dimension (length). To create a vector, use the c() to combine elements into a vector. And, we use the assignment operator (&lt;-) to assign the vector to an object named exampleVector, so we can access it later.\n\nCodeexampleVector &lt;- c(40, 30, 24, 20, 18, 23, 27, 32, 26, 23, NA, 37)\n\n\nWe can then access the contents of the object by calling its name:\n\nCodeexampleVector\n\n [1] 40 30 24 20 18 23 27 32 26 23 NA 37",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createDF",
    "href": "getting-started.html#sec-createDF",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.8 Create a Data Frame",
    "text": "3.8 Create a Data Frame\nA data frame has two dimensions: rows and columns. Here is an example of creating a data frame, while using the assignment operator (&lt;-) to assign the data frame to an object so we can access it later:\n\nCodeplayers &lt;- data.frame(\n  ID = 1:12,\n  name = c(\n    \"Ken Cussion\",\n    \"Ben Sacked\",\n    \"Chuck Downfield\",\n    \"Ron Ingback\",\n    \"Rhonda Ball\",\n    \"Hugo Long\",\n    \"Lionel Scrimmage\",\n    \"Drew Blood\",\n    \"Chase Emdown\",\n    \"Justin Time\",\n    \"Spike D'Ball\",\n    \"Isac Ulooz\"),\n  position = c(\"QB\",\"QB\",\"QB\",\"RB\",\"RB\",\"WR\",\"WR\",\"WR\",\"WR\",\"TE\",\"TE\",\"LB\"),\n  age = c(40, 30, 24, 20, 18, 23, 27, 32, 26, 23, NA, 37)\n  )\n\nfantasyPoints &lt;- data.frame(\n  ID = c(2, 7, 13, 14),\n  fantasyPoints = c(250, 170, 65, 15)\n)\n\nfantasyPoints_weekly &lt;- expand.grid(\n  ID = 1:12,\n  season = c(2022, 2023),\n  week = 1:17\n)\n\nset.seed(52242)\nfantasyPoints_weekly$fantasyPoints &lt;- sample(\n  0:35,\n  size = nrow(fantasyPoints_weekly),\n  replace = TRUE\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createList",
    "href": "getting-started.html#sec-createList",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.9 Create a List",
    "text": "3.9 Create a List\nA list can store multiple data frames in one object:\n\nCodeexampleList &lt;- list(players, fantasyPoints, fantasyPoints_weekly)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingStartedLoadData",
    "href": "getting-started.html#sec-gettingStartedLoadData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.10 Load a Data Frame",
    "text": "3.10 Load a Data Frame\nHere is how you load a .RData file using a relative path (i.e., a path relative to the working directory, where the working directory is represented by a period):\n\nCodeload(file = \"./data/nfl_players.RData\")\n\n\nThe following code loads a file from an absolute path:\n\nCodenfl_players &lt;- read.csv(\"C:/Users/myusername/nfl_players.RData\")\n\n\nHere is how you load a .csv file:\n\nCodenfl_players &lt;- read.csv(\"./data/nfl_players.csv\") # relative path\nnfl_players &lt;- read.csv(\"C:/Users/myusername/nfl_players.csv\") # absolute path",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingStartedSaveData",
    "href": "getting-started.html#sec-gettingStartedSaveData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.11 Save a Data Frame",
    "text": "3.11 Save a Data Frame\nHere is how you save a .RData file using a relative path:\n\nCodesave(\n  nfl_players,\n  file = \"./data/nfl_players.RData\")\n\n\nThe following code saves a file to an absolute path:\n\nCodesave(\n  nfl_players,\n  file = \"C:/Users/myusername/nfl_players.RData\")\n\n\nHere is how you save a .csv file:\n\nCodewrite.csv(\n  nfl_players,\n  file = \"./data/nfl_players.csv\") # relative path\n\nwrite.csv(\n  nfl_players,\n  file = \"C:/Users/myusername/nfl_players.csv\") # absolute path",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-variableNames",
    "href": "getting-started.html#sec-variableNames",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.12 Variable Names",
    "text": "3.12 Variable Names\nTo see the names of variables in a data frame, use the following syntax:\n\nCodenames(nfl_players)\n\n [1] \"gsis_id\"                  \"status\"                  \n [3] \"display_name\"             \"first_name\"              \n [5] \"last_name\"                \"esb_id\"                  \n [7] \"birth_date\"               \"college_name\"            \n [9] \"position_group\"           \"position\"                \n[11] \"jersey_number\"            \"height\"                  \n[13] \"weight\"                   \"years_of_experience\"     \n[15] \"team_abbr\"                \"team_seq\"                \n[17] \"current_team_id\"          \"football_name\"           \n[19] \"entry_year\"               \"rookie_year\"             \n[21] \"draft_club\"               \"draft_number\"            \n[23] \"college_conference\"       \"status_description_abbr\" \n[25] \"status_short_description\" \"gsis_it_id\"              \n[27] \"short_name\"               \"smart_id\"                \n[29] \"headshot\"                 \"suffix\"                  \n[31] \"uniform_number\"           \"draft_round\"             \n\nCodenames(players)\n\n[1] \"ID\"       \"name\"     \"position\" \"age\"     \n\nCodenames(fantasyPoints)\n\n[1] \"ID\"            \"fantasyPoints\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-logicalOperators",
    "href": "getting-started.html#sec-logicalOperators",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.13 Logical Operators",
    "text": "3.13 Logical Operators\n\n3.13.1 Is Equal To: ==\n\n\nCodeplayers$position == \"RB\"\n\n [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n3.13.2 Is Not Equal To: !=\n\n\nCodeplayers$position != \"RB\"\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\n3.13.3 Is Greater Than: &gt;\n\n\nCodeplayers$age &gt; 30\n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE    NA  TRUE\n\n\n\n3.13.4 Is Less Than: &lt;\n\n\nCodeplayers$age &lt; 30\n\n [1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA FALSE\n\n\n\n3.13.5 Is Greater Than or Equal To: &gt;=\n\n\nCodeplayers$age &gt;= 30\n\n [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE    NA  TRUE\n\n\n\n3.13.6 Is Less Than or Equal To: &lt;=\n\n\nCodeplayers$age &lt;= 30\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA FALSE\n\n\n\n3.13.7 Is In a Value of Another Vector: %in%\n\n\nCodeplayers$position %in% c(\"RB\",\"WR\")\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\n\n3.13.8 Is Not In a Value of Another Vector: !(%in%)\n\n\nCode!(players$position %in% c(\"RB\",\"WR\"))\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\n\n3.13.9 Is Missing: is.na()\n\n\nCodeis.na(players$age)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n3.13.10 Is Not Missing: !is.na()\n\n\nCode!is.na(players$age)\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n\n\n\n3.13.11 And: &\n\n\nCodeplayers$position == \"WR\" & players$age &gt; 26\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n3.13.12 Or: |\n\n\nCodeplayers$position == \"WR\" | players$age &gt; 23\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE    NA  TRUE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-piping",
    "href": "getting-started.html#sec-piping",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.14 Piping",
    "text": "3.14 Piping\nIn base R, if you want to perform multiple operations, it is common to either a) nest the operations, or b) save the object at each step.\nBelow is an example of nested operations:\n\nCodelength(names(nfl_players))\n\n[1] 32\n\n\nBelow is an example of saving the intermediate object at each step:\n\nCodevariableNames &lt;- names(nfl_players)\nvariableNames\n\n [1] \"gsis_id\"                  \"status\"                  \n [3] \"display_name\"             \"first_name\"              \n [5] \"last_name\"                \"esb_id\"                  \n [7] \"birth_date\"               \"college_name\"            \n [9] \"position_group\"           \"position\"                \n[11] \"jersey_number\"            \"height\"                  \n[13] \"weight\"                   \"years_of_experience\"     \n[15] \"team_abbr\"                \"team_seq\"                \n[17] \"current_team_id\"          \"football_name\"           \n[19] \"entry_year\"               \"rookie_year\"             \n[21] \"draft_club\"               \"draft_number\"            \n[23] \"college_conference\"       \"status_description_abbr\" \n[25] \"status_short_description\" \"gsis_it_id\"              \n[27] \"short_name\"               \"smart_id\"                \n[29] \"headshot\"                 \"suffix\"                  \n[31] \"uniform_number\"           \"draft_round\"             \n\nCodelengthOfVariableNames &lt;- length(variableNames)\nlengthOfVariableNames\n\n[1] 32\n\n\nCode for performing nested operations can be challenging to read. Saving the intermediate object can be a waste of time to do if you are not interested in the intermediate object, and can take up unnecessary memory and computational resources. An alternative approach is to use piping. Piping allows taking the result from one computation and sending it to the next computation, thus allowing a chain of computations without saving the intermediate object at each step.\nIn base R, you can perform piping with the |&gt; expression. In tidyverse you can perform piping with the %&gt;% expression.\n\n3.14.0.1 Base R\n\n\nCodenfl_players |&gt;\n  names() |&gt;\n  length()\n\n[1] 32\n\n\n\n3.14.0.2 Tidyverse\n\nCodenfl_players %&gt;%\n  names() %&gt;%\n  length()\n\n[1] 32",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-subset",
    "href": "getting-started.html#sec-subset",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.15 Subset",
    "text": "3.15 Subset\nTo subset a data frame, use brackets to specify the subset of rows and columns to keep, where the value/vector before the comma specifies the rows to keep, and the value/vector after the comma specifies the columns to keep:\n\nCodedataframe[rowsToKeep, columnsToKeep]\n\n\nYou can subset by using any of the following:\n\nnumeric indices of the rows/columns to keep (or drop)\nnames of the rows/columns to keep (or drop)\nvalues of TRUE and FALSE corresponding to which rows/columns to keep\n\n\n3.15.1 One Variable\nTo subset one variable, use the following syntax:\n\nCodeplayers$name\n\n [1] \"Ken Cussion\"      \"Ben Sacked\"       \"Chuck Downfield\"  \"Ron Ingback\"     \n [5] \"Rhonda Ball\"      \"Hugo Long\"        \"Lionel Scrimmage\" \"Drew Blood\"      \n [9] \"Chase Emdown\"     \"Justin Time\"      \"Spike D'Ball\"     \"Isac Ulooz\"      \n\n\nor:\n\nCodeplayers[,\"name\"]\n\n [1] \"Ken Cussion\"      \"Ben Sacked\"       \"Chuck Downfield\"  \"Ron Ingback\"     \n [5] \"Rhonda Ball\"      \"Hugo Long\"        \"Lionel Scrimmage\" \"Drew Blood\"      \n [9] \"Chase Emdown\"     \"Justin Time\"      \"Spike D'Ball\"     \"Isac Ulooz\"      \n\n\n\n3.15.2 Particular Rows of One Variable\nTo subset one variable, use the following syntax:\n\nCodeplayers$name[which(players$position == \"RB\")]\n\n[1] \"Ron Ingback\" \"Rhonda Ball\"\n\n\nor:\n\nCodeplayers[which(players$position == \"RB\"), \"name\"]\n\n[1] \"Ron Ingback\" \"Rhonda Ball\"\n\n\n\n3.15.3 Particular Columns (Variables)\nTo subset particular columns/variables, use the following syntax:\n\n3.15.3.1 Base R\n\n\nCodesubsetVars &lt;- c(\"name\",\"age\")\n\nplayers[,c(2,4)]\n\n\n  \n\n\nCodeplayers[,c(\"name\",\"age\")]\n\n\n  \n\n\nCodeplayers[,subsetVars]\n\n\n  \n\n\n\nOr, to drop columns:\n\nCodedropVars &lt;- c(\"name\",\"age\")\n\nplayers[,-c(2,4)]\n\n\n  \n\n\nCodeplayers[,!(names(players) %in% c(\"name\",\"age\"))]\n\n\n  \n\n\nCodeplayers[,!(names(players) %in% dropVars)]\n\n\n  \n\n\n\n\n3.15.3.2 Tidyverse\n\nCodeplayers %&gt;%\n  select(name, age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(name:age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(all_of(subsetVars))\n\n\n  \n\n\n\nOr, to drop columns:\n\nCodeplayers %&gt;%\n  select(-name, -age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(-c(name:age))\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(-all_of(dropVars))\n\n\n  \n\n\n\n\n3.15.4 Particular Rows\nTo subset particular rows, use the following syntax:\n\n3.15.4.1 Base R\n\n\nCodesubsetRows &lt;- c(4,5)\n\nplayers[c(4,5),]\n\n\n  \n\n\nCodeplayers[subsetRows,]\n\n\n  \n\n\nCodeplayers[which(players$position == \"RB\"),]\n\n\n  \n\n\n\n\n3.15.4.2 Tidyverse\n\nCodeplayers %&gt;%\n  filter(position == \"WR\")\n\n\n  \n\n\nCodeplayers %&gt;%\n  filter(position == \"WR\", age &lt;= 26)\n\n\n  \n\n\nCodeplayers %&gt;%\n  filter(position == \"WR\" | age &gt;= 26)\n\n\n  \n\n\n\n\n3.15.5 Particular Rows and Columns\nTo subset particular rows and columns, use the following syntax:\n\n3.15.5.1 Base R\n\n\nCodeplayers[c(4,5), c(2,4)]\n\n\n  \n\n\nCodeplayers[subsetRows, subsetVars]\n\n\n  \n\n\nCodeplayers[which(players$position == \"RB\"), subsetVars]\n\n\n  \n\n\n\n\n3.15.5.2 Tidyverse\n\nCodeplayers %&gt;%\n  filter(position == \"RB\") %&gt;%\n  select(all_of(subsetVars))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-viewData",
    "href": "getting-started.html#sec-viewData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.16 View Data",
    "text": "3.16 View Data\n\n3.16.1 All Data\nTo view data, use the following syntax:\n\nCodeView(players)\n\n\n\n3.16.2 First 6 Rows/Elements\nTo view only the first six rows (if a data frame) or elements (if a vector), use the following syntax:\n\nCodehead(nfl_players)\n\n\n  \n\n\nCodehead(nfl_players$display_name)\n\n[1] \"'Omar Ellison\"    \"A'Shawn Robinson\" \"A.J. Arcuri\"      \"A.J. Barner\"     \n[5] \"A.J. Bouye\"       \"A.J. Brown\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-dataCharacteristics",
    "href": "getting-started.html#sec-dataCharacteristics",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.17 Data Characteristics",
    "text": "3.17 Data Characteristics\n\n3.17.1 Data Structure\n\nCodestr(nfl_players)\n\nnflvrs_d [20,751 × 32] (S3: nflverse_data/tbl_df/tbl/data.table/data.frame)\n $ gsis_id                 : chr [1:20751] \"00-0004866\" \"00-0032889\" \"00-0037845\" \"00-0039793\" ...\n $ status                  : chr [1:20751] \"RET\" \"ACT\" \"DEV\" \"ACT\" ...\n $ display_name            : chr [1:20751] \"'Omar Ellison\" \"A'Shawn Robinson\" \"A.J. Arcuri\" \"A.J. Barner\" ...\n $ first_name              : chr [1:20751] \"'Omar\" \"A'Shawn\" \"A.J.\" \"A.J.\" ...\n $ last_name               : chr [1:20751] \"Ellison\" \"Robinson\" \"Arcuri\" \"Barner\" ...\n $ esb_id                  : chr [1:20751] \"ELL711319\" \"ROB367960\" \"ARC716900\" \"BAR235889\" ...\n $ birth_date              : chr [1:20751] \"1971-10-08\" \"1995-03-21\" \"1997-08-13\" \"2002-05-03\" ...\n $ college_name            : chr [1:20751] NA \"Alabama\" \"Michigan State\" \"Michigan\" ...\n $ position_group          : chr [1:20751] \"WR\" \"DL\" \"OL\" \"TE\" ...\n $ position                : chr [1:20751] \"WR\" \"DT\" \"T\" \"TE\" ...\n $ jersey_number           : int [1:20751] 84 94 61 88 24 11 60 6 81 63 ...\n $ height                  : num [1:20751] 73 76 79 78 72 72 75 76 69 76 ...\n $ weight                  : int [1:20751] 200 330 320 251 191 226 325 220 190 280 ...\n $ years_of_experience     : chr [1:20751] \"2\" \"9\" \"2\" \"0\" ...\n $ team_abbr               : chr [1:20751] \"LAC\" \"CAR\" \"LA\" \"SEA\" ...\n $ team_seq                : int [1:20751] NA 1 NA NA 1 1 1 1 NA NA ...\n $ current_team_id         : chr [1:20751] \"4400\" \"0750\" \"2510\" \"4600\" ...\n $ football_name           : chr [1:20751] NA \"A'Shawn\" \"A.J.\" \"A.J.\" ...\n $ entry_year              : int [1:20751] NA 2016 2022 2024 2013 2019 2015 2019 NA NA ...\n $ rookie_year             : int [1:20751] NA 2016 2022 2024 2013 2019 2015 2019 NA NA ...\n $ draft_club              : chr [1:20751] NA \"DET\" \"LA\" \"SEA\" ...\n $ draft_number            : int [1:20751] NA 46 261 121 NA 51 67 NA NA NA ...\n $ college_conference      : chr [1:20751] NA \"Southeastern Conference\" \"Big Ten Conference\" \"Big Ten Conference\" ...\n $ status_description_abbr : chr [1:20751] NA \"A01\" \"P01\" \"A01\" ...\n $ status_short_description: chr [1:20751] NA \"Active\" \"Practice Squad\" \"Active\" ...\n $ gsis_it_id              : int [1:20751] NA 43335 54726 57242 40688 47834 42410 48335 NA NA ...\n $ short_name              : chr [1:20751] NA \"A.Robinson\" \"A.Arcuri\" \"A.Barner\" ...\n $ smart_id                : chr [1:20751] \"3200454c-4c71-1319-728e-d49d3d236f8f\" \"3200524f-4236-7960-bf20-bc060ac0f49c\" \"32004152-4371-6900-5185-8cdd66b2ad11\" \"32004241-5223-5889-95d9-0ba3aeeb36ed\" ...\n $ headshot                : chr [1:20751] NA \"https://static.www.nfl.com/image/private/f_auto,q_auto/league/qgiwxchd1lmgszfunys8\" NA \"https://static.www.nfl.com/image/upload/f_auto,q_auto/league/msnzbeyjoemcas9dm8vt\" ...\n $ suffix                  : chr [1:20751] NA NA NA NA ...\n $ uniform_number          : chr [1:20751] NA \"94\" \"61\" \"88\" ...\n $ draft_round             : chr [1:20751] NA NA NA NA ...\n - attr(*, \"nflverse_type\")= chr \"players\"\n - attr(*, \"nflverse_timestamp\")= POSIXct[1:1], format: \"2024-09-04 02:56:52\"\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\n\n3.17.2 Data Dimensions\nNumber of rows and columns:\n\nCodedim(nfl_players)\n\n[1] 20751    32\n\n\nNumber of rows:\n\nCodenrow(nfl_players)\n\n[1] 20751\n\n\nNumber of columns:\n\nCodencol(nfl_players)\n\n[1] 32\n\n\n\n3.17.3 Number of Elements\n\nCodelength(nfl_players$display_name)\n\n[1] 20751\n\n\n\n3.17.4 Number of Missing Elements\n\nCodelength(nfl_players$college_name[which(is.na(nfl_players$college_name))])\n\n[1] 12126\n\n\n\n3.17.5 Number of Non-Missing Elements\n\nCodelength(nfl_players$college_name[which(!is.na(nfl_players$college_name))])\n\n[1] 8625\n\nCodelength(na.omit(nfl_players$college_name))\n\n[1] 8625",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createNewVars",
    "href": "getting-started.html#sec-createNewVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.18 Create New Variables",
    "text": "3.18 Create New Variables\nTo create a new variable, use the following syntax:\n\nCodeplayers$newVar &lt;- NA\n\n\nHere is an example of creating a new variable:\n\nCodeplayers$newVar &lt;- 1:nrow(players)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-recodeVars",
    "href": "getting-started.html#sec-recodeVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.19 Recode Variables",
    "text": "3.19 Recode Variables\nHere is an example of recoding a variable:\n\nCodeplayers$oldVar1 &lt;- NA\nplayers$oldVar1[which(players$position == \"QB\")] &lt;- \"quarterback\"\nplayers$oldVar1[which(players$position == \"RB\")] &lt;- \"running back\"\nplayers$oldVar1[which(players$position == \"WR\")] &lt;- \"wide receiver\"\nplayers$oldVar1[which(players$position == \"TE\")] &lt;- \"tight end\"\n\nplayers$oldVar2 &lt;- NA\nplayers$oldVar2[which(players$age &lt; 30)] &lt;- \"young\"\nplayers$oldVar2[which(players$age &gt;= 30)] &lt;- \"old\"\n\n\nRecode multiple variables:\n\nCodeplayers %&gt;%\n  mutate(across(c(\n    oldVar1:oldVar2),\n    ~ case_match(\n      .,\n      c(\"quarterback\",\"old\",\"running back\") ~ 0,\n      c(\"wide receiver\",\"tight end\",\"young\") ~ 1)))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#renameVars",
    "href": "getting-started.html#renameVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.20 Rename Variables",
    "text": "3.20 Rename Variables\n\nCodeplayers &lt;- players %&gt;% \n  rename(\n    newVar1 = oldVar1,\n    newVar2 = oldVar2)\n\n\nUsing a vector of variable names:\n\nCodevarNamesFrom &lt;- c(\"oldVar1\",\"oldVar2\")\nvarNamesTo &lt;- c(\"newVar1\",\"newVar2\")\n\nplayers &lt;- players %&gt;% \n  rename_with(~ varNamesTo, all_of(varNamesFrom))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#convertVarTypes",
    "href": "getting-started.html#convertVarTypes",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.21 Convert the Types of Variables",
    "text": "3.21 Convert the Types of Variables\nOne variable:\n\nCodeplayers$factorVar &lt;- factor(players$ID)\nplayers$numericVar &lt;- as.numeric(players$age)\nplayers$integerVar &lt;- as.integer(players$newVar1)\nplayers$characterVar &lt;- as.character(players$newVar2)\n\n\nMultiple variables:\n\nCodeplayers %&gt;%\n  mutate(across(c(\n    ID,\n    age),\n    as.numeric))\n\n\n  \n\n\nCodeplayers %&gt;%\n  mutate(across(\n    age:newVar1,\n    as.character))\n\n\n  \n\n\nCodeplayers %&gt;%\n  mutate(across(where(is.factor), as.character))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-merging",
    "href": "getting-started.html#sec-merging",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.22 Merging/Joins",
    "text": "3.22 Merging/Joins\n\n3.22.1 Overview\nMerging (also called joining) merges two data objects using a shared set of variables called “keys.” The keys are the variable(s) that are used to align the rows from the two objects. The data for the given key(s) in the first object get paired with (i.e., get placed in the same row as) the data for that same key in the second object. In general, each row should have a value on each of the keys; there should be no missingness in the keys. To merge two objects, the key(s) that will be used to match the records must be present in both objects. The keys are used to merge the variables in object 1 (x) with the variables in object 2 (y). Different merge types select different rows to merge.\nFor some data objects, you might want to combine information for the same player from multiple data objects. If each data object is in player form (i.e., player_id uniquely identifies each row), you might merge by the player’s identification number (e.g., player_id). In this case, the key uniquely identifies each row.\nHowever, some data objects have multiple keys. For instance, in long form data objects, each player may have multiple rows corresponding to multiple seasons. In this case, the keys may be player_id and season—that is, the data are in player-season form. If object 1 and object 2 are both in player-season form, we would use player_id and season as the keys to merge the two objects. In this case, the keys uniquely identify each row; that is, they account for the levels of nesting.\nHowever, if the data objects are of different form, we would select the keys as the variable(s) that represent the lowest common denominator of variables used to join the data objects that are present in both objects. For instance, assume that object 1 is in player-season form. For object 2, each player has multiple rows corresponding to seasons and games/weeks—in this case, object 2 is in player-season-week form. Object 1 does not have the week variable, so it cannot be used to join the objects. Thus, we would use player_id and season as the keys to merge the two objects, because both variables are present in both objects.\nIt is important not to have rows with duplicate values on the keys. For instance, if there is more than one row with the same player_id in each object (or multiple rows in object 2 with the same combination of player_id, season, and week), then each row with that player_id in object 1 gets paired with each row with that player_id in object 2. The many possible combinations can lead to the resulting object greatly expanding in terms of the number of rows. Thus, you want the keys to uniquely identify each row. In the example below, player is present in each object, so we can merge by player; however, each object has multiple rows with the same player. For example, mergeExample1A has three rows for player A; mergeExample1B has two rows for player A. Thus, when we merge them, the resulting object has many more rows than each respective object (even though neither object has players that the other object does not).\n\nCodemergeExample1A &lt;- data.frame(\n  player = c(\"A\",\"A\",\"A\",\"B\",\"B\"),\n  age = c(20,22,24,26,28)\n)\n\nmergeExample1B &lt;- data.frame(\n  player = c(\"A\",\"A\",\"B\",\"B\"),\n  points = c(10,15,20,25)\n)\n\nmergeExample1 &lt;- full_join(\n  mergeExample1A,\n  mergeExample1B,\n  by = \"player\")\n\nmergeExample1\n\n\n  \n\n\nCodedim(mergeExample1)\n\n[1] 10  3\n\n\nNote: if the two objects include variables with the same name (apart from the keys), R will not know how you want each to appear in the merged object. So, it will add a suffix (e.g., .x, .y) to each common variable to indicate which object (i.e., object x or object y) the variable came from, where object x is the first object—i.e., the object to which object y (the second object) is merged. In general, apart from the keys, you should not include variables with the same name in two objects to be merged. To prevent this, either remove or rename the shared variable in one of the objects, or include the shared variable as a key. However, as described above, you should include it as a key only if you want to use its values to align the rows from each object. Below is an example of merging two objects with the same variable name (i.e., points) that is not used as a key.\n\nCodemergeExample2A &lt;- data.frame(\n  player = c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  points = c(20,22,24,26,28)\n)\n\nmergeExample2B &lt;- data.frame(\n  player = c(\"A\",\"B\",\"C\",\"F\"),\n  points = c(10,15,20,25)\n)\n\nmergeExample2 &lt;- full_join(\n  mergeExample2A,\n  mergeExample2B,\n  by = \"player\")\n\nmergeExample2\n\n\n  \n\n\n\nWhen two objects are merged that have different formats, the resulting data object inherits the format of the data object that has more levels of nesting. For instance, consider that you want to merge two objects, object A and object B. Object A is in player form and object B is in player-season-week form. When you merge them, the resulting data object will be in player-season-week form.\n\nCodemergeExample3A &lt;- data.frame(\n  player = c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  weight = c(225,250,275,300,325)\n)\n\nmergeExample3B &lt;- data.frame(\n  player = c(\"A\",\"A\",\"A\",\"A\",\"B\",\"B\"),\n  season = c(2023,2023,2024,2024,2024,2024),\n  week = c(1,2,1,2,3,4),\n  points = c(10,15,20,25,30,35)\n)\n\nmergeExample3 &lt;- full_join(\n  mergeExample3A,\n  mergeExample3B,\n  by = \"player\")\n\nmergeExample3\n\n\n  \n\n\n\n\n3.22.2 Data Before Merging\nHere are the data in the players object:\n\nCodeplayers\n\n\n  \n\n\nCodedim(players)\n\n[1] 12 10\n\n\nThe data are structured in ID form. That is, every row in the dataset is uniquely identified by the variable, ID.\nHere are the data in the fantasyPoints object:\n\nCodefantasyPoints\n\n\n  \n\n\nCodedim(fantasyPoints)\n\n[1] 4 2\n\n\n\n3.22.3 Types of Joins\n\n3.22.3.1 Visual Overview of Join Types\nBelow is a visual that depicts various types of merges/joins. Object x is the circle labeled as x. Object y is the circle labeled as y. The area of overlap in the Venn diagram indicates the rows on the keys that are shared between the two objects (e.g., the same player_id, season, and week). The non-overlapping area indicates the rows on the keys that are unique to each object. The shaded blue area indicates which rows (on the keys) are kept in the merged object from each of the two objects, when using each of the merge types. For instance, a left outer join keeps the shared rows and the rows that are unique to object x, but it drops the rows that are unique to object y.\n\n\n3.22.3.2 Full Outer Join\nA full outer join includes all rows in x or y. It returns columns from x and y. Here is how to merge two data frames using a full outer join (i.e., “full join”):\n\nCodefullJoinData &lt;- full_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nfullJoinData\n\n\n  \n\n\nCodedim(fullJoinData)\n\n[1] 14 11\n\n\n\n3.22.3.3 Left Outer Join\nA left outer join includes all rows in x. It returns columns from x and y. Here is how to merge two data frames using a left outer join (“left join”):\n\nCodeleftJoinData &lt;- left_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nleftJoinData\n\n\n  \n\n\nCodedim(leftJoinData)\n\n[1] 12 11\n\n\n\n3.22.3.4 Right Outer Join\nA right outer join includes all rows in y. It returns columns from x and y. Here is how to merge two data frames using a right outer join (“right join”):\n\nCoderightJoinData &lt;- right_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nrightJoinData\n\n\n  \n\n\nCodedim(rightJoinData)\n\n[1]  4 11\n\n\n\n3.22.3.5 Inner Join\nAn inner join includes all rows that are in both x and y. An inner join will return one row of x for each matching row of y, and can duplicate values of records on either side (left or right) if x and y have more than one matching record. It returns columns from x and y. Here is how to merge two data frames using an inner join:\n\nCodeinnerJoinData &lt;- inner_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\ninnerJoinData\n\n\n  \n\n\nCodedim(innerJoinData)\n\n[1]  2 11\n\n\n\n3.22.3.6 Semi Join\nA semi join is a filter. A left semi join returns all rows from x with a match in y. That is, it filters out records from x that are not in y. Unlike an inner join, a left semi join will never duplicate rows of x, and it includes columns from only x (not from y). Here is how to merge two data frames using a left semi join:\n\nCodesemiJoinData &lt;- semi_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nsemiJoinData\n\n\n  \n\n\nCodedim(semiJoinData)\n\n[1]  2 10\n\n\n\n3.22.3.7 Anti Join\nAn anti join is a filter. A left anti join returns all rows from x without a match in y. That is, it filters out records from x that are in y. It returns columns from only x (not from y). Here is how to merge two data frames using a left anti join:\n\nCodeantiJoinData &lt;- anti_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nantiJoinData\n\n\n  \n\n\nCodedim(antiJoinData)\n\n[1] 10 10\n\n\n\n3.22.3.8 Cross Join\nA cross join combines each row in x with each row in y.\n\nCodecrossJoinData &lt;- cross_join(\n  players,\n  fantasyPoints)\n\ncrossJoinData\n\n\n  \n\n\nCodedim(crossJoinData)\n\n[1] 48 12",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-longToWide",
    "href": "getting-started.html#sec-longToWide",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.23 Transform Data from Long to Wide",
    "text": "3.23 Transform Data from Long to Wide\nDepending on the analysis, it may be important to restructure the data to be in long or wide form. When the data are in wide form, each player has only one row. When the data are in long form, each player has multiple rows—e.g., a row for each game. The data structure is called wide or long form because a dataset in wide form has more columns and fewer rows (i.e., it appears wider and shorter), whereas a dataset in long form has more rows and fewer columns (i.e., it appears narrower and taller).\nHere are the original data in long form. The data are structured in “player-season-week form”. That is, every row in the dataset is uniquely identified by the combination of variables, ID, season, and week—these are the keys. This is an example of long form, because each player has multiple rows.\n\nCodedataLong &lt;- full_join(\n  players %&gt;% select(-age),\n  fantasyPoints_weekly,\n  by = c(\"ID\")\n)\n\ndataLong\n\n\n  \n\n\nCodedim(dataLong)\n\n[1] 408  12\n\nCodenames(dataLong)\n\n [1] \"ID\"            \"name\"          \"position\"      \"newVar1\"      \n [5] \"newVar2\"       \"factorVar\"     \"numericVar\"    \"integerVar\"   \n [9] \"characterVar\"  \"season\"        \"week\"          \"fantasyPoints\"\n\n\nBelow, we widen the data by two variables (season and week), using tidyverse, so that the data are now in “player form” (where each row is uniquely identified by the ID variable):\n\nCodedataWide &lt;- dataLong %&gt;% \n  pivot_wider(\n    names_from = c(season, week),\n    names_glue = \"{.value}_{season}_week{week}\",\n    values_from = fantasyPoints)\n\ndataWide\n\n\n  \n\n\nCodedim(dataWide)\n\n[1] 12 43\n\nCodenames(dataWide)\n\n [1] \"ID\"                        \"name\"                     \n [3] \"position\"                  \"newVar1\"                  \n [5] \"newVar2\"                   \"factorVar\"                \n [7] \"numericVar\"                \"integerVar\"               \n [9] \"characterVar\"              \"fantasyPoints_2022_week1\" \n[11] \"fantasyPoints_2023_week1\"  \"fantasyPoints_2022_week2\" \n[13] \"fantasyPoints_2023_week2\"  \"fantasyPoints_2022_week3\" \n[15] \"fantasyPoints_2023_week3\"  \"fantasyPoints_2022_week4\" \n[17] \"fantasyPoints_2023_week4\"  \"fantasyPoints_2022_week5\" \n[19] \"fantasyPoints_2023_week5\"  \"fantasyPoints_2022_week6\" \n[21] \"fantasyPoints_2023_week6\"  \"fantasyPoints_2022_week7\" \n[23] \"fantasyPoints_2023_week7\"  \"fantasyPoints_2022_week8\" \n[25] \"fantasyPoints_2023_week8\"  \"fantasyPoints_2022_week9\" \n[27] \"fantasyPoints_2023_week9\"  \"fantasyPoints_2022_week10\"\n[29] \"fantasyPoints_2023_week10\" \"fantasyPoints_2022_week11\"\n[31] \"fantasyPoints_2023_week11\" \"fantasyPoints_2022_week12\"\n[33] \"fantasyPoints_2023_week12\" \"fantasyPoints_2022_week13\"\n[35] \"fantasyPoints_2023_week13\" \"fantasyPoints_2022_week14\"\n[37] \"fantasyPoints_2023_week14\" \"fantasyPoints_2022_week15\"\n[39] \"fantasyPoints_2023_week15\" \"fantasyPoints_2022_week16\"\n[41] \"fantasyPoints_2023_week16\" \"fantasyPoints_2022_week17\"\n[43] \"fantasyPoints_2023_week17\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-wideToLong",
    "href": "getting-started.html#sec-wideToLong",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.24 Transform Data from Wide to Long",
    "text": "3.24 Transform Data from Wide to Long\nConversely, we can also restructure data from wide to long. Here are the data in long form, after they have been transformed from wide form using tidyverse:\n\nCodedataLong &lt;- dataWide %&gt;% \n  pivot_longer(\n    cols = fantasyPoints_2022_week1:fantasyPoints_2023_week17,\n    names_to = c(\"season\", \"week\"),\n    names_pattern = \"fantasyPoints_(.*)_week(.*)\",\n    values_to = \"fantasyPoints\")\n\ndataLong\n\n\n  \n\n\nCodedim(dataLong)\n\n[1] 408  12\n\nCodenames(dataLong)\n\n [1] \"ID\"            \"name\"          \"position\"      \"newVar1\"      \n [5] \"newVar2\"       \"factorVar\"     \"numericVar\"    \"integerVar\"   \n [9] \"characterVar\"  \"season\"        \"week\"          \"fantasyPoints\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-loops",
    "href": "getting-started.html#sec-loops",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.25 Loops",
    "text": "3.25 Loops\nIf you want to perform the same computation multiple times, it can be faster to do it in a loop compared to writing out the same computation many times. For instance, here is a loop that runs from 1 to 12 (the number of players in the players object), incrementing by 1 after each iteration. The loop prints each element of a vector (i.e., the player’s name) and the loop index (i) that indicates where the loop is in terms of its iterations:\n\nCodefor(i in 1:length(players$ID)){\n  print(paste(\"The loop is at index:\", i, sep = \" \"))\n  print(paste(\"My favorite player is:\", players$name[i], sep = \" \"))\n}\n\n[1] \"The loop is at index: 1\"\n[1] \"My favorite player is: Ken Cussion\"\n[1] \"The loop is at index: 2\"\n[1] \"My favorite player is: Ben Sacked\"\n[1] \"The loop is at index: 3\"\n[1] \"My favorite player is: Chuck Downfield\"\n[1] \"The loop is at index: 4\"\n[1] \"My favorite player is: Ron Ingback\"\n[1] \"The loop is at index: 5\"\n[1] \"My favorite player is: Rhonda Ball\"\n[1] \"The loop is at index: 6\"\n[1] \"My favorite player is: Hugo Long\"\n[1] \"The loop is at index: 7\"\n[1] \"My favorite player is: Lionel Scrimmage\"\n[1] \"The loop is at index: 8\"\n[1] \"My favorite player is: Drew Blood\"\n[1] \"The loop is at index: 9\"\n[1] \"My favorite player is: Chase Emdown\"\n[1] \"The loop is at index: 10\"\n[1] \"My favorite player is: Justin Time\"\n[1] \"The loop is at index: 11\"\n[1] \"My favorite player is: Spike D'Ball\"\n[1] \"The loop is at index: 12\"\n[1] \"My favorite player is: Isac Ulooz\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingStartedSessionInfo",
    "href": "getting-started.html#sec-gettingStartedSessionInfo",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.26 Session Info",
    "text": "3.26 Session Info\nAt the end of each chapter in which R code is used, I provide the session information, which describes the system and operating system the code was run on and the versions of each package. That way, if you get different results from me, you can see which differ, to help with reproducibility. If you run the (all of) the exact same code as is provided in the text, in the exact same order, with the exact same setup (platform, operating system, package versions, etc.), you should get the exact same answer as is in the text. That is the idea of reproducibility—getting the exact same result with the exact same inputs. Reproducibility is crucial for studies to achieve greater confidence in their findings and to ensure better replicability of findings across studies.\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_1.9.1    compiler_4.4.3    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.6.1         \n [9] generics_0.1.3    knitr_1.50        htmlwidgets_1.6.4 munsell_0.5.1    \n[13] pillar_1.10.1     tzdb_0.5.0        rlang_1.1.5       stringi_1.8.4    \n[17] xfun_0.51         timechange_0.3.0  cli_3.6.4         withr_3.0.2      \n[21] magrittr_2.0.3    digest_0.6.37     grid_4.4.3        hms_1.1.3        \n[25] lifecycle_1.0.4   vctrs_0.6.5       evaluate_1.0.3    glue_1.8.0       \n[29] colorspace_2.1-1  rmarkdown_2.29    tools_4.4.3       pkgconfig_2.0.3  \n[33] htmltools_0.5.8.1\n\n\n\n\n\n\nAden-Buie, G., Schloerke, B., Allaire, J., & Rossell Hayes, A. (2023). learnr: Interactive tutorials for R. https://rstudio.github.io/learnr/\n\n\nAndersen, D., Petersen, I. T., & Tungate, A. (2025). ffanalytics: Scrape data for fantasy football. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nBryan, J., Hester, J., Robinson, D., Wickham, H., Dervieux, C., & Posit. (2025). Reprex do’s and don’ts. https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nCsárdi, G., Hester, J., Wickham, H., Chang, W., Morgan, M., & Tenenbaum, D. (2024). remotes: R package installation from remote repositories, including GitHub. https://remotes.r-lib.org\n\n\nR Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nSalmon, M. (2018). Where to get help with your R question? https://masalmon.eu/2018/07/22/wheretogethelp/\n\n\nStack Overflow. (2018). How to make a great R reproducible example. https://stackoverflow.com/a/5963610\n\n\nStack Overflow. (2025). How to create a minimal, reproducible example. https://stackoverflow.com/help/minimal-reproducible-example\n\n\nWickham, H. (2023). tidyverse: Easily install and load the Tidyverse. https://tidyverse.tidyverse.org\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "download-football-data.html",
    "href": "download-football-data.html",
    "title": "4  Download and Process NFL Football Data",
    "section": "",
    "text": "4.1 Load Packages\nCodelibrary(\"ffanalytics\") # to install: install.packages(\"remotes\"); remotes::install_github(\"FantasyFootballAnalytics/ffanalytics\")\nlibrary(\"petersenlab\") # to install: install.packages(\"remotes\"); remotes::install_github(\"DevPsyLab/petersenlab\")\nlibrary(\"nflreadr\")\nlibrary(\"nflfastR\")\nlibrary(\"nfl4th\")\nlibrary(\"nflplotR\")\nlibrary(\"progressr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-dataDictionary",
    "href": "download-football-data.html#sec-dataDictionary",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n4.2 Data Dictionaries of NFL Data",
    "text": "4.2 Data Dictionaries of NFL Data\nData Dictionaries are metadata that describe the meaning of the variables in a dataset. Ho & Carl (2025a) provide Data Dictionaries for the various National Football League (NFL) datasets at the following link: https://nflreadr.nflverse.com/articles/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-downloadFootballDataOverview",
    "href": "download-football-data.html#sec-downloadFootballDataOverview",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n4.3 Types of NFL Data",
    "text": "4.3 Types of NFL Data\nBelow, we provide examples for how to download various types of NFL data using the nflreadr package (Ho & Carl, 2024). For additional resources, Congelio (2023) provides a helpful introductory text for working with NFL data in R. We save each data file after downloading it, so we can use the data in subsequent chapters. If you have difficulty downloading the data files using the nflreadr package (Ho & Carl, 2024), we also saved the data files so they are publicly available on the Open Science Framework: https://osf.io/z6pg4.\nThis chapter extensively uses merging to process the data for later use. See Section 3.22 for a reminder of how to perform merging, the types of merges, and what you can expect when you merge data objects with different formats. Guidance for how to merge the various NFL-related data files is provided by Sharpe (2020a): https://github.com/nflverse/nfldata/blob/master/DATASETS.md.\n\n4.3.1 Players\n\nCodenfl_players_raw &lt;- progressr::with_progress(\n  nflreadr::load_players())\n\n\n\nCodesave(\n  nfl_players_raw,\n  file = \"./data/nfl_players_raw.RData\"\n)\n\n\nThe nfl_players object is in player form. That is, each row should be uniquely identified by gsis_id. Let’s rearrange the data accordingly:\n\nCodenfl_players &lt;- nfl_players_raw %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  arrange(display_name)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_players %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.1.1 Processing\nLet’s do some data cleanup:\n\nCode# Convert missing values to NA\nnfl_players[nfl_players == \"\"] &lt;- NA\n\n# Drop players with missing values for gsis_id\nnfl_players &lt;- nfl_players %&gt;% \n  filter(!is.na(gsis_id))\n\n\n\nCodesave(\n  nfl_players,\n  file = \"./data/nfl_players.RData\"\n)\n\n\n\n4.3.2 Teams\n\nCodenfl_teams_raw &lt;- progressr::with_progress(\n  nflreadr::load_teams(current = TRUE))\n\n\n\nCodesave(\n  nfl_teams_raw,\n  file = \"./data/nfl_teams_raw.RData\"\n)\n\n\nThe nfl_teams object is in team form. That is, each row should be uniquely identified by team_id. Let’s rearrange the data accordingly:\n\nCodenfl_teams &lt;- nfl_teams_raw %&gt;% \n  select(team_id, everything())\n\n\nLet’s check for duplicate team instances:\n\nCodenfl_teams %&gt;% \n  group_by(team_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_teams,\n  file = \"./data/nfl_teams.RData\"\n)\n\n\n\n4.3.3 Fantasy Player IDs\nHo & Carl (2025h) provide a Data Dictionary for fantasy player ID data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_playerids.html\n\nCodenfl_playerIDs_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_playerids())\n\n\n\nCodesave(\n  nfl_playerIDs_raw,\n  file = \"./data/nfl_playerIDs_raw.RData\"\n)\n\n\nThe nfl_playerIDs object is in player form. That is, each row should be uniquely identified by mfl_id.\n\nCodenfl_playerIDs &lt;- nfl_playerIDs_raw %&gt;% \n  arrange(name, mfl_id)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_playerIDs %&gt;% \n  group_by(mfl_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_playerIDs %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s do some data processing to help with merging the dataset with other datasets:\n\nCodenfl_playerIDs$name[which(nfl_playerIDs$name == \"Bennett,Michael\")] &lt;- \"Michael Bennett\"\n\nnfl_playerIDs &lt;- nfl_playerIDs %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(name, lowercase = TRUE)\n  )\n\n\n\nCodesave(\n  nfl_playerIDs,\n  file = \"./data/nfl_playerIDs.RData\"\n)\n\n\n\n4.3.4 Player Info\n\n4.3.5 Rosters\nA Data Dictionary for rosters is located at the following links:\n\n\nhttps://nflreadr.nflverse.com/articles/dictionary_rosters.html (Ho & Carl, 2025r)\n\n\nhttps://github.com/nflverse/nfldata/blob/master/DATASETS.md#rosters (Sharpe, 2020c)\n\n\n\nCodenfl_rosters_raw &lt;- progressr::with_progress(\n  nflreadr::load_rosters(seasons = TRUE))\n\nnfl_rosters_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_rosters_weekly(seasons = TRUE))\n\nrosters &lt;- read_csv(\"https://raw.githubusercontent.com/leesharpe/nfldata/master/data/rosters.csv\")\n\n\n\nCodesave(\n  nfl_rosters_raw,\n  rosters,\n  file = \"./data/nfl_rosters_raw.RData\"\n)\n\nsave(\n  nfl_rosters_weekly_raw,\n  file = \"./data/nfl_rosters_weekly_raw.RData\"\n)\n\n\nThe nfl_rosters object is in player-season-team form. That is, each row should be uniquely identified by the combination of gsis_id, season, and team. Let’s rearrange the data accordingly:\n\nCodenfl_rosters &lt;- nfl_rosters_raw %&gt;% \n  left_join(\n    rosters %&gt;% select(playerid, season, team, side, category, games, starts, years, av),\n    by = c(\"season\",\"team\",\"pfr_id\" = \"playerid\"),\n    na_matches = \"never\"\n  ) %&gt;% \n  select(gsis_id, season, team, week, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, team, week)\n\n\nLet’s check for duplicate player-season-team instances:\n\nCodenfl_rosters %&gt;% \n  group_by(gsis_id, season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.5.1 Processing\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for gsis_id\nnfl_rosters &lt;- nfl_rosters %&gt;% \n  filter(!is.na(gsis_id))\n\n# Fill in missing values for a player in their duplicate instances, and then keep only the first of the duplicate instances\nnfl_rosters &lt;- nfl_rosters %&gt;% \n  group_by(gsis_id, season, team) %&gt;% \n  fill(names(.), .direction = \"downup\") %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\n\nLet’s check again for duplicate player-season-team instances:\n\nCodenfl_rosters %&gt;% \n  group_by(gsis_id, season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_rosters_weekly object is in player-season-week form. That is, each row should be uniquely identified by the combination of gsis_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_rosters_weekly &lt;- nfl_rosters_weekly_raw %&gt;% \n  select(gsis_id, season, week, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_rosters_weekly %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for gsis_id\nnfl_rosters_weekly &lt;- nfl_rosters_weekly %&gt;% \n  filter(!is.na(gsis_id))\n\n# Fill in missing values for a player in their duplicate instances, and then keep only the first of the duplicate instances\nnfl_rosters_weekly &lt;- nfl_rosters_weekly %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  fill(names(.), .direction = \"downup\") %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\n\nLet’s check again for duplicate player-season-week instances:\n\nCodenfl_rosters_weekly %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_rosters,\n  file = \"./data/nfl_rosters.RData\"\n)\n\nsave(\n  nfl_rosters_weekly,\n  file = \"./data/nfl_rosters_weekly.RData\"\n)\n\n\n\n4.3.6 Game Schedules\nHo & Carl (2025s) provide a Data Dictionary for game schedules data at the following link: https://nflreadr.nflverse.com/articles/dictionary_schedules.html\n\nCodenfl_schedules_raw &lt;- progressr::with_progress(\n  nflreadr::load_schedules(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_schedules_raw,\n  file = \"./data/nfl_schedules_raw.RData\"\n)\n\n\nThe nfl_schedules object is in game form and in season-week (and -game type) form. That is, each row should be uniquely identified by game_id. Each row should also be uniquely identified by the combination of season and week (and game type).\n\nCodenfl_schedules &lt;- nfl_schedules_raw\n\n\nLet’s check for duplicate game instances:\n\nCodenfl_schedules %&gt;% \n  group_by(game_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_schedules,\n  file = \"./data/nfl_schedules.RData\"\n)\n\n\n\n4.3.7 Standings\nSharpe (2020d) provides a Data Dictionary for standings data at the following link: https://github.com/nflverse/nfldata/blob/master/DATASETS.md#standings\n\nCodenfl_standings_raw &lt;- read_csv(\"http://www.habitatring.com/standings.csv\")\n\n\n\nCodesave(\n  nfl_standings_raw,\n  file = \"./data/nfl_standings_raw.RData\"\n)\n\n\nThe nfl_standings object is in season-team form. That is, each row should be uniquely identified by the combination of season and team.\n\nCodenfl_standings &lt;- nfl_standings_raw\n\n\nLet’s check for duplicate season-team instances:\n\nCodenfl_standings %&gt;% \n  group_by(season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_standings,\n  file = \"./data/nfl_standings.RData\"\n)\n\n\n\n4.3.8 The Combine\nHo & Carl (2025b) provide a Data Dictionary for data from the combine at the following link: https://nflreadr.nflverse.com/articles/dictionary_combine.html\n\nCodenfl_combine_raw &lt;- progressr::with_progress(\n  nflreadr::load_combine(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_combine_raw,\n  file = \"./data/nfl_combine_raw.RData\"\n)\n\n\nThe nfl_combine object is in player form. That is, each row should be uniquely identified by the player’s id. However, there is no gsis_id variable to merge it easily with other datasets. Some of the players have other id variables, including pfr_id and cfb_id. Let’s rearrange the data accordingly:\n\nCodenfl_combine &lt;- nfl_combine_raw %&gt;% \n  select(pfr_id, cfb_id, everything()) %&gt;% \n  arrange(season, player_name)\n\n\nLet’s do some data processing to help with merging the dataset with other datasets:\n\nCodenfl_combine &lt;- nfl_combine %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player_name, lowercase = TRUE)\n  )\n\n# First, merge on both pfr_id and cfb_id\nmerged_data1 &lt;- left_join(\n  nfl_combine,\n  nfl_playerIDs %&gt;% select(pfr_id, cfbref_id, gsis_id),\n  by = c(\"pfr_id\", \"cfb_id\" = \"cfbref_id\"),\n  na_matches = \"never\"\n)\n\n# Second, merge on pfr_id\nmerged_data2 &lt;- left_join(\n  nfl_combine,\n  nfl_playerIDs %&gt;% select(pfr_id, merge_name, position, gsis_id),\n  by = c(\"pfr_id\",\"merge_name\",\"pos\" = \"position\"),\n  na_matches = \"never\"\n)\n\n# Third, merge on cfb_id\nmerged_data3 &lt;- left_join(\n  nfl_combine,\n  nfl_playerIDs %&gt;% select(cfbref_id, merge_name, position, gsis_id),\n  by = c(\"cfb_id\" = \"cfbref_id\",\"merge_name\",\"pos\" = \"position\"),\n  na_matches = \"never\"\n)\n\n# Combine gsis_id across merges\nnfl_combine$gsis_id &lt;- coalesce(\n  merged_data1$gsis_id,\n  merged_data2$gsis_id,\n  merged_data3$gsis_id\n  )\n\n# Rearrange the data\nnfl_combine &lt;- nfl_combine %&gt;% \n  select(gsis_id, pfr_id, cfb_id, player_name, everything()) %&gt;% \n  arrange(season, player_name)\n\n\nLet’s check for duplicate gsis_id, pfr_id, and cfb_id instances:\n\nCodenfl_combine %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1, !is.na(gsis_id)) %&gt;% \n  arrange(gsis_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(pfr_id) %&gt;% \n  filter(n() &gt; 1, !is.na(pfr_id)) %&gt;% \n  arrange(pfr_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(cfb_id) %&gt;% \n  filter(n() &gt; 1, !is.na(cfb_id)) %&gt;% \n  arrange(cfb_id) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s do some additional data processing:\n\nCode# Drop Stanford Samuels Jr.\nnfl_combine$gsis_id[which(nfl_combine$cfb_id == \"stanford-samuels-1\")] &lt;- NA\n\nnfl_combine &lt;- nfl_combine %&gt;% \n  separate_wider_delim(\n    ht,\n    names = c(\"feet\", \"inches\"),\n    delim = \"-\") %&gt;%\n  mutate(\n    feet = as.numeric(feet),\n    inches = as.numeric(inches),\n    ht = feet * 12 + inches\n  ) %&gt;%\n  select(-feet, -inches)\n\n\nHowever, these apparent duplicates appear to be different players at different positions:\n\nCodenfl_combine %&gt;% \n  group_by(season, gsis_id, pos) %&gt;% \n  filter(n() &gt; 1, !is.na(gsis_id)) %&gt;% \n  arrange(gsis_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(season, pfr_id, pos) %&gt;% \n  filter(n() &gt; 1, !is.na(pfr_id)) %&gt;% \n  arrange(pfr_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(season, cfb_id, pos) %&gt;% \n  filter(n() &gt; 1, !is.na(cfb_id)) %&gt;% \n  arrange(cfb_id) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_combine,\n  file = \"./data/nfl_combine.RData\"\n)\n\n\n\n4.3.9 Draft Picks\nHo & Carl (2025e) provide a Data Dictionary for draft picks data at the following link: https://nflreadr.nflverse.com/articles/dictionary_draft_picks.html\n\nCodenfl_draftPicks_raw &lt;- progressr::with_progress(\n  nflreadr::load_draft_picks(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_draftPicks_raw,\n  file = \"./data/nfl_draftPicks_raw.RData\"\n)\n\n\nThe nfl_draftPicks object is in player form. That is, each row should be uniquely identified by gsis_id. Let’s rearrange the data accordingly:\n\nCodenfl_draftPicks &lt;- nfl_draftPicks_raw %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  arrange(pfr_player_name)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_draftPicks %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.9.1 Processing\nLet’s do some data cleanup:\n\nCode# Convert missing values to NA\nnfl_draftPicks[nfl_draftPicks == \"\"] &lt;- NA\n\n# Drop players with missing values for gsis_id\nnfl_draftPicks &lt;- nfl_draftPicks %&gt;% \n  filter(!is.na(gsis_id))\n\n\nLet’s check again for duplicate player instances:\n\nCodenfl_draftPicks %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_draftPicks,\n  file = \"./data/nfl_draftPicks.RData\"\n)\n\n\n\n4.3.10 Draft Values\nSharpe (2020b) provides a Data Dictionary for draft values data at the following link: https://github.com/nflverse/nfldata/blob/master/DATASETS.md#draft_values\n\nCodenfl_draftValues_raw &lt;- read_csv(\"https://raw.githubusercontent.com/leesharpe/nfldata/master/data/draft_values.csv\")\n\n\n\nCodesave(\n  nfl_draftValues_raw,\n  file = \"./data/nfl_draftValues_raw.RData\"\n)\n\n\nThe nfl_draftValues object is in pick form. That is, each row should be uniquely identified by pick. Let’s rearrange the data accordingly:\n\nCodenfl_draftValues &lt;- nfl_draftValues_raw %&gt;% \n  select(pick, everything()) %&gt;% \n  arrange(pick)\n\n\nLet’s check for duplicate pick instances:\n\nCodenfl_draftValues %&gt;% \n  group_by(pick) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_draftValues,\n  file = \"./data/nfl_draftValues.RData\"\n)\n\n\n\n4.3.11 Depth Charts\nHo & Carl (2025d) provide a Data Dictionary for data from weekly depth charts at the following link: https://nflreadr.nflverse.com/articles/dictionary_depth_charts.html\n\nCodenfl_depthCharts_raw &lt;- progressr::with_progress(\n  nflreadr::load_depth_charts(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_depthCharts_raw,\n  file = \"./data/nfl_depthCharts_raw.RData\"\n)\n\n\nThe nfl_depthCharts object is in player-season-week-position form. That is, each row should be uniquely identified by the combination of gsis_id, season, week, and depth_position. Let’s rearrange the data accordingly:\n\nCodenfl_depthCharts &lt;- nfl_depthCharts_raw %&gt;% \n  select(gsis_id, season, week, depth_position, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, week, depth_position)\n\n\nLet’s check for duplicate player-season-week-position instances:\n\nCodenfl_depthCharts %&gt;% \n  group_by(gsis_id, season, week, depth_position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.11.1 Processing\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for gsis_id\nnfl_depthCharts &lt;- nfl_depthCharts %&gt;% \n  filter(!is.na(gsis_id))\n\n# Fill in missing values for a player in their duplicate instances, and then keep only the first of the duplicate instances\nnfl_depthCharts &lt;- nfl_depthCharts %&gt;% \n  group_by(gsis_id, season, week, depth_position) %&gt;% \n  fill(names(.), .direction = \"downup\") %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\n\nLet’s check again for duplicate player-season-week-position instances:\n\nCodenfl_depthCharts %&gt;% \n  group_by(gsis_id, season, week, depth_position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_depthCharts,\n  file = \"./data/nfl_depthCharts.RData\"\n)\n\n\n\n4.3.12 Trades\nSharpe (2020e) provides a Data Dictionary for NFL trade data at the following link: https://github.com/nflverse/nfldata/blob/master/DATASETS.md#trades\n\nCodenfl_trades_raw &lt;- read_csv(\"https://raw.githubusercontent.com/leesharpe/nfldata/master/data/trades.csv\")\n\n\n\nCodesave(\n  nfl_trades_raw,\n  file = \"./data/nfl_trades_raw.RData\"\n)\n\n\nThe nfl_trades object is in trade-player form. That is, each row should be uniquely identified by the combination of trade_id and pfr_id.\n\nCodenfl_trades &lt;- nfl_trades_raw\n\n\nLet’s check for duplicate trade-player instances:\n\nCodenfl_trades %&gt;%\n  filter(!is.na(pfr_id)) %&gt;% \n  group_by(trade_id, pfr_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_trades,\n  file = \"./data/nfl_trades.RData\"\n)\n\n\n\n4.3.13 Play-By-Play Data\nTo download play-by-play data from prior weeks and seasons, we can use the load_pbp() function of the nflreadr package (Ho & Carl, 2024). We add a progress bar using the with_progress() function from the progressr package (Bengtsson, 2024) because it takes a while to run. Ho & Carl (2025n) provide a Data Dictionary for the play-by-play data at the following link: https://nflreadr.nflverse.com/articles/dictionary_pbp.html\n\n\n\n\n\n\nNote 4.1: Downloading play-by-play data\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_pbp_raw &lt;- progressr::with_progress(\n  nflreadr::load_pbp(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_pbp_raw,\n  file = \"./data/nfl_pbp_raw.RData\"\n)\n\n\nThe nfl_pbp object is in game-drive-play form. That is, each row should be uniquely identified by the combination of game_id, fixed_drive, play_id. Let’s rearrange the data accordingly:\n\nCodenfl_pbp &lt;- nfl_pbp_raw %&gt;% \n  select(game_id, drive, play_id, everything()) %&gt;% \n  arrange(game_id, drive, play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_pbp %&gt;% \n  group_by(game_id, fixed_drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_pbp,\n  file = \"./data/nfl_pbp.RData\"\n)\n\n\n\n4.3.14 4th Down Data\nWe use the nfl4th package (Baldwin, 2023) to download fourth down data.\n\n\n\n\n\n\nNote 4.2: Downloading 4th down data\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_4thdown_raw &lt;- nfl4th::load_4th_pbp(\n  seasons = 2014:nflreadr::most_recent_season())\n\n\n\nCodesave(\n  nfl_4thdown_raw,\n  file = \"./data/nfl_4thdown_raw.RData\"\n)\n\n\nThe nfl_4thdown object is in game-drive-play form. That is, each row should be uniquely identified by the combination of game_id, drive, play_id. Let’s rearrange the data accordingly:\n\nCodenfl_4thdown &lt;- nfl_4thdown_raw %&gt;% \n  select(game_id, drive, play_id, everything()) %&gt;% \n  arrange(game_id, drive, play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_4thdown %&gt;% \n  group_by(game_id, drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_4thdown,\n  file = \"./data/nfl_4thdown.RData\"\n)\n\n\n\n4.3.15 Participation\nHo & Carl (2025m) provide a Data Dictionary for the participation data at the following link: https://nflreadr.nflverse.com/articles/dictionary_participation.html\n\nCodenfl_participation_raw &lt;- progressr::with_progress(\n  nflreadr::load_participation(\n    seasons = TRUE,\n    include_pbp = TRUE))\n\n\n\nCodesave(\n  nfl_participation_raw,\n  file = \"./data/nfl_participation_raw.RData\"\n)\n\n\nThe nfl_participation object is in game-drive-play form. That is, each row should be uniquely identified by the combination of nflverse_game_id, drive, play_id. Let’s rearrange the data accordingly:\n\nCodenfl_participation &lt;- nfl_participation_raw %&gt;% \n  select(nflverse_game_id, drive, play_id, everything()) %&gt;% \n  arrange(nflverse_game_id, drive, play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_participation %&gt;% \n  group_by(nflverse_game_id, drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_participation,\n  file = \"./data/nfl_participation.RData\"\n)\n\n\n\n4.3.16 Historical Actual Player Statistics\n\n4.3.16.1 Career Statistics\n\n4.3.16.2 Season-by-Season Statistics\n\n4.3.16.3 Week-by-Week Statistics\nWe can download historical week-by-week actual player statistics using the load_player_stats() function from the nflreadr package (Ho & Carl, 2024). Ho & Carl (2025p) provide a Data Dictionary for statistics for offensive players at the following link: https://nflreadr.nflverse.com/articles/dictionary_player_stats.html. Ho & Carl (2025q) provide a Data Dictionary for statistics for defensive players at the following link: https://nflreadr.nflverse.com/articles/dictionary_player_stats_def.html.\n\nCodenfl_actualStats_offense_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"offense\"))\n\nnfl_actualStats_defense_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"defense\"))\n\nnfl_actualStats_kicking_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"kicking\"))\n\n\n\nCodesave(\n  nfl_actualStats_offense_weekly_raw, nfl_actualStats_defense_weekly_raw, nfl_actualStats_kicking_weekly_raw,\n  file = \"./data/nfl_actualStats_position_weekly_raw.RData\"\n)\n\n\nThe nfl_actualStats_weekly objects are in player-season-week form. That is, each row should be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_offense_weekly &lt;- nfl_actualStats_offense_weekly_raw %&gt;% \n  rename(team = recent_team) %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualStats_defense_weekly &lt;- nfl_actualStats_defense_weekly_raw %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualStats_kicking_weekly &lt;- nfl_actualStats_kicking_weekly_raw %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualStats_offense_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_defense_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_kicking_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualStats_offense_weekly, nfl_actualStats_defense_weekly, nfl_actualStats_kicking_weekly,\n  file = \"./data/nfl_actualStats_position_weekly.RData\"\n)\n\n\n\n4.3.17 Injuries\nHo & Carl (2025k) provide a Data Dictionary for injury data at the following link: https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n\nCodenfl_injuries_raw &lt;- progressr::with_progress(\n  nflreadr::load_injuries(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_injuries_raw,\n  file = \"./data/nfl_injuries_raw.RData\"\n)\n\n\nThe nfl_injuries object is in player-season-week form. That is, each row should be uniquely identified by the combination of gsis_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_injuries &lt;- nfl_injuries_raw %&gt;% \n  select(gsis_id, season, week, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_injuries %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_injuries,\n  file = \"./data/nfl_injuries.RData\"\n)\n\n\n\n4.3.18 Snap Counts\nHo & Carl (2025t) provide a Data Dictionary for snap counts data at the following link: https://nflreadr.nflverse.com/articles/dictionary_snap_counts.html\n\nCodenfl_snapCounts_raw &lt;- progressr::with_progress(\n  nflreadr::load_snap_counts(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_snapCounts_raw,\n  file = \"./data/nfl_snapCounts_raw.RData\"\n)\n\n\nThe nfl_snapCounts object is in game-player form. That is, each row should be uniquely identified by the combination of game_id and pfr_player_id. Let’s rearrange the data accordingly:\n\nCodenfl_snapCounts &lt;- nfl_snapCounts_raw %&gt;% \n  select(game_id, pfr_player_id, everything()) %&gt;% \n  arrange(game_id, pfr_player_id)\n\n\nLet’s check for duplicate game instances:\n\nCodenfl_snapCounts %&gt;% \n  group_by(game_id, pfr_player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_snapCounts,\n  file = \"./data/nfl_snapCounts.RData\"\n)\n\n\n\n4.3.19 ESPN QBR\nHo & Carl (2025f) provide a Data Dictionary for ESPN QBR data at the following link: https://nflreadr.nflverse.com/articles/dictionary_espn_qbr.html\n\nCodenfl_espnQBR_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_espn_qbr(\n    seasons = TRUE,\n    summary_type = c(\"season\")))\n\nnfl_espnQBR_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_espn_qbr(\n    seasons = TRUE,\n    summary_type = c(\"week\")))\n\n\n\nCodesave(\n  nfl_espnQBR_seasonal_raw,\n  file = \"./data/nfl_espnQBR_seasonal_raw.RData\"\n)\n\nsave(\n  nfl_espnQBR_weekly_raw,\n  file = \"./data/nfl_espnQBR_weekly_raw.RData\"\n)\n\n\nThe nfl_espnQBR_seasonal object is in player-season-season type form, where season type refers to regular season versus postseason. That is, each row should be uniquely identified by the combination of player_id, season, and season_type. Let’s rearrange the data accordingly:\n\nCodenfl_espnQBR_seasonal &lt;- nfl_espnQBR_seasonal_raw %&gt;% \n  select(player_id, season, season_type, everything()) %&gt;% \n  arrange(name_display, player_id, season, season_type)\n\n\nLet’s check for duplicate player-season-team instances:\n\nCodenfl_espnQBR_seasonal %&gt;% \n  group_by(player_id, season, season_type) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_espnQBR_weekly object is in both game-player form and player-season-season type-week form, where season type refers to regular season versus postseason. That is, each row should be uniquely identified by the combination of gsis_id, season, and week or by the combination of player_id, season, season_type, and week_num. Let’s rearrange the data accordingly:\n\nCodenfl_espnQBR_weekly &lt;- nfl_espnQBR_weekly_raw %&gt;% \n  select(player_id, season, season_type, week_num, everything()) %&gt;% \n  arrange(name_display, player_id, season, season_type, week_num)\n\n\nLet’s check for duplicate game-player or player-season-season type-week instances:\n\nCodenfl_espnQBR_weekly %&gt;% \n  arrange(game_id, player_id) %&gt;% \n  group_by(game_id, player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_espnQBR_weekly %&gt;% \n  arrange(player_id, season, season_type, week_num) %&gt;% \n  group_by(player_id, season, season_type, week_num) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_espnQBR_seasonal,\n  file = \"./data/nfl_espnQBR_seasonal.RData\"\n)\n\nsave(\n  nfl_espnQBR_weekly,\n  file = \"./data/nfl_espnQBR_weekly.RData\"\n)\n\n\n\n4.3.20 NFL Next Gen Stats\nHo & Carl (2025l) provide a Data Dictionary for NFL Next Gen Stats data at the following link: https://nflreadr.nflverse.com/articles/dictionary_nextgen_stats.html\n\nCodenfl_nextGenStats_pass_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"passing\")))\n\nnfl_nextGenStats_rush_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"rushing\")))\n\nnfl_nextGenStats_rec_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"receiving\")))\n\nnfl_nextGenStats_weekly_raw &lt;- bind_rows(\n  nfl_nextGenStats_pass_weekly_raw,\n  nfl_nextGenStats_rush_weekly_raw,\n  nfl_nextGenStats_rec_weekly_raw\n)\n\n\n\nCodesave(\n  nfl_nextGenStats_weekly_raw,\n  file = \"./data/nfl_nextGenStats_weekly_raw.RData\"\n)\n\n\nThe nfl_nextGenStats_weekly object is in player-season-season type-week form, where season type refers to regular season versus postseason. That is, each row should be uniquely identified by the combination of player_gsis_id, season, season_type, and week. Let’s rearrange the data accordingly:\n\nCodenfl_nextGenStats_weekly &lt;- nfl_nextGenStats_weekly_raw %&gt;% \n  select(player_gsis_id, season, season_type, week, everything()) %&gt;% \n  arrange(player_display_name, player_gsis_id, season, season_type)\n\n\nLet’s check for duplicate player-season-season type-week instances:\n\nCodenfl_nextGenStats_weekly %&gt;% \n  group_by(player_gsis_id, season, season_type, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_nextGenStats_weekly,\n  file = \"./data/nfl_nextGenStats_weekly.RData\"\n)\n\n\n\n4.3.21 Advanced Stats from Pro Football Reference\nThere is a Data Dictionary for Pro Football Reference passing data at the following links:\n\n\nhttps://nflreadr.nflverse.com/articles/dictionary_pfr_passing.html (Ho & Carl, 2025o)\n\n\nhttps://www.pro-football-reference.com/about/advanced_stats.htm [Pro Football Reference (2025); archived at https://perma.cc/LKH5-92PQ]\n\nPro Football Reference (2024) provides advanced stats from the 2024 season at the following link: https://www.pro-football-reference.com/years/2024/advanced.htm\n\nCode# Seasonal Data\nnfl_advancedStatsPFR_pass_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"pass\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_rush_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rush\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_rec_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rec\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_def_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"def\"),\n    summary_level = c(\"season\")))\n\n# Weekly Data\nnfl_advancedStatsPFR_pass_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"pass\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_rush_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rush\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_rec_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rec\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_def_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"def\"),\n    summary_level = c(\"week\")))\n\n\n\nCodesave(\n  nfl_advancedStatsPFR_pass_seasonal_raw,\n  nfl_advancedStatsPFR_rush_seasonal_raw,\n  nfl_advancedStatsPFR_rec_seasonal_raw,\n  nfl_advancedStatsPFR_def_seasonal_raw,\n  file = \"./data/nfl_advancedStatsPFR_seasonal_raw.RData\"\n)\n\nsave(\n  nfl_advancedStatsPFR_pass_weekly_raw,\n  nfl_advancedStatsPFR_rush_weekly_raw,\n  nfl_advancedStatsPFR_rec_weekly_raw,\n  nfl_advancedStatsPFR_def_weekly_raw,\n  file = \"./data/nfl_advancedStatsPFR_weekly_raw.RData\"\n)\n\n\n\n4.3.21.1 Processing\n\nCode# Clean up player name for merging; name variables based on which data object they're from\n\n## Seasonal Data\nnfl_advancedStatsPFR_pass_seasonal &lt;- nfl_advancedStatsPFR_pass_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".pass\"),\n    -c(pfr_id, merge_name, season, team))\n\nnfl_advancedStatsPFR_rush_seasonal &lt;- nfl_advancedStatsPFR_rush_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  ) %&gt;% \n  rename(\n    team = tm\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rush\"),\n    -c(pfr_id, merge_name, season, team))\n\nnfl_advancedStatsPFR_rec_seasonal &lt;- nfl_advancedStatsPFR_rec_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  ) %&gt;% \n  rename(\n    team = tm\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rec\"),\n    -c(pfr_id, merge_name, season, team))\n\nnfl_advancedStatsPFR_def_seasonal &lt;- nfl_advancedStatsPFR_def_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  )  %&gt;% \n  rename(\n    team = tm\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".def\"),\n    -c(pfr_id, merge_name, season, team))\n\n## Weekly Data\nnfl_advancedStatsPFR_pass_weekly &lt;- nfl_advancedStatsPFR_pass_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".pass\"),\n    -c(game_id, pfr_player_id))\n\nnfl_advancedStatsPFR_rush_weekly &lt;- nfl_advancedStatsPFR_rush_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rush\"),\n    -c(game_id, pfr_player_id))\n\nnfl_advancedStatsPFR_rec_weekly &lt;- nfl_advancedStatsPFR_rec_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rec\"),\n    -c(game_id, pfr_player_id))\n\nnfl_advancedStatsPFR_def_weekly &lt;- nfl_advancedStatsPFR_def_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".def\"),\n    -c(game_id, pfr_player_id))\n\n## Merge across positions\nnfl_advancedStatsPFR_seasonal_list &lt;- list(\n  nfl_advancedStatsPFR_pass_seasonal,\n  nfl_advancedStatsPFR_rush_seasonal,\n  nfl_advancedStatsPFR_rec_seasonal,\n  nfl_advancedStatsPFR_def_seasonal)\n\nnfl_advancedStatsPFR_weekly_list &lt;- list(\n  nfl_advancedStatsPFR_pass_weekly,\n  nfl_advancedStatsPFR_rush_weekly,\n  nfl_advancedStatsPFR_rec_weekly,\n  nfl_advancedStatsPFR_def_weekly)\n\nnfl_advancedStatsPFR_seasonalByTeam &lt;- nfl_advancedStatsPFR_seasonal_list %&gt;% \n  purrr::reduce(\n    full_join,\n    by = c(\"pfr_id\",\"merge_name\",\"season\",\"team\"))\n\nnfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly_list %&gt;% \n  purrr::reduce(\n    full_join,\n    by = c(\"game_id\",\"pfr_player_id\")) #merge_name\n\n#nfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly_list %&gt;% \n#  purrr::reduce(\n#    full_join,\n#    by = c(\"pfr_player_id\",\"merge_name\",\"season\",\"week\"))\n\nnfl_advancedStatsPFR_seasonalByTeam &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  mutate(\n    pfr_player_name = coalesce(\n      player.pass,\n      player.rush,\n      player.rec,\n      player.def\n    ),\n    age = coalesce(\n      #age.pass,\n      age.rush,\n      age.rec,\n      age.def\n    ),\n    pos = coalesce(\n      #pos.pass,\n      pos.rush,\n      pos.rec,\n      pos.def\n    ),\n    g = coalesce(\n      #g.pass,\n      g.rush,\n      g.rec,\n      g.def\n    ),\n    gs = coalesce(\n      #gs.pass,\n      gs.rush,\n      gs.rec,\n      gs.def\n    )\n  ) %&gt;% \n  select(-c(\n    starts_with(\"player.\"),\n    starts_with(\"age.\"),\n    starts_with(\"pos.\"),\n    starts_with(\"g.\"),\n    starts_with(\"gs.\")))\n\nnfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly %&gt;% \n  mutate(\n    pfr_player_name = coalesce(\n      pfr_player_name.pass,\n      pfr_player_name.rush,\n      pfr_player_name.rec,\n      pfr_player_name.def\n    ),\n    season = coalesce(\n      season.pass,\n      season.rush,\n      season.rec,\n      season.def\n    ),\n    week = coalesce(\n      week.pass,\n      week.rush,\n      week.rec,\n      week.def\n    ),\n    team = coalesce(\n      team.pass,\n      team.rush,\n      team.rec,\n      team.def\n    ),\n    merge_name = coalesce(\n      merge_name.pass,\n      merge_name.rush,\n      merge_name.rec,\n      merge_name.def\n    ),\n    pfr_game_id = coalesce(\n      pfr_game_id.pass,\n      pfr_game_id.rush,\n      pfr_game_id.rec,\n      pfr_game_id.def\n    ),\n    game_type = coalesce(\n      game_type.pass,\n      game_type.rush,\n      game_type.rec,\n      game_type.def\n    ),\n    opponent = coalesce(\n      opponent.pass,\n      opponent.rush,\n      opponent.rec,\n      opponent.def\n    )\n  ) %&gt;% \n  select(-c(\n    starts_with(\"pfr_player_name.\"),\n    starts_with(\"season.\"),\n    starts_with(\"week.\"),\n    starts_with(\"team.\"),\n    starts_with(\"merge_name.\"),\n    starts_with(\"pfr_game_id.\"),\n    starts_with(\"game_type.\"),\n    starts_with(\"opponent.\"),\n    ))\n\n\nThe nfl_advancedStatsPFR_seasonalByTeam object is in player-season-team form. That is, each row should be uniquely identified by the combination of pfr_id, season, and team. Let’s rearrange the data accordingly:\n\nCodenfl_advancedStatsPFR_seasonalByTeam &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  select(pfr_id, season, team, pfr_player_name, everything()) %&gt;% \n  arrange(pfr_player_name, pfr_id, season, team)\n\n\nLet’s check for duplicate player-season-team instances:\n\nCodenfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  group_by(pfr_id, season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nAggregate variables within each pass/rush/rec/def object by team for seasonal data (so seasonal data are in player-season form, not player-season-team form). Depending on the variable, aggregation was performed using a sum, weighted mean (weighted by the number of games played for each team), or a recomputed percentage.\n\nCodepfrVars &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  select(pass_attempts.pass:m_tkl_percent.def, g, gs) %&gt;% \n  names()\n\nweightedAverageVars &lt;- c(\n  \"pocket_time.pass\",\n  \"ybc_att.rush\",\"yac_att.rush\",\n  \"ybc_r.rec\",\"yac_r.rec\",\"adot.rec\",\"rat.rec\",\n  \"yds_cmp.def\",\"yds_tgt.def\",\"dadot.def\",\"m_tkl_percent.def\",\"rat.def\"\n)\n\nrecomputeVars &lt;- c(\n  \"drop_pct.pass\", # drops.pass / pass_attempts.pass\n  \"bad_throw_pct.pass\", # bad_throws.pass / pass_attempts.pass\n  \"on_tgt_pct.pass\", # on_tgt_throws.pass / pass_attempts.pass\n  \"pressure_pct.pass\", # times_pressured.pass / pass_attempts.pass\n  \"drop_percent.rec\", # drop.rec / tgt.rec\n  \"rec_br.rec\", # rec.rec / brk_tkl.rec\n  \"cmp_percent.def\" # cmp.def / tgt.def\n)\n\nsumVars &lt;- pfrVars[pfrVars %ni% c(\n  weightedAverageVars, recomputeVars,\n  \"merge_name\", \"loaded.pass\", \"loaded.rush\", \"loaded.rec\", \"loaded.def\")]\n\nnfl_advancedStatsPFR_seasonal &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  group_by(pfr_id, merge_name, season) %&gt;% \n  summarise(\n    across(all_of(weightedAverageVars), ~ weighted.mean(.x, w = g, na.rm = TRUE)),\n    across(all_of(sumVars), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\") %&gt;% \n  mutate(\n    drop_pct.pass = drops.pass / pass_attempts.pass,\n    bad_throw_pct.pass = bad_throws.pass / pass_attempts.pass,\n    on_tgt_pct.pass = on_tgt_throws.pass / pass_attempts.pass,\n    pressure_pct.pass = times_pressured.pass / pass_attempts.pass,\n    drop_percent.rec = drop.rec / tgt.rec,\n    rec_br.rec = drop.rec / tgt.rec,\n    cmp_percent.def = cmp.def / tgt.def\n  )\n\n# Merge with other player info\nnfl_advancedStatsPFR_seasonalByTeam_1stTeam &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  slice(1)\n\nnfl_advancedStatsPFR_seasonalByTeam_1stTeam_mergeVars &lt;- nfl_advancedStatsPFR_seasonalByTeam_1stTeam %&gt;% \n  select(pfr_id, season, team, pfr_player_name, age, pos) #, g, gs\n\nnfl_advancedStatsPFR_seasonal &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  left_join(\n    nfl_advancedStatsPFR_seasonalByTeam_1stTeam_mergeVars,\n    by = c(\"pfr_id\",\"season\")\n  ) %&gt;% \n  select(\n    pfr_id, season, pfr_player_name, pos, age, team, g, gs,\n    contains(\".pass\"), contains(\".rush\"), contains(\".rec\"), contains(\".def\"),\n    everything())\n\n\nLet’s check for duplicate player-season instances:\n\nCodenfl_advancedStatsPFR_seasonal %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_advancedStatsPFR_weekly object is in both game-player form and player-season-week form. That is, each row should be uniquely identified by the combination of pfr_player_id, season, and week or by the combination of pfr_player_id, season, game_type, and week. Let’s rearrange the data accordingly:\n\nCodenfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly %&gt;% \n  select(pfr_player_id, season, week, game_type, game_id, pfr_player_name, everything()) %&gt;% \n  arrange(pfr_player_name, pfr_player_id, season, week)\n\n\nLet’s check for duplicate game-player or player-season-week instances:\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(game_id, pfr_player_id) %&gt;% \n  group_by(game_id, pfr_player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(pfr_player_id, season, week) %&gt;% \n  group_by(pfr_player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nMerge with gsis_id for merging with other datasets:\n\nCode# Prepare data for merging\nnfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly %&gt;% \n  rename(pfr_id = pfr_player_id)\n\n# Identify duplicates\nnfl_advancedStatsPFR_seasonal %&gt;% \n  select(pfr_id, season, age) %&gt;% \n  na.omit() %&gt;% \n  unique() %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  arrange(pfr_id, season) %&gt;% \n  head()\n\n\n  \n\n\nCode# Merge seasonal data with the player IDs\nnfl_advancedStatsPFR_seasonal &lt;- left_join(\n  nfl_advancedStatsPFR_seasonal,\n  nfl_playerIDs %&gt;% \n    filter(!is.na(pfr_id)) %&gt;% \n    filter(gsis_id != \"00-0039137\") %&gt;% # drop DL Byron Young, keep OLB Byron Young\n    select(pfr_id, gsis_id) %&gt;% \n    unique(),\n  by = \"pfr_id\"\n)\n\n# Merge weekly data with the player IDs\nnfl_advancedStatsPFR_weekly &lt;- left_join(\n  nfl_advancedStatsPFR_weekly,\n  nfl_playerIDs %&gt;% \n    filter(!is.na(pfr_id)) %&gt;% \n    filter(gsis_id != \"00-0039137\") %&gt;% # drop DL Byron Young, keep OLB Byron Young\n    select(pfr_id, gsis_id) %&gt;% \n    unique(),\n  by = \"pfr_id\"\n)\n\n# Remove distinct players who were given the same `pfr_id` (to allow merging)\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0035665\" & nfl_advancedStatsPFR_seasonal$pos %in% c(\"LB\",\"LILB\",\"RILB\"))] &lt;- NA # drop LB David Young, keep DB David Young\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0035665\" & nfl_advancedStatsPFR_weekly$team %in% c(\"TEN\",\"MIA\"))] &lt;- NA # drop LB David Young, keep DB David Young\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0035292\" & nfl_advancedStatsPFR_seasonal$pos %in% c(\"LB\",\"LILB\",\"RILB\"))] &lt;- NA # drop LB David Young, keep DB David Young\nnfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0035292\" & nfl_advancedStatsPFR_weekly$team %in% c(\"TEN\",\"MIA\"))] &lt;- NA # drop LB David Young, keep DB David Young\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0033894\" & nfl_advancedStatsPFR_seasonal$pos == \"DB\")] &lt;- NA # drop S Marcus Williams, keep DB David Young\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0033894\" & nfl_advancedStatsPFR_weekly$pos == \"DB\")] &lt;- NA # drop S Marcus Williams\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0038407\" & nfl_advancedStatsPFR_seasonal$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0038407\" & nfl_advancedStatsPFR_weekly$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0037106\" & nfl_advancedStatsPFR_seasonal$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0037106\" & nfl_advancedStatsPFR_weekly$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0038549\" & nfl_advancedStatsPFR_seasonal$pos == \"WR\")] &lt;- NA # drop WR DJ TUrner, keep CB DJ Turner\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0038549\" & nfl_advancedStatsPFR_weekly$pos == \"WR\")] &lt;- NA # drop WR DJ TUrner, keep CB DJ Turner\n\n\nNow, each row of the nfl_advancedStatsPFR_seasonal object should be uniquely identified by the combination of gsis_id (or pfr_id), and season. Each row of the nfl_advancedStatsPFR_weekly object should be uniquely identified by the combination of gsis_id (or pfr_id), season, and week or by the combination of gsis_id (or pfr_id), season, game_type, and week.\nLet’s check again for duplicate game-player or player-season-week instances:\n\nCode# Based on gsis_id\nnfl_advancedStatsPFR_seasonal %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  group_by(gsis_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  arrange(game_id, gsis_id) %&gt;% \n  group_by(game_id, gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  arrange(gsis_id, season, week) %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCode# Based on pfr_id\nnfl_advancedStatsPFR_seasonal %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(game_id, pfr_id) %&gt;% \n  group_by(game_id, pfr_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(pfr_id, season, week) %&gt;% \n  group_by(pfr_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_advancedStatsPFR_seasonal,\n  file = \"./data/nfl_advancedStatsPFR_seasonal.RData\"\n)\n\nsave(\n  nfl_advancedStatsPFR_weekly,\n  file = \"./data/nfl_advancedStatsPFR_weekly.RData\"\n)\n\n\n\n4.3.22 Player Contracts\nHo & Carl (2025c) provide a Data Dictionary for player contracts data at the following link: https://nflreadr.nflverse.com/articles/dictionary_contracts.html\n\nCodenfl_playerContracts_raw &lt;- progressr::with_progress(\n  nflreadr::load_contracts())\n\n\n\nCodesave(\n  nfl_playerContracts_raw,\n  file = \"./data/nfl_playerContracts_raw.RData\"\n)\n\n\nThe nfl_playerContracts object is in player-year-team-value form. That is, each row should be uniquely identified by the combination of otc_id, year_signed, team, and value. Let’s rearrange the data accordingly:\n\nCodenfl_playerContracts &lt;- nfl_playerContracts_raw %&gt;% \n  select(otc_id, year_signed, team, everything()) %&gt;% \n  arrange(player, otc_id, year_signed, team)\n\n\nLet’s check for duplicate player-year-team-value instances:\n\nCodenfl_playerContracts %&gt;% \n  group_by(otc_id, year_signed, team, value) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_playerContracts,\n  file = \"./data/nfl_playerContracts.RData\"\n)\n\n\n\n4.3.23 FTN Charting Data\nHo & Carl (2025j) provide a Data Dictionary for FTN Charting data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ftn_charting.html\n\nCodenfl_ftnCharting_raw &lt;- progressr::with_progress(\n  nflreadr::load_ftn_charting(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_ftnCharting_raw,\n  file = \"./data/nfl_ftnCharting_raw.RData\"\n)\n\n\nThe nfl_ftnCharting object is in game-play form. That is, each row should be uniquely identified by the combination of nflverse_game_id and play_id. Let’s rearrange the data accordingly:\n\nCodenfl_ftnCharting &lt;- nfl_ftnCharting_raw %&gt;% \n  select(nflverse_game_id, nflverse_play_id, everything()) %&gt;% \n  arrange(nflverse_game_id, nflverse_play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_ftnCharting %&gt;% \n  group_by(nflverse_game_id, nflverse_play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_ftnCharting,\n  file = \"./data/nfl_ftnCharting.RData\"\n)\n\n\n\n4.3.24 FantasyPros Rankings\nHo & Carl (2025i) provide a Data Dictionary for FantasyPros ranking data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_rankings.html\n\nCode#nfl_rankings_raw &lt;- progressr::with_progress( # currently throws error\n#  nflreadr::load_ff_rankings(type = \"all\"))\n\nnfl_rankings_draft_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_rankings(type = \"draft\"))\n\nnfl_rankings_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_rankings(type = \"week\"))\n\n\n\nCodesave(\n  nfl_rankings_draft_raw,\n  file = \"./data/nfl_rankings_draft_raw.RData\"\n)\n\nsave(\n  nfl_rankings_weekly_raw,\n  file = \"./data/nfl_rankings_weekly_raw.RData\"\n)\n\n\nThe nfl_rankings_draft object is in player-page_type form. That is, each row should be uniquely identified by the player’s id. Let’s rearrange the data accordingly:\n\nCodenfl_rankings_draft &lt;- nfl_rankings_draft_raw %&gt;% \n  select(id, page_type, player, pos, team, everything()) %&gt;% \n  arrange(player, id, pos, page_type)\n\n\nLet’s check for duplicate player-page_type instances:\n\nCodenfl_rankings_draft %&gt;% \n  group_by(id, page_type) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_rankings_weekly object is in player-page form. That is, each row should be uniquely identified by fantasypros_id and page. Let’s rearrange the data accordingly:\n\nCodenfl_rankings_weekly &lt;- nfl_rankings_weekly_raw %&gt;% \n  select(fantasypros_id, page, player_name, pos, team, everything()) %&gt;% \n  arrange(player_name, fantasypros_id, page, pos)\n\n\nLet’s check for duplicate player-page instances:\n\nCodenfl_rankings_weekly %&gt;% \n  group_by(fantasypros_id, page) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_rankings_draft,\n  file = \"./data/nfl_rankings_draft.RData\"\n)\n\nsave(\n  nfl_rankings_weekly,\n  file = \"./data/nfl_rankings_weekly.RData\"\n)\n\n\n\n4.3.25 Expected Fantasy Points\nHo & Carl (2025g) provide a Data Dictionary for expected fantasy points data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_opportunity.html\n\nCodenfl_expectedFantasyPoints_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"weekly\",\n    model_version = \"latest\"\n  ))\n\nnfl_expectedFantasyPoints_pass_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"pbp_pass\",\n    model_version = \"latest\"\n  ))\n\nnfl_expectedFantasyPoints_rush_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"pbp_rush\",\n    model_version = \"latest\"\n  ))\n\n\n\nCodenfl_expectedFantasyPoints_weekly_raw &lt;- nflreadr::load_ff_opportunity(\n  seasons = TRUE,\n  stat_type = \"weekly\",\n  model_version = \"latest\"\n  )\n\nnfl_expectedFantasyPoints_pass_raw &lt;- nflreadr::load_ff_opportunity(\n  seasons = TRUE,\n  stat_type = \"pbp_pass\",\n  model_version = \"latest\"\n  )\n\nnfl_expectedFantasyPoints_rush_raw &lt;- nflreadr::load_ff_opportunity(\n  seasons = TRUE,\n  stat_type = \"pbp_rush\",\n  model_version = \"latest\"\n  )\n\n\n\nCodesave(\n  nfl_expectedFantasyPoints_weekly_raw,\n  file = \"./data/nfl_expectedFantasyPoints_weekly_raw.RData\"\n)\n\nsave(\n  nfl_expectedFantasyPoints_pass_raw,\n  nfl_expectedFantasyPoints_rush_raw,\n  file = \"./data/nfl_expectedFantasyPoints_pbp_raw.RData\"\n)\n\n\n\nCodenfl_expectedFantasyPoints_pbp_list &lt;- list(\n  nfl_expectedFantasyPoints_pass_raw,\n  nfl_expectedFantasyPoints_rush_raw)\n\nnfl_expectedFantasyPoints_pbp &lt;- full_join(\n  nfl_expectedFantasyPoints_pass_raw,\n  nfl_expectedFantasyPoints_rush_raw,\n  by = c(\"game_id\",\"fixed_drive\",\"play_id\"),\n  suffix = c(\".pass\", \".rush\")\n)\n\n\nThe nfl_expectedFantasyPoints_weekly object is in game-player form and in player-season-week form. That is, each row should be uniquely identified by the combination of game_id and player_id. Each row should also be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_expectedFantasyPoints_weekly &lt;- nfl_expectedFantasyPoints_weekly_raw %&gt;% \n  select(player_id, season, week, game_id, full_name, posteam, position, everything()) %&gt;% \n  arrange(full_name, player_id, season, week)\n\n\nLet’s check for duplicate game-player instances and player-season-week instances:\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(game_id, player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.25.1 Processing\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for player_id\nnfl_expectedFantasyPoints_weekly &lt;- nfl_expectedFantasyPoints_weekly %&gt;% \n  filter(!is.na(player_id))\n\n\nLet’s check again for duplicate game-player instances and season-week-player instances:\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(game_id, player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_expectedFantasyPoints_pbp object is in game-drive-play form. That is, each row should be uniquely identified by the combination of game_id, fixed_drive, and play_id. Let’s rearrange the data accordingly:\n\nCodenfl_expectedFantasyPoints_pbp &lt;- nfl_expectedFantasyPoints_pbp %&gt;% \n  select(game_id, fixed_drive, play_id, everything()) %&gt;% \n  arrange(game_id, fixed_drive, play_id)\n\n\nLet’s check for duplicate game-player instances and season-week-player instances:\n\nCodenfl_expectedFantasyPoints_pbp %&gt;% \n  group_by(game_id, fixed_drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_expectedFantasyPoints_weekly,\n  file = \"./data/nfl_expectedFantasyPoints_weekly.RData\"\n)\n\nsave(\n  nfl_expectedFantasyPoints_pbp,\n  file = \"./data/nfl_expectedFantasyPoints_pbp.RData\"\n)\n\n\n\n4.3.26 Fantasy Football Projections\n\n4.3.26.1 Download Players’ Projections\n\n\n\n\n\n\nNote 4.3: Downloading players’ seasonal projections\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCode# Seasonal Projections\nplayers_projections_seasonal_raw &lt;- ffanalytics::scrape_data(\n  season = NULL, # NULL grabs the current season\n  week = 0) # 0 grabs seasonal projections\n\n# Weekly Projections\nplayers_projections_weekly_raw &lt;- ffanalytics::scrape_data(\n  season = NULL, # NULL grabs the current season\n  week = NULL) # NULL grabs the current week\n\n\n\nCodesave(\n  players_projections_seasonal_raw,\n  file = \"./data/players_projections_seasonal_raw.RData\"\n)\n\nsave(\n  players_projections_weekly_raw,\n  file = \"./data/players_projections_weekly_raw.RData\"\n)\n\n\nThe data file is saved in the project repository and can be loaded using the following command:\n\nCodeload(file = \"./data/players_projections_seasonal_raw.RData\")\nload(file = \"./data/players_projections_weekly_raw.RData\")\n\n\nThe players_projections_seasonal_raw and players_projections_weekly_raw object is in player-position-projection source form. That is, each row should be uniquely identified by the combination of id, pos and data_src. Each row should also be uniquely identified by the combination of player, pos, and data_src.\nLet’s check for duplicate player-position-projection source instances:\n\nCodeplayers_projections_seasonal_raw %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_seasonal_raw %&gt;% \n  bind_rows() %&gt;% \n  select(player, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_weekly_raw %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_weekly_raw %&gt;% \n  bind_rows() %&gt;% \n  select(player, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.26.2 Specify League Scoring Settings\nFirst, create a scoring object using the default scoring object:\n\nCodescoring_obj_default &lt;- ffanalytics::scoring\n\n\nView the default scoring settings:\n\nCodescoring_obj_default\n\n$pass\n$pass$pass_att\n[1] 0\n\n$pass$pass_comp\n[1] 0\n\n$pass$pass_inc\n[1] 0\n\n$pass$pass_yds\n[1] 0.04\n\n$pass$pass_tds\n[1] 4\n\n$pass$pass_int\n[1] -3\n\n$pass$pass_40_yds\n[1] 0\n\n$pass$pass_300_yds\n[1] 0\n\n$pass$pass_350_yds\n[1] 0\n\n$pass$pass_400_yds\n[1] 0\n\n\n$rush\n$rush$all_pos\n[1] TRUE\n\n$rush$rush_yds\n[1] 0.1\n\n$rush$rush_att\n[1] 0\n\n$rush$rush_40_yds\n[1] 0\n\n$rush$rush_tds\n[1] 6\n\n$rush$rush_100_yds\n[1] 0\n\n$rush$rush_150_yds\n[1] 0\n\n$rush$rush_200_yds\n[1] 0\n\n\n$rec\n$rec$all_pos\n[1] TRUE\n\n$rec$rec\n[1] 0\n\n$rec$rec_yds\n[1] 0.1\n\n$rec$rec_tds\n[1] 6\n\n$rec$rec_40_yds\n[1] 0\n\n$rec$rec_100_yds\n[1] 0\n\n$rec$rec_150_yds\n[1] 0\n\n$rec$rec_200_yds\n[1] 0\n\n\n$misc\n$misc$all_pos\n[1] TRUE\n\n$misc$fumbles_lost\n[1] -3\n\n$misc$fumbles_total\n[1] 0\n\n$misc$sacks\n[1] 0\n\n$misc$two_pts\n[1] 2\n\n\n$kick\n$kick$xp\n[1] 1\n\n$kick$fg_0019\n[1] 3\n\n$kick$fg_2029\n[1] 3\n\n$kick$fg_3039\n[1] 3\n\n$kick$fg_4049\n[1] 4\n\n$kick$fg_50\n[1] 5\n\n$kick$fg_miss\n[1] 0\n\n\n$ret\n$ret$all_pos\n[1] TRUE\n\n$ret$return_tds\n[1] 6\n\n$ret$return_yds\n[1] 0\n\n\n$idp\n$idp$all_pos\n[1] TRUE\n\n$idp$idp_solo\n[1] 1\n\n$idp$idp_asst\n[1] 0.5\n\n$idp$idp_sack\n[1] 2\n\n$idp$idp_int\n[1] 3\n\n$idp$idp_fum_force\n[1] 3\n\n$idp$idp_fum_rec\n[1] 2\n\n$idp$idp_pd\n[1] 1\n\n$idp$idp_td\n[1] 6\n\n$idp$idp_safety\n[1] 2\n\n\n$dst\n$dst$dst_fum_rec\n[1] 2\n\n$dst$dst_int\n[1] 2\n\n$dst$dst_safety\n[1] 2\n\n$dst$dst_sacks\n[1] 1\n\n$dst$dst_td\n[1] 6\n\n$dst$dst_blk\n[1] 1.5\n\n$dst$dst_ret_yds\n[1] 0\n\n$dst$dst_pts_allowed\n[1] 0\n\n\n$pts_bracket\n$pts_bracket[[1]]\n$pts_bracket[[1]]$threshold\n[1] 0\n\n$pts_bracket[[1]]$points\n[1] 10\n\n\n$pts_bracket[[2]]\n$pts_bracket[[2]]$threshold\n[1] 6\n\n$pts_bracket[[2]]$points\n[1] 7\n\n\n$pts_bracket[[3]]\n$pts_bracket[[3]]$threshold\n[1] 20\n\n$pts_bracket[[3]]$points\n[1] 4\n\n\n$pts_bracket[[4]]\n$pts_bracket[[4]]$threshold\n[1] 34\n\n$pts_bracket[[4]]$points\n[1] 0\n\n\n$pts_bracket[[5]]\n$pts_bracket[[5]]$threshold\n[1] 99\n\n$pts_bracket[[5]]$points\n[1] -4\n\n\nNow, modify the scoring settings to match your league settings. Below, we use the scoring settings for fantasy leagues on NFL.com, which happen to be point-per-reception leagues (i.e., PPR leagues):\n\nCodescoring_obj &lt;- scoring_obj_default\n\n# Offense\nscoring_obj$pass$pass_int &lt;- -2\nscoring_obj$rec$rec &lt;- 1\nscoring_obj$misc$fumbles_lost &lt;- -2\n\n# Kickers\nscoring_obj$kick$fg_4049 &lt;- 3\n\n# Defense/Special Teams\nscoring_obj$pts_bracket &lt;- list(\n  list(threshold = 0, points = 10),\n  list(threshold = 6, points = 7),\n  list(threshold = 13, points = 4),\n  list(threshold = 20, points = 1),\n  list(threshold = 27, points = 0),\n  list(threshold = 34, points = -1),\n  list(threshold = 99, points = -4)\n)\n\n\nView our scoring settings:\n\nCodescoring_obj\n\n$pass\n$pass$pass_att\n[1] 0\n\n$pass$pass_comp\n[1] 0\n\n$pass$pass_inc\n[1] 0\n\n$pass$pass_yds\n[1] 0.04\n\n$pass$pass_tds\n[1] 4\n\n$pass$pass_int\n[1] -2\n\n$pass$pass_40_yds\n[1] 0\n\n$pass$pass_300_yds\n[1] 0\n\n$pass$pass_350_yds\n[1] 0\n\n$pass$pass_400_yds\n[1] 0\n\n\n$rush\n$rush$all_pos\n[1] TRUE\n\n$rush$rush_yds\n[1] 0.1\n\n$rush$rush_att\n[1] 0\n\n$rush$rush_40_yds\n[1] 0\n\n$rush$rush_tds\n[1] 6\n\n$rush$rush_100_yds\n[1] 0\n\n$rush$rush_150_yds\n[1] 0\n\n$rush$rush_200_yds\n[1] 0\n\n\n$rec\n$rec$all_pos\n[1] TRUE\n\n$rec$rec\n[1] 1\n\n$rec$rec_yds\n[1] 0.1\n\n$rec$rec_tds\n[1] 6\n\n$rec$rec_40_yds\n[1] 0\n\n$rec$rec_100_yds\n[1] 0\n\n$rec$rec_150_yds\n[1] 0\n\n$rec$rec_200_yds\n[1] 0\n\n\n$misc\n$misc$all_pos\n[1] TRUE\n\n$misc$fumbles_lost\n[1] -2\n\n$misc$fumbles_total\n[1] 0\n\n$misc$sacks\n[1] 0\n\n$misc$two_pts\n[1] 2\n\n\n$kick\n$kick$xp\n[1] 1\n\n$kick$fg_0019\n[1] 3\n\n$kick$fg_2029\n[1] 3\n\n$kick$fg_3039\n[1] 3\n\n$kick$fg_4049\n[1] 3\n\n$kick$fg_50\n[1] 5\n\n$kick$fg_miss\n[1] 0\n\n\n$ret\n$ret$all_pos\n[1] TRUE\n\n$ret$return_tds\n[1] 6\n\n$ret$return_yds\n[1] 0\n\n\n$idp\n$idp$all_pos\n[1] TRUE\n\n$idp$idp_solo\n[1] 1\n\n$idp$idp_asst\n[1] 0.5\n\n$idp$idp_sack\n[1] 2\n\n$idp$idp_int\n[1] 3\n\n$idp$idp_fum_force\n[1] 3\n\n$idp$idp_fum_rec\n[1] 2\n\n$idp$idp_pd\n[1] 1\n\n$idp$idp_td\n[1] 6\n\n$idp$idp_safety\n[1] 2\n\n\n$dst\n$dst$dst_fum_rec\n[1] 2\n\n$dst$dst_int\n[1] 2\n\n$dst$dst_safety\n[1] 2\n\n$dst$dst_sacks\n[1] 1\n\n$dst$dst_td\n[1] 6\n\n$dst$dst_blk\n[1] 1.5\n\n$dst$dst_ret_yds\n[1] 0\n\n$dst$dst_pts_allowed\n[1] 0\n\n\n$pts_bracket\n$pts_bracket[[1]]\n$pts_bracket[[1]]$threshold\n[1] 0\n\n$pts_bracket[[1]]$points\n[1] 10\n\n\n$pts_bracket[[2]]\n$pts_bracket[[2]]$threshold\n[1] 6\n\n$pts_bracket[[2]]$points\n[1] 7\n\n\n$pts_bracket[[3]]\n$pts_bracket[[3]]$threshold\n[1] 13\n\n$pts_bracket[[3]]$points\n[1] 4\n\n\n$pts_bracket[[4]]\n$pts_bracket[[4]]$threshold\n[1] 20\n\n$pts_bracket[[4]]$points\n[1] 1\n\n\n$pts_bracket[[5]]\n$pts_bracket[[5]]$threshold\n[1] 27\n\n$pts_bracket[[5]]$points\n[1] 0\n\n\n$pts_bracket[[6]]\n$pts_bracket[[6]]$threshold\n[1] 34\n\n$pts_bracket[[6]]$points\n[1] -1\n\n\n$pts_bracket[[7]]\n$pts_bracket[[7]]$threshold\n[1] 99\n\n$pts_bracket[[7]]$points\n[1] -4\n\n\n\n4.3.26.3 Calculate Projected Points\nCalculate projected points by source:\n\nCodeplayers_projectedPoints_seasonal &lt;- ffanalytics:::impute_and_score_sources(\n  data_result = players_projections_seasonal_raw,\n  scoring_rules = scoring_obj)\n\nplayers_projectedPoints_weekly &lt;- ffanalytics:::impute_and_score_sources(\n  data_result = players_projections_weekly_raw,\n  scoring_rules = scoring_obj)\n\n\nCalculate projected statistics and points, averaged across sources:\n\nCode# Seasonal Projections\nplayers_projectedStatsAverage_seasonal &lt;- ffanalytics::projections_table(\n  players_projections_seasonal_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = TRUE)\n\nplayers_projectedPointsAverage_seasonal &lt;- ffanalytics::projections_table(\n  players_projections_seasonal_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = FALSE)\n\n# Weekly Projections\nplayers_projectedStatsAverage_weekly &lt;- ffanalytics::projections_table(\n  players_projections_weekly_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = TRUE)\n\nplayers_projectedPointsAverage_weekly &lt;- ffanalytics::projections_table(\n  players_projections_weekly_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = FALSE)\n\n\nThe players_projectedPoints_seasonal, players_projectedStatsAverage_seasonal, players_projectedPointsAverage_seasonal, players_projectedPoints_weekly, players_projectedStatsAverage_weekly, and players_projectedPointsAverage_weekly objects are in player-average type-position form. That is, each row should be uniquely identified by the combination of id, avg_type and pos (or position).\nLet’s check for duplicate player-position-projection source instances:\n\nCodeplayers_projectedPoints_seasonal %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedStatsAverage_seasonal %&gt;% \n  group_by(id, avg_type, position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedPointsAverage_seasonal %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedPoints_weekly %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedStatsAverage_weekly %&gt;% \n  group_by(id, avg_type, position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedPointsAverage_weekly %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s merge the two averaged projected statistics and points objects:\n\nCodeplayers_projections_seasonal_average &lt;- full_join(\n  players_projectedPointsAverage_seasonal,\n  players_projectedStatsAverage_seasonal,\n  by = c(\"id\",\"avg_type\",\"pos\" = \"position\")\n)\n\nplayers_projections_weekly_average &lt;- full_join(\n  players_projectedPointsAverage_weekly,\n  players_projectedStatsAverage_weekly,\n  by = c(\"id\",\"avg_type\",\"pos\" = \"position\")\n)\n\n\nLet’s again check for duplicate player-position-projection source instances:\n\nCodeplayers_projections_seasonal_average %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_weekly_average %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.26.4 Add Additional Player Information\n\nCodeplayers_projections_seasonal_average &lt;- players_projections_seasonal_average %&gt;% \n  add_ecr() %&gt;% \n  add_adp() %&gt;% \n  add_aav() %&gt;%\n  add_uncertainty() %&gt;% \n  add_player_info()\n\nplayers_projections_weekly_average &lt;- players_projections_weekly_average %&gt;% \n  add_ecr() %&gt;% \n  #add_uncertainty() %&gt;% # currently throws an error\n  add_player_info()\n\n\n\nCodesave(\n  players_projectedPoints_seasonal, players_projections_seasonal_average,\n  file = \"./data/players_projectedPoints_seasonal.RData\"\n)\n\nsave(\n  players_projectedPoints_weekly, players_projections_weekly_average,\n  file = \"./data/players_projectedPoints_weekly.RData\"\n)\n\n\nThe data file is saved in the project repository and can be loaded using the following command:\n\nCodeload(file = \"./data/players_projectedPoints_seasonal.RData\")\nload(file = \"./data/players_projectedPoints_weekly.RData\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-calculations",
    "href": "download-football-data.html#sec-calculations",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n4.4 Calculations",
    "text": "4.4 Calculations\n\n4.4.1 Historical Actual Player Statistics\nIn addition to week-by-week actual player statistics, we can also compute historical actual player statistics as a function of different timeframes, including season-by-season and career statistics.\n\n4.4.1.1 Career Statistics\nFirst, we can compute the players’ career statistics using the calculate_stats() function from the nflfastR package (Carl & Baldwin, 2024).\nTODO: Insert code example.\n\n4.4.1.1.1 OLD (DELETE CODE BELOW)\nFirst, we can compute the players’ career statistics using the calculate_player_stats(), calculate_player_stats_def(), and calculate_player_stats_kicking() functions from the nflfastR package (Carl & Baldwin, 2024) for offensive players, defensive players, and kickers, respectively.\n\n\n\n\n\n\nNote 4.4: Calculating players’ career statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_actualStats_offense_career_raw &lt;- nflfastR::calculate_player_stats(\n  nfl_pbp,\n  weekly = FALSE)\n\nnfl_actualStats_defense_career_raw &lt;- nflfastR::calculate_player_stats_def(\n  nfl_pbp,\n  weekly = FALSE)\n\nnfl_actualStats_kicking_career_raw &lt;- nflfastR::calculate_player_stats_kicking(\n  nfl_pbp,\n  weekly = FALSE)\n\n\n\nCodesave(\n  nfl_actualStats_offense_career_raw, nfl_actualStats_defense_career_raw, nfl_actualStats_kicking_career_raw,\n  file = \"./data/nfl_actualStats_career_raw.RData\"\n)\n\n\nThe nfl_actualStats_career objects are in player form. That is, each row should be uniquely identified by the combination of player_id. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_offense_career &lt;- nfl_actualStats_offense_career_raw %&gt;% \n  arrange(player_display_name, player_id)\n\nnfl_actualStats_defense_career &lt;- nfl_actualStats_defense_career_raw %&gt;% \n  arrange(player_display_name, player_id)\n\nnfl_actualStats_kicking_career &lt;- nfl_actualStats_kicking_career_raw %&gt;% \n  arrange(player_display_name, player_id)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_actualStats_offense_career %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_defense_career %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_kicking_career %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s do some data cleanup:\n\nCode# Sum statistics across a player's years with multiple teams\nnfl_actualStats_defense_career &lt;- nfl_actualStats_defense_career %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(\n    across(where(is.numeric), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\"\n  )\n\nnfl_actualStats_kicking_career &lt;- nfl_actualStats_kicking_career %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(\n    across(where(is.numeric), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\"\n  )\n\n# Re-calculate percentage stats\nnfl_actualStats_kicking_career &lt;- nfl_actualStats_kicking_career %&gt;% \n  mutate(\n    fg_pct = fg_made / fg_att,\n    pat_pct = pat_made / pat_att\n  )\n\n# Merge data back with player info\nnfl_actualStats_defense_career_raw_playerInfo &lt;- nfl_actualStats_defense_career_raw %&gt;% \n  select(player_id, player_name, player_display_name, position, position_group, team, headshot_url) %&gt;% \n  unique() %&gt;% \n  group_by(player_id) %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\nnfl_actualStats_kicking_career_raw_playerInfo &lt;- nfl_actualStats_kicking_career_raw %&gt;% \n  select(player_id, player_name, player_display_name, position, position_group, team, headshot_url) %&gt;% \n  unique() %&gt;% \n  group_by(player_id) %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\nnfl_actualStats_defense_career &lt;- nfl_actualStats_defense_career_raw_playerInfo %&gt;% \n  right_join(\n    nfl_actualStats_defense_career,\n    by = \"player_id\"\n  )\n\nnfl_actualStats_kicking_career &lt;- nfl_actualStats_kicking_career_raw_playerInfo %&gt;% \n  right_join(\n    nfl_actualStats_kicking_career,\n    by = \"player_id\"\n  )\n\n# Rearrange data\nnfl_actualStats_defense_career &lt;- nfl_actualStats_defense_career %&gt;% \n  arrange(player_display_name, player_id)\n\nnfl_actualStats_kicking_career &lt;- nfl_actualStats_kicking_career %&gt;% \n  arrange(player_display_name, player_id)\n\n\nLet’s check again for duplicate player instances:\n\nCodenfl_actualStats_offense_career %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_defense_career %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_kicking_career %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.4.1.2 Season-by-Season Statistics\nSecond, we can compute the players’ season-by-season statistics.\nTODO: Save/update data file in repo with the data generated from this code, and follow the data file through the rest of the book, updating code as necessary.\n\n\n\n\n\n\nNote 4.5: Downloading season-by-season statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_actualStats_seasonal_player_raw &lt;- calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  season_type = \"REG\")\n\nnfl_actualStats_seasonal_team_raw &lt;- calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  stat_type = \"team\",\n  season_type = \"REG\")\n\nnfl_actualStats_seasonal_player_inclPost_raw &lt;- calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  season_type = \"REG+POST\")\n\nnfl_actualStats_seasonal_team_inclPost_raw &lt;- calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  stat_type = \"team\",\n  season_type = \"REG+POST\")\n\n\n\nCodesave(\n  nfl_actualStats_seasonal_player_raw, nfl_actualStats_seasonal_team_raw,\n  nfl_actualStats_seasonal_player_inclPost_raw, nfl_actualStats_seasonal_team_inclPost_raw,\n  file = \"./data/nfl_actualStats_seasonal_raw.RData\"\n)\n\n\nA Data Dictionary for the variables is available in the nfl_stats_variables object that is returned when running the calculate_stats() function:\n\nCodenfl_stats_variables\n\n\n  \n\n\n\nThe nfl_actualStats_seasonal_player_raw object is in player-season form. That is, each row should be uniquely identified by the combination of player_id and season. The nfl_actualStats_seasonal_team_raw object is in team-season form. That is, each row should be uniquely identified by the combination of team and season. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_seasonal_player &lt;- nfl_actualStats_seasonal_player_raw %&gt;% \n  rename(team = recent_team) %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nnfl_actualStats_seasonal_team &lt;- nfl_actualStats_seasonal_team_raw %&gt;% \n  select(season, team, everything()) %&gt;% \n  arrange(season, team)\n\nnfl_actualStats_seasonal_player_inclPost &lt;- nfl_actualStats_seasonal_player_inclPost_raw %&gt;% \n  rename(team = recent_team) %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nnfl_actualStats_seasonal_team_inclPost &lt;- nfl_actualStats_seasonal_team_inclPost_raw %&gt;% \n  select(season, team, everything()) %&gt;% \n  arrange(season, team)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_actualStats_seasonal_player %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_seasonal_team %&gt;% \n  group_by(team, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_seasonal_player_inclPost %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_seasonal_team_inclPost %&gt;% \n  group_by(team, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.4.1.2.1 OLD (DELETE CODE BELOW)\n\nCodeseasons &lt;- unique(nfl_pbp$season)\n\nnfl_pbp_seasonalList &lt;- list()\nnfl_actualStats_offense_seasonalList &lt;- list()\nnfl_actualStats_defense_seasonalList &lt;- list()\nnfl_actualStats_kicking_seasonalList &lt;- list()\n\n\n\n\n\n\n\n\nNote 4.6: Calculating players’ season-by-season statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodepb &lt;- txtProgressBar(\n  min = 0,\n  max = length(seasons),\n  style = 3)\n\nfor(i in 1:length(seasons)){\n  # Subset play-by-play data by season\n  nfl_pbp_seasonalList[[i]] &lt;- nfl_pbp %&gt;% \n    dplyr::filter(season == seasons[i])\n  \n  # Compute actual statistics by season\n  nfl_actualStats_offense_seasonalList[[i]] &lt;- \n    nflfastR::calculate_player_stats(\n      nfl_pbp_seasonalList[[i]],\n      weekly = FALSE)\n  \n  nfl_actualStats_defense_seasonalList[[i]] &lt;- \n    nflfastR::calculate_player_stats_def(\n      nfl_pbp_seasonalList[[i]],\n      weekly = FALSE)\n  \n  nfl_actualStats_kicking_seasonalList[[i]] &lt;- \n    nflfastR::calculate_player_stats_kicking(\n      nfl_pbp_seasonalList[[i]],\n      weekly = FALSE)\n  \n  nfl_actualStats_offense_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualStats_defense_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualStats_kicking_seasonalList[[i]]$season &lt;- seasons[i]\n  \n  print(\n    paste(\"Completed computing projections for season: \", seasons[i], sep = \"\"))\n  \n  # Update the progress bar\n  setTxtProgressBar(pb, i)\n}\n\n# Close the progress bar\nclose(pb)\n\nnfl_actualStats_offense_seasonal_raw &lt;- nfl_actualStats_offense_seasonalList %&gt;% \n  dplyr::bind_rows()\nnfl_actualStats_defense_seasonal_raw &lt;- nfl_actualStats_defense_seasonalList %&gt;% \n  dplyr::bind_rows()\nnfl_actualStats_kicking_seasonal_raw &lt;- nfl_actualStats_kicking_seasonalList %&gt;% \n  dplyr::bind_rows()\n\n\n\nCodesave(\n  nfl_actualStats_offense_seasonal_raw, nfl_actualStats_defense_seasonal_raw, nfl_actualStats_kicking_seasonal_raw,\n  file = \"./data/nfl_actualStats_seasonal_raw.RData\"\n)\n\n\nThe nfl_actualStats_seasonal objects are in player-season form. That is, each row should be uniquely identified by the combination of player_id and season. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_offense_seasonal &lt;- nfl_actualStats_offense_seasonal_raw %&gt;% \n  rename(team = recent_team) %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nError: object 'nfl_actualStats_offense_seasonal_raw' not found\n\nCodenfl_actualStats_defense_seasonal &lt;- nfl_actualStats_defense_seasonal_raw %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nError: object 'nfl_actualStats_defense_seasonal_raw' not found\n\nCodenfl_actualStats_kicking_seasonal &lt;- nfl_actualStats_kicking_seasonal_raw %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nError: object 'nfl_actualStats_kicking_seasonal_raw' not found\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_actualStats_offense_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\nError: object 'nfl_actualStats_offense_seasonal' not found\n\nCodenfl_actualStats_defense_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\nError: object 'nfl_actualStats_defense_seasonal' not found\n\nCodenfl_actualStats_kicking_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\nError: object 'nfl_actualStats_kicking_seasonal' not found\n\n\nLet’s do some data cleanup:\n\nCode# Sum statistics across a player's years with multiple teams\nnfl_actualStats_defense_seasonal &lt;- nfl_actualStats_defense_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  summarise(\n    across(where(is.numeric), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\"\n  )\n\nError: object 'nfl_actualStats_defense_seasonal' not found\n\nCodenfl_actualStats_kicking_seasonal &lt;- nfl_actualStats_kicking_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  summarise(\n    across(where(is.numeric), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\"\n  )\n\nError: object 'nfl_actualStats_kicking_seasonal' not found\n\nCode# Re-calculate percentage stats\nnfl_actualStats_kicking_seasonal &lt;- nfl_actualStats_kicking_seasonal %&gt;% \n  mutate(\n    fg_pct = fg_made / fg_att,\n    pat_pct = pat_made / pat_att\n  )\n\nError: object 'nfl_actualStats_kicking_seasonal' not found\n\nCode# Merge data back with player info\nnfl_actualStats_defense_seasonal_raw_playerInfo &lt;- nfl_actualStats_defense_seasonal_raw %&gt;% \n  select(player_id, season, player_name, player_display_name, position, position_group, team, headshot_url) %&gt;% \n  unique() %&gt;% \n  group_by(player_id) %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\nError: object 'nfl_actualStats_defense_seasonal_raw' not found\n\nCodenfl_actualStats_kicking_seasonal_raw_playerInfo &lt;- nfl_actualStats_kicking_seasonal_raw %&gt;% \n  select(player_id, season, player_name, player_display_name, position, position_group, team, headshot_url) %&gt;% \n  unique() %&gt;% \n  group_by(player_id) %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\nError: object 'nfl_actualStats_kicking_seasonal_raw' not found\n\nCodenfl_actualStats_defense_seasonal &lt;- nfl_actualStats_defense_seasonal_raw_playerInfo %&gt;% \n  right_join(\n    nfl_actualStats_defense_seasonal,\n    by = c(\"player_id\",\"season\")\n  )\n\nError: object 'nfl_actualStats_defense_seasonal_raw_playerInfo' not found\n\nCodenfl_actualStats_kicking_seasonal &lt;- nfl_actualStats_kicking_seasonal_raw_playerInfo %&gt;% \n  right_join(\n    nfl_actualStats_kicking_seasonal,\n    by = c(\"player_id\",\"season\")\n  )\n\nError: object 'nfl_actualStats_kicking_seasonal_raw_playerInfo' not found\n\nCode# Rearrange data\nnfl_actualStats_defense_seasonal &lt;- nfl_actualStats_defense_seasonal %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nError: object 'nfl_actualStats_defense_seasonal' not found\n\nCodenfl_actualStats_kicking_seasonal &lt;- nfl_actualStats_kicking_seasonal %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nError: object 'nfl_actualStats_kicking_seasonal' not found\n\n\nLet’s check again for duplicate player instances:\n\nCodenfl_actualStats_offense_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\nError: object 'nfl_actualStats_offense_seasonal' not found\n\nCodenfl_actualStats_defense_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\nError: object 'nfl_actualStats_defense_seasonal' not found\n\nCodenfl_actualStats_kicking_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\nError: object 'nfl_actualStats_kicking_seasonal' not found\n\n\n\n4.4.1.3 Week-by-Week Statistics\nWe already load players’ week-by-week statistics above. Nevertheless, we could compute players’ weekly statistics from the play-by-play data using the following syntax:\nTODO:\n\nDecide whether to use calculate_stats() from nflfastR (Carl & Baldwin, 2024) or load_player_stats() from nflreadr (Ho & Carl, 2024).\nSave/update data file in repo with the data generated from the relevant code, and follow the data file through the rest of the book, updating code as necessary.\n\n\n\n\n\n\n\nNote 4.7: Downloading week-by-week statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_actualStats_weekly_player_raw &lt;- calculate_stats(\n  seasons = TRUE,\n  summary_level = \"week\")\n\nnfl_actualStats_weekly_team_raw &lt;- calculate_stats(\n  seasons = TRUE,\n  summary_level = \"week\",\n  stat_type = \"team\")\n\n\n\nCodesave(\n  nfl_actualStats_weekly_player_raw, nfl_actualStats_weekly_team_raw,\n  file = \"./data/nfl_actualStats_weekly_raw.RData\"\n)\n\n\nThe nfl_actualStats_weekly objects are in player-season-week form. That is, each row should be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_weekly_player &lt;- nfl_actualStats_weekly_player_raw %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualStats_weekly_team &lt;- nfl_actualStats_weekly_team_raw %&gt;% \n  select(season, week, team, everything()) %&gt;% \n  arrange(season, week, team)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualStats_weekly_player %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_weekly_team %&gt;% \n  group_by(team, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualStats_weekly_player, nfl_actualStats_weekly_team,\n  file = \"./data/nfl_actualStats_weekly.RData\"\n)\n\n\n\n4.4.1.3.1 OLD (DELETE CODE BELOW)\n\nCodenfl_actualStats_offense_weekly &lt;- nflfastR::calculate_player_stats(\n  nfl_pbp,\n  weekly = TRUE)\n\nnfl_actualStats_defense_weekly &lt;- nflfastR::calculate_player_stats_def(\n  nfl_pbp,\n  weekly = TRUE)\n\nnfl_actualStats_kicking_weekly &lt;- nflfastR::calculate_player_stats_kicking(\n  nfl_pbp,\n  weekly = TRUE)\n\n\n\n4.4.2 Historical Actual Fantasy Points\nSpecify scoring settings:\n\n4.4.2.1 Week-by-Week\n\nCodenfl_actualFantasyPoints_player_weekly_seasonalList &lt;- list()\nnfl_actualFantasyPoints_dst_weekly_seasonalList &lt;- list()\n\n\n\n\n\n\n\n\nNote 4.8: Calculating players’ week-by-week fantasy points\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCode#nfl_actualFantasyPoints_weekly_raw &lt;- ffanalytics:::actual_points_scoring(\n#  season = 2023,\n#  summary_level = c(\"week\"),\n#  stat_type = c(\"player\", \"dst\", \"team\"),\n#  season_type = c(\"REG\", \"POST\", \"REG+POST\"),\n#  scoring_rules = scoring_obj,\n#  vor_baseline = NULL,\n#  rename_colums = FALSE\n#)\n\npb &lt;- txtProgressBar(\n  min = 0,\n  max = length(seasons),\n  style = 3)\n\nfor(i in 1:length(seasons)){\n  # Compute actual statistics by season\n  nfl_actualFantasyPoints_player_weekly_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"week\"),\n      stat_type = c(\"player\"),\n      #season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_dst_weekly_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"week\"),\n      stat_type = c(\"dst\"),\n      #season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_player_weekly_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualFantasyPoints_dst_weekly_seasonalList[[i]]$season &lt;- seasons[i]\n  \n  print(\n    paste(\"Completed computing actual fantasy points for season: \", seasons[i], sep = \"\"))\n  \n  # Update the progress bar\n  setTxtProgressBar(pb, i)\n}\n\n# Close the progress bar\nclose(pb)\n\nnfl_actualFantasyPoints_player_weekly_raw &lt;- nfl_actualFantasyPoints_player_weekly_seasonalList %&gt;% \n  dplyr::bind_rows()\n\nnfl_actualFantasyPoints_dst_weekly_raw &lt;- nfl_actualFantasyPoints_dst_weekly_seasonalList %&gt;% \n  dplyr::bind_rows()\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_weekly_raw, nfl_actualFantasyPoints_dst_weekly_raw,\n  file = \"./data/nfl_actualFantasyPoints_weekly_raw.RData\"\n)\n\n\nThe nfl_actualFantasyPoints_weekly objects are in player-season-week (or team-season-week) form. That is, each row should be uniquely identified by the combination of player_id, season, and week (or team-season-week). Let’s rearrange the data accordingly:\n\nCodenfl_actualFantasyPoints_player_weekly &lt;- nfl_actualFantasyPoints_player_weekly_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualFantasyPoints_dst_weekly &lt;- nfl_actualFantasyPoints_dst_weekly_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(season, week, team, everything()) %&gt;% \n  arrange(season, week, team)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualFantasyPoints_player_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualFantasyPoints_dst_weekly %&gt;% \n  group_by(team, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_weekly, nfl_actualFantasyPoints_dst_weekly,\n  file = \"./data/nfl_actualFantasyPoints_weekly.RData\"\n)\n\n\n\n4.4.2.1.1 OLD (DELETE CODE BELOW)\n\nCodenfl_actualStats_offense_weekly\nnfl_actualStats_defense_weekly\nnfl_actualStats_kicking_weekly\n\ntest_weekly_offense &lt;- ffanalytics:::actual_points_scoring(\n  nflr_player_stats_df = nfl_actualStats_offense_weekly %&gt;% filter(season == 2023),\n  nflr_pbp_data = nfl_pbp %&gt;% filter(season == 2023),\n  scoring_rules = scoring_obj,\n  vor_baseline = NULL,\n  rename_colums = TRUE)\n\ntest_weekly_defense &lt;- ffanalytics:::actual_points_scoring(\n  nflr_player_stats_df = nfl_actualStats_defense_weekly %&gt;% filter(season == 2023),\n  nflr_pbp_data = nfl_pbp %&gt;% filter(season == 2023),\n  scoring_rules = scoring_obj,\n  vor_baseline = NULL,\n  rename_colums = TRUE)\n\ntest_weekly_kicking &lt;- ffanalytics:::actual_points_scoring(\n  nflr_player_stats_df = nfl_actualStats_kicking_weekly %&gt;% filter(season == 2023),\n  nflr_pbp_data = nfl_pbp %&gt;% filter(season == 2023),\n  scoring_rules = scoring_obj,\n  vor_baseline = NULL,\n  rename_colums = TRUE)\n\n\n\n4.4.2.2 Season-by-Season\n\nCodenfl_actualFantasyPoints_player_seasonal_seasonalList &lt;- list()\nnfl_actualFantasyPoints_dst_seasonal_seasonalList &lt;- list()\n\n\n\n\n\n\n\n\nNote 4.9: Calculating players’ season-by-season fantasy points\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCode#nfl_actualFantasyPoints_seasonal_raw &lt;- ffanalytics:::actual_points_scoring(\n#  season = 2023,\n#  summary_level = c(\"season\"),\n#  stat_type = c(\"player\", \"dst\", \"team\"),\n#  season_type = c(\"REG\"),\n#  scoring_rules = scoring_obj,\n#  vor_baseline = NULL,\n#  rename_colums = TRUE\n#)\n\npb &lt;- txtProgressBar(\n  min = 0,\n  max = length(seasons),\n  style = 3)\n\nfor(i in 1:length(seasons)){\n  # Compute actual statistics by season\n  nfl_actualFantasyPoints_player_seasonal_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"season\"),\n      stat_type = c(\"player\"),\n      season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_dst_seasonal_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"season\"),\n      stat_type = c(\"dst\"),\n      season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_player_seasonal_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualFantasyPoints_dst_seasonal_seasonalList[[i]]$season &lt;- seasons[i]\n  \n  print(\n    paste(\"Completed computing actual fantasy points for season: \", seasons[i], sep = \"\"))\n  \n  # Update the progress bar\n  setTxtProgressBar(pb, i)\n}\n\n# Close the progress bar\nclose(pb)\n\nnfl_actualFantasyPoints_player_seasonal_raw &lt;- nfl_actualFantasyPoints_player_seasonal_seasonalList %&gt;% \n  dplyr::bind_rows()\n\nnfl_actualFantasyPoints_dst_seasonal_raw &lt;- nfl_actualFantasyPoints_dst_seasonal_seasonalList %&gt;% \n  dplyr::bind_rows()\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_seasonal_raw, nfl_actualFantasyPoints_dst_seasonal_raw,\n  file = \"./data/nfl_actualFantasyPoints_seasonal_raw.RData\"\n)\n\n\nThe nfl_actualFantasyPoints_seasonal objects are in player-season (or team-season) form. That is, each row should be uniquely identified by the combination of player_id and season (or team-season). Let’s rearrange the data accordingly:\n\nCodenfl_actualFantasyPoints_player_seasonal &lt;- nfl_actualFantasyPoints_player_seasonal_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nnfl_actualFantasyPoints_dst_seasonal &lt;- nfl_actualFantasyPoints_dst_seasonal_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(season, team, everything()) %&gt;% \n  arrange(season, team)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualFantasyPoints_player_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualFantasyPoints_dst_seasonal %&gt;% \n  group_by(team, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_seasonal, nfl_actualFantasyPoints_dst_seasonal,\n  file = \"./data/nfl_actualFantasyPoints_seasonal.RData\"\n)\n\n\n\n4.4.2.3 Career\n\nCodenfl_actualFantasyPoints_player_career_raw &lt;- nfl_actualFantasyPoints_player_seasonal %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(fantasyPoints = sum(fantasyPoints, na.rm = TRUE))\n\nnfl_actualFantasyPoints_player_career_raw &lt;- full_join(\n  nfl_actualFantasyPoints_player_career_raw,\n  nfl_players %&gt;% select(gsis_id, display_name, first_name, last_name, position_group, position),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_career,\n  file = \"./data/nfl_actualFantasyPoints_career_raw.RData\"\n)\n\n\n\nCodenfl_actualFantasyPoints_player_career &lt;- nfl_actualFantasyPoints_player_career_raw %&gt;% \n  select(player_id, display_name, first_name, last_name, position_group, position, fantasyPoints)\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_career,\n  file = \"./data/nfl_actualFantasyPoints_career.RData\"\n)\n\n\n\n4.4.3 Player Age and Experience\n\n4.4.3.1 Weekly\nWe calculate the player’s age based on the difference between dates using the lubridate package (Spinu et al., 2024):\n\nCode# Reshape from wide to long format\nnfl_actualFantasyPoints_player_weekly_long &lt;- nfl_actualFantasyPoints_player_weekly %&gt;% \n  tidyr::pivot_longer(\n    cols = c(team, opponent_team),\n    names_to = \"role\",\n    values_to = \"team\")\n\n# Perform separate inner join operations for the home_team and away_team\nnfl_actualFantasyPoints_player_weekly_home &lt;- dplyr::inner_join(\n  nfl_actualFantasyPoints_player_weekly_long,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"home_team\")) %&gt;% \n  mutate(home_away = \"home_team\")\n\nnfl_actualFantasyPoints_player_weekly_away &lt;- dplyr::inner_join(\n  nfl_actualFantasyPoints_player_weekly_long,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"away_team\")) %&gt;% \n  mutate(home_away = \"away_team\")\n\n# Combine the results of the join operations\nnfl_actualFantasyPoints_player_weekly_schedules_long &lt;- dplyr::bind_rows(\n  nfl_actualFantasyPoints_player_weekly_home,\n  nfl_actualFantasyPoints_player_weekly_away)\n\n# Reshape from long to wide\nplayer_game_gameday &lt;- nfl_actualFantasyPoints_player_weekly_schedules_long %&gt;%\n  dplyr::distinct(player_id, season, week, game_id, home_away, team, gameday) %&gt;% #, .keep_all = TRUE\n  tidyr::pivot_wider(\n    names_from = home_away,\n    values_from = team)\n\n# Merge player birthdate and the game date\nplayer_game_birthdate_gameday &lt;- dplyr::left_join(\n  player_game_gameday,\n  unique(nfl_players[,c(\"gsis_id\",\"birth_date\")]),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\nplayer_game_birthdate_gameday$birth_date &lt;- lubridate::ymd(player_game_birthdate_gameday$birth_date)\nplayer_game_birthdate_gameday$gameday &lt;- lubridate::ymd(player_game_birthdate_gameday$gameday)\n\n# Calculate player's age for a given week as the difference between their birthdate and the game date\nplayer_game_birthdate_gameday$age &lt;- lubridate::interval(\n  start = player_game_birthdate_gameday$birth_date,\n  end = player_game_birthdate_gameday$gameday\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\n# Merge with Pro Football Reference Data on Player Age by Season\nplayer_game_birthdate_gameday &lt;- player_game_birthdate_gameday %&gt;% \n  dplyr::left_join(\n    nfl_advancedStatsPFR_seasonal %&gt;% filter(!is.na(gsis_id), !is.na(season), !is.na(age)) %&gt;% select(gsis_id, season, age) %&gt;% unique(),\n    by = c(\"player_id\" = \"gsis_id\", \"season\")\n  )\n\n# Set age as first non-missing value from calculation above or from PFR\nplayer_game_birthdate_gameday &lt;- player_game_birthdate_gameday %&gt;% \n  mutate(age = coalesce(age.x, age.y)) %&gt;% \n  select(-age.x, -age.y)\n\n# Calculate ageCentered and ageCenteredQuadratic\nplayer_game_birthdate_gameday$ageCentered20 &lt;- player_game_birthdate_gameday$age - 20\nplayer_game_birthdate_gameday$ageCentered20Quadratic &lt;- player_game_birthdate_gameday$ageCentered20 ^ 2\n\n# Merge with player info\nplayer_age &lt;- dplyr::left_join(\n  player_game_birthdate_gameday,\n  nfl_players %&gt;% select(-birth_date, -team_abbr, - team_seq),\n  by = c(\"player_id\" = \"gsis_id\"))\n\n# Add game_id to weekly stats to facilitate merging\nnfl_actualFantasyPoints_player_weekly &lt;- nfl_actualFantasyPoints_player_weekly %&gt;% \n  dplyr::left_join(\n    player_age[,c(\"season\",\"week\",\"player_id\",\"game_id\")],\n    by = c(\"season\",\"week\",\"player_id\"))\n\n# Merge with player weekly stats\nplayer_stats_weekly &lt;- dplyr::full_join(\n  player_age %&gt;% select(-position, -position_group),\n  nfl_actualFantasyPoints_player_weekly,\n  by = c(\"season\",\"week\",\"player_id\",\"game_id\"))\n\nplayer_stats_weekly$total_years_of_experience &lt;- as.integer(player_stats_weekly$years_of_experience)\n\nplayer_stats_weekly$years_of_experience &lt;- NULL\n\ndistinct_seasons &lt;- player_stats_weekly %&gt;%\n  dplyr::select(player_id, season) %&gt;%\n  dplyr::distinct() %&gt;% \n  dplyr::left_join(\n    nfl_players[,c(\"gsis_id\",\"years_of_experience\")],\n    by = c(\"player_id\" = \"gsis_id\")\n  ) %&gt;% \n  dplyr::mutate(total_years_of_experience = as.integer(years_of_experience)) %&gt;% \n  dplyr::select(-years_of_experience)\n\nyears_of_experience &lt;- distinct_seasons %&gt;% \n  dplyr::arrange(player_id, -season) %&gt;% \n  dplyr::group_by(player_id) %&gt;%\n  dplyr::mutate(years_of_experience = first(total_years_of_experience) - (row_number() - 1)) %&gt;%\n  dplyr::ungroup()\n\nyears_of_experience$years_of_experience[which(years_of_experience$years_of_experience &lt; 0)] &lt;- 0\n\nplayer_stats_weekly &lt;- player_stats_weekly %&gt;% \n  dplyr::left_join(\n    years_of_experience[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\n\nThe player_stats_weekly objects are in player-season-week form. That is, each row should be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodeplayer_stats_weekly &lt;- player_stats_weekly %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodeplayer_stats_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCode# Save data\nsave(\n  player_stats_weekly,\n  file = \"./data/player_stats_weekly.RData\"\n)\n\n\n\n4.4.3.2 Seasonal\n\nCode# Merge player info with seasonal stats\nplayer_stats_seasonal &lt;- dplyr::full_join(\n  nfl_actualFantasyPoints_player_seasonal,\n  nfl_players %&gt;% select(-position, -position_group, -team_abbr, - team_seq),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\n# Calculate age\nseason_startdate &lt;- nfl_schedules %&gt;% \n  dplyr::group_by(season) %&gt;% \n  dplyr::summarise(startdate = min(gameday, na.rm = TRUE))\n\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  dplyr::left_join(\n    season_startdate,\n    by = \"season\"\n  )\n\nplayer_stats_seasonal$age &lt;- lubridate::interval(\n  start = player_stats_seasonal$birth_date,\n  end = player_stats_seasonal$startdate\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\n# Merge with Pro Football Reference Data on Player Age by Season\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  dplyr::left_join(\n    nfl_advancedStatsPFR_seasonal %&gt;% filter(!is.na(gsis_id), !is.na(season), !is.na(age)) %&gt;% select(gsis_id, season, age) %&gt;% unique(),\n    by = c(\"player_id\" = \"gsis_id\", \"season\")\n  )\n\n# Set age as first non-missing value from calculation above or from PFR\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  mutate(age = coalesce(age.x, age.y)) %&gt;% \n  select(-age.x, -age.y)\n\n# Calculate ageCentered and ageCenteredQuadratic\nplayer_stats_seasonal$ageCentered20 &lt;- player_stats_seasonal$age - 20\nplayer_stats_seasonal$ageCentered20Quadratic &lt;- player_stats_seasonal$ageCentered20 ^ 2\n\n# Years of experience\nplayer_stats_seasonal$years_of_experience &lt;- NULL\n\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  dplyr::left_join(\n    years_of_experience[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\n\nThe player_stats_seasonal objects are in player-season form. That is, each row should be uniquely identified by the combination of player_id and season. Let’s rearrange the data accordingly:\n\nCodeplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\n\nLet’s check for duplicate player-season instances:\n\nCodeplayer_stats_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\nCode# Save data\nsave(\n  player_stats_seasonal,\n  file = \"./data/player_stats_seasonal.RData\"\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-downloadFootballDataSessionInfo",
    "href": "download-football-data.html#sec-downloadFootballDataSessionInfo",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n4.5 Session Info",
    "text": "4.5 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_1.0.0          stringr_1.5.1          dplyr_1.1.4           \n [4] purrr_1.0.4            readr_2.1.5            tidyr_1.3.1           \n [7] tibble_3.2.1           ggplot2_3.5.1          tidyverse_2.0.0       \n[10] lubridate_1.9.4        progressr_0.15.1       nflplotR_1.4.0        \n[13] nfl4th_1.0.4           nflfastR_5.0.0         nflreadr_1.4.1        \n[16] petersenlab_1.1.1      ffanalytics_3.1.4.0000\n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.3          mnormt_2.1.1       gridExtra_2.3      httr2_1.1.1       \n [5] readxl_1.4.5       rlang_1.1.5        magrittr_2.0.3     snakecase_0.11.1  \n [9] furrr_0.3.1        compiler_4.4.3     mgcv_1.9-1         vctrs_0.6.5       \n[13] reshape2_1.4.4     quadprog_1.5-8     rvest_1.0.4        pkgconfig_2.0.3   \n[17] fastmap_1.2.0      backports_1.5.0    pbivnorm_0.6.0     promises_1.3.2    \n[21] rmarkdown_2.29     tzdb_0.5.0         ps_1.9.0           xfun_0.51         \n[25] cachem_1.1.0       jsonlite_1.9.1     later_1.4.1        rrapply_1.2.7     \n[29] psych_2.5.3        parallel_4.4.3     lavaan_0.6-19      cluster_2.1.8     \n[33] R6_2.6.1           stringi_1.8.4      RColorBrewer_1.1-3 parallelly_1.42.0 \n[37] rpart_4.1.24       cellranger_1.1.0   xgboost_1.7.8.1    Rcpp_1.0.14       \n[41] knitr_1.50         base64enc_0.1-3    Matrix_1.7-2       splines_4.4.3     \n[45] nnet_7.3-20        timechange_0.3.0   tidyselect_1.2.1   rstudioapi_0.17.1 \n[49] yaml_2.3.10        codetools_0.2-20   websocket_1.4.2    curl_6.2.1        \n[53] processx_3.8.6     listenv_0.9.1      lattice_0.22-6     plyr_1.8.9        \n[57] withr_3.0.2        evaluate_1.0.3     foreign_0.8-88     future_1.34.0     \n[61] xml2_1.3.8         pillar_1.10.1      checkmate_2.3.2    stats4_4.4.3      \n[65] generics_0.1.3     chromote_0.5.0     hms_1.1.3          mix_1.0-13        \n[69] munsell_0.5.1      scales_1.3.0       globals_0.16.3     xtable_1.8-4      \n[73] glue_1.8.0         janitor_2.2.1      Hmisc_5.2-3        tools_4.4.3       \n[77] data.table_1.17.0  mvtnorm_1.3-3      grid_4.4.3         mitools_2.4       \n[81] colorspace_2.1-1   nlme_3.1-167       htmlTable_2.4.3    Formula_1.2-5     \n[85] cli_3.6.4          rappdirs_0.3.3     viridisLite_0.4.2  gt_0.11.1         \n[89] gtable_0.3.6       fastrmodels_1.0.2  digest_0.6.37      htmlwidgets_1.6.4 \n[93] memoise_2.0.1      htmltools_0.5.8.1  lifecycle_1.0.4    httr_1.4.7        \n\n\n\n\n\n\nBaldwin, B. (2023). nfl4th: Functions to calculate optimal fourth down decisions in the National Football League. https://www.nfl4th.com/\n\n\nBengtsson, H. (2024). progressr: An inclusive, unifying API for progress updates. https://progressr.futureverse.org\n\n\nCarl, S., & Baldwin, B. (2024). nflfastR: Functions to efficiently access NFL play by play data. https://www.nflfastr.com/\n\n\nCongelio, B. J. (2023). Introduction to NFL analytics with R. CRC Press. https://bradcongelio.com/nfl-analytics-with-r-book\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nHo, T., & Carl, S. (2024). nflreadr: Download nflverse data. https://nflreadr.nflverse.com\n\n\nHo, T., & Carl, S. (2025a). Articles. https://nflreadr.nflverse.com/articles/index.html\n\n\nHo, T., & Carl, S. (2025b). Data dictionary - combine. https://nflreadr.nflverse.com/articles/dictionary_combine.html\n\n\nHo, T., & Carl, S. (2025c). Data dictionary - contracts. https://nflreadr.nflverse.com/articles/dictionary_contracts.html\n\n\nHo, T., & Carl, S. (2025d). Data dictionary - depth charts. https://nflreadr.nflverse.com/articles/dictionary_depth_charts.html\n\n\nHo, T., & Carl, S. (2025e). Data dictionary - draft picks. https://nflreadr.nflverse.com/articles/dictionary_draft_picks.html\n\n\nHo, T., & Carl, S. (2025f). Data dictionary - ESPN QBR. https://nflreadr.nflverse.com/articles/dictionary_espn_qbr.html\n\n\nHo, T., & Carl, S. (2025g). Data dictionary - FF opportunity. https://nflreadr.nflverse.com/articles/dictionary_ff_opportunity.html\n\n\nHo, T., & Carl, S. (2025h). Data dictionary - FF player IDs. https://nflreadr.nflverse.com/articles/dictionary_ff_playerids.html\n\n\nHo, T., & Carl, S. (2025i). Data dictionary - FF rankings. https://nflreadr.nflverse.com/articles/dictionary_ff_rankings.html\n\n\nHo, T., & Carl, S. (2025j). Data dictionary - FTN charting. https://nflreadr.nflverse.com/articles/dictionary_ftn_charting.html\n\n\nHo, T., & Carl, S. (2025k). Data dictionary - injuries. https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n\n\nHo, T., & Carl, S. (2025l). Data dictionary - next gen stats. https://nflreadr.nflverse.com/articles/dictionary_nextgen_stats.html\n\n\nHo, T., & Carl, S. (2025m). Data dictionary - participation. https://nflreadr.nflverse.com/articles/dictionary_participation.html\n\n\nHo, T., & Carl, S. (2025n). Data dictionary - PBP. https://nflreadr.nflverse.com/articles/dictionary_pbp.html\n\n\nHo, T., & Carl, S. (2025o). Data dictionary - PFR passing. https://nflreadr.nflverse.com/articles/dictionary_pfr_passing.html\n\n\nHo, T., & Carl, S. (2025p). Data dictionary - player stats. https://nflreadr.nflverse.com/articles/dictionary_player_stats.html\n\n\nHo, T., & Carl, S. (2025q). Data dictionary - player stats defense. https://nflreadr.nflverse.com/articles/dictionary_player_stats_def.html\n\n\nHo, T., & Carl, S. (2025r). Data dictionary - rosters. https://nflreadr.nflverse.com/articles/dictionary_rosters.html\n\n\nHo, T., & Carl, S. (2025s). Data dictionary - schedules. https://nflreadr.nflverse.com/articles/dictionary_schedules.html\n\n\nHo, T., & Carl, S. (2025t). Data dictionary - snap counts. https://nflreadr.nflverse.com/articles/dictionary_snap_counts.html\n\n\nPro Football Reference. (2024). 2024 NFL advanced stats. https://www.pro-football-reference.com/years/2024/advanced.htm\n\n\nPro Football Reference. (2025). About our advanced stats. https://www.pro-football-reference.com/about/advanced_stats.htm\n\n\nSharpe, L. (2020a). NFL data sets. https://github.com/nflverse/nfldata/blob/master/DATASETS.md\n\n\nSharpe, L. (2020b). NFL data sets - draft values. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#draft_values\n\n\nSharpe, L. (2020c). NFL data sets - rosters. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#rosters\n\n\nSharpe, L. (2020d). NFL data sets - standings. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#standings\n\n\nSharpe, L. (2020e). NFL data sets - trades. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#trades\n\n\nSpinu, V., Grolemund, G., & Wickham, H. (2024). lubridate: Make dealing with dates a little easier. https://lubridate.tidyverse.org",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "data-visualization.html",
    "href": "data-visualization.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationGettingStarted",
    "href": "data-visualization.html#sec-dataVisualizationGettingStarted",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1.1 Load Packages\n\nCodelibrary(\"nflplotR\")\nlibrary(\"plotly\")\nlibrary(\"gghighlight\")\nlibrary(\"ggridges\")\nlibrary(\"ggExtra\")\nlibrary(\"tidyverse\")\n\n\n\n5.1.2 Load Data\n\nCodeload(file = \"./data/nfl_pbp.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationOverview",
    "href": "data-visualization.html#sec-dataVisualizationOverview",
    "title": "5  Data Visualization",
    "section": "\n5.2 Overview",
    "text": "5.2 Overview\n\n5.2.1 Principles of Graphic Design\nWhen designing graphics, it is important to understand general principles of graphic design. Adobe Express (2020) describes important principles about graphic design at the following link: https://www.adobe.com/express/learn/blog/8-basic-design-principles-to-help-you-create-better-graphics (archived at https://perma.cc/29P9-NNSK). The important principles include:\n\n\nFocus on alignment.\nUse hierarchy to help focus your design.\nLeverage contrast to accentuate important design elements.\nUse repetition to your advantage.\nConsider proximity when organizing your graphic elements.\nMake sure your designs have balance.\nOptimize color to support your design.\nLeave negative space.\n\n\n\n5.2.2 Principles of Data Visualization\nData visualization involves graphic design in a particular domain—the visualization of data (numeric-derived information). Schwabish (2021) describes five principles in data visualization:\n\n\nShow the data.\nReduce the clutter.\nIntegrate the graphics and text.\nAvoid the spaghetti chart.\nStart with gray.\n\n\n“Showing the data” involves showing the data that matters the most. “Reducing the clutter” involves removing non-data things that obscure the data—for example, extraneous gridlines, tick marks, data markers (e.g., symbols to distinguish between series), and complex shadings (e.g., textured or filled gradients). “Integrating the graphics and text” involves using headline titles, clear and useful labels (instead of legends), and helpful annotations. Headline or newspaper-like titles are titles that are succinct with active phasing and that indicate the take-away message (e.g., “Quarterbacks Threw Fewer Touchdowns in 2024 than in Previous Years”). In terms of labels, Schwabish (2021) advocates to label the data directly instead of using a legend. In terms of helpful annotations, you can provide additional text that helps explain the data (e.g., peaks or valleys, outliers, or other variations that deserve explanation), including how to interpret the chart. “Avoiding the spaghetti chart” means avoiding packed charts with too much information that makes them difficult to interpret. Spaghetti charts are lines with many lines that, make the plot look like a bunch of spaghetti. However, Schwabish (2021) also advocates against using charts of other types that are complicated and difficult to interpret due to too much information, such as complicated maps or bar plots with too many colors, icons, or bars. If there are too many lines or series, Schwabish (2021) advocates breaking it up into multiple charts (i.e., facets, trellis charts, or small multiples). An example of faceted charts is depicted in Figure 5.27. “Starting with gray” refers to the idea of using gray as the default color for most lines/points/bars, so that you can use a color to highlight the lines/points/bars of interest. In addition, as noted by Schwabish (2021), it is important to treat data as objectively as possible and not to present figures in a biased way as to mislead.\nIn his classic book, Tufte (2001) states that effective data visualizations should follow principles of graphical excellence and integrity. He notes that “Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.” (p. 51). That is, data visualizations should seek to maximize the data-to-ink ratio (within reason), and should spend less space on “fluff” (i.e., non-data things that can be erased without losing meaning, such as grid lines, redundancies, etc.). This is consistent with Schwabish’s (2021) principles of showing the data and reducing the clutter. Tufte (2001) describes six principles of graphical integrity:\n\n\nThe representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the numerical quantities represented.\nClear, detailed, and thorough labeling should be used to defeat graphic distortion and ambiguity. Write out explanations of the data on the graphic itself. Label important events in the data.\nShow data variation, not design variation.\nIn time-series displays of money, deflated and standardized units of monetary measurement are nearly always better than nominal units.\nThe number of information-carrying (variable) dimensions depicted should not exceed the number of dimensions in the data.\nGraphics must not quote data out of context.\n\n— Tufte (2001, p. 77)\n\nTufte (2001) also provides recommendations for friendly, accessible graphics, including:\n\nspell words out (rather than using abbreviations)\nhave words run from left to write (including the y-axis title)\ninclude little messages to help explain the data\nplace labels on the graphic so no legend is needed\navoid elaborately encoded shadings, cross-hatching, and colors\navoid “chartjunk”—i.e., unnecessary or distracting elements (e.g., excessive decoration, overly complex graphics, graphical effects, and irrelevant information such as moiré vibration, heavy grids, and self-promoting graphs) that do not improve viewers’ understanding of the data\nif colors are used, use colors that are distinguishable by color-deficient and color-blind viewers (red–green is a common form of color-blindness)\nuse type (i.e., of the text) that is clear, precise, and modest\nuse text that is upper-and-lower case, not all capitals\n\nAn example figure that applies these principles of data visualization is in Figure 5.1.\n\nCodeconfidenceLevel &lt;- .95 # for 95% confidence interval\n\nplayer_stats_seasonal_offense_summary &lt;- player_stats_seasonal %&gt;%\n  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  group_by(position_group) %&gt;%\n  summarise(\n    n = sum(!is.na(fantasyPoints)),\n    mean = mean(fantasyPoints, na.rm = TRUE),\n    sd = sd(fantasyPoints, na.rm = TRUE)\n  ) %&gt;%\n  mutate(se = sd/sqrt(n)) %&gt;%\n  mutate(\n    ci_lower = mean - qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se,\n    ci_upper = mean + qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se,\n    positionLabel = case_match(\n      position_group,\n      \"QB\" ~ \"Quarterback\",\n      \"RB\" ~ \"Running Back\",\n      \"WR\" ~ \"Wide Receiver\",\n      \"TE\" ~ \"Tight End\"\n      )\n  )\n\nggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = positionLabel,\n    y = mean,\n    fill = positionLabel\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  geom_errorbar(\n    aes(\n      ymin = ci_lower,\n      ymax = ci_upper),\n    width = 0.2,\n    color = \"black\"\n  ) +\n  gghighlight::gghighlight(\n    positionLabel == \"Quarterback\",\n    label_key = positionLabel) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Quarterbacks Score More Fantasy Points than Other Positions\"\n  ) +\n  annotate(\n    \"segment\",\n    x = 3.5,\n    xend = 3.2,\n    y = 70,\n    yend = 35,\n    color = \"blue\",\n    linewidth = 1.5,\n    alpha = 0.6,\n    arrow = arrow()) +\n  annotate(\n    \"text\",\n    x = 2.75,\n    y = 75,\n    label = \"Tight Ends score fewer fantasy\\npoints than other positions\",\n    hjust = 0) + # left-justify\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.1: Example Figure that Applies the Principles of Data Visualization.\n\n\n\n\n\n5.2.3 Creating Data Visualizations in R\n\nThe Data Visualization Catalogue provides examples of various types of plots depending on one’s goal: https://datavizcatalogue.com/search.html. The R Graph Gallery provides examples of various types of plots and how to create them in R: https://r-graph-gallery.com. Books on data visualization in R include ggplot2: Elegant Graphics for Data Analysis (Wickham, 2024) and R Graphics Cookbook: Practical Recipes for Visualizing Data (Chang, 2018). In this chapter, we will examine how to create statistical graphics to visualize data. We will create the plots using the ggplot2 package (Wickham et al., 2024; Wickham, 2024). When creating plots in ggplot2 with multiple points or lines (e.g., multiple players or levels of a predictor variable), it is easiest to do so with the data in long form (as opposed to wide form).\nA key principle of graphic design and data visualization is the importance of contrast. Each visual component (e.g., line) that is important to see should be easy to distinguish. For instance, you can highlight lines or points of interest to draw people’s attention to the target of interest (Schwabish, 2021). For examples of highlighting in figures, see Figures 5.23 (Section 5.5.1) and 14.4.\nIt is also important to use color schemes with distinguishable colors. Good color schemes for sequential, diverging, and qualitative (i.e., categorical) data are provided by ColorBrewer (https://colorbrewer2.org) and are available using the scale_color_brewer() and scale_fill_brewer() functions of the ggplot2 package (Wickham et al., 2024), as demonstrated in Figure 5.26 (Section 5.6.2). There are a variety of resources for color schemes that are accessible to color-blind viewers:\n\n\nhttps://www.datylon.com/blog/data-visualization-for-colorblind-readers [Kilin (2022); archived at https://perma.cc/7VTA-Y8YS]\nthe viridis package (Garnier, 2024; Garnier et al., 2024)\n\n\nhttps://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html (archived at https://perma.cc/NK9K-HL7L)\n\n\nNuñez et al. (2018)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-univariateDistribution",
    "href": "data-visualization.html#sec-univariateDistribution",
    "title": "5  Data Visualization",
    "section": "\n5.3 Univariate Distribution",
    "text": "5.3 Univariate Distribution\nScherer (2021) describes various ways of visualizing univariate distributions; see here (archived at https://perma.cc/EEJ8-LND2).\n\n5.3.1 Histogram\nA histogram of fantasy points is depicted in Figure 5.2.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Count\",\n    title = \"Histogram of Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.2: Histogram of Fantasy Points.\n\n\n\n\n\n5.3.2 Density Plot\nA histogram of fantasy points is depicted in Figure 5.3.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_density(alpha = 0.7) + # add transparency\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    fill = \"Position\",\n    title = \"Density Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.3: Density Plot of Fantasy Points by Position.\n\n\n\n\n\n5.3.3 Density Plot with Histogram and Rug Plot\nA density plot of fantasy points with a histogram and rug plot is depicted in Figure 5.4.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Fantasy Points with Histogram and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.4: Density Plot with Histogram and Rug Plot.\n\n\n\n\n\n5.3.4 Histogram with Overlaid Density and Rug Plot\nA histogram of fantasy points with an overlaid density and rug plot is depicted in Figure 5.5.\n\nCodebinWidth &lt;- 15\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    fill = \"#0099F8\",\n    binwidth = binWidth\n  ) +\n  geom_density(\n    aes(y = after_stat(count) * binWidth),\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Count\",\n    title = \"Histogram of Fantasy Points with Overlaid Density and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.5: Histogram with Overlaid Density and Rug Plot.\n\n\n\n\n\n5.3.5 Box-and-Whisker Plot\nIn a box-and-whisker plot, the box is created using the 1st and 3rd quartiles (i.e., the 25th and 75th percentiles, respectively). The length of the box is equal to the interquartile range, which is calculated as: \\(\\text{IQR} = Q_3 - Q_1\\), where \\(Q_3\\) and \\(Q_1\\) are the third and first quartiles, respectively. The line in the middle of the box is located at the median (i.e., the 2nd quartile or 50th percentile). The whiskers commonly extend \\(1.5 \\times \\text{IQR}\\) units from the box. That is, the upper whisker is commonly located at \\(1.5 \\times \\text{IQR}\\) units above the third quartile. The lower whisker is commonly located at \\(1.5 \\times \\text{IQR}\\) units below the first quartile. The points represent extreme values (i.e., outliers) that are outside the whiskers.\nA box-and-whisker plot of fantasy points is depicted in Figure 5.6.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = fantasyPoints,\n    fill = position_group)\n) +\n  geom_boxplot(staplewidth = 0.25) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Box-and-Whisker Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.6: Box-and-Whisker Plot.\n\n\n\n\n\n5.3.6 Violin Plot\nA violin plot of fantasy points is depicted in Figure 5.7.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = fantasyPoints,\n    fill = position_group)\n) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Violin Plot of Fantasy Points by Position\",\n    subtitle = \"Lines represent the 25th, 50th, and 75th quantiles\"\n  ) +\n  theme_classic() + \n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.7: Violin Plot. Lines represent the 25th, 50th, and 75th quantiles.\n\n\n\n\n\n5.3.7 Ridgeline Plot\nA ridgeline plot of fantasy points is depicted in Figure 5.8 using the ggridges package (Wilke, 2024).\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_group,\n    group = position_group,\n    fill = position_group)\n) +\n  ggridges::geom_density_ridges(\n    rel_min_height = 0.0085, # remove trailing tails\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Position\",\n    title = \"Ridgeline Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.8: Ridgeline Plot.\n\n\n\n\nWe can add lines at the quartiles, as depicted in Figure 5.9.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_group,\n    group = position_group,\n    fill = factor(after_stat(quantile)))\n) +\n  ggridges::stat_density_ridges(\n    rel_min_height = 0.0085, # remove trailing tails\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = 4,\n    quantile_lines = TRUE\n  ) +\n   scale_fill_viridis_d() + # use viridis color scheme\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Position\",\n    title = \"Ridgeline Plot of Fantasy Points by Position\",\n    subtitle = \"Vertical lines represent the 25th, 50th, and 75th quantiles\",\n    fill = \"Quartile\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.9: Ridgeline Plot. Lines represent the 25th, 50th, and 75th quantiles.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-scatterplot",
    "href": "data-visualization.html#sec-scatterplot",
    "title": "5  Data Visualization",
    "section": "\n5.4 Scatterplot",
    "text": "5.4 Scatterplot\nAs a tutorial, we walk through some of the (many) modifications that can be made to create an advanced, customized plot in ggplot2.\nFirst, we prepare the data:\n\nCode# Subset Data\nrb_seasonal &lt;- player_stats_seasonal %&gt;% \n  filter(position_group == \"RB\")\n\n\n\n5.4.1 Base Layer\nSecond, we create the base layer of the plot using the ggplot() function of the ggplot2 package (Wickham et al., 2024), as in Figure 5.10. We specify the data object and the variables in the data object that are associated with the x- and y-axes:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal, # specify data object\n  aes(\n    x = age, # specify variable on x-axis\n    y = rushing_yards)) # specify variable on y-axis\n\n\n\n\n\n\nFigure 5.10: Base Plot.\n\n\n\n\n\n5.4.2 Add Points\nThird, we create a scatterplot using the geom_point() function from the ggplot2 package (Wickham et al., 2024), as in Figure 5.11:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() # add points for scatterplot\n\n\n\n\n\n\nFigure 5.11: Scatterplot.\n\n\n\n\n\n5.4.3 Best-Fit Line\nFourth, we add a linear best-fit line using the geom_smooth(), as in Figure 5.12:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # add linear best-fit line\n\n\n\n\n\n\nFigure 5.12: Scatterplot with Linear Best-Fit Line.\n\n\n\n\nWe could also estimate a quadratic polynomial best-fit line, as in Figure 5.13:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    formula = y ~ poly(x, 2)) # add quadratic best-fit line\n\n\n\n\n\n\nFigure 5.13: Scatterplot with Quadratic Best-Fit Line.\n\n\n\n\nOr, we could estimate a smooth best-fit line using locally estimated scatterplot smoothing (LOESS) to allow for any form of nonlinearity, as in Figure 5.14:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") # add smooth best-fit (LOESS) line\n\n\n\n\n\n\nFigure 5.14: Scatterplot with Best-Fit Line Using Locally Estimated Scatterplot Smoothing (LOESS).\n\n\n\n\nBy default, the best-fit line is based on a generalized additive model, which allows for nonlinearity, as in Figure 5.15:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() # add GAM best-fit line; same as specifying method = \"gam\"\n\n\n\n\n\n\nFigure 5.15: Scatterplot with Best-Fit Line from Generalized Additive Model.\n\n\n\n\n\n5.4.4 Modify Axes\nThen, we can change the axes, as in Figure 5.16:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40), # set limits of x-axis\n    ylim = c(0,NA), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5) # specify x-axis labels\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250) # specify y-axis labels\n  )\n\n\n\n\n\n\nFigure 5.16: Scatterplot with Modified Axes.\n\n\n\n\n\n5.4.5 Plot Labels\nThen, we can add plot labels, as in Figure 5.17:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs( # add plot labels\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  )\n\n\n\n\n\n\nFigure 5.17: Scatterplot with Plot Labels.\n\n\n\n\n\n5.4.6 Theme\nThen, we can use a theme such as the classic theme (theme_classic()) to make it more visually presentable, as in Figure 5.18:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  ) +\n  theme_classic() # use the classic theme\n\n\n\n\n\n\nFigure 5.18: Scatterplot with Classic Theme.\n\n\n\n\nOr, we could use a different theme, such as the dark theme (theme_dark()) in Figure 5.19. For a list of themes available in ggplot2, see here: https://ggplot2-book.org/themes#sec-themes (Wickham, 2024).\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  ) +\n  theme_dark() # use the dark theme\n\n\n\n\n\n\nFigure 5.19: Scatterplot with Dark Theme.\n\n\n\n\n\n5.4.7 Interactive\nAfter creating our plot, we can make the plot interactive using the ggplotly() function from the plotly package (Sievert, 2020; Sievert et al., 2024), as in Figure 5.20.\n\nCodeplot_ypcByPlayerAge &lt;- ggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season)) + # add season for mouse over tooltip\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  ) +\n  theme_classic()\n\nggplotly(plot_ypcByPlayerAge)\n\n\n\n\n\n\nFigure 5.20: Interactive Scatterplot Using Plotly.\n\n\n\n\n5.4.8 Marginal Density/Histogram\nWe can also add a marginal density/histogram, as in Figure 5.21 using the ggExtra package (Attali & Baker, 2023):\n\nCodeggExtra::ggMarginal(\n  plot_ypcByPlayerAge,\n  type = \"densigram\")\n\n\n\n\n\n\nFigure 5.21: Scatterplot With Marginal Density/Histogram.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-lineChart",
    "href": "data-visualization.html#sec-lineChart",
    "title": "5  Data Visualization",
    "section": "\n5.5 Line Chart",
    "text": "5.5 Line Chart\nA bar plot of Tom Brady’s fantasy points by season is depicted in Figure 5.22.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name == \"Tom Brady\"),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints\n    )\n) +\n  geom_line(\n    linewidth = 1.5,\n    color = \"blue\"\n  ) +\n  labs(\n    x = \"Season\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Tom Brady's Fantasy Points by Season\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.22: Line Chart.\n\n\n\n\n\n5.5.1 With Highlighting\nWe use the gghighlight package (Yutani, 2023) to highlight particular elements:\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% \n    filter(position_group %in% c(\"QB\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_id,\n    color = player_display_name)\n) +\n  geom_line(linewidth = 2) +\n  gghighlight::gghighlight(\n    player_display_name == \"Tom Brady\",\n    label_key = player_display_name,\n    unhighlighted_params = list(linewidth = 0.5)) +\n  labs(\n    x = \"Season\",\n    y = \"Fantasy Points\",\n    title = \"Fantasy Points by Season and Player\",\n    subtitle = \"(Tom Brady in Red)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.23: Line Chart with Highlighting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-barPlot",
    "href": "data-visualization.html#sec-barPlot",
    "title": "5  Data Visualization",
    "section": "\n5.6 Bar Plot",
    "text": "5.6 Bar Plot\nTo create a bar plot, we first compute summary statistics:\n\nCodeconfidenceLevel &lt;- .95 # for 95% confidence interval\n\nplayer_stats_seasonal_offense_summary &lt;- player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  group_by(position_group) %&gt;%\n  summarise( \n    n = sum(!is.na(fantasyPoints)),\n    mean = mean(fantasyPoints, na.rm = TRUE),\n    sd = sd(fantasyPoints, na.rm = TRUE)\n  ) %&gt;%\n  mutate(se = sd/sqrt(n)) %&gt;%\n  mutate(\n    ci_lower = mean - qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se,\n    ci_upper = mean + qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se\n  )\n\n\nThe summary statistics are in Table 5.1.\n\nCodeplayer_stats_seasonal_offense_summary\n\n\nTable 5.1: Table of Summary Statistics.\n\n\n\n  \n\n\n\n\n\n\nA bar plot of fantasy points by position is depicted in Figure 5.24.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary,\n  mapping = aes(\n    x = position_group,\n    y = mean,\n    fill = position_group\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.24: Bar Plot.\n\n\n\n\n\n5.6.1 With Error Bars\nBased on the summary statistics in Table 5.1, we create a bar plot with bars representing the 95% confidence interval in Figure 5.25.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = mean,\n    fill = position_group\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  geom_errorbar(\n    aes(\n      ymin = ci_lower,\n      ymax = ci_upper),\n    width = 0.2,\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.25: Bar Plot with Bars Representing the 95% Confidence Interval.\n\n\n\n\n\n5.6.2 Modified Color Scheme\nWe can also modify the color scheme, as in Figure 5.26\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = mean,\n    fill = position_group\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  geom_errorbar(\n    aes(\n      ymin = ci_lower,\n      ymax = ci_upper),\n    width = 0.2,\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.26: Bar Plot with Bars Representing the 95% Confidence Interval.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-faceting",
    "href": "data-visualization.html#sec-faceting",
    "title": "5  Data Visualization",
    "section": "\n5.7 Faceting",
    "text": "5.7 Faceting\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% \n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  aes(\n    x = age,\n    y = fantasyPoints)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  labs(\n    x = \"Player's Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age\"\n  ) +\n  theme_bw() +\n  facet_wrap(vars(position_group)) # facet by position_group\n\n\n\n\n\n\nFigure 5.27: Faceted Scatterplot.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-scatterplotExamples",
    "href": "data-visualization.html#sec-scatterplotExamples",
    "title": "5  Data Visualization",
    "section": "\n5.8 Examples",
    "text": "5.8 Examples\n\n5.8.1 Players\n\n5.8.1.1 Running Back Performance By Player Age\n\nCode# Prepare Data\nrushing_attempts &lt;- nfl_pbp %&gt;% \n  dplyr::filter(season_type == \"REG\") %&gt;% \n  dplyr::filter(\n    rush == 1,\n    rush_attempt == 1,\n    qb_scramble == 0,\n    qb_dropback == 0,\n    !is.na(rushing_yards))\n\nrb_yardsPerCarry &lt;- rushing_attempts %&gt;% \n  dplyr::group_by(rusher_id, season) %&gt;% \n  dplyr::summarise(\n    ypc = mean(rushing_yards, na.rm = TRUE),\n    rush_attempts = n(),\n    .groups = \"drop\") %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(\n    player_stats_seasonal,\n    by = c(\"rusher_id\" = \"player_id\", \"season\")\n  ) %&gt;% \n  dplyr::filter(\n    position_group == \"RB\",\n    rush_attempts &gt;= 50)\n\n\n\n5.8.1.1.1 Rushing Yards Per Carry\nRushing yards per carry over the course of the season is depicted as a function of the Running Back’s age in Figure 5.28.\n\nCodeplot_ypcByPlayerAge2 &lt;- ggplot2::ggplot(\n  data = rb_yardsPerCarry,\n  aes(\n    x = age,\n    y = ypc)) +\n  geom_point(\n    aes(\n      text = player_display_name,\n      label = season)) +\n  geom_smooth() +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards Per Carry (Season)\",\n    title = \"NFL Rushing Yards Per Carry (Season) by Player Age\",\n    subtitle = \"(minimum 50 rushing attempts)\"\n  ) +\n  theme_classic()\n\nggplotly(plot_ypcByPlayerAge2)\n\n\n\n\n\n\nFigure 5.28: Rushing Yards Per Carry (Season) by Player Age.\n\n\n\n\n5.8.1.1.2 Rushing EPA Per Season\nRushing expected points added (EPA) over the course of the season is depicted as a function of the Running Back’s age in Figure 5.29.\n\nCodeplot_rushEPAbyPlayerAge &lt;- ggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_epa)) +\n  geom_point(\n    aes(\n      text = player_display_name,\n      label = season)) +\n  geom_smooth() +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing EPA (Season)\",\n    title = \"NFL Rushing Expected Points Added (Season) by Player Age\"\n  ) +\n  theme_classic()\n\nggplotly(plot_rushEPAbyPlayerAge)\n\n\n\n\n\n\nFigure 5.29: Rushing Expected Points Added (Season) by Player Age.\n\n\n\n\n5.8.2 Teams\n\n5.8.2.1 Defensive and Offensive EPA per Play\nExpected points added (EPA) per play by the team with possession.\n\nCodepbp_regularSeason &lt;- nfl_pbp %&gt;% \n  dplyr::filter(\n    season == 2024,\n    season_type == \"REG\") %&gt;%\n  dplyr::filter(!is.na(posteam) & (rush == 1 | pass == 1))\n\nepa_offense &lt;- pbp_regularSeason %&gt;%\n  dplyr::group_by(team = posteam) %&gt;%\n  dplyr::summarise(off_epa = mean(epa, na.rm = TRUE))\n\nepa_defense &lt;- pbp_regularSeason %&gt;%\n  dplyr::group_by(team = defteam) %&gt;%\n  dplyr::summarise(def_epa = mean(epa, na.rm = TRUE))\n\nepa_combined &lt;- epa_offense %&gt;%\n  dplyr::inner_join(\n    epa_defense,\n    by = \"team\")\n\n\nDefensive EPA per play during the 2024 NFL season is depicted as a function of offensive EPA per play in Figure 5.30 using the nflplotR package (Carl, 2024).\n\nCodeggplot2::ggplot(\n  data = epa_combined,\n  aes(\n    x = off_epa,\n    y = def_epa)) +\n  nflplotR::geom_mean_lines(\n    aes(\n      x0 = off_epa,\n      y0 = def_epa)) +\n  nflplotR::geom_nfl_logos(\n    aes(\n      team_abbr = team),\n      width = 0.065,\n      alpha = 0.7) +\n  labs(\n    x = \"Offense EPA/play\",\n    y = \"Defense EPA/play\",\n    title = \"2024 NFL Offensive and Defensive EPA per Play\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) + # horizontal y-axis title\n  scale_y_reverse()\n\n\n\n\n\n\nFigure 5.30: 2024 NFL Offensive and Defensive EPA Per Play.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationConclusion",
    "href": "data-visualization.html#sec-dataVisualizationConclusion",
    "title": "5  Data Visualization",
    "section": "\n5.9 Conclusion",
    "text": "5.9 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationSessionInfo",
    "href": "data-visualization.html#sec-dataVisualizationSessionInfo",
    "title": "5  Data Visualization",
    "section": "\n5.10 Session Info",
    "text": "5.10 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] tidyverse_2.0.0   ggExtra_0.10.1    ggridges_0.5.6    gghighlight_0.4.1\n[13] plotly_4.10.4     ggplot2_3.5.1     nflplotR_1.4.0   \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       xfun_0.51          htmlwidgets_1.6.4  ggrepel_0.9.6     \n [5] lattice_0.22-6     tzdb_0.5.0         crosstalk_1.2.1    vctrs_0.6.5       \n [9] tools_4.4.3        generics_0.1.3     pkgconfig_2.0.3    Matrix_1.7-2      \n[13] data.table_1.17.0  RColorBrewer_1.1-3 gt_0.11.1          lifecycle_1.0.4   \n[17] compiler_4.4.3     farver_2.1.2       munsell_0.5.1      httpuv_1.6.15     \n[21] htmltools_0.5.8.1  yaml_2.3.10        lazyeval_0.2.2     later_1.4.1       \n[25] pillar_1.10.1      cachem_1.1.0       magick_2.8.5       nlme_3.1-167      \n[29] mime_0.13          tidyselect_1.2.1   digest_0.6.37      stringi_1.8.4     \n[33] labeling_0.4.3     splines_4.4.3      fastmap_1.2.0      grid_4.4.3        \n[37] colorspace_2.1-1   cli_3.6.4          magrittr_2.0.3     withr_3.0.2       \n[41] nflreadr_1.4.1     scales_1.3.0       promises_1.3.2     ggpath_1.0.2      \n[45] timechange_0.3.0   rmarkdown_2.29     httr_1.4.7         hms_1.1.3         \n[49] memoise_2.0.1      shiny_1.10.0       evaluate_1.0.3     knitr_1.50        \n[53] miniUI_0.1.1.1     viridisLite_0.4.2  mgcv_1.9-1         rlang_1.1.5       \n[57] Rcpp_1.0.14        xtable_1.8-4       glue_1.8.0         xml2_1.3.8        \n[61] jsonlite_1.9.1     R6_2.6.1          \n\n\n\n\n\n\nAdobe Express. (2020). 8 basic design principles to help you make awesome graphics. https://www.adobe.com/express/learn/blog/8-basic-design-principles-to-help-you-create-better-graphics\n\n\nAttali, D., & Baker, C. (2023). ggExtra: Add marginal histograms to ggplot2, and more ggplot2 enhancements. https://github.com/daattali/ggExtra\n\n\nCarl, S. (2024). nflplotR: NFL logo plots in ggplot2 and gt. https://nflplotr.nflverse.com\n\n\nChang, W. (2018). R graphics cookbook: Practical recipes for visualizing data (2nd ed.). O’Reilly Media. https://r-graphics.org\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGarnier, S. (2024). viridis: Colorblind-friendly color maps for R. https://sjmgarnier.github.io/viridis/\n\n\nGarnier, S., Ross, N., Rudis, B., Sciaini, M., Camargo, A. P., & Scherer, C. (2024). viridis(Lite) - colorblind-friendly color maps for R. https://doi.org/10.5281/zenodo.4679423\n\n\nKilin, I. (2022). The best charts for color blind viewers. https://www.datylon.com/blog/data-visualization-for-colorblind-readers\n\n\nNuñez, J. R., Anderton, C. R., & Renslow, R. S. (2018). Optimizing colormaps with consideration for color vision deficiency to enable accurate interpretation of scientific data. PLOS ONE, 13(7), e0199239. https://doi.org/10.1371/journal.pone.0199239\n\n\nScherer, C. (2021). Beyond bar and box plots. https://z3tt.github.io/beyond-bar-and-box-plots\n\n\nSchwabish, J. (2021). Better data visualizations: A guide for scholars, researchers, and wonks. Columbia University Press. https://doi.org/10.7312/schw19310\n\n\nSievert, C. (2020). Interactive web-based data visualization with R, plotly, and shiny. Chapman; Hall/CRC. https://plotly-r.com\n\n\nSievert, C., Parmer, C., Hocking, T., Chamberlain, S., Ram, K., Corvellec, M., & Despouy, P. (2024). plotly: Create interactive web graphics via plotly.js. https://plotly-r.com\n\n\nTufte, E. R. (2001). The visual display of quantitative information. Graphics Press.\n\n\nWickham, H. (2024). ggplot2: Elegant graphics for data analysis (3rd ed.). Springer. https://ggplot2-book.org\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., Dunnington, D., & van den Brand, T. (2024). ggplot2: Create elegant data visualisations using the grammar of graphics. https://ggplot2.tidyverse.org\n\n\nWilke, C. O. (2024). ggridges: Ridgeline plots in ggplot2. https://wilkelab.org/ggridges/\n\n\nYutani, H. (2023). gghighlight: Highlight lines and points in ggplot2. https://yutannihilation.github.io/gghighlight/",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html",
    "href": "player-evaluation.html",
    "title": "6  Player Evaluation",
    "section": "",
    "text": "6.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-playerEvaluationGettingStarted",
    "href": "player-evaluation.html#sec-playerEvaluationGettingStarted",
    "title": "6  Player Evaluation",
    "section": "",
    "text": "6.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#overview",
    "href": "player-evaluation.html#overview",
    "title": "6  Player Evaluation",
    "section": "\n6.2 Overview",
    "text": "6.2 Overview\nEvaluating players for fantasy football could be thought of as similar to the process of evaluating companies when picking stocks to buy. You want to evaluate and compare various assets so that you get the assets with the best value.\nThere are various domains of criteria we can consider when evaluating a football player’s fantasy prospects. Potential domains to consider include:\n\nathletic profile\nhistorical performance\nhealth\nage and career stage\nsituational factors\nmatchups\ncognitive and motivational factors\nfantasy value\n\nThe discussion that follows is based on my and others’ impressions of some of the characteristics that may be valuable to consider when evaluating players. However, the extent to which any factor is actually relevant for predicting future performance is an empirical question and should be evaluated empirically.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAthleticProfile",
    "href": "player-evaluation.html#sec-evalAthleticProfile",
    "title": "6  Player Evaluation",
    "section": "\n6.3 Athletic Profile",
    "text": "6.3 Athletic Profile\nFactors related to a player’s athletic profile include factors such as:\n\nbody shape\n\nheight\nweight\nhand size\nwing span (arm length)\n\n\nbody function\n\nagility\nstrength\nspeed\nacceleration/explosiveness\njumping ability\n\n\n\nIn terms of body shape, we might consider a player’s height, weight, hand size, and wing span (arm length). Height allows players to see over opponents and to reach balls higher in the air. Thus, greater height is particularly valuable for Quarterbacks and Wide Receivers. Heavier players are tougher to budge and to tackle. Greater weight is particularly valuable for Linemen, Fullbacks, and Tight Ends, but it can also be valuable—to a deree—for Quarterbacks, Running Backs, and Wide Receivers. Hand size and wing span is particularly valuable for people catching the ball; thus, a larger hand size and longer wing span are particularly valuable for Wide Receivers and Tight Ends.\nIn terms of body function, we can consider a player’s agility, strength, speed, acceleration/explosiveness, and jumping ability. For Wide Receivers, speed, explosiveness, and jumping ability are particularly valuable. For Running Backs, agility, strength, speed, and explosiveness are particularly valuable.\nMany aspects of a player’s athletic profile, including tests of speed (40-yard dash), strength (bench press), agility (20-yard shuttle run; three cone drill), and jumping ability (vertical jump; broad jump) are available from the National Football League (NFL) Combine, which is especially relevant for evaluating rookies. We demonstrate how to import data from the NFL Combine in Section 4.3.8. There are also calculators that integrate information about body shape and information from the NFL Combine to determine a player’s relative athletic score (RAS) for their position: https://ras.football/ras-calculator/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-skill",
    "href": "player-evaluation.html#sec-skill",
    "title": "6  Player Evaluation",
    "section": "\n6.4 Skill",
    "text": "6.4 Skill\nWhen scouting players, scouts consider not only the player’s athletic profile, but also their position-relevant skill. For instance, how good are they are reading the defense, passing the ball, running routes, catching balls, making defenders miss tackles, taking care of the ball, consistency, etc. Scouting and evaluating skill is a complicated endeavor, and even the professional scouts frequently make mistakes in their evaluations and predictions. You can certainly read skill evaluations about various players; however, unlike metrics of athletic profile, we do not have direct access to the player’s underlying skill. Some may say, “You know it when you see it.” But, this is not particularly useful when trying to identify players who are undervalued or overvalued—because the skill evaluations are likely already “baked into” a player’s projections. Because we do not have direct access to a player’s skill, we tend to rely on indirect metrics of their ability, such as historical performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalHistoricalPerformance",
    "href": "player-evaluation.html#sec-evalHistoricalPerformance",
    "title": "6  Player Evaluation",
    "section": "\n6.5 Historical Performance",
    "text": "6.5 Historical Performance\n\n6.5.1 Overview\n\n“The best predictor of future behavior is past behavior.” – Unknown\n\n\n“Past performance does not guarantee future results.” – A common disclaimer about investments.\n\nFactors relating to historical performance to consider could include:\n\nperformance in college\n\ndraft position\n\n\nperformance in the NFL\nefficiency\nconsistency\n\nIt is important to consider a player’s past performance. However, the extent to which historical performance may predict future performance may depend on many factors such as (a) the similarity of the prior situation to the current situation, (b) how long ago the prior situation was, and (c) the extent to which the player (or situation) has changed in the interim. For rookies, the player does not have prior seasons of performance in the NFL to draw upon. Thus, when evaluating rookies, it can be helpful to consider their performance in college or in their prior leagues. However, there are large differences between the situation in college and the situation in the NFL, so prior success in college may not portend future success in the NFL. An indicator that intends to be prognostic of future performance, and that accounts for past performance, is a player’s draft position—that is, how early (or late) was a player selected in the NFL Draft. The earlier a player was selected in the NFL Draft, the greater likelihood that the player will perform well; however, this is somewhat countered by the fact that the teams with the highest draft picks tend to be the worst based on the prior season’s record.\nFor players who have played in the NFL, past performance becomes more relevant because, presumably, the prior situation is more similar (than was their situation in college) to their current situation. Nevertheless, as described below, lots of things change from game to game and season to season, and such changes are important to monitor because they can render prior situations less relevant to the player’s current situation. Thus, it is important not to rely just on a player’s historical performance from last season. Nevertheless, historical performance is one of the best indicators we have.\nWe demonstrate how to import historical player statistics in Section 4.3.16. We demonstrate how to calculate historical player statistics in Section 4.4.1. We demonstrate how to calculate historical fantasy points in Section 4.4.2.\n\n6.5.1.1 Changes in Situational Factors\nLots of things change from game to game and season to season: injuries, coaches, coaching strategies, teammates, etc. Just because a player performed well or poorly in a given game or season does not necessarily mean that they will perform similarly in subsequent games/seasons. Thus, it is crucial to consider the player’s current situation—and what has changed since the prior game or season. Consider what has changed in terms of situational factors:\n\nIs the player on a new team?\nDo they have a new head coach or coordinator?\nHow will the team’s offensive scheme change?\nDoes the team have a better offensive line?\nDoes the team have better receiving targets?\nWill the player’s position change on the depth chart?\nWill there be more competition for targets?\n\nThere is often greater uncertainty in selecting a player who has changed teams or whose situation has greatly changed, but such risk can be highly rewarded when the player’s new team or situation is better suited to the player. If a player underachieved on a given team, they may perform better for another team.\n\n6.5.2 Efficiency\nIn addition to how many fantasy points a player scores in terms of historical performance, we also care about efficiency and consistency. How efficient were they given the number of opportunities they had? This is important to consider because different players have different opportunities. For example, some Running Backs had more opportunity than others (e.g., more carries), so comparing players merely on rushing yards would be misleading. If they were relatively more efficient, they will likely score more points than many of their peers when given more opportunities. If they were relativelly inefficient, their capacity to score fantasy points may be more dependent on touches/opportunities. Efficiency might be operationalized by indicators such as yards per passing attempt, yards per rushing attempt, yards per target, yards per reception, etc.\n\n6.5.3 Consistency\nIn terms of consistency, how consistent was the player they from game to game and from season to season? For instance, we could examine the standard deviations of players’ fantasy points across games in a given season. However, the standard deviation tends to be upwardly biased as the mean increases. So, we can account for the player’s mean fantasy points per game by dividing their game-to-game standard deviation of fantasy points (\\(\\sigma\\)) by their mean fantasy points across games (\\(\\mu\\)). This is known as the coefficient of variation (CV), which is provided in Equation 6.1.\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\tag{6.1}\\]\nPlayers with a lower standard deviation and a lower coefficient of variation (of fantasy points across games) are more consistent. In the example below, Player 2 might be preferable to Player 1 because Player 2 is more consistent; Player 1 is more “boom-or-bust.” Despite showing a similar mean of fantasy points across weeks, Player 2 shows a smaller week-to-week standard deviation and coefficient of variation.\n\nCodeset.seed(1)\n\nplayerScoresByWeek &lt;- data.frame(\n  player1_scores = rnorm(17, mean = 20, sd = 7),\n  player2_scores = rnorm(17, mean = 20, sd = 4),\n  player3_scores = rnorm(17, mean = 10, sd = 4),\n  player4_scores = rnorm(17, mean = 10, sd = 1)\n)\n\nconsistencyData &lt;- data.frame(t(playerScoresByWeek))\n\nweekNames &lt;- paste(\"week\", 1:17, sep = \"\")\n\nnames(consistencyData) &lt;- weekNames\nrow.names(consistencyData) &lt;- NULL\n\nconsistencyData$mean &lt;- rowMeans(consistencyData[,weekNames])\nconsistencyData$sd &lt;- apply(consistencyData, 1, sd)\nconsistencyData$cv &lt;- consistencyData$sd / consistencyData$mean\n\nconsistencyData$player &lt;- c(1, 2, 3, 4)\n\nconsistencyData &lt;- consistencyData %&gt;% \n  select(player, mean, sd, cv, week1:week17)\n\nround(consistencyData, 2)\n\n\n  \n\n\n\nHowever, another perspective is to embrace the chaos and uncertainty that is part of fantasy football; one is better positioned to win a given week if you have one or a few players that “go off”—i.e., that score lots of points. Thus, starting boom-or-bust players can make it more likely that you win in the “boom weeks”; however, this may also mean having some “bust weeks”.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalHealth",
    "href": "player-evaluation.html#sec-evalHealth",
    "title": "6  Player Evaluation",
    "section": "\n6.6 Health",
    "text": "6.6 Health\nHealth-related factors to consider include:\n\ncurrent injury status\ninjury history\n\nIt is also important to consider a player’s past and current health status. In terms of a player’s current health status, it is important to consider whether they are injured or are playing at less than 100% of their typical health. In terms of a player’s prior health status, one can consider their injury history, including the frequency and severity of injuries and their prognosis.\nWe demonstrate how to import injury reports in Section 4.3.17.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAvailability",
    "href": "player-evaluation.html#sec-evalAvailability",
    "title": "6  Player Evaluation",
    "section": "\n6.7 Availability",
    "text": "6.7 Availability\nIn addition to injuries, other factors can affect a player’s availability, including contract hold-outs or league suspensions, for instance, due to violating the league’s conduct policy. Some players may be “holding out”, which means that they may refuse to play until they sign a more desirable contract with the team (e.g., for more money or for a longer-term contract).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAgeCareerStage",
    "href": "player-evaluation.html#sec-evalAgeCareerStage",
    "title": "6  Player Evaluation",
    "section": "\n6.8 Age and Career Stage",
    "text": "6.8 Age and Career Stage\nAge and career stage-related factors include:\n\nage\nexperience\ntouches\n\nA player’s age is relevant because of important age-related changes in a player’s speed, ability to recover from injury, etc. A player’s experience is relevant because players develop knowledge and skills with greater experience. A player’s prior touches/usage is also relevant, because it speaks to how many hits a player may have taken. For players who take more hits, it may be more likely that their bodies “break down” sooner.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalSituation",
    "href": "player-evaluation.html#sec-evalSituation",
    "title": "6  Player Evaluation",
    "section": "\n6.9 Situational Factors",
    "text": "6.9 Situational Factors\nSituational factors one could consider include:\n\nteam quality\nrole on team\nteammates\nopportunity and usage\n\nsnap count\ntouches/targets\nred zone usage\n\n\n\nFootball is a team sport. A player is embedded within a broader team context; it is important to consider the strength of their team context insofar as it may support— or detract from—a player’s performance. For instance, for a Quarterback, it is important to consider how strong the pass blocking is from the Offensive Line. Will they have enough time to throw the ball, or will they be constantly under pressure to be sacked? It is also important to consider the strength of the pass catchers—the Wide Receivers and Tight Ends. For a Running Back, it is important to consider how strong the run blocking is from the Offensive Line. For a Wide Receiver, it is important to consider how strong the pass blocking is, and how strong the Quarterback is.\nIt is also important to consider a player’s role on the team. Is the player a starter or a backup? Related to this, it is important to consider the strength of one’s teammates. For a given Running Back, if a teammate is better at running the ball, this may take away from how much the player sees the field. For a given Wide Receiver, if a teammate is better at catching the ball, this may take some targets away from the player. However, the team’s top defensive back is often matched up against the team’s top Wide Receiver. So, if the team’s top Wide Receiver is matched up against a particularly strong Defensive Back, the second- and third-best Wide Receivers may more targets than usual.\nIt is also important to consider a player’s opportunity and usage, which are influenced by many factors, including the skill of the player, the skill of their teammates, the role of the player on the team, the coaching style, the strategy of the opposing team, game scripts, etc. In terms of the player’s opportunity and usage, how many snaps do they get? How many touches and/or targets do they receive? Being on the field for more snaps and receiving more touches and/or targets means that the player has more opportunities to score fantasy points. Are they targeted in the red zone? Red zone targets are more likely to lead to touchdown scoring opportunities, which are particularly valuable in fantasy football.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalMatchups",
    "href": "player-evaluation.html#sec-evalMatchups",
    "title": "6  Player Evaluation",
    "section": "\n6.10 Matchups",
    "text": "6.10 Matchups\nMatchup-related factors to consider include:\n\nstrength of schedule\nweekly matchup\n\nAnother aspect to consider is how challenging their matchup(s) and strength of schedule is. For a Quarterback, it is valuable to consider how strong the oppenent’s passing defense is. For a Running Back, how strong is the running defense? For a Wide Receiver, how strong is the passing defense and the Defensive Back that is likely to be assigned to guard them?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalCogMotivational",
    "href": "player-evaluation.html#sec-evalCogMotivational",
    "title": "6  Player Evaluation",
    "section": "\n6.11 Cognitive and Motivational Factors",
    "text": "6.11 Cognitive and Motivational Factors\nOther factors to consider include cognitive and motivational factors. Some coaches refer to these as the “X Factor” or “the intangibles.” However, just as any other construct in psychology, we can devise ways to operationalize them. Insofar as they are observable, they are measurable.\nCognitive and motivational factors one could consider include:\n\nreaction time\nknowledge and intelligence\nwork ethic and mental toughness\nincentives\n\ncontract performance incentives\nwhether they are in a contract year\n\n\n\nA player’s knowledge, intelligence, and reaction time can help them gain an upper-hand even when they may not be the fastest or strongest. A player’s work ethic and mental toughness may help them be resilient and persevere in the face of challenges. Contact-related incentives may lead a player to put forth greater effort. For instance, a contract may have a performance incentive that provides a player greater compensation if they achieve a particular performance milestone (e.g., receiving yards). Another potential incentive is if a player is in what is called their “contract year” (i.e., the last year of their current contract). If a player is in the last year of their current contract, they have an incentive to perform well so they can get re-signed to a new contract.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-fantasyValue",
    "href": "player-evaluation.html#sec-fantasyValue",
    "title": "6  Player Evaluation",
    "section": "\n6.12 Fantasy Value",
    "text": "6.12 Fantasy Value\n\n6.12.1 Sources From Which to Evaluate Fantasy Value\nThere are several sources that one can draw upon to evaluate a player’s fantasy value:\n\nexpert or aggregated rankings\nlayperson rankings\n\nplayers’ Average Draft Position (ADP) in other league snake drafts\n\nplayers’ Average Auction Value (AAV) in other league auction drafts\n\n\n\nexpert or aggregated projections\n\n\n6.12.1.1 Expert Fantasy Rankings\nFantasy rankings (by so-called “experts”) are provided by many sources. To reduce some of the bias due to a given source, some services aggregate projections across sources, consistent with a “wisdom of the crowd” approach. FantasyPros aggregates fantasy rankings across sources. Fantasy Football Analytics creates fantasy rankings from projections that are aggregated across sources (see the webapp here: https://apps.fantasyfootballanalytics.net).\n\n6.12.1.2 Layperson Fantasy Rankings: ADP and AAV\nAverage Draft Position (ADP) and Average Auction Value (AAV), are based on league drafts, mostly composed of everyday people. ADP is based on snake drafts, whereas AAV is based on auction drafts. Thus, ADP and AAV are consistent with a “wisdom of the crowd” approach, and I refer to them as forms of rankings by laypeople. ADP data are provided by FantasyPros. AAV data are also provided by FantasyPros.\n\n6.12.1.3 Projections\nProjections are provided by various sources. Projections (and rankings, for that matter) are a bit of a black box. It is often unclear how they were derived by a particular source. That is, it is unclear how much of the projection was based on statistical analysis versus conjecture.\nTo reduce some of the bias due to a given source, some services aggregate projections across sources, consistent with a “wisdom of the crowd” approach. Projections that are aggregated across sources are provided by Fantasy Football Analytics (see the webapp here: https://apps.fantasyfootballanalytics.net) and by FantasyPros. Moreover, sites also provide season-long player futures for the over/under betting lines, which are based on Vegas lines and how people bet: https://www.rotowire.com/betting/nfl/player-futures.php. In addition, sites also provide weekly projections based on Vegas lines: https://vegasprojections.com; https://www.actionnetwork.com/nfl/props.\n\n6.12.1.4 Benefits of Using Projections Rather than Rankings\nIt is important to keep in mind that rankings, ADP, and AAV are specific to roster and scoring settings of a particular league. For instance, in point-per-reception (PPR) leagues, players who catch lots of passes (Wide Receivers, Tight Ends, and some Running Backs) are valued more highly. As another example, Quarterbacks are valued more highly in 2-Quarterback leagues. Thus, if using rankings, ADP, or AAV, it is important to find ones from leagues that mirror—as closely as possible—your league settings.\nProjected statistics (e.g., projected passing touchdowns) are agnostic to league settings and can thus be used to generate league-specific fantasy projections and rankings. Thus, projected statisitics may be more useful than rankings because they can be used to generate rankings for your particular league settings. For instance, if you know how many touchdowns, yards, and interceptions a Quarterback is a projected to throw (in addition to any other relevant categories for the player, e.g., rushing yards and touchdowns), you can calculate how many fantasy points the Quarterback is expected to gain in your league (or in any league). Thus, you can calculate ranking from projections, but you cannot reverse engineer projections from rankings.\n\n6.12.2 Indices to Evaluate Fantasy Value\nBased on the sources above (rankings, ADP, AAV, and projections), we can derive multiple indices to evaluate fantasy value. There are many potential indices that can be worthwhile to consider, including a player’s:\n\ndropoff\nvalue over replacement player (VORP)\nuncertainty\n\n\n6.12.2.1 Dropoff\nA player’s dropoff is the difference between (a) the player’s projected points and (b) the projected points of the next-best player at that position.\n\n6.12.2.2 Value Over Replacement Player\nBecause players from some positions (e.g., Quarterbacks) tend to score more points than players from other positions (e.g., Wide Receivers), it would be inadvisable to compare players across different positions based on projected points. In order to more fairly compare players across positions, we can consider a player’s value over a typical replacement player at that position (shortened to “value over replacement player”). A player’s value over a replacement player (VORP) is the difference between (a) a player’s projected fantasy points and (b) the fantasy points that you would be expected to get from a typical bench player at that position. Thus, VORP provides an index of how much added value a player provides.\n\n6.12.2.3 Uncertainty\nA player’s uncertainty is how much variability there is in projections or rankings for a given player across sources. For instance, consider a scenario where three experts provide ratings about two players, Player A and Player B. Player A is projected to score 300, 310, and 290 points by experts 1, 2, and 3, respectively. Player B is projected to score 400, 300, and 200 points by experts 1, 2, and 3, respectively. In this case, both players are (on average) projected to score the same number of points (300).\n\nCodeexampleData &lt;- data.frame(\n  player = c(rep(\"A\", 3), rep(\"B\", 3)),\n  expert = c(1:3, 1:3),\n  projectedPoints = c(300, 310, 290, 400, 300, 200)\n)\n\nplayerA_mean &lt;- mean(exampleData$projectedPoints[which(exampleData$player == \"A\")])\nplayerB_mean &lt;- mean(exampleData$projectedPoints[which(exampleData$player == \"B\")])\n\nplayerA_sd &lt;- sd(exampleData$projectedPoints[which(exampleData$player == \"A\")])\nplayerB_sd &lt;- sd(exampleData$projectedPoints[which(exampleData$player == \"B\")])\n\nplayerA_cv &lt;- playerA_mean / playerA_sd\nplayerB_cv &lt;- playerB_mean / playerB_sd\n\n\n\nCodeplayerA_mean\n\n[1] 300\n\nCodeplayerB_mean\n\n[1] 300\n\n\nHowever, the players differ considerably in their uncertainty (i.e., the source-to-source variability in their projections), as operationalized with the standard deviation and coefficient variation of projected points across sources for a given player.\n\nCodeplayerA_sd\n\n[1] 10\n\nCodeplayerB_sd\n\n[1] 100\n\nCodeplayerA_cv\n\n[1] 30\n\nCodeplayerB_cv\n\n[1] 3\n\n\nHere is a depiction of a density plot of projected points for a player with a low, medium, and high uncertainty:\n\nCodeplayerA &lt;- rnorm(1000000, mean = 150, sd = 5)\nplayerB &lt;- rnorm(1000000, mean = 150, sd = 15)\nplayerC &lt;- rnorm(1000000, mean = 150, sd = 30)\n\nmydata &lt;- data.frame(playerA, playerB, playerC)\n\nmydata_long &lt;- mydata %&gt;% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"player\",\n    values_to = \"points\"\n  ) %&gt;% \n  mutate(\n    name = case_match(\n      player,\n      \"playerA\" ~ \"Player A\",\n      \"playerB\" ~ \"Player B\",\n      \"playerC\" ~ \"Player C\",\n    )\n  )\n\nggplot2::ggplot(\n  data = mydata_long,\n  ggplot2::aes(\n    x = points,\n    fill = name\n  )\n) +\n  ggplot2::geom_density(alpha = .3) + \n  ggplot2::labs(\n    x = \"Players' Projected Points\",\n    title = \"Density Plot of Projected Points for Three Players\"\n  ) +\n  ggplot2::theme_classic() +\n  ggplot2::theme(legend.title = element_blank())\n\n\n\n\n\n\nFigure 6.1: Density Plot of Projected Points for Three Players\n\n\n\n\nUncertainty is not necessarily a bad characteristic of a player’s projected points. It just means we have less confidence about how the player may be expected to perform. Thus, players with greater uncertainty are risky and tend to have a higher upside (or ceiling) and a lower downside (or floor).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-signsSamples",
    "href": "player-evaluation.html#sec-signsSamples",
    "title": "6  Player Evaluation",
    "section": "\n6.13 Signs Versus Samples",
    "text": "6.13 Signs Versus Samples\nWhen thinking about the most important domains to assess for evaluating players and predicting their performance, it is important to distinguish between signs and samples (Den Hartigh et al., 2018). Signs are indicators of underlying states. The signs approach to assessment involves assessing processes that may predict performance, where the emphasis is on what the sign indicates about the underlying attribute, rather than on the specific behavior itself. In football, the signs approach might involve measuring skills using separate tests where players’ skills are tested in isolation. For instance, the NFL Combine takes a signs approach to assessment, in which players have a separate test to assess for speed (e.g., 40-yard dash) and jumping ability (e.g., vertical jump).\nSamples reflect behaviors that are close to the behavior of interest. The samples approach to asesssment tries to assess the person’s performance in a behavior, situation, and context that are as reflective as possible of the kinds of behaviors, situations, and contexts that the person would have to perform in for the position or occupation. In the samples approach, the behavior itself is the main emphasis, rather than an underlying attribute. In football, the samples approach might involve measuring the player’s performance during games or game-like situations. For instance, to assess the skills of a Wide Receiver, you might observe—during games or game-like situations—how well they are able to catch passes when closely guarded or double-teamed, or how well they are able to catch poorly thrown passes.\nIn general, samples tend to be stronger than signs for predicting player performance in sports (Den Hartigh et al., 2018). For instance, compared to tests of speed, power, and agility at the NFL Combine, collegiate performance is a stronger predictor of performance in the NFL (Lyons et al., 2011). That is, previous sports performance is the best predictor of future performance (for a review, see Den Hartigh et al., 2018).\nAccording to the theoretical perspective to sports performance known as the ecological dynamics approach, successful performance in sports involves the coordination of multiple, intertwined skills that are contextually embedded (Den Hartigh et al., 2018). For instance, a Wide Receiver needs to coordinate skills in speed, route running, jumping, and good hands to create space from defenders and to catch poorly thrown passes in a game. The ecological dynamics approach considers the important interaction of the player, task, and environment. Thus, to assess players in a way that is likely to be most predictive of their future performance, it is important for the assessment to retain the player–task–environment interaction (Den Hartigh et al., 2018). For example, it would be valuable to use assessments from games or game-like situations in which players must perform tasks that leverage multiple, intertwined skills and that are similar to the tasks you want to predict.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalIntegration",
    "href": "player-evaluation.html#sec-evalIntegration",
    "title": "6  Player Evaluation",
    "section": "\n6.14 Putting it Altogether",
    "text": "6.14 Putting it Altogether\nAfter performing an evaluation of the relevant domain(s) for a given player, one must integrate the evaluation information across domains to make a judgment about a player’s overall value. When considering how much weight to give to each of various factors, it is important to evaluate how much predictive validity each factor has for predicting a player’s successful performance in the NFL. Then, one can weight each variable according to its predictive validity using an actuarial approach. Actuarial approaches are described in a later chapter, in Section 15.2.2. For now, suffice it to say that actuarial approaches leverage statistical formulas, as opposed to using judgment alone. As described in Chapter 14, people’s judgment—including judgments by experts (e.g., professional scouts and coaches)—tends to be riddled with biases (Den Hartigh et al., 2018). Moreover, experts tend to disagree in terms of their judgments/predictions about players (Den Hartigh et al., 2018). In addition, there are benefits to leveraging multiple perpectives, consistent with the “wisdom of the crowd”, as described in Section 24.3. In general, as noted in Section 6.13, give more weight to samples of relevant behavior—such as past performance in games—than to signs such as NFL Combine metrics (e.g., bench press, 40-yard dash, etc.). Where you can, use assessments from games or game-like situations in which players must perform tasks that leverage multiple, intertwined skills and that are similar to the tasks you want to predict.\nWhen thinking about a player’s value, it can be worth thinking of a player’s upside and a player’s downside. Players that are more consistent may show higher downside but a lower upside. Younger, less experienced players may show a higher upside but a lower downside.\nThe extent to which you prioritize a higher upside versus a higher downside may depend on many factors. For instance, when drafting players, you may prioritize drafting players with the highest downside (i.e., the safest players), whereas you may draft sleepers (i.e., players with higher upside) for your bench. When choosing which players to start in a given week, if you are predicted to beat a team handily, it may make sense to start the players with the highest downside. By contrast, if you are predicted to lose to a team by a good margin, it may make sense to start the players with the highest upside.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-playerEvaluationSessionInfo",
    "href": "player-evaluation.html#sec-playerEvaluationSessionInfo",
    "title": "6  Player Evaluation",
    "section": "\n6.15 Session Info",
    "text": "6.15 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_1.9.1    compiler_4.4.3    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.6.1         \n [9] labeling_0.4.3    generics_0.1.3    knitr_1.50        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.10.1     tzdb_0.5.0        rlang_1.1.5      \n[17] stringi_1.8.4     xfun_0.51         timechange_0.3.0  cli_3.6.4        \n[21] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.3       \n[25] hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5       evaluate_1.0.3   \n[29] glue_1.8.0        farver_2.1.2      colorspace_2.1-1  rmarkdown_2.29   \n[33] tools_4.4.3       pkgconfig_2.0.3   htmltools_0.5.8.1\n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J. (2011). On the predictive efficiency of past performance and physical ability: The case of the National Football League. Human Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "draft.html",
    "href": "draft.html",
    "title": "7  The Fantasy Draft",
    "section": "",
    "text": "7.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftGettingStarted",
    "href": "draft.html#sec-draftGettingStarted",
    "title": "7  The Fantasy Draft",
    "section": "",
    "text": "7.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftTypes",
    "href": "draft.html#sec-draftTypes",
    "title": "7  The Fantasy Draft",
    "section": "\n7.2 Types of Fantasy Drafts",
    "text": "7.2 Types of Fantasy Drafts\nThere are several types of drafts in fantasy football. The most common types of drafts are snake drafts and auction drafts.\n\n7.2.1 Snake Draft\nIn a snake draft, the participants (i.e., managers) are assigned a draft order. In the first round, the managers draft in that order. In the second round, the managers draft in reverse order. It continues to “snake” in this way, round after round, so that the person who has the first pick in a given round has the last pick in the next round, and whoever has the last pick in a given round has the first pick in the next round.\n\n7.2.2 Auction Draft\nIn an auction draft, the managers are assigned a nomination order and there is a salary cap (e.g., $200). The first manager chooses which player to nominate. Then, the managers bid on that player like in an auction. In order to bid, the manager must raise the price by at least $1. If two managers want to obtain the same player, they may continue to raise the amount until one manager backs out and is no longer to bid by raising the price. The highest bidder wins (i.e., drafts) that player. Then, the second manager nominates a player, and the managers bid on that player. This process repeats until all teams have drafted their allotment of players.\n\n7.2.3 Comparison\nSnake drafts are more common than auction drafts. Snake drafts tend to be quicker than auction drafts. However, auction drafts are more fair than snake drafts. In an auction draft, unlike a snake draft, all players are available to all teams. For instance, in a snake draft, the first 9 players drafted are unavailable to the 10th pick of the first round. So, if you have the 10th pick and want the top-ranked player, this player would not be available to you in the snake draft. However, in the auction draft, every player is available to every manager, so long as the manager is able and willing to bid enough.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftStrategy",
    "href": "draft.html#sec-draftStrategy",
    "title": "7  The Fantasy Draft",
    "section": "\n7.3 Draft Strategy",
    "text": "7.3 Draft Strategy\n\n7.3.1 Overview\nThere is no one “right” draft strategy. As noted by Lee & Liu (2022) in their analysis of fantasy drafts, the effectiveness of any draft strategy depends on the strategies of the other managers in the league. Sometimes it works best to “zig” when everyone else is “zagging”. For instance, Lee & Liu (2022) found that the most successful draft strategies in terms of roster composition (i.e., the number of players for each position) were not the most common draft strategies. For instance, if you notice that everyone else is drafting Wide Receivers, this may mean that other managers are over-valuing Wide Receivers, and this could be a nice opportunity to draft a Running Back for good value.\nIn general, you will first want to generate the rankings you will use to select which players to prioritize. You may generate your rankings based one or more of the following:\n\nyour evaluation of players\n\nexpert or aggregated rankings\n\nlayperson rankings\n\nplayers’ Average Draft Position (ADP) in other league drafts (for snake drafts)\nplayers’ Average Auction Value (AAV) in other league drafts (for auction drafts)\n\n\nexpert or aggregated projections\nindices derived from rankings and projections\n\nAs described in Section 24.3, there can be benefits of leveraging the wisdom of the crowd by using rankings or projections that are averaged across many people and perspectives. Section 6.12.1 describes where to obtain aggregated rankings, aggregated projections, ADP, and AAV data. As described in Chapter 15, there are also benefits to using the actuarial approach to prediction rather than (merely) using judgment.\nIt is not sufficient to compare players in terms of projected points because some positions have more depth than other positions. Some positions show positional scarcity—that is, a limited number of high performing players.\nAn important concept in the draft is “dropoff”, which is described in Section 6.12.2.1. Dropoff at a given position, is the difference—in terms of projected fantasy points—between (a) the best available player remaining at that position and (b) the second-best available player remaining at that position. If there is a bigger dropoff at a given position, there may be greater value in drafting the top player from that position. For instance, consider the following scenario: “Quarterback A” is projected to score 325 points, and “Quarterback B” is projected to score 320 points. “Tight End A” is projected to score 230 points, and “Tight End B” is projected to score 150 points. In this example, there is a much greater dropoff for Tight Ends than there is for Quarterbacks. Thus, even though “Quarterback A” is projected to score more points than “Tight End A”, “Tight End A” may be more valuable, relatively, because there is still a good Quarterback available if someone else drafts “Quarterback A”. In general, Kickers and Defenses tend to have the lowest dropoff (i.e., the lowest expected drop in fantasy points) by positional rank, so it makes sense to draft Kickers and Defenses late in the draft (Lee & Liu, 2022). Defenses, in particular, appear to be among the least predictable of the positions (Lee & Liu, 2022).\nAnother important concept is a player’s value over a typical replacement player at that position (shortened to “value over replacement player”; VORP), which is described in Section 6.12.2.2. A player’s value over a typical replacement player provides a way to more fairly compare (and thus rank) players across different positions.\nAnother important concept is a player’s uncertainty, which is described in Section 6.12.2.3.\nIn both snake and auction draft formats, your goal is to draft the team whose weekly starting lineup scores the most points and thus the collection of players with the greatest VORP. For your starting lineup, it may make sense—especially with your earliest selections—when comparing two players with equivalent VORP, to prioritize players with higher consistency and lower uncertainty, because they may be considered “safer” with a higher floor. However, when drafting players for your bench, it make make more sense to prioritize high-risk, high reward players with greater uncertainty, because they may have a higher ceiling. Players with a higher ceiling have a potential to be “sleepers”—players who are valued low (i.e., with a high ADP or low AAV) and who outperform their valuation. Note that, although players with greater uncertainty are high-risk, high-reward players, selecting this kind of a player for your bench (i.e., in a late round or for a small cost) is a lower risk selection, because you have less to lose with later/lower-cost picks. That is, even though the player is higher risk, selecting a higher risk player for your bench is a lower risk decision.\nThe Spurs in the National Basketball Association (NBA) were well-reputed for excelling in this draft strategy [Ryan (2013); archived at https://perma.cc/X7NW-WZC6]. They frequently used their second-round picks to draft high-risk, high-reward players. Sometimes, the secound round pick was a bust, but they have little to lose with a failed second round pick. Other times, their second round picks—including Willie Anderson, DeJuan Blair, Goran Dragic, Luis Scola, and Manu Ginóbili—greatly outperformed expectations. Thanks, in part, to this draft strategy, the team showed strong extended success for nearly three decades from 1989 through the late-2010s.\nHowever, the draft strategies to achieve the “optimal lineup” differ between snake versus auction drafts.\nOne factor that is not included above is whether a player is on your favorite team. Managers commonly like to draft players on their favorite teams (e.g., Cowboys, Eagles). That is fine—fantasy football is a game. Do what is fun for you. However, if your goal is to select the best players, leave your allegiances at the door. Selecting players based on their playing for your favorite team—rather than based on performance—is a form of cognitive bias.\nIn general, the most consequential decisions tend to be those made early in the draft regarding the top projected players (Lee & Liu, 2022). That is, the earlier selections tend to have the greatest impact on the success of one’s fantasy season. So, make sure to spend time in your draft preparation to identify the players you want to select early in the draft. Moreover, some teams like to hedge their bet on their top Running Backs in the case that the player were to get injured, by drafting the player’s backup, a strategy known as handcuffing. However, there is not strong evidence that handcuffing leads to better outcomes (Lee & Liu, 2022).\nIn general, Lee & Liu (2022) found that teams that drafted more Running Backs and Wide Receivers tended to outperform other teams.\n\n7.3.2 Snake Draft\nIn general, your goal is to draft the team whose weekly starting lineup has the greatest VORP. Consequently, you are often looking to pick the player with the highest VORP at a given selection, while keeping in mind (a) the dropoff of players at other positions and (b) which players may be available at subsequent picks so that you do not sacrifice too much later value with a given selection. For instance, if a particular Quarterback has a slightly higher VORP than a particular Running Back, but the Quarterback is likely to be available at the manager’s next pick but the Running Back is likely to be unavailable at their next pick, it might make more sense to draft the Running Back.\nHerding behavior is common in snake drafts. As described in Section 24.3, herding occurs when people align their behavior with others. For instance, Lee & Liu (2022) found evidence that when Quarterbacks, Kickers, and Defenses were drafted by one team, subsequent teams were more likely to draft a player of that position. The same was not the case for Running Backs, Wide Receivers, and Tight Ends.\n\n7.3.3 Auction Draft\nAccording to an analysis by the Harvard Sports Analysis Collective, the majority of the manager’s salary cap should be spent on the starting lineup, and you should spend less on bench players [Chakravarthy (2012); archived at https://perma.cc/P7RX-92UU]. This is known as the “stars and scrubs” draft strategy. Based on the analysis, the author recommended applying a 10% premium to the top players and a 10% discount to the lower-tiered players. The idea behind the approach is that a player on your bench does not contribute to the team’s points and, thus, most players drafted to your bench do not contribute much to the team’s points throughout the season. That said, bench players can be important in the case of a starter’s injury or under-performance. So, it is recommended to draft starters with lower uncertainty who are safer. In contrast to your starting lineup, you may look to draft players on your bench who have greater uncertainty for their high reward potential in a low-risk selection given the lower price.\nAn alternative to the “stars and scrubs” approach is to wait to draft more “high-value” players after other managers have over-paid for players. In any case, having some small amount of cap left over toward the end of the draft can help you draft good value players to fill out your bench spots for cheap (e.g., $2). Having some depth can help you offset the risk of injuries and the possibility that some of your starters may underperform their expectation, which is quite likely given how challenging it is to predict fantasy performance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftSessionInfo",
    "href": "draft.html#sec-draftSessionInfo",
    "title": "7  The Fantasy Draft",
    "section": "\n7.4 Session Info",
    "text": "7.4 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.3    fastmap_1.2.0     cli_3.6.4        \n [5] tools_4.4.3       htmltools_0.5.8.1 rmarkdown_2.29    knitr_1.50       \n [9] jsonlite_1.9.1    xfun_0.51         digest_0.6.37     rlang_1.1.5      \n[13] evaluate_1.0.3   \n\n\n\n\n\n\nChakravarthy, P. (2012). Optimizing draft strategies in fantasy football. https://harvardsportsanalysis.wordpress.com/wp-content/uploads/2012/04/fantasyfootballdraftanalysis1.pdf\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nLee, M. D., & Liu, S. (2022). Drafting strategies in fantasy football: A study of competitive sequential human decision making. Judgment and Decision Making, 17(4), 691–719. https://doi.org/10.1017/S1930297500008901\n\n\nRyan, J. (2013). Beating the NBA draft: Does any team outperform expectations? https://harvardsportsanalysis.org/2013/11/beating-the-nba-draft-does-any-team-outperform-expectations",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "research-methods.html",
    "href": "research-methods.html",
    "title": "8  Research Methods",
    "section": "",
    "text": "8.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsGettingStarted",
    "href": "research-methods.html#sec-researchMethodsGettingStarted",
    "title": "8  Research Methods",
    "section": "",
    "text": "8.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-sampleVsPopulation",
    "href": "research-methods.html#sec-sampleVsPopulation",
    "title": "8  Research Methods",
    "section": "\n8.2 Sample vs Population",
    "text": "8.2 Sample vs Population\nIn research, it is important to distinguish between the sample and the target population. The target population is who you want your study’s findings to generalize to. For instance, if we want our findings to lead to inferences we can draw regarding all current NFL players, then NFL players are our target population. However, despite our best efforts to recruit all NFL players into our study, we may not succeed in doing that. The participants (i.e., people or players) who we successfully recruit to be in our study represent our sample. The number of participants in the study is our sample size.\nIt is rare for the sample to include all people who are in the target population. It can be costly to recruit large samples, and many potential participants may decline to participate for a variety of reasons (insufficient time, lack of interest in the study, distrust of scientists, etc.). Thus, our goals are (a) to recruit as many people from the population as possible and (b) for the sample to be as representative of the population as possible.\nFor increasing the representativeness of the sample (with respect to the population), we might conduct a random sample, in which each person in the population (i.e., each NFL player) has equal likelihood of being selected. For instance, we might randomly select 250 players to recruit to the study. True random samples, though strong in aspiration, are difficult and costly to achieve. In reality, many researchers conduct convenience sampling. A convenience sample is recruited because it is convenient (i.e., less costly and time-consuming).\nFor instance, many studies examine college students—in part, because they are easy to recruit. If our target population is NFL players but we are unable to recruit NFL players into our study, we could easily recruit a large sample of college students. Although the convenience sample may afford a very large sample, the college student sample may not be representative of the target population (NFL players). Thus, the findings in our study may not generalize to NFL players—that is, what we learn in college students may not apply in the same way among NFL players. For instance, if we learn that consumption of sports drinks (compared to drinking only water) improves running speed among college students, that may not be the case among NFL players.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-hypothesisVsPrediction",
    "href": "research-methods.html#sec-hypothesisVsPrediction",
    "title": "8  Research Methods",
    "section": "\n8.3 Research Questions, Hypotheses, and Predictions",
    "text": "8.3 Research Questions, Hypotheses, and Predictions\nA research question is a question that the investigator (you!) wants to know the answer to. For example, a research question might be: “Does consumption of sports drink improve player performance?” A hypothesis is a proposed explanation. A prediction is “the expected result of a test that is derived, by deduction, from a hypothesis or theory” [Eastwell (2014), p. 17; archived at https://perma.cc/8EX4-8JYN]. Here is an example of a hypothesis and the resulting prediction:\n\nThe present study evaluates whether consumption of sports drink improves player performance. I hypothesize that consumption of sports drink leads football players to perform better in games because of greater endurance owing to restoration of electrolytes. If the hypothesis is true, I predict that players who consume sports drink during a game will score more fantasy points than players who do not consume sports drink during the game.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchDesigns",
    "href": "research-methods.html#sec-researchDesigns",
    "title": "8  Research Methods",
    "section": "\n8.4 Research Designs",
    "text": "8.4 Research Designs\nThere are three broad types of research designs:\n\nexperiment\ncorrelational/observational study\ncase study\n\n\n8.4.1 Experiment\nIn an experiment, there are one or more things (i.e., variables) that we manipulate to see how the manipulation influences the process of interest. The variable that we manipulate is the independent variable. By contrast, the dependent variable is the variable that we evaluate to determine whether it was influenced by the manipulation (i.e., by the independent variable). Besides the independent and dependent variables, the researcher attempts to hold everything else constant through processes including standardization and random assignment. Standardization involves using the same procedures to assess each participant, so that scores can be fairly compared across participants (and groups). Random assignment involves randomly assigning participants to conditions of the independent variable, so the people in each condition are comparable and do not differ systematically. However, not all things are feasible or ethical to manipulate. For instance, it would not be ethical to randomly assign some players to receive a head trauma. In addition, it would not be feasible to manipulate the weather of a game or the performance of players.\n\n8.4.1.1 Intervention Study\nAn intervention study is a study that involves some modification (e.g., a treatment) with the intent to improve people’s standing on the dependent variable (e.g., depression). Some intervention studies have a control group, whereas intervention studies do not. Inclusion of a control group is valuable; without a control group, you do not know whether any apparent gains in the treatment condition were due to the treatment per se versus just the mere passage of time, regression effects, or other things that were going on in the participants’ lives. An intervention that includes random assignment (e.g., to the intervention or control group) is an experiment. A randomized controlled trial (RCT) is an example of an experiment because it is an intervention with random assignment.\nFor instance, we may be interested to evaluate whether players perform better (e.g., run faster) if they drink a sports drink compared when they drink only water. Our hypothesis might be that players will be expected to perform better when they drink a sports drink (compared to when they drink only water), for the reasons specified in Section 8.3. To this this research question and hypothesis, we might conduct an experiment by randomly assigning some players during practice to receive a sports drink and some players to receive only water. In this case, our independent variable is whether the player receives a sports drink. Our dependent variable might be their 40-yard dash time during practice.\n\n8.4.2 Correlational/Observational Study\nIn a correlational (aka observational) study, we do not manipulate a variable to see how the manipulation influences another variable. Instead, we examine how two variables, a predictor and an outcome variable, are associated. The hypothesized cause is called the predictor variable. The hypothesized effect is called the outcome variable. In this way, the predictor variable is similar to the independent variable, and the outcome variable is similar to the dependent variable. However, unlike the independent and dependent variables in an experiment, the predictor and outcome variables in a correlational study are not manipulated.\nFor instance, to use a correlational study to test the possibility that players who drink sports drinks perform better than players who drink only water, we could examine whether the players who drink sports drinks during a game score more fantasy points than players who drink only water during the game. In this case, our predictor variable is whether the players drinks sports drinks during a game. Our outcome variable is the number of fantasy points the player scored.\n\n8.4.2.1 Correlation Does Not Imply Causation\nAs the maxim goes, “correlation does not imply causation”—just because two variables are associated does not necessarily mean that they are causally related.\nJust because X is associated with Y does not mean that X causes Y. Consider that you find an association between variables X and Y.\nThere are several reasons why you might observe an association between X and Y:\n\n\nX causes Y\n\n\nY causes X\n\n\nX and Y are bidirectional: X causes Y and Y causes X\n\na third variable (i.e., confound), Z, influences both X and Y\n\nthe association between X and Y is spurious\n\nFor instance, one possibility is that the association we observed reflects our hypothesis that X causes Y, as depicted in Figure 8.1. That is, consumption of more sports drink may improve players’ performance.\n\n\n\n\n\nFigure 8.1: Hypothesized Causal Effect Based on an Observed Association Between X and Y, Such That X Causes Y.\n\n\nHowever, a second possibility is that the association reflects the opposite direction of effect, where Y actually causes X, as depicted in Figure 8.2. For instance, greater performance may lead players to drink more sports drink (rather than the reverse).\n\n\n\n\n\nFigure 8.2: Reverse (Opposite) Direction of Effect From the Hypothesized Effect, Where Y Causes X.\n\n\nA third possibility is that the association reflects a bidirectional effect, where X causes Y and Y causes X, as depicted in Figure 8.3. For instance, consumption of more sports drink may improve players’ performance, and greater performance in turn may lead players to drink more sports drink.\n\n\n\n\n\nFigure 8.3: Bidirectional Effect Between X and Y, such that X causes Y and Y causes X.\n\n\nA fourth possibility is that the association could reflect the influence of a third variable. If a third variable is a common cause of each and accounts for their association, it is a confound. An observed association between X and Y could reflect a confound—i.e., a cause (Z) that influences both X and Y, which explains why X and Y are correlated even though they are not causally related. A third variable confound that is a common cause of both X and Y is depicted in Figure 8.4. For instance, it may not be that sport drink consumption per se influences player performance; rather, it may be that players who are more intelligent or have more financial resources tend to drink more sports drinks and also tend to perform better. In this case, intelligence or financial resources may be a confound that influences both sports drink consumption and player performance, but sports drink consumptions—though correlated with player performance—does not influence player performance.\nFor another example, consider that ice cream sales are associated with shark attacks. It is unlikely that more people eating ice creams leads to shark attacks. There is a likely a third variable—heat waves—that is a confound because it influences both ice cream sales and shark attacks and explains their association.\n\n\n\n\n\nFigure 8.4: Confounded Association Between X and Y due to a Common Cause, Z.\n\n\nLastly, the association might be spurious. It might just reflect random variation (i.e., chance), and that when tested on an independent sample, what appeared as an association in the original dataset may not hold when testing the association in a new dataset.\n\n8.4.3 Case Study\nIn a case study, we assess a small sample of individuals (commonly only one person or a few people), often with rich qualitative information. Themes may be coded from the qualitative information, which may help inform inferences about whether some process may have played a role in influencing the outcome of interest. The inferences are then drawn in a subjective, qualitative way. Testimonials and anecdotes are examples are case studies.\nFor instance, to use a case study to evalute the possibilty that players who drink sports drinks perform better than players who drink only water, we could conduct an in-depth interview with a player. In the interview, we might ask the player how they performed in games with versus without a sports drink and have them discuss whether they believe the sports drink improved their performance (and if so, how). Then, based on the player’s responses, we might code the responses to extract themes and to make a qualitative judgement of whether or not the player likely performed better during games in which they had a sports drink.\n\n8.4.4 Other Features of the Research Design\n\n8.4.4.1 Number of Timepoints\nIn addition to whether the research design is an experiment, correlational/observational study, or a case study, a research design can also have one or multiple timepoints. The differing number of timepoints allow studies to be characterized as one of the following:\n\ncross-sectional\nlongitudinal\n\n\n8.4.4.1.1 Cross-Sectional\nA cross-sectional study is a study with one timepoint.\nFor instance, in a cross-sectional study evaluating whether having a sports drink improves player performance, we might assess players’ drinking behavior and performance during only game 1.\nCross-sectional studies are more common than longitudinal studies because cross-sectional studies are less costly and time-consuming. They can provide a helpful starting point to test findings more rigorously in subsequent longitudinal studies.\n\n8.4.4.1.2 Longitudinal Design\nA longitudinal study is a study with more than one timepoint. When the same measures are assessed at each of multiple timepoints, we refer to this as a “repeated measures” design.\nIn a longitudinal study evaluating whether having a sports drink improves player performance, we might assess players’ drinking behavior and performance during each game of the season, and possibly across multiple seasons.\nLongitudinal studies are less common than cross-sectional studies because longitudinal studies are more costly and time-consuming. Nevertheless, longitudinal studies can allow us test our hypotheses more rigorously, because they can allow us to test whether changes in the predictor/indepdnent variable leads to changes in the outcome/dependent variable. Thus, compared to cross-sectional studies, longitudinal studies can provide greater confidence in causal inferences.\n\n8.4.4.2 Within- or Between-Subject\nA research design can also be within-subject, between-subject, or both. A study can involve both within-subject and between-subject comparisons if one predictor/independent variable is within-subject and another predictor/independent variable is between-subject.\n\n8.4.4.2.1 Within-Subject Design\nA within-subject design is one in which each participant (i.e., person or player) receives multiple levels of the independent variable (or predictor).\nFor instance, in an experiment evaluating whether having a sports drink improves player performance, we might assign players to drink the sports drink in the first half of the game and to drink only water in the second half of the game. Or we could assign some of the players to drink sports drink in the first half and water in the second half, and assign the other players to drink water in the first half and sports drink in the second half.\nIn a correlational study evaluating whether having a sports drink improves player performance, we might evaluate how within-person changes in sports drink consumption are associated with within-person changes in performance. That is, we could evaluate, when a given player has a sports drink (or more sports drinks), do they perform better than when the same individual has only water (or fewer sports drinks)?\nWithin-subject designs tend to have greater statistical power than between-subject designs. However, within-subject designs often have carryover effects. For instance, consider the study in which we assign players to drink only water in the first and third quarters and to drink sports drink in the second and fourth quarters (an A-B-A-B design). Drinking sports drink in the second quarter could increase how much hydration a player has throughout the rest of the game, which could lead to altered performance in the third and fourth quarters that is not due to what they drink in third and fourth quarters.\n\n8.4.4.2.2 Between-Subject Design\nA between-subject design is one in which each participant (i.e., person or player) receives only one level of the independent variable.\nFor instance, in an experiment evaluating whether having a sports drink improves player performance, we might assign some players to drink the sports drink but the other players to drink only water.\nIn a correlational study evaluating whether having a sports drink improves player performance, we might evaluate whether people who drink sports drinks tend to perform better than players who drink only water. Or, we could evaluate whether players who drink more sports drinks perform better than players who drink fewer sports drinks (i.e., whether the number of sports drinks consumed during a game is correlated with player performance).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchDesignValidity",
    "href": "research-methods.html#sec-researchDesignValidity",
    "title": "8  Research Methods",
    "section": "\n8.5 Research Design Validity",
    "text": "8.5 Research Design Validity\nResearch design validity involves the accuracy of inferences from a study. There are three types of research design validity:\n\ninternal validity\nexternal validity\nconclusion validity\n\n\n8.5.1 Internal Validity\nInternal validity is the extent to which we can be confident that the associations identified in the study are causal.\n\n8.5.2 External Validity\nExternal validity is the extent to which we can be confident that findings from the study play out similarly in the real world—that is, the findings generalize to the target population.\n\n8.5.3 Tradeoffs Between Internal and External Validity\nThere is a tradeoff between internal and external validity—a single research design cannot have both high internal and high external validity. Each study and design has weaknesses. Some research designs are better suited for making causal inferences, whereas other designs tend to be better suited for making inferences that generalize to the real world. The research design that is best suited to making causal inferences is an experiment because it is the design in which the researcher has the greatest control over the variables. Thus, experiments tend to have higher internal validity than other research designs. However, by manipulating one variable and holding everything else constant, the research takes place in a very standardized fashion that can become like studying a process in a vacuum. So, even if a process is theoretically causal in a vacuum, it may act differently in the real world when it interacts with other processes.\nCorrelational designs have greater capacity for external validity than experimental designs because the participants can be observed in their natural environments to evaluate how variables are related in the real world. However, the greater external validity comes at a cost of lower internal validity. Correlational designs are not well-positioned to make causal inferences. Correlational studies can account for potential confounds using covariates or for the reverse direction of effect using longitudinal designs, but the researcher has less control over the variables than in an experiment.\nAs the internal validity of a study’s design increases, its external validity tends to decrease. The greater control we have over variables (and, therefore, have greater confidence about causal inferences), the lower the likelihood that the findings reflect what happens in the real world because it is studying things in a metaphorical vacuum. Because no single research design can have both high internal and external validity, scientific inquiry needs a combination of many different research designs so we can be more confident in our inferences—experimental designs for making causal inferences and correlational designs for making inferences that are more likely to reflect the real world.\nCase studies, because they have smaller sample sizes and inferences drawn in a subjective, qualitative way, tend to have lower external validity than both experimental and correlational studies. Case studies also tend to have lower internal validity because they have less control over variables, and thus fail to remove the possibility of illusory correlations, potential confounds, or the reverse direction of effect. Thus, case studies are among the weakest forms of evidence. Nevertheless, case studies can still be useful for generating hypotheses that can then be tested empirically with a larger sample in experimental or correlational studies.\n\n8.5.4 Conclusion Validity\nConclusion validity is the extent to which a study’s conclusions are reasonable about the association among variables based on the data. That is, were the correct statistical analyses performed, and are the interpretations of the findings from those analyses correct?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-mediationModeration",
    "href": "research-methods.html#sec-mediationModeration",
    "title": "8  Research Methods",
    "section": "\n8.6 Mediation vs Moderation",
    "text": "8.6 Mediation vs Moderation\nBoth types of effects involve (at least) three variables:\n\nAn independent/predictor variable, which will be labeled as X.\nA dependent/outcome variable, which will be labeled as Y.\nThe mediator or moderator variable, which will be labeled as M.\n\nA mnemonic to help remember the difference between mediation and moderation is in Figure 8.5.\n\n\n\n\n\nFigure 8.5: Mediation Versus Moderation Mnemonic.\n\n\n\n8.6.1 Mediation\n\n8.6.1.1 Overview\nMediation is a causal chain of events, where one variable (a mediator variable) at least partially explains (or accounts for) the association between two other variables (the predictor variable and the outcome variable). In mediation, a predictor (X) leads to a mediator (M), which leads to an outcome (Y). Mediation answers the question of, “Why (or how) does X influence Y? A mediator (M) is a variable that helps explain the assocation between two other variables, and it answers the question of why/how X influences Y. That is, the mediator is the variable that helps explain how/why X is related to Y. In other words, you can think of the mediator as the mechanism that helps explain why X has an impact on Y. The association between X and Y gets smaller when accounting for M. Visually this can be written as in Figure 8.6:\n\n\n\n\n\nFigure 8.6: Mediation.\n\n\nwhere X is causing M, which in turn is causing Y. In other words, X leads to M, and M leads to Y.\nFor instance, if we determine that consuming sports drinks improves player performance, we may want to know how/why. That is, what is the mechanism that leads consumption of sports drinks to improve player performance? We might hypothesize that consumption of sports drink helps increase a player’s hydration, which in turn will improve the player’s performance. In this case, increased hydration mediates (i.e., helps explain or account for) the effect of the sports drink consumption on improved player performance.\nQuestion: Why/how does sports drink consumpion lead players to perform better?\nAnswer: increased hydration\nAs a picture, we can draw this assocation as in Figure 8.7:\n\n\n\n\n\nFigure 8.7: Mediation Example.\n\n\n\n8.6.1.2 Types of Mediation\n\n8.6.1.2.1 Full Mediation\nWhen one mechanism fully accounts for the effect of the predictor variable on the outcome variable, this is known as full mediation, as depicted in Figure 13.15:\n\n\n\n\n\nFigure 8.8: Full Mediation.\n\n\n\n8.6.1.2.2 Partial Mediation\nWhen a single process partially—but does not fully—accounts for the effect of the predictor variable on the outcome variable; this is known as partial mediation and is depicted in Figure 13.16:\n\n\n\n\n\nFigure 8.9: Partial Mediation.\n\n\n\n8.6.1.2.3 Multiple Mediators\nIn addition, there can be multiple mediators/mechanisms that account for the effect of a predictor variable on an outcome variable, as depicted in Figure 8.10:\n\n\n\n\n\nFigure 8.10: Multiple Mediators.\n\n\n\n8.6.2 Moderation (i.e., Interaction)\n\n8.6.2.1 Overview\nModeration (sometimes called an “interaction”), on the other hand, occurs when there is a variable or condition (M; called a “moderator”) that changes the assocation between X and Y. That is, the effect of the predictor variable on the outcome variable differs at different levels of the moderator variable. In these cases, X and M work together to have an effect on Y; here X does not have a direct effect on M. Moderation answers the question of, “For whom does X influence Y?” If X influences Y more strongly for some people or in some circumstances, we would say that there is an interaction such that the effect of X on Y depends on M, as depicted in Figure 8.11:\n\n\n\n\n\nFigure 8.11: Moderation.\n\n\nFor example, if the effect of consuming sports drinks on player performance differs for Quarterbacks and Wide Receivers, the interaction could be depicted in Figures 8.12 and 8.13:\n\n\n\n\n\nFigure 8.12: Moderation Example: Path Diagram.\n\n\n\n\n\n\n\nFigure 8.13: Moderation Example: Interaction Graph.\n\n\nAn interaction can be identified visually by non-parallel lines at different levels of the moderator. In this example, the player’s position moderates the effect consuming sports drinks on player performance. In particular, there is a strong positive association between consuming sports drinks and player performance for Wide Receivers (as evidenced by the upward slope of the best-fit regression line), whereas there is no association between consuming sports drinks and player performance for Quarterbacks (as evidenced by the flat line).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-levelsOfMeasurement",
    "href": "research-methods.html#sec-levelsOfMeasurement",
    "title": "8  Research Methods",
    "section": "\n8.7 Levels of Measurement",
    "text": "8.7 Levels of Measurement\nIt is important to know the levels of measurement of your data, because the level(s) of measurement of your data constrain the types of comparisons and analyses that you can meaningfully perform. There are four levels of measurement that any variable can have:\n\nnominal\nordinal\ninterval\nratio\n\nEach is described below:\n\n8.7.1 Nominal\nA variable is considered nominal if it is composed of qualitative classifications. You cannot meaningfully evaluate whether one number in the variable is larger than another number in the variable because higher numbers do not reflect higher levels of the concept. Examples of nominal variables include:\n\nsex (e.g., 1 = male; 2 = female)\nrace (e.g., 1 = American Indian; 2 = Asian; 3 = Black; 4 = Pacific Islander; 5 = White)\nethnicity (e.g., 0 = Non-Hispanic/Latino; 1 = Hispanic/Latino)\nzip code\njersey number\n\nA football player’s jersey number is an example of a nominal variable. A jersey number of 7 is not higher on whatever concept of interest compared to a jersey number of 6.\nTo examine the central tendency of a nominal variable, you can determine the mode, but you cannot calculate a mean or median.\n\n8.7.2 Ordinal\nA variable is considered ordinal if the classifications are ordered. However, ordinal variables do not have equally spaced intervals. Examples of ordinal intervals include:\n\nlikert response scales (e.g., 1 = strongly disagree; 2 = disagree; 3 = neutral; 4 = agree; 5 = strongly agree)\neducational attainment (e.g., 1 = no formal education; 2 = elementary school; 3 = middle school; 4 = high school; 5 = college; 6 = graduate degree)\nacademic grades on A–F scale (e.g., 1 = A; 2 = B; 3 = C; 4 = D; 5 = F)\nplayer rank (1 = 1st; 2 = 2nd; 3 = 3rd, etc.)\n\nA football player’s fantasy rank is an example of an ordinal variable. A player with a fantasy rank of 1 has a higher rank than a player with a rank of 2, but it is not known how far apart each player is—i.e., the intervals do not all reflect the same distance. For instance, the distance between the top-ranked player and the 2nd-best player might be 30 points, whereas the distance between the 2nd-best player and the 3rd-best player might be 2 points.\nTo examine the central tendency of ordinal data, the median and mode are most appropriate; however, the mean may be used (unlike for nominal data).\n\n8.7.3 Interval\nA variable is considered interval if the classifications are ordered (similar to ordinal data) and have equally spaced intervals (unlike ordinal data). However, interval variables do not have a meaningful zero that reflects absence. Examples of interval data include:\n\ntemperature on the Fahrenheit or Celsius scale\ntime of day\n\nFor instance, the temperature difference between 80 and 90 degrees Fahrenheit is the same as the temperature difference between 90 and 100 degrees Fahrenheit. However, 0 degrees Fahrenheit does not reflect absence of temperature/heat.\nInterval data can be meaningfully added or subtracted. For instance, if a game starts at 4 pm and ends at 7 pm, you know the game lasted 3 hours (\\(7 - 4 = 3\\)). However, interval data cannot be meaningfully multiplied or divided. For instance, 100 degrees Fahrenheit is not twice as hot as 50 degrees Fahrenheit.\nTo examine the central tendency of interval data, you can compute the mean, median, or mode.\n\n8.7.4 Ratio\nA variable is considered ratio if the classifications are ordered (similar to ordinal data), have equally spaced intervals (like interval data), and have an absolute zero point that reflects absence of the concept. Examples of ratio data include:\n\ntemperature on the Kelvin scale\nheight\nweight\nage\ndistance\nspeed\nvolume\ntime elapsed\nincome\nstock price\nyears of formal education\npoints in football\n\nFor instance, points in football has order, equally spaced intervals, and an absolute zero—a team cannot score less than zero points, and zero points reflects absence of points (though it could be argued to be interval data because zero points does not reflect absence of skill.)\nRatio data can be meaningfully added, subtracted, multiplied, or divided. A player who weighs 350 pounds weighs twice as much as someone who weighs 175 pounds.\nTo examine the central tendency of ratio data, you can compute the mean, median, or mode.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-psychometrics",
    "href": "research-methods.html#sec-psychometrics",
    "title": "8  Research Methods",
    "section": "\n8.8 Psychometrics",
    "text": "8.8 Psychometrics\nBelow, I provide brief discussions of various aspects of measurement reliability and validity. For more information on these and other aspects of psychometrics, see Petersen (2024) and Petersen (2025).\n\n8.8.1 Measurement Reliability\nThe reliability of a measure’s scores deals with the consistency of measurement. This book focuses on the following types of reliability:\n\ntest–retest reliability\ninter-rater reliability\nintra-rater reliability\ninternal consistency\nparallel-forms reliability\n\nFor more information on these and other aspects of reliability, see https://isaactpetersen.github.io/Principles-Psychological-Assessment/reliability.html (Petersen, 2024, 2025).\n\n8.8.1.1 Test–Retest Reliability\nTest–retest reliability evaluates the consistency of scores across time. For a construct that is expected to be stable across time (e.g., hand size in adults), we would expect our measurements to be consistent across time. The consistency of scores across time can be examined in terms of relative or absolute test–retest reliability. Relative test–retest reliability—i.e., the consistency of individual differences across time—is commonly evaluated using the coefficient of stability (i.e., the Pearson correlation coefficent). Absolute test–retest reliability—i.e., the absolute consistency of people’s scores across time—is commonly evaluated using the coefficient of repeatability.\n\n8.8.1.2 Inter-Rater Reliability\nInter-rater reliability evaluates the consistency of scores across raters. For instance, if we have a strong measure for assessing college players’ aptitude to succeed in the NFL, the measure should yield a similar score for a given player regardless of which (trained) rater (e.g., coach or talent scout) uses it to rate the player. The consistency of scores across raters is commonly evaluated using the intraclass correlation coefficient (for continuous variables) and Cohen’s kappa (\\(\\kappa\\); for categorical variables).\n\n8.8.1.3 Intra-Rater Reliability\nIntra-rater reliability evaluates the consistency of scores within a given rater. If we have a strong measure for assessing college players’ aptitude to succeed in the NFL, the measure should yield a similar score for a given player from the same (trained) rater (e.g., coach or talent scout) each time they rate the same player (assuming the player’s aptitude has not changed). The consistency of scores within raters can be evaluated using similar approaches as those evaluating inter-rater reliability.\n\n8.8.1.4 Internal Consistency\nInternal consistency evaluates the consistency of scores across items within a measure. If we develop a strong questionnaire measure to assess a college players’ aptitude to succeed in the NFL, the scores should be relatively consistent across items. The consistency of scores across items within a measure is commonly evaluated using Cronbach’s alpha (\\(\\alpha\\)) or McDonald’s omega (\\(\\omega\\)).\n\n8.8.1.5 Parallel-Forms Reliability\nParallel-forms reliability evaluates the consistency of scores across different but equivalent forms of a measure. If we develop two equivalent versions of the Wonderlic Contemporary Cognitive Ability Test (Form A and Form B) so that players sitting next to each other do not receive the same items, we would expect a player’s score on Form A would be similar to their score on Form B. Parallel-forms reliability is is commonly evaluated using the coefficient of equivalence (i.e., the Pearson correlation coefficent).\n\n8.8.2 Measurement Validity\nThe validity of a measure’s scores deals with the accuracy of measurement. This book focuses on the following types of validity:\n\nface validity\ncontent validity\n\ncriterion-related validity\n\nconcurrent (criterion-related) validity\npredictive (criterion-related) validity\n\n\nconstruct validity\nconvergent validity\ndiscriminant validity\nincremental validity\necological validity\n\nFor more information on these and other aspects of validity, see https://isaactpetersen.github.io/Principles-Psychological-Assessment/validity.html (Petersen, 2024, 2025).\n\n8.8.2.1 Face Validity\nFace validity evaluates the extent to which a measure “looks like” (on its face) it assesses the construct of interest. For instance, if a measure is developed to assess aptitude of Wide Receivers for the position, it would be considered to have face validity if everyday (lay) people believe that it assesses aptitude for being a successful Wide Receiver.\n\n8.8.2.2 Content Validity\nContent validity evaluates the extent to which the measure assesses the full breadth of the content, as determined by context experts. For the measure to have content validity, it should not have gaps (missing content facets) or intrusions (facets of other constructs). For instance, a strong measure for assessing a player’s aptitude to succeed in the NFL might need to include a player’s speed, strength, size, lateral quickness, etc. If the measure is missing their speed, this would be a content gap. If the measure assesses a construct-irrelevant facet (e.g., their attractiveness), this would be a content intrusion.\n\n8.8.2.3 Criterion-Related Validity\nCriterion-related validity evaluates the extent to which the measure’s scores are related to meaningful variables of interest. Criterion-related validity is commonly evaluated using a Pearson correlation or some form of regression.\nThere are two types of criterion-related validity:\n\nconcurrent (criterion-related) validity\npredictive (criterion-related) validity\n\n\n8.8.2.3.1 Concurrent (Criterion-Related) Validity\nConcurrent criterion-related validity (aka concurrent validity) evaluates the extent to which the measure’s scores are related to meaningful variables of interest assessed at the same point in time. That is, concurrent validity could evaluate whether current player statistics (e.g., passing yards) are associated with their fantasy points.\n\n8.8.2.3.2 Predictive (Criterion-Related) Validity\nPredictive criterion-related validity (aks predictive validity) evaluates the extent to which the measure’s scores are related to meaningful variables of interest that are assessed at a later point in time. For example, predictive validity could evaluate whether scores on the measure we developed to assess a player’s aptitude to succeed in the NFL predicts later performance in the NFL.\n\n8.8.2.4 Construct Validity\nConstruct validity evaluates the extent to which the measure’s scores accurately assess the construct of interest. If we develop a measure with intent to assess aptitude for being a successful Running Back, and it appears to more accurately assess aptitude for being a successful Wide Receiver, then our measure has poor construct validity for assessing aptitude for being a successful Running Back. Construct validity subsumes convergent and discriminant validity, in addition to all of the other forms of measurement validity.\n\n8.8.2.5 Convergent Validity\nConvergent validity evaluates the extent to which the measure’s scores are related to other measures of the same construct. For instance, if we develop a new measure to assess intelligence, its scores should be related to scores from other measures designed to assess intelligence (e.g., Wonderlic Contemporary Cognitive Ability Test).\n\n8.8.2.6 Discriminant Validity\nDiscriminant validity evaluates the extent to which the measure’s scores are unrelated to measures of the different constructs. For instance, if we develop a new measure to assess intelligence, its scores should be less strongly associated with measures of other constructs (e.g., measures of happiness).\n\n8.8.2.7 Incremental Validity\nIncremental validity evaluates the extent to which the measure’s scores provide an increase in predictive accuracy compared to other information that is easily and cheaply available. That is, in order to be useful, a strong measure should tell us something that we did not already know. For instance, if we develop a strong measure of intelligence, it should result in increased predictive accuracy (for success in the NFL) compared to when just relying on the Wonderlic Contemporary Cognitive Ability Test.\n\n8.8.2.8 Ecological Validity\nEcological validity evaluates the extent to which the measures’ scores are indicative of the behavior of a person in the natural environment. For instance, measures of a players’ speed during a game has higher ecological validity (and is more predictive of their performance) than their speed during the NFL Combine (Lyons et al., 2011). For instance, compared to tests of speed, power, and agility at the NFL Combine, collegiate performance is a stronger predictor of performance in the NFL (Lyons et al., 2011). That is, previous sports performance is the best predictor of future performance (for a review, see Den Hartigh et al., 2018).\n\n8.8.3 Reliability vs Validity\nReliability and validity are different but related. Reliability refers to the consistency of scores, whereas accuracy refers to the accuracy of scores. Validity depends on reliability. Reliability is necessary—but insufficient for—validity. That is, consistency is necessary—but insufficient for—accuracy. As depicted in Figure 8.14, a measure can be no more valid than it is reliable. A measure can be consistent but inaccurate; however, a measure cannot be accurate but inconsistent.\n\n\n\n\n\nFigure 8.14: Reliability Versus Validity.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsConclusion",
    "href": "research-methods.html#sec-researchMethodsConclusion",
    "title": "8  Research Methods",
    "section": "\n8.9 Conclusion",
    "text": "8.9 Conclusion\nThere are various types of research designs. Each type of research design differs in the extent to which it supports the ability to draw causal inferences (internal validity) versus the extent to which it supports the ability to identify processes that generalize to the real-world (external validity). In addition, it is important to understand the distinction between sample and population, and the distinction between mediation and moderation. It is also important to consider the levels of measurement used because they constrain the types of analyses that may be performed. In addition, it is important to consider the psychometrics of measurements, including multiple aspects of reliability (consistency) and validity (accuracy).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsSessionInfo",
    "href": "research-methods.html#sec-researchMethodsSessionInfo",
    "title": "8  Research Methods",
    "section": "\n8.10 Session Info",
    "text": "8.10 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.3    fastmap_1.2.0     cli_3.6.4        \n [5] tools_4.4.3       htmltools_0.5.8.1 rmarkdown_2.29    knitr_1.50       \n [9] jsonlite_1.9.1    xfun_0.51         digest_0.6.37     rlang_1.1.5      \n[13] evaluate_1.0.3   \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nEastwell, P. (2014). Understanding hypotheses, predictions, laws, and theories. Science Education Review, 13(1), 16–21. https://eric.ed.gov/?id=EJ1057150\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J. (2011). On the predictive efficiency of past performance and physical ability: The case of the National Football League. Human Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html",
    "href": "basic-statistics.html",
    "title": "9  Basic Statistics",
    "section": "",
    "text": "9.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsGettingStarted",
    "href": "basic-statistics.html#sec-basicStatsGettingStarted",
    "title": "9  Basic Statistics",
    "section": "",
    "text": "9.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"DescTools\")\nlibrary(\"pwr\")\nlibrary(\"pwrss\")\nlibrary(\"WebPower\")\nlibrary(\"grid\")\nlibrary(\"tidyverse\")\n\n\n\n9.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_seasonal.RData object in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-descriptiveStatistics",
    "href": "basic-statistics.html#sec-descriptiveStatistics",
    "title": "9  Basic Statistics",
    "section": "\n9.2 Descriptive Statistics",
    "text": "9.2 Descriptive Statistics\nDescriptive statistics are used to describe data. For instance, they may be used to describe the center, spread, or shape of the data. There are various indices of each.\n\n9.2.1 Center\nIndices to describe the center (central tendency) of a variable’s data include:\n\nmean (aka “average”)\nmedian\nHodges-Lehmann statistic (aka pseudomedian)\nmode\nweighted mean\nweighted median\n\nThe mean of \\(X\\) (written as: \\(\\bar{X}\\)) is calculated as in Equation 9.5:\n\\[\n\\bar{X} = \\frac{\\sum X_i}{n} = \\frac{X_1 + X_2 + ... + X_n}{n}\n\\tag{9.1}\\]\n\nCodeexampleValues &lt;- c(0, 0, 10, 15, 20, 30, 1000)\nexampleValues_mean &lt;- apa(mean(exampleValues), 2)\n\n\nThat is, to compute the mean, sum all of the values and divide by the number of values (\\(n\\)). One issue with the mean is that it is sensitive to extreme (outlying) values. For instance, the mean of the values of 0, 0, 10, 15, 20, 30, and 1000 is 153.57.\n\nCodeexampleValues_median &lt;- median(exampleValues)\n\n\nThe median is determined as the value at the 50th percentile (i.e., the value that is higher than 50% of the values and is lower than the other 50% of values). Compared to the mean, the median is less influenced by outliers. The median of the values of 0, 0, 10, 15, 20, 30, and 1000 is 15.\n\nCodeexampleValues_pseudomedian &lt;- DescTools::HodgesLehmann(exampleValues)\n\n\nThe Hodges-Lehmann statistic (aka pseudomedian) is computed as the median of all pairwise means, and it is also robust to outliers. The pseudomedian can be computed using the DescTools package (Signorell, 2025). The pseudomedian of the values of 0, 0, 10, 15, 20, 30, and 1000 is 15.\n\nCodeexampleValues_mode &lt;- petersenlab::Mode(exampleValues)\n\n\nThe mode is the most common/frequent value. The mode of the values of 0, 0, 10, 15, 20, 30, and 1000 is 0. The petersenlab package (Petersen, 2025a) contains the Mode() function for computing the mode of a set of data.\nIf you want to give some values more weight to others, you can calculate a weighted mean and a weighted median (or other quantile), while assigning a weight to each value. The petersenlab package (Petersen, 2025a) contains various functions for computing the weighted median (i.e., a weighted quantile at the 0.5 quantile, which is equivalent to the 50th percentile) based on Akinshin (2023). Because some projections are outliers, we use a trimmed version of the weighted Harrell-Davis quantile estimator for greater robustness.\nBelow is R code to estimate each:\n\nCodemean(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 42.56248\n\nCodemedian(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 26\n\nCodeDescTools::HodgesLehmann(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 33.85\n\nCodepetersenlab::Mode(player_stats_seasonal$fantasyPoints)\n\n[1] 1\n\nCodeweighted.mean(\n  player_stats_seasonal$fantasyPoints,\n  weights = sample( # randomly generate weights (could specify them manually)\n    x = 1:3,\n    size = length(player_stats_seasonal$fantasyPoints),\n    replace = TRUE),\n  na.rm = TRUE)\n\n[1] 42.56248\n\nCodepetersenlab::wthdquantile(\n  player_stats_seasonal$fantasyPoints,\n  w = sample( # randomly generate weights (could specify them manually)\n    x = 1:3,\n    size = length(player_stats_seasonal$fantasyPoints),\n    replace = TRUE),\n  probs = 0.5)\n\n[1] 26.28229\n\n\n\n9.2.2 Spread\nIndices to describe the spread (variability) of a variable’s data include:\n\nstandard deviation\nvariance\nrange\nminimum and maximum\ninterquartile range (IQR)\nmedian absolute deviation\n\nThe (sample) variance of \\(X\\) (written as: \\(s^2\\)) is calculated as in Equation 9.2:\n\\[\ns^2 = \\frac{\\sum (X_i - \\bar{X})^2}{n-1}\n\\tag{9.2}\\]\nwhere \\(X_i\\) is each data point, \\(\\bar{X}\\) is the mean of \\(X\\), and \\(n\\) is the number of data points.\nThe (sample) standard deviation of \\(X\\) (written as: \\(s\\)) is calculated as in Equation 9.3:\n\\[\ns = \\sqrt{\\frac{\\sum (X_i - \\bar{X})^2}{n-1}}\n\\tag{9.3}\\]\nThe range is calculated of \\(X\\) is calculated as in Equation 9.4:\n\\[\n\\text{range} = \\text{maximum} - \\text{minimum}\n\\tag{9.4}\\]\nThe interquartile range (IQR) is calculated as in Equation 9.5:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\tag{9.5}\\]\nwhere \\(Q_3\\) is the score at the third quartile (i.e., 75th percentile), and \\(Q_1\\) is the score at the first quartile (i.e., 25th percentile).\nThe median absolute deviation (MAD) is the median of all deviations from the median, and is calculated as in Equation 9.6:\n\\[\n\\text{MAD} = \\text{median}(|X_i - \\tilde{X}|)\n\\tag{9.6}\\]\nwhere \\(\\tilde{X}\\) is the median of X. Compared to the standard deviation, the median absolute deviation is more robust to outliers.\nBelow is R code to estimate each:\n\n9.2.3 Shape\nIndices to describe the shape of a variable’s data include:\n\nskewness\nkurtosis\n\nPositive skewness (right-skewed) reflects a longer or heavier right-tailed distribution, whereas negative skewness (left-skewed) reflects a longer or heavier left-tailed distribution. Fantasy points tend to be positively skewed.\nThe kurtosis reflects the extent of extreme (outlying) values in a distribution relative to a normal distribution (or bell curve). A mesokurtic distribution (with a kurtosis value near zero) reflects a normal amount of tailedness. Positive kurtosis values reflect a leptokurtic distribution, where there are lighter tails and a sharper peak than a normal distribution. Negative kurtosis values reflect a platykurtic distribution, where there are heavier tails and a flatter peak than a normal distribution. Fantasy points tend to have a leptokurtic distribution.\nBelow is R code to estimate each:\n\n9.2.4 Combination\nTo estimate multiple indices of center, spread, and shape of the data, you can use the following code:\n\nCodepsych::describe(player_stats_seasonal[\"fantasyPoints\"])\n\n\n  \n\n\nCodeplayer_stats_seasonal %&gt;% \n  select(age, years_of_experience, fantasyPoints) %&gt;% \n  summarise(across(\n      everything(),\n      .fns = list(\n        n = ~ length(na.omit(.)),\n        missingness = ~ mean(is.na(.)) * 100,\n        M = ~ mean(., na.rm = TRUE),\n        SD = ~ sd(., na.rm = TRUE),\n        min = ~ min(., na.rm = TRUE),\n        max = ~ max(., na.rm = TRUE),\n        range = ~ max(., na.rm = TRUE) - min(., na.rm = TRUE),\n        IQR = ~ IQR(., na.rm = TRUE),\n        MAD = ~ mad(., na.rm = TRUE),\n        median = ~ median(., na.rm = TRUE),\n        pseudomedian = ~ DescTools::HodgesLehmann(., na.rm = TRUE),\n        mode = ~ petersenlab::Mode(., multipleModes = \"mean\"),\n        skewness = ~ psych::skew(., na.rm = TRUE),\n        kurtosis = ~ psych::kurtosi(., na.rm = TRUE)),\n      .names = \"{.col}.{.fn}\")) %&gt;%\n    pivot_longer(\n      cols = everything(),\n      names_to = c(\"variable\",\"index\"),\n      names_sep = \"\\\\.\") %&gt;% \n    pivot_wider(\n      names_from = index,\n      values_from = value)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-scoresAndScales",
    "href": "basic-statistics.html#sec-scoresAndScales",
    "title": "9  Basic Statistics",
    "section": "\n9.3 Scores and Scales",
    "text": "9.3 Scores and Scales\nThere are many different types of scores and scales. This book focuses on raw scores and z-scores. For information on other scores and scales, including percentile ranks, T-scores, standard scores, scaled scores, and stanine scores, see here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/scoresScales.html#scoreTransformation (Petersen, 2025b).\n\n9.3.1 Raw Scores\nRaw scores are the original data on the original metric. Thus, raw scores are considered unstandardized. For example, raw scores that represent the players’ age may range from 20 to 40. Raw scores depend on the construct and unit; thus raw scores may not be comparable across variables.\n\n9.3.2 z Scores\nz scores have a mean of zero and a standard deviation of one. z scores are frequently used to render scores across variables more comparable. Thus, z scores are considered a form of a standardized score.\nz scores are calculated using Equation 9.7:\n\\[\nz = \\frac{X - \\bar{X}}{\\sigma}\n\\tag{9.7}\\]\nwhere \\(X\\) is the observed score, \\(\\bar{X}\\) is the mean observed score, and \\(\\sigma\\) is the standard deviation of the observed scores.\nYou can easily convert a variable to a z score using the scale() function:\n\nCodescale(variable)\n\n\nWith a standard normal curve, 68% of scores fall within one standard deviation of the mean. 95% of scores fall within two standard deviations of the mean. 99.7% of scores fall within three standard deviations of the mean.\nThe area under a normal curve within one standard deviation of the mean is calculated below using the pnorm() function, which calculates the cumulative density function for a normal curve.\n\nCodestdDeviations &lt;- 1\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.6826895\n\n\nThe area under a normal curve within one standard deviation of the mean is depicted in Figure 9.1.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 9.1: Density of Standard Normal Distribution. The blue region represents the area within one standard deviation of the mean.\n\n\n\n\nThe area under a normal curve within two standard deviations of the mean is calculated below:\n\nCodestdDeviations &lt;- 2\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.9544997\n\n\nThe area under a normal curve within two standard deviations of the mean is depicted in Figure 9.2.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 9.2: Density of Standard Normal Distribution. The blue region represents the area within two standard deviations of the mean.\n\n\n\n\nThe area under a normal curve within three standard deviations of the mean is calculated below:\n\nCodestdDeviations &lt;- 3\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.9973002\n\n\nThe area under a normal curve within three standard deviations of the mean is depicted in Figure 9.3.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 9.3: Density of Standard Normal Distribution. The blue region represents the area within three standard deviations of the mean.\n\n\n\n\nIf you want to determine the z score associated with a particular percentile in a normal distribution, you can use the qnorm() function. For instance, the z score associated with the 37th percentile is:\n\nCodeqnorm(.37)\n\n[1] -0.3318533",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-inferentialStatistics",
    "href": "basic-statistics.html#sec-inferentialStatistics",
    "title": "9  Basic Statistics",
    "section": "\n9.4 Inferential Statistics",
    "text": "9.4 Inferential Statistics\nInferential statistics are used to draw inferences regarding whether there is (a) a difference in level on variable across groups or (b) an association between variables. For instance, inferential statistics may be used to evaluate whether Quarterbacks tend to have longer careers compared to Running Backs. Or, they could be used to evaluate whether number of carries is associated with injury likelihood. To apply inferential statistics, we make use of the null hypothesis (\\(H_0\\)) and the alternative hypothesis (\\(H_1\\)).\n\n9.4.1 Null Hypothesis Significance Testing\nTo draw statistical inferences, the frequentist statistics paradigm leverages null hypothesis significance testing. Frequentist statistics is the most widely used statistical paradigm. However, frequentist statistics is not the only statistical paradigm. Other statistical paradigms exist, including Bayesian statistics, which is based on Bayes’ theorem. This chapter focuses on the frequentist approach to hypothesis testing, known as null hypothesis significance testing. We discuss Bayesian statistics in Chapter 16.\n\n9.4.1.1 Null Hypothesis (\\(H_0\\))\nWhen testing whether there are differences in level across groups on a variable of interest, the null hypothesis (\\(H_0\\)) is that there is no difference in level across groups. For instance, when testing whether Quarterbacks tend to have longer careers compared to Running Backs, the null hypothesis (\\(H_0\\)) is that Quarterbacks do not systematically differ from Running Backs in the length of their career.\nWhen testing whether there is an association between variables, the null hypothesis (\\(H_0\\)) is that there is no association between the variables. For instance, when testing whether number of carries is associated with injury likelihood, the null hypothesis (\\(H_0\\)) is that there is no association between number of carries and injury likelihood.\n\n9.4.1.2 Alternative Hypothesis (\\(H_1\\))\nThe alternative hypothesis (\\(H_1\\)) is the researcher’s hypothesis that they want to evaluate. An alternative hypothesis (\\(H_1\\)) might be directional (i.e., one-sided) or non-directional (i.e., two-sided).\nDirectional hypotheses specify a particular direction, such as which group will have larger scores or which direction (positive or negative) two variables will be associated. Examples of directional hypotheses include:\n\nQuarterbacks have longer careers compared to Running Backs\nNumber of carries is positively associated with injury likelihood\n\nNon-directional hypotheses do not specify a particular direction. For instance, non-directional hypotheses may state that two groups differ but do not specify which group will have larger scores. Or, non-directional hypotheses may state that two variables are associated but do not state what the sign is of the association—i.e., positive or negative. Examples of non-directional hypotheses include:\n\nQuarterbacks differ in the length of their careers compared to Running Backs\nNumber of carries is associated with injury likelihood\n\n9.4.1.3 Statistical Significance\nIn science, statistical significance is evaluated with the p-value. The p-value does not represent the probability that you observed the result by chance. The p-value represents a conditional probability—it examines the probability of one event given another event. In particular, the p-value evaluates the likelihood that you would detect a result as at least as extreme as the one observed (in terms of the magnitude of the difference or of the association) given that the null hypothesis (\\(H_0\\)) is true.\nThis can be expressed in conditional probability notation, \\(P(A | B)\\), which is the probability (likelihood) of event A occurring given that event B occurred (or given condition B).\nThe conditional probability notation for a left-tailed directional test (i.e., Quarterbacks have shorter careers than Running Backs; or number of carries is negatively associated with injury likelihood) is in Equation 9.8.\n\\[\np\\text{-value} = P(T \\le t | H_0)\n\\tag{9.8}\\]\nwhere \\(T\\) is the test statistic of interest (e.g., the distribution of \\(t\\)-, \\(r-\\), or \\(F\\) values, depending on the test) and \\(t\\) is the observed test statistic (e.g., \\(t\\)-, \\(r-\\), or \\(F\\)-coefficient, depending on the test).\nThe conditional probability notation for a right-tailed directional test (i.e., Quarterbacks have longer careers than Running Backs; or number of carries is positively associated with injury likelihood) is in Equation 9.9.\n\\[\np\\text{-value} = P(T \\ge t | H_0)\n\\tag{9.9}\\]\nThe conditional probability notation for a two-tailed non-directional test (i.e., Quarterbacks differ in the length of their careers compared to Running Backs; or number of carries is associated with injury likelihood) is in Equation 9.10.\n\\[\np\\text{-value} = 2 \\times \\text{min}(P(T \\le t | H_0), P(T \\ge t | H_0))\n\\tag{9.10}\\]\nwhere min(a, b) is the smaller number of a and b.\nIf the distribution of the test statistic is symmetric around zero, the p-value for the two-tailed non-directional test simplifies to Equation 9.11.\n\\[\np\\text{-value} = 2 \\times P(T \\ge |t| | H_0)\n\\tag{9.11}\\]\nNevertheless, to be conservative (i.e., to avoid false positive/Type I errors), many researchers use two-tailed p-values regardless whether their hypothesis is one- or two-tailed.\nFor a test of group differences, the p-value evaluates the likelihood that you would observe a difference as large or larger than the one you observed between the groups if there were no systematic difference between the groups in the population, as depicted in Figure 9.4. For instance, when evaluating whether Quarterbacks have longer careers than Running Backs, and you observed a mean difference of 0.03 years, the p-value evaluates the likelihood that you would observe a difference as large or larger than 0.03 years between the groups if, in truth among all Quarterbacks and Running Backs in the NFL, Quarterbacks do not differ from Running Backs in terms of the length of their career.\nCodeset.seed(52242)\n\nnObserved &lt;- 1000\nnPopulation &lt;- 1000000\n\nobservedGroups &lt;- data.frame(\n  score = c(rnorm(nObserved, mean = 47, sd = 3), rnorm(nObserved, mean = 52, sd = 3)),\n  group = as.factor(c(rep(\"Group 1\", nObserved), rep(\"Group 2\", nObserved)))\n)\n\npopulationGroups &lt;- data.frame(\n  score = c(rnorm(nPopulation, mean = 50, sd = 3.03), rnorm(nPopulation, mean = 50, sd = 3)),\n  group = as.factor(c(rep(\"Group 1\", nPopulation), rep(\"Group 2\", nPopulation)))\n)\n\nggplot2::ggplot(\n  data = observedGroups,\n  mapping = aes(\n    x = score,\n    fill = group,\n    color = group\n  )\n) +\n  geom_density(alpha = 0.5) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_fill_manual(values = c(\"red\",\"blue\")) +\n  geom_vline(xintercept = mean(observedGroups$score[which(observedGroups$group == \"Group 1\")])) +\n  geom_vline(xintercept = mean(observedGroups$score[which(observedGroups$group == \"Group 2\")])) +\n  ggplot2::labs(\n    x = \"Score\",\n    y = \"Frequency\",\n    title = \"What is the probability my data would look like this...\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    #plot.title.position = \"plot\"\n    legend.position = \"inside\",\n    legend.margin = margin(0, 0, 0, 0),\n    legend.justification.top = \"left\",\n    legend.justification.left = \"top\",\n    legend.justification.bottom = \"right\",\n    legend.justification.inside = c(1, 1),\n    legend.location = \"plot\")\n\nggplot2::ggplot(\n  data = populationGroups,\n  mapping = aes(\n    x = score,\n    fill = group,\n    color = group\n  )\n) +\n  geom_density(alpha = 0.5) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_fill_manual(values = c(\"red\",\"blue\")) +\n  geom_vline(xintercept = mean(populationGroups$score[which(populationGroups$group == \"Group 1\")])) +\n  geom_vline(xintercept = mean(populationGroups$score[which(populationGroups$group == \"Group 2\")])) +\n  ggplot2::labs(\n    x = \"Score\",\n    y = \"Frequency\",\n    title = \"...if in the population, the groups were really this:\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    #plot.title.position = \"plot\",\n    legend.position = \"inside\",\n    legend.margin = margin(0, 0, 0, 0),\n    legend.justification.top = \"left\",\n    legend.justification.left = \"top\",\n    legend.justification.bottom = \"right\",\n    legend.justification.inside = c(1, 1),\n    legend.location = \"plot\")\n\n\n\n\n\n\n\n\n\n(a) What is the probability my data would look like this…\n\n\n\n\n\n\n\n\n\n(b) …if in the population, the groups were really this?\n\n\n\n\n\n\nFigure 9.4: Interpretation of p-Values When Examining The Differences Between Groups. The vertical black lines reflect the group means.\n\n\nFor a test of whether two variables are associated, the p-value evaluates the likelihood that you would observe an association as strong or stronger than the one you observed if there were no actual association between the variables in the population, as depicted in Figure 9.5. For instance, when evaluating whether number of carries is positively associated with injury likelihood, and you observed a correlation coefficient of \\(r = .25\\) between number of carries and injury likelihood, the p-value evaluates the likelihood that you would observe a correlation as strong or stronger than \\(r = .25\\) between the variables if, in truth among all NFL Running Backs, number of carries is not associated with injury likelihood.\nCodeset.seed(52242)\n\nobservedCorrelation &lt;- 0.9\n\ncorrelations &lt;- data.frame(criterion = rnorm(2000))\ncorrelations$sample &lt;- NA\ncorrelations$sample[1:100] &lt;- complement(correlations$criterion[1:100], observedCorrelation)\ncorrelations$population &lt;- complement(correlations$criterion, 0)\n\nggplot2::ggplot(\n  data = correlations,\n  mapping = aes(\n    x = sample,\n    y = criterion\n  )\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(\n    limits = c(-3.5,3)\n  ) +\n  annotate(\n    x = 0,\n    y = 4,\n    label = paste(\"italic(r) != \", 0, sep = \"\"),\n    parse = TRUE,\n    geom = \"text\",\n    size = 7) + \n  labs(\n    x = \"Predictor Variable\",\n    y = \"Outcome Variable\",\n    title = \"What is the probability my data would look like this...\"\n  ) +\n  theme_classic(\n    base_size = 16) +\n  theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\nggplot2::ggplot(\n  data = correlations,\n  mapping = aes(\n    x = population,\n    y = criterion\n  )\n) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  scale_x_continuous(\n    limits = c(-2.5,2.5)\n  ) +\n  annotate(\n    x = 0,\n    y = 4,\n    label = paste(\"italic(r) == '\", \"0.00\", \"'\", sep = \"\"),\n    parse = TRUE,\n    geom = \"text\",\n    size = 7) + \n  labs(\n    x = \"Predictor Variable\",\n    y = \"Outcome Variable\",\n    title = \"...if in the population, the association was really this:\"\n  ) +\n  theme_classic(\n    base_size = 16) +\n  theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\n(a) What is the probability my data would look like this…\n\n\n\n\n\n\n\n\n\n(b) …if in the population, the association was really this?\n\n\n\n\n\n\nFigure 9.5: Interpretation of p-Values When Examining The Association Between Variables.\n\n\nUsing what is called null-hypothesis significance testing (NHST), we consider an effect to be statistically significant if the p-value is less than some threshold, called the alpha level. In science, we typically want to be conservative because a false positive (i.e., Type I error) is considered more problematic than a false negative (i.e., Type II error). That is, we would rather say an effect does not exist when it really does than to say an effect does exist when it really does not. Thus, we typically set the alpha level to a low value, commonly .05. Then, we would consider an effect to be statistically significant if the p-value is less than .05. That is, there is a small chance (5%; or 1 in 20 times) that we would observe an effect at least as extreme as the effect observed, if the null hypothesis were true. So, you might expect around 5% of tests where the null hypothesis is true to be statistically significant just by chance. We could lower the rate of Type II (i.e., false negative) errors—i.e., we could detect more effects—if we set the alpha level to a higher value (e.g., .10); however, raising the alpha level would raise the possibility of Type I (false positive) errors.\nIf the p-value is less than .05, we reject the null hypothesis (\\(H_0\\)) that there was no difference or association. Thus, we conclude that there was a statistically significant (non-zero) difference or association. If the p-value is greater than .05, we fail to reject the null hypothesis; the difference/association was not statistically significant. Thus, we do not have confidence that there was a difference or association. However, we do not accept the null hypothesis; it could be there we did not observe an effect because we did not have adequate power to detect the effect—e.g., if the effect size was small, the data were noisy, and the sample size was small and/or unrepresentative.\nThere are four general possibilities of decision making outcomes when performing null-hypothesis significance testing:\n\nWe (correctly) reject the null hypothesis when it is in fact false (\\(1 - \\beta\\)). This is a true positive. For instance, we may correctly determine that Quarterbacks have longer careers than Running Backs.\nWe (correctly) fail to reject the null hypothesis when it is in fact true (\\(1 - \\alpha\\)). This is a true negative. For instance, we may correctly determine that Quarterbacks do not have longer careers than Running Backs.\nWe (incorrectly) reject the null hypothesis when it is in fact true (\\(\\alpha\\)). This is a false positive. When performing null hypothesis testing, a false positive is known as a Type I error. For instance, we may incorrectly determine that Quarterbacks have longer careers than Running Backs when, in fact, Quarterbacks and Running Backs do not differ in their career length.\nWe (incorrectly) fail to reject the null hypothesis when it is in fact false (\\(\\beta\\)). This is a false negative. When performing null hypothesis testing, a false negative is known as a Type II error. For instance, we may incorrectly determine that Quarterbacks and Running Backs do not differ in their career length when, in fact, Quarterbacks have longer careers than Running Backs.\n\nA two-by-two confusion matrix for null-hypothesis significance testing is in Figure 9.6.\n\n\n\n\n\nFigure 9.6: A Two-by-Two Confusion Matrix for Null-Hypothesis Significance Testing.\n\n\nIn statistics, power is the probability of detecting an effect, if, in fact, the effect exists. Otherwise said, power is the probability of rejecting the null hypothesis, if, in fact, the null hypothesis is false. Power is influenced by several variables:\n\nthe sample size (N): the larger the N, the greater the power\n\nfor group comparisons, the power depends on the sample size of each group\n\n\nthe effect size: the larger the effect, the greater the power\n\nfor group comparisons, larger effect sizes reflect:\n\nlarger between-group variance, and\nsmaller within-group variance (i.e., strong measurement precision, i.e., reliability)\n\n\n\n\nthe alpha level: the researcher specifies the alpha level (though it is typically set at .05); the higher the alpha level, the greater the power; however, the higher we set the alpha level, the higher the likelihood of Type I errors (false positives)\none- versus two-tailed tests: one-tailed tests have higher power than two-tailed tests\n\nwithin-subject versus between-subject comparisons: within-subject designs tend to have greater power than between-subject designs\n\n\nA plot of statistical power is in Figure 9.7, as adapted from Magnusson (2013) (archived at https://perma.cc/FG3J-85L6).\n\nCodem1 &lt;- 0  # mu H0\nsd1 &lt;- 1.5 # sigma H0\nm2 &lt;- 3.5 # mu HA\nsd2 &lt;- 1.5 # sigma HA\n \nz_crit &lt;- qnorm(1-(0.05/2), m1, sd1)\n \n# set length of tails\nmin1 &lt;- m1-sd1*4\nmax1 &lt;- m1+sd1*4\nmin2 &lt;- m2-sd2*4\nmax2 &lt;- m2+sd2*4\n# create x sequence\nx &lt;- seq(min(min1,min2), max(max1, max2), .01)\n# generate normal dist #1\ny1 &lt;- dnorm(x, m1, sd1)\n# put in data frame\ndf1 &lt;- data.frame(\"x\" = x, \"y\" = y1)\n# generate normal dist #2\ny2 &lt;- dnorm(x, m2, sd2)\n# put in data frame\ndf2 &lt;- data.frame(\"x\" = x, \"y\" = y2)\n \n# Alpha polygon\ny.poly &lt;- pmin(y1,y2)\npoly1 &lt;- data.frame(x=x, y=y.poly)\npoly1 &lt;- poly1[poly1$x &gt;= z_crit, ]\npoly1&lt;-rbind(poly1, c(z_crit, 0))  # add lower-left corner\n \n# Beta polygon\npoly2 &lt;- df2\npoly2 &lt;- poly2[poly2$x &lt;= z_crit,]\npoly2&lt;-rbind(poly2, c(z_crit, 0))  # add lower-left corner\n \n# power polygon; 1-beta\npoly3 &lt;- df2\npoly3 &lt;- poly3[poly3$x &gt;= z_crit,]\npoly3 &lt;-rbind(poly3, c(z_crit, 0))  # add lower-left corner\n \n# combine polygons\npoly1$id &lt;- 3 # alpha, give it the highest number to make it the top layer\npoly2$id &lt;- 2 # beta\npoly3$id &lt;- 1 # power; 1 - beta\npoly &lt;- rbind(poly1, poly2, poly3)\npoly$id &lt;- factor(poly$id,  labels = c(\"power\", \"beta\", \"alpha\"))\n\n# plot with ggplot2\nggplot(poly, aes(x,y, fill=id, group=id)) +\n  geom_polygon(show.legend=F, alpha=I(8/10)) +\n  # add line for treatment group\n  geom_line(data=df1, aes(x, y, color = \"H0\", group = NULL, fill = NULL), linewidth = 1.5, show_guide = FALSE) + \n  # add line for treatment group. These lines could be combined into one dataframe.\n  geom_line(data=df2, aes(color = \"HA\", group = NULL, fill = NULL), linewidth = 1.5, show_guide = FALSE) +\n  # add vlines for z_crit\n  geom_vline(xintercept = z_crit, linewidth=1, linetype=\"dashed\") +\n  # change colors\n  scale_color_manual(\n    \"Group\",\n    values = c(\"HA\" = \"#981e0b\", \"H0\" = \"black\")) +\n  scale_fill_manual(\"test\", values= c(\"alpha\" = \"#0d6374\", \"beta\" = \"#be805e\", \"power\"=\"#7cecee\")) +\n  # beta arrow\n  annotate(\"segment\", x = 0.1, y = 0.045, xend = 1.3, yend = 0.01, arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1) +\n  annotate(\"text\", label=\"beta\", x=0, y=0.05, parse=T, size=8) +\n  # alpha arrow\n  annotate(\"segment\", x = 4, y = 0.043, xend = 3.4, yend = 0.01, arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1) +\n  annotate(\"text\", label=\"frac(alpha,2)\", x=4.2, y=0.05, parse=T, size=8) +\n  # power arrow\n  annotate(\"segment\", x = 6, y = 0.2, xend = 4.5, yend = 0.15, arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1) +\n  annotate(\"text\", label = expression(paste(1-beta, \"  (\\\"power\\\")\")), x = 6.1, y = 0.21, parse = TRUE, size = 8) +\n  # H_0 title\n  annotate(\"text\", label = \"H[0]\", x = m1, y = 0.28, parse = TRUE, size = 8) +\n  # H_a title\n  annotate(\"text\", label = \"H[1]\", x = m2, y = 0.28, parse = TRUE, size = 8) +\n  ggtitle(\"Statistical Power\") +\n  # remove some elements\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill=\"white\"),\n    panel.border = element_blank(),\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=22))\n\n\n\n\n\n\nFigure 9.7: Statistical Power (Adapted from Kristoffer Magnusson: https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics; archived at https://perma.cc/FG3J-85L6). The dashed line represents the critical value or threshold.\n\n\n\n\nInteractive visualizations by Kristoffer Magnusson on p-values and null-hypothesis significance testing are below:\n\n\nhttps://rpsychologist.com/pvalue [Magnusson (2021); archived at https://perma.cc/JP9F-9ZVY]\n\nhttps://rpsychologist.com/d3/pdist [Magnusson (2015); archived at https://perma.cc/BE96-8LSJ]\n\nhttps://rpsychologist.com/d3/nhst [Magnusson (2014); archived at https://perma.cc/ZU9A-37F3]\n\nTwelve misconceptions about p-values (Goodman, 2008) are in Table 9.1.\n\n\nTable 9.1: Twelve Misconceptions About p-Values from Goodman (2008). Goodman also provides a discussion about why each statement is false.\n\n\n\n\n\n\n\nNumber\nMisconception\n\n\n\n1\nIf \\(p = .05\\), the null hypothesis has only a 5% chance of being true.\n\n\n2\nA nonsignificant difference (eg, \\(p &gt; .05\\)) means there is no difference between groups.\n\n\n3\nA statistically significant finding is clinically important.\n\n\n4\nStudies with \\(p\\)-values on opposite sides of .05 are conflicting.\n\n\n5\nStudies with the same \\(p\\)-value provide the same evidence against the null hypothesis.\n\n\n6\n\n\\(p = .05\\) means that we have observed data that would occur only 5% of the time under the null hypothesis.\n\n\n7\n\n\\(p = .05\\) and \\(p &lt; .05\\) mean the same thing.\n\n\n8\n\n\\(p\\)-values are properly written as inequalities (e.g., “\\(p \\le .05\\)” when \\(p = .015\\)).\n\n\n9\n\n\\(p = .05\\) means that if you reject the null hypothesis, the probability of a Type I error is only 5%.\n\n\n10\nWith a \\(p = .05\\) threshold for significance, the chance of a Type I error will be 5%.\n\n\n11\nYou should use a one-sided \\(p\\)-value when you don’t care about a result in one direction, or a difference in that direction is impossible.\n\n\n12\nA scientific conclusion or treatment policy should be based on whether or not the \\(p\\)-value is significant.\n\n\n\n\n\n\nThat is, the p-value is not:\n\nthe probability that the effect was due to chance\nthe probability that the null hypothesis is true\nthe size of the effect\nthe importance of the effect\nwhether the effect is true, real, or causal\n\nStatistical significance involves the consistency of an effect/association/difference; it suggests that the association/difference is reliably non-zero. However, just because something is statistically significant does not mean that it is important. For instance, consider that we discover that players who consume sports drink before a game tend to perform better than players who do not (\\(p &lt; .05\\)). However, what if consumption of sports drinks is associated with an average improvement of 0.002 points per game. A small effect such as this might be detectable with a large sample size. This effect would be considered to be reliable/consistent because it is statistically significant. However, such an effect is so small that it results in differences that are not practically important. Thus, in addition to statistical significance, it is also important to consider practical significance.\n\n9.4.2 Practical Significance\nPractical significance deals with how large or important the effect/association/difference is. It is based on the magnitude of the effect, called the effect size. Effect size can be quantified in various ways including:\n\nCohen’s \\(d\\)\n\nStandardized regression coefficient (beta; \\(\\beta\\))\nCorrelation coefficient (\\(r\\))\nCohen’s \\(\\omega\\) (omega)\nCohen’s \\(f\\)\n\nCohen’s \\(f^2\\)\n\nCoefficient of determination (\\(R^2\\))\nEta squared (\\(\\eta^2\\))\nPartial eta squared (\\(\\eta_p^2\\))\n\n\n9.4.2.1 Cohen’s \\(d\\)\n\nCohen’s \\(d\\) is calculated as in Equation 9.12:\n\\[\n\\begin{aligned}\n  d &= \\frac{\\text{mean difference}}{\\text{pooled standard deviation}} \\\\\n   &= \\frac{\\bar{X_1} - \\bar{X_2}}{s} \\\\\n\\end{aligned}\n\\tag{9.12}\\]\nwhere:\n\\[\ns = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\tag{9.13}\\]\nwhere \\(n_1\\) and \\(n_2\\) is the sample size of group 1 and group 2, respectively, and \\(s_1\\) and \\(s_2\\) is the standard deviation of group 1 and group 2, respectively.\n\n9.4.2.2 Standardized Regression Coefficient (Beta; \\(\\beta\\))\nThe standardized regression coefficient (beta; \\(\\beta\\)) is used in multiple regression, and is calculated as in Equation 9.14:\n\\[\n\\beta_x = B_x \\times \\frac{s_x}{s_y}\n\\tag{9.14}\\]\nwhere \\(B_x\\) is the unstandardized regression coefficient of the predictor variable \\(x\\) in predicting the outcome variable \\(y\\), \\(s_x\\) is the standard deviation of \\(x\\), and \\(s_y\\) is the standard deviation of \\(y\\).\n\n9.4.2.3 Correlation Coefficient (\\(r\\))\nThe formula for the correlation coefficient is in Chapter 10.\n\n9.4.2.4 Cohen’s \\(\\omega\\)\n\nCohen’s \\(\\omega\\) is used in chi-square tests, and is calculated as in Equation 9.15:\n\\[\n\\omega = \\sqrt{\\frac{\\chi^2}{N} - \\frac{df}{N}}\n\\tag{9.15}\\]\nwhere \\(\\chi^2\\) is the chi-square statistic from the test, \\(N\\) is the sample size, and \\(df\\) is the degrees of freedom.\n\n9.4.2.5 Cohen’s \\(f\\)\n\nCohen’s \\(f\\) is commonly used in ANOVA, and is calculated as in Equation 9.16:\n\\[\n\\begin{aligned}\n  f &= \\sqrt{\\frac{R^2}{1 - R^2}} \\\\\n    &= \\sqrt{\\frac{\\eta^2}{1 - \\eta^2}}\n\\end{aligned}\n\\tag{9.16}\\]\n\n9.4.2.6 Cohen’s \\(f^2\\)\n\nCohen’s \\(f^2\\) is commonly used in regression, and is calculated as in Equation 9.17:\n\\[\n\\begin{aligned}\n  f^2 &= \\frac{R^2}{1 - R^2} \\\\\n      &= \\frac{\\eta^2}{1 - \\eta^2}\n\\end{aligned}\n\\tag{9.17}\\]\nTo calculate the effect size of a particular predictor, you can calculate \\(\\Delta f^2\\) as in Equation 9.18:\n\\[\n\\begin{aligned}\n  \\Delta f^2 &= \\frac{R^2_{\\text{model}} - R^2_{\\text{reduced}}}{1 - R^2_{\\text{model}}} \\\\\n             &= \\frac{\\eta^2_{\\text{model}} - \\eta^2_{\\text{reduced}}}{1 - \\eta^2_{\\text{model}}}\n\\end{aligned}\n\\tag{9.18}\\]\nwhere \\(R^2_{\\text{model}}\\) is the \\(R^2\\) of the model with the predictor variable of interest and \\(R^2_{\\text{reduced}}\\) is the \\(R^2\\) of the model without the predictor variable of interest.\n\n9.4.2.7 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome variable that is explained by the predictor variable(s). \\(R^2\\) is commonly used in regression, and is calculated as in Equation 9.19:\n\\[\n\\begin{aligned}\n  R^2 &= 1 - \\frac{\\sum (Y_i - \\hat{Y}_i)^2}{\\sum (Y_i - \\bar{Y})^2} \\\\\n      &= 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{\\text{sum of squared residuals}}{\\text{total sum of squares}} \\\\\n      &= \\frac{f^2}{1 + f^2} \\\\\n      &= \\eta^2 \\\\\n      &= \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\n\\end{aligned}\n\\tag{9.19}\\]\nwhere \\(Y_i\\) is the observed value of the outcome variable for the \\(i\\)th observation, \\(\\hat{Y}_i\\) is the model predicted value for the \\(i\\)th observation, \\(\\bar{Y}\\) is the mean of the observed values of the outcome variable. The total sum of squares is an index of the total variation in the outcome variable.\n\n9.4.2.8 Eta Squared (\\(\\eta^2\\)) and Partial Eta Squared (\\(\\eta_p^2\\))\nLike \\(R^2\\), eta squared (\\(\\eta^2\\)) reflects the proportion of variance in the dependent variable that is explained by the independent variable(s). \\(\\eta^2\\) is commonly used in ANOVA, and is calculated as in Equation 9.20:\n\\[\n\\begin{aligned}\n  \\eta^2 &= \\frac{SS_{\\text{effect}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{\\text{sum of squared residuals}}{\\text{total sum of squares}} \\\\\n      &= \\frac{f^2}{1 + f^2} \\\\\n      &= R^2\n\\end{aligned}\n\\tag{9.20}\\]\nwhere \\(SS_{\\text{effect}}\\) is the sum of squares for the effect of interest and \\(SS_{\\text{total}}\\) is the total sum of squares.\nPartial eta squared (\\(\\eta_p^2\\)) reflects the proportion of variance in the dependent variable that is explained by the independent variable while controlling for the other independent variables. \\(\\eta_p^2\\) is commonly used in ANOVA, and is calculated as in Equation 9.21:\n\\[\n\\eta_p^2 = \\frac{SS_{\\text{effect}}}{SS_{\\text{effect}} + SS_{\\text{error}}}\n\\tag{9.21}\\]\nwhere \\(SS_{\\text{effect}}\\) is the sum of squares for the effect of interest and \\(SS_{\\text{error}}\\) is the sum of squares for the residual error term.\n\n9.4.2.9 Effect Size Thresholds\nEffect size thresholds (Cohen, 1988; McGrath & Meyer, 2006) for small, medium, and large effect sizes are in Table 9.2.\n\n\nTable 9.2: Effect Size Thresholds for Small, Medium, and Large Effect Sizes.\n\n\n\n\n\n\n\n\n\nEffect Size Index\nSmall\nMedium\nLarge\n\n\n\nCohen’s \\(d\\)\n\n\\(\\ge |.20|\\)\n\\(\\ge |.50|\\)\n\\(\\ge |.80|\\)\n\n\nStandardized regression coefficient (beta; \\(\\beta\\))\n\\(\\ge |.10|\\)\n\\(\\ge |.24|\\)\n\\(\\ge |.37|\\)\n\n\nCorrelation coefficient (\\(r\\))\n\\(\\ge |.10|\\)\n\\(\\ge |.24|\\)\n\\(\\ge |.37|\\)\n\n\nCohen’s \\(\\omega\\)\n\n\\(\\ge .10\\)\n\\(\\ge .30\\)\n\\(\\ge .50\\)\n\n\nCohen’s \\(f\\)\n\n\\(\\ge .10\\)\n\\(\\ge .25\\)\n\\(\\ge .40\\)\n\n\nCohen’s \\(f^2\\)\n\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .16\\)\n\n\nCoefficient of determination (\\(R^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)\n\n\nEta squared (\\(\\eta^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)\n\n\nPartial eta squared (\\(\\eta_p^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-statisticalDecisionTree",
    "href": "basic-statistics.html#sec-statisticalDecisionTree",
    "title": "9  Basic Statistics",
    "section": "\n9.5 Statistical Decision Tree",
    "text": "9.5 Statistical Decision Tree\nA statistical decision tree is a flowchart or decision tree that depicts which statistical test to use given the purpose of analysis, the type of data, etc. An example statistical decision tree is depicted in Figure 9.8.\n\n\n\n\n\nFigure 9.8: A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R. & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including the addition of several statistical tests. Note: “Interval” as a level of measurement includes data with an “interval” or higher level of measurement; thus, it also includes data with a “ratio” level of measurement.\n\n\nThis statistical decision tree can be generally summarized such that associations are examined with the correlation/regression family, and differences are examined with the t-test/ANOVA family, as depicted in Figure 9.9.\n\n\n\n\n\nFigure 9.9: Summary of A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure.\n\n\nHowever, many statistical tests can be re-formulated in a regression framework, as in Figure 9.10.\n\n\n\n\n\nFigure 9.10: A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure, Re-Formulated in a Regression Framework. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R. & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including re-formulating the tests in a regression framework.\n\n\nBoth associations and differences can be examined with the regression family, which greatly simplifies our summary of the statistical decision tree, as depicted in Figure 9.11.\n\n\n\n\n\nFigure 9.11: Summary of A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure.\n\n\nThus, in most cases, the regression framework can be used to examine most questions regarding associations between variables or differences between groups.\nJackson-Wood (2017) provides an online, interactive statistical decision tree to help you decide which statistical analysis to use: https://www.statsflowchart.co.uk",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-statisticalTests",
    "href": "basic-statistics.html#sec-statisticalTests",
    "title": "9  Basic Statistics",
    "section": "\n9.6 Statistical Tests",
    "text": "9.6 Statistical Tests\n\n9.6.1 t-Test\nThere are several t-tests:\n\none-sample t-test\ntwo-samples t-test\n\nindependent samples t-test\npaired samples t-test\n\n\n\nA one-sample t-test is used to evaluate whether a sample mean differs systematically from a particular value. The null hypothesis is that the sample mean does not differ systematically from the pre-specified value. The alternative hypothesis is that the sample mean differs systematically from the pre-specified value. For instance, let’s say you want to test out a new draft strategy. You could participate in a mock draft and draft players using the new strategy. Then, you could use a one-sample t-test to evaluate whether your new draft strategy yields players with more projected points than the average of players’ projected points for other teams.\nTwo-samples t-tests are used to test for differences between scores of two groups. If the two groups are independent, the independent samples t-test is used. If the two groups involve paired samples, the paired samples t-test is used. The null hypothesis is that the mean of group 1 does not differ systematically from the mean of group 2. The alternative hypothesis is that the mean of group 1 differs systematically from the mean of group 2. For instance, you could use an independent-samples t-test if you want to examine whether Quarterbacks tend to have have longer careers than Running Backs. By contrast, you could use a paired samples t-test if you want to examine whether Quarterbacks tend to score more points in the second year of their contract compared to their rookie year, because the same subjects were assessed twice (i.e., a within-subject design).\n\n9.6.2 Analysis of Variance\nAnalysis of variance (ANOVA) allows examining whether groups differ systematically as a function of one or more factors. There are multiple variants of ANOVA:\n\none-way ANOVA\nfactorial ANOVA\nrepeated measures ANOVA (RM-ANOVA)\nmultivariate ANOVA (MANOVA)\n\nLike two-samples t-tests, ANOVA allows examining whether groups differ as a function of an independent variable. However, unlike a t-test, ANOVA allows examining multiple multiple independent variables and more than two groups. The null hypothesis is that the the groups’ mean value does not differ systematically. The alternative hypothesis is that the groups’ mean value differs systematically.\nA one-way ANOVA examines whether two or more groups differ as a function of an independent variable. For instance, you could use a one-way ANOVA to evaluate if you want to evaluate whether multiple positions differ in their length of career. Factorial ANOVA examines whether two or more groups differ as a function of multiple independent variables. For instance, you could use factorial ANOVA to evaluate whether one’s length of career depends on one’s position and weight. Repeated measures ANOVA examines whether scores differ across repeated measures (e.g., across time) for the same participants. For instance, you could use repeated-measures ANOVA to evaluate whether rookies score more points as the season progresses. Multivariate ANOVA examines whether multiple dependent variables differ as a function of one or more factor(s). For instance, you could use MANOVA to evaluate whether one’s contract length and pay differ as a function of one’s position.\n\n9.6.3 Correlation\nCorrelation examines the association between a predictor and outcome variable. The null hypothesis is that the the two variables are not associated. The alternative hypothesis is that the two variables are associated.\nThe Pearson correlation coefficient (\\(r\\)) is calculated as in Equation 9.22:\n\\[\nr = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2 \\sum (Y_i - \\bar{Y})^2}}\n\\tag{9.22}\\]\nwhere \\(X\\) is the predictor variable and \\(Y\\) is the outcome variable.\n\n9.6.4 (Multiple) Regression\nRegression, like correlation, examines the association between a predictor and outcome variable. However, unlike correlation, regression allows multiple predictor variables.\nRegression with a single predictor takes the form in Equation 11.1. A regression line is depicted in Figure 11.4. Multiple regression (i.e., regression with multiple predictors) takes the form in Equation 11.2.\nThe null hypothesis is that the the predictor variable(s) are not associated with the outcome variable. The alternative hypothesis is that the predictor variable(s) are associated with the outcome variable.\n\n9.6.5 Chi-Square Test\nThere are two primary types of chi-square tests:\n\nchi-square goodness-of-fit test\nchi-square test for association (aka test of independence)\n\nThe chi-square goodness-of-fit test evaluates whether a set of categorical data came from a specified distribution. The null hypothesis is that the data came from the specified distribution. The alternative hypothesis is that the data did not come from the specified distribution.\nThe chi-square test for association evaluates whether two categorical variables are associated. The null hypothesis is that the two variables are not associated. The alternative hypothesis is that the two variables are associated.\n\n9.6.6 Formulating Statistical Tests in Terms of Partitioned Variance\nMany statistical tests can be formulated in terms of partitioned variance.\nFor instance, the t statistic from the independent-samples t-test and the F statistic from ANOVA can be thought of as the ratio of between-group variance to within-group variance, as in Equation 9.23:\n\\[\nt \\text{ or } F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\n\\tag{9.23}\\]\nThe correlation coefficient can be thought of as the ratio of shared variance (i.e., covariance) to total variance, as in Equation 9.24:\n\\[\nr = \\frac{\\text{shared variance}}{\\text{total variance}}\n\\tag{9.24}\\]\nThe coefficient of determination (\\(R^2\\)) is the proportion of variance in the outcome variable that is explained by the predictor variables. \\(\\eta^2\\) is the proportion of variance in the dependent variable that is explained by the independent variables. The coefficient of determination and \\(\\eta^2\\) can be expressed as the ratio of variance explained in the outcome or dependent variable to the total variance in the outcome or dependent variable, as in Equation 9.25:\n\\[\nR^2 \\text{ or } \\eta^2 = \\frac{\\text{variance explained in the outcome variable}}{\\text{total variance in the outcome variable}}\n\\tag{9.25}\\]\n\n9.6.7 Critical Value\nThe critical value is the test value for a given test, above which the effect is considered to be statistically significant. The critical value for statistical significance for each test can be determined based on the degrees of freedom and alpha level. The degrees of freedom (df) refer to the number of values in the calculation of a test statistic that are free to vary.\n\nCodealpha &lt;- .05\nN &lt;- 200\nnGroup1 &lt;- 150\nnGroup2 &lt;- 150\nnumGroups &lt;- 4\nnumLevelsFactorA &lt;- 3\nnumLevelsFactorB &lt;- 4\nnumMeasurements &lt;- 4\nnumPredictors &lt;- 5\nnumCategories &lt;- 6\nnumRows &lt;- 5\nnumColumns &lt;- 2\n\n\n\n9.6.7.1 One-Sample t-Test\nFor a one-sample t-test, the degrees of freedom is in Equation 9.26:\n\\[\ndf = N - 1\n\\tag{9.26}\\]\nwhere \\(N\\) is sample size.\n\nCodedf_oneSampleTtest &lt;- N - 1\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_oneSampleTtest)\n\n[1] 1.652547\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_oneSampleTtest)\n\n[1] 1.971957\n\n\n\n9.6.7.2 Independent-Samples t-Test\nFor an independent-samples t-test, the degrees of freedom is in Equation 9.27:\n\\[\ndf = n_1 + n_2 - 2\n\\tag{9.27}\\]\nwhere \\(n_1\\) is the sample size of group 1 and \\(n_2\\) is the sample size of group 2.\n\nCodedf_independentSamplesTtest &lt;- nGroup1 + nGroup2 - 2\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_independentSamplesTtest)\n\n[1] 1.649983\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_independentSamplesTtest)\n\n[1] 1.967957\n\n\n\n9.6.7.3 Paired-Samples t-Test\nFor a paired-samples t-test, the degrees of freedom is in Equation 9.28:\n\\[\ndf = N - 1\n\\tag{9.28}\\]\nwhere \\(N\\) is sample size (i.e., the number of paired observations).\n\nCodedf_pairedSamplesTtest &lt;- N - 1\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_pairedSamplesTtest)\n\n[1] 1.652547\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_pairedSamplesTtest)\n\n[1] 1.971957\n\n\n\n9.6.7.4 One-Way ANOVA\nFor a one-way ANOVA, the degrees of freedom is in Equation 9.29:\n\\[\n\\begin{aligned}\n  df_\\text{between} &= g - 1 \\\\\n  df_\\text{within} &= N - g\n\\end{aligned}\n\\tag{9.29}\\]\nwhere \\(N\\) is sample size and \\(g\\) is the number of groups.\n\nCodedf_betweenOneWayANOVA &lt;- numGroups - 1\ndf_withinOneWayANOVA &lt;- N - numGroups\n\n\nOne-tailed test:\n\nCodeqf(1 - alpha, df_betweenOneWayANOVA, df_withinOneWayANOVA)\n\n[1] 2.650677\n\n\nTwo-tailed test:\n\nCodeqf(1 - alpha/2, df_betweenOneWayANOVA, df_withinOneWayANOVA)\n\n[1] 3.183378\n\n\n\n9.6.7.5 Factorial ANOVA\nFor a factorial two-way ANOVA, the degrees of freedom is in Equation 9.30:\n\\[\n\\begin{aligned}\n  df_\\text{Factor A} &= a - 1 \\\\\n  df_\\text{Factor B} &= b - 1 \\\\\n  df_\\text{Interaction} &= (a - 1)(b - 1) \\\\\n  df_\\text{error} &= ab(N - 1)\n\\end{aligned}\n\\tag{9.30}\\]\nwhere \\(N\\) is sample size, \\(a\\) is the number of levels for factor A, and \\(b\\) is the number of levels for factor B.\n\nCodedf_factorA &lt;- numLevelsFactorA - 1\ndf_factorB &lt;- numLevelsFactorB - 1\ndf_interaction &lt;- df_factorA * df_factorB\ndf_error &lt;- numLevelsFactorA * numLevelsFactorB * (N - 1)\n\n\nFactor A (one-tailed test):\n\nCodeqf(1 - alpha, df_factorA, df_error)\n\n[1] 2.999494\n\n\nFactor B (one-tailed test):\n\nCodeqf(1 - alpha, df_factorB, df_error)\n\n[1] 2.608629\n\n\nInteraction (one-tailed test):\n\nCodeqf(1 - alpha, df_interaction, df_error)\n\n[1] 2.102376\n\n\nFactor A (two-tailed test):\n\nCodeqf(1 - alpha/2, df_factorA, df_error)\n\n[1] 3.694584\n\n\nFactor B (two-tailed test):\n\nCodeqf(1 - alpha/2, df_factorB, df_error)\n\n[1] 3.121587\n\n\nInteraction (two-tailed test):\n\nCodeqf(1 - alpha/2, df_interaction, df_error)\n\n[1] 2.413504\n\n\n\n9.6.7.6 Repeated Measures ANOVA\nFor a repeated measures ANOVA, the degrees of freedom is in Equation 9.31:\n\\[\n\\begin{aligned}\n  df_1 &= T - 1 \\\\\n  df_2 &= (T - 1)(N - 1)\n\\end{aligned}\n\\tag{9.31}\\]\nwhere \\(N\\) is sample size and \\(T\\) is the number of measurements (i.e., the number of levels of the within-person factor: e.g., timepoints or conditions).\n\nCodedf1_RMANOVA &lt;- numMeasurements - 1\ndf2_RMANOVA &lt;- (numMeasurements - 1) * (N - 1)\n\n\nOne-tailed test:\n\nCodeqf(1 - alpha, df1_RMANOVA, df2_RMANOVA)\n\n[1] 2.619828\n\n\nTwo-tailed test:\n\nCodeqf(1 - alpha/2, df1_RMANOVA, df2_RMANOVA)\n\n[1] 3.138017\n\n\n\n9.6.7.7 Correlation\nFor a correlation, the degrees of freedom is in Equation 9.32:\n\\[\ndf = N - 2\n\\tag{9.32}\\]\nwhere \\(N\\) is sample size.\n\nCodedf_correlation &lt;- N - 2\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_correlation)\n\n[1] 1.652586\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_correlation)\n\n[1] 1.972017\n\n\n\n9.6.7.8 Multiple Regression\nFor multiple regression, the degrees of freedom is in Equation 9.33:\n\\[\n\\begin{aligned}\n  df_1 &= p \\\\\n  df_2 &= N - p - 1\n\\end{aligned}\n\\tag{9.33}\\]\nwhere \\(N\\) is sample size and \\(p\\) is the number of predictors.\n\nCodedf1_regression &lt;- numPredictors\ndf2_regression &lt;- N - numPredictors - 1\n\n\nOne-tailed test:\n\nCodeqf(1 - alpha, df1_regression, df2_regression)\n\n[1] 2.260647\n\n\nTwo-tailed test:\n\nCodeqf(1 - alpha/2, df1_regression, df2_regression)\n\n[1] 2.63243\n\n\n\n9.6.7.9 Chi-Square Goodness-of-Fit Test\nFor the chi-square goodness-of-fit test, the degrees of freedom is in Equation 9.34:\n\\[\ndf = c - 1\n\\tag{9.34}\\]\nwhere \\(c\\) is the number of categories.\n\nCodedf_chisquareGOF &lt;- numCategories - 1\n\n\nOne-tailed test:\n\nCodeqchisq(1 - alpha, df_chisquareGOF)\n\n[1] 11.0705\n\n\nTwo-tailed test:\n\nCodeqchisq(1 - alpha/2, df_chisquareGOF)\n\n[1] 12.8325\n\n\n\n9.6.7.10 Chi-Square Test for Association\nFor the chi-square test for association, the degrees of freedom is in Equation 9.35:\n\\[\ndf = (r - 1) \\times (c - 1)\n\\tag{9.35}\\]\nwhere \\(r\\) is the number of rows in the contingency table and \\(c\\) is the number of columns in the contingency table.\n\nCodedf_chisquareAssociation &lt;- (numRows - 1) * (numColumns - 1)\n\n\nOne-tailed test:\n\nCodeqchisq(1 - alpha, df_chisquareAssociation)\n\n[1] 9.487729\n\n\nTwo-tailed test:\n\nCodeqchisq(1 - alpha/2, df_chisquareAssociation)\n\n[1] 11.14329\n\n\n\n9.6.8 Statistical Power\nAs described above, statistical power is the probability of detecting an effect, if, in fact, the effect exists. Statistical power for a given test can be calculated based on three factors:\n\neffect size\nsample size\nalpha level\n\nKnowing any three of the following, you can calculate the fourth: statistical power, effect size, sample size, and alpha level. Below is R code for calculating power for each of various statistical tests (i.e., a power analysis). We use the pwr (Champely, 2020) and pwrss (Bulus, 2023) packages to estimate statistical power. For free point-and-click software for calculating statistical power, see G*Power: https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html\n\nCodepower &lt;- .8\neffectSize_d &lt;- .5\neffectSize_r &lt;- .24\neffectSize_beta &lt;- .24\neffectSize_f &lt;- .25\neffectSize_fSquared &lt;- .06\neffectSize_omega &lt;- .3\n\n\nWhen designing a study, it is important to consider power and the sample size needed to detect the hypothesized effect size. If your sample size is too small and you do not detect an effect (i.e., \\(p &gt; .05\\)), you do not know whether your failure to detect the effect was because a) the effect does not exist, or b) the effect exists but you did not have enough power to detect it.\n\n9.6.8.1 One-Sample t-Test\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9999998\n    alternative = two.sided\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 200\n              d = 0.1990655\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n9.6.8.2 Independent-Samples t-Test\n\n9.6.8.2.1 Balanced Group Sizes\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9987689\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 200\n              d = 0.2808267\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n9.6.8.2.2 Unbalanced Group Sizes\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  n1 = nGroup1,\n  n2 = nGroup2,\n  d = effectSize_d,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 150\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9907677\n    alternative = two.sided\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  power = power,\n  n1 = nGroup1,\n  d = effectSize_d,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 40.22483\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  power = power,\n  n1 = nGroup1,\n  n2 = nGroup2,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 150\n              d = 0.3245459\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n9.6.8.3 Paired-Samples t-Test\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9999998\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 200\n              d = 0.1990655\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n9.6.8.4 One-Way ANOVA\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999962\n\nNOTE: n is number in each group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.117038\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nThe power analysis code above assumes the groups are of equal size (i.e., a balanced design). If the design is unbalanced (i.e., there are different numbers of participants in each group), it may be necessary to conduct a power analysis via a simulation. Below is an example of evaluating the statistical power for detecting an effect unbalanced designs via simulation:\n\nCodenSim &lt;- 1000 # number of simulations\n\n# Function to generate data and perform ANOVA\nsimulate_anova &lt;- function(nGroup1, nGroup2, f, alpha) {\n  # Means for each group\n  mean1 &lt;- 0\n  mean2 &lt;- f * sqrt((nGroup1 + nGroup2) / 2)\n  \n  # Generate data\n  group1 &lt;- rnorm(nGroup1, mean = mean1, sd = 1)\n  group2 &lt;- rnorm(nGroup2, mean = mean2, sd = 1)\n  \n  # Combine data\n  data &lt;- data.frame(\n    value = c(group1, group2),\n    group = factor(rep(c(\"Group1\", \"Group2\"), c(nGroup1, nGroup2)))\n  )\n  \n  # Perform ANOVA\n  aov_result &lt;- aov(value ~ group, data = data)\n  p_value &lt;- summary(aov_result)[[1]][[\"Pr(&gt;F)\"]][1]\n  \n  # Check if p-value is less than alpha\n  return(p_value &lt; alpha)\n}\n\n# Run simulations\nset.seed(52242) # for reproducibility\npowerSimulationOneWayAnova &lt;- replicate(\n  nSim,\n  simulate_anova(\n    nGroup1 = 10,\n    nGroup2 = 25,\n    f = effectSize_f,\n    alpha = alpha))\n\n# Estimate power\nmean(powerSimulationOneWayAnova)\n\n[1] 0.774\n\n\n\n9.6.8.5 Factorial ANOVA\nThe power analysis code below assumes the groups are of equal size (i.e., a balanced design). If the design is unbalanced (i.e., there are different numbers of participants in each group), it may be necessary to conduct a power analysis via a simulation. See Section 9.6.8.4 for an example power analysis simulation for one-way ANOVA.\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999238\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999962\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 1\n\nNOTE: n is number in each group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 32.05196\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 200\n              f = 0.1270373\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.117038\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 200\n              f = 0.09889082\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n9.6.8.6 Repeated Measures ANOVA\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.8484718\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.8536292\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.6756298\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    178.3971 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    175.7692 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    253.2369 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2358259  4  4     1  0.05   0.8\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2342726  4  4     1  0.05   0.8\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2817486  4  4     1  0.05   0.8\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\n\n9.6.8.7 Correlation\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.r.test(\n  n = N,\n  r = effectSize_r,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 200\n              r = 0.24\n      sig.level = 0.05\n          power = 0.9310138\n    alternative = two.sided\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.r.test(\n  power = power,\n  r = effectSize_r,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 133.1299\n              r = 0.24\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.r.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 200\n              r = 0.1965767\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n9.6.8.8 Multiple Regression\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.f2.test(\n  f2 = effectSize_fSquared,\n  sig.level = alpha,\n  u = numPredictors,\n  v = N - numPredictors - 1)\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 194\n             f2 = 0.06\n      sig.level = 0.05\n          power = 0.7548031\n\nCodepwrss::pwrss.t.reg(\n  n = N,\n  beta1 = effectSize_beta,\n  k = numPredictors,\n  alpha = alpha,\n  alternative = \"not equal\")\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.936 \n  n = 200 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 194 \n Non-centrality parameter = 3.496 \n Type I error rate = 0.05 \n Type II error rate = 0.064 \n\n\nSolving for sample size needed (given effect size, power, and alpha level)—\\(v = N - \\text{numberOfPredictors} - 1\\); thus, \\(N = v + \\text{numberOfPredictors} + 1\\):\n\nCodemultipleRegressionSampleSizeModel &lt;- pwr::pwr.f2.test(\n  power = power,\n  f2 = effectSize_fSquared,\n  sig.level = alpha,\n  u = numPredictors)\n\nmultipleRegressionSampleSizeModel\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 213.3947\n             f2 = 0.06\n      sig.level = 0.05\n          power = 0.8\n\nCodevNeeded &lt;- multipleRegressionSampleSizeModel$v\nsampleSizeNeeded &lt;- vNeeded + numPredictors + 1\nsampleSizeNeeded\n\n[1] 219.3947\n\nCodepwrss::pwrss.t.reg(\n  power = power,\n  beta1 = effectSize_beta,\n  k = numPredictors,\n  alpha = alpha,\n  alternative = \"not equal\")\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 131 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 124.427 \n Non-centrality parameter = 2.823 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.f2.test(\n  power = power,\n  sig.level = alpha,\n  u = numPredictors,\n  v = N - numPredictors - 1)\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 194\n             f2 = 0.06597765\n      sig.level = 0.05\n          power = 0.8\n\n\n\n9.6.8.9 Chi-Square Goodness-of-Fit Test\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  N = N,\n  w = effectSize_omega,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 200\n             df = 5\n      sig.level = 0.05\n          power = 0.9269225\n\nNOTE: N is the number of observations\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  w = effectSize_omega,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 142.529\n             df = 5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  N = N,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.2532543\n              N = 200\n             df = 5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\n\n9.6.8.10 Chi-Square Test for Association\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  N = N,\n  w = effectSize_omega,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 200\n             df = 4\n      sig.level = 0.05\n          power = 0.9431195\n\nNOTE: N is the number of observations\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  w = effectSize_omega,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 132.6143\n             df = 4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  N = N,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.2442875\n              N = 200\n             df = 4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\n\n9.6.8.11 Multilevel Modeling\nPower analysis for multilevel modeling approaches is more complicated than it is for other statistical analyses, such as correlation, multiple regression, t-tests, ANOVA, etc.\nThere are free web applications for calculating power in multilevel modeling:\n\n\nhttps://aguinis.shinyapps.io/ml_power (Mathieu et al., 2012)\n\n\nhttps://koumurayama.shinyapps.io/summary_statistics_based_power (Murayama et al., 2022)\n\n\nhttps://webpower.psychstat.org/wiki/models/index (Zhang & Yuan, 2018)\n\n\n9.6.8.12 Path Analysis, Factor Analysis, and Structural Equation Modeling\nPower analysis for latent variable modeling approaches like structural equation modeling (SEM) is more complicated than it is for other statistical analyses, such as correlation, multiple regression, t-tests, ANOVA, etc.\nI provide an example of power analysis in SEM using Monte Carlo simulation in R here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/sem.html#monteCarloPowerAnalysis (Petersen, 2025b).\nThere are also free web applications for calculating power in SEM:\n\n\nhttps://sjak.shinyapps.io/power4SEM (Jak et al., 2020)\n\n\nhttps://sempower.shinyapps.io/sempower (Moshagen & Bader, 2024)\n\n\nhttps://yilinandrewang.shinyapps.io/pwrSEM (Wang & Rhemtulla, 2021)\n\n\nhttps://webpower.psychstat.org/wiki/models/index (Zhang & Yuan, 2018)\n\n\n9.6.8.13 Mediation and Moderation\nThere are free tools for calculating power for tests of mediation and moderation:\n\n\nhttps://schoemanna.shinyapps.io/mc_power_med (Schoemann et al., 2017)\n\n\nhttps://www.causalevaluation.org/power-analysis.html (web application: https://powerupr.shinyapps.io/index/) (Ataneka et al., 2023), based on the PowerUpR package (Bulus et al., 2021)\n\n\nhttps://webpower.psychstat.org/wiki/models/index (Zhang & Yuan, 2018)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsConclusion",
    "href": "basic-statistics.html#sec-basicStatsConclusion",
    "title": "9  Basic Statistics",
    "section": "\n9.7 Conclusion",
    "text": "9.7 Conclusion\nDescriptive statistics are used to describe the data, including the center, spread, or shape of data. Inferential statistics are used to draw inferences regarding differences between groups or associations between variables. Null hypothesis signficance testing is a framework for inferential statistics, in which there is a null hypothesis and alternative hypothesis. The null hypothesis is that there is no difference between groups or that there is no association between variables. Statistical significance is evaluated with a \\(p\\)-value, which represents the probability of obtaining a result at least as extreme as the result observed if the null hypothesis is true. Effects with p-values less than .05 are considered statistically significant. However, it is also important to consider practical significance and effect sizes. When designing a study, it is important to consider statistical power and the sample size needed to detect the hypothesized effect size. You can determine a study’s power based on the effect size, sample size, and alpha level.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsSessionInfo",
    "href": "basic-statistics.html#sec-basicStatsSessionInfo",
    "title": "9  Basic Statistics",
    "section": "\n9.8 Session Info",
    "text": "9.8 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] grid      parallel  stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   WebPower_0.9.4    PearsonDS_1.3.1  \n[13] lavaan_0.6-19     lme4_1.1-36       Matrix_1.7-2      MASS_7.3-64      \n[17] pwrss_0.3.1       pwr_1.3-0         DescTools_0.99.59 petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] Rdpack_2.6.3       DBI_1.2.3          mnormt_2.1.1       gridExtra_2.3     \n [5] gld_2.6.7          readxl_1.4.5       rlang_1.1.5        magrittr_2.0.3    \n [9] e1071_1.7-16       compiler_4.4.3     mgcv_1.9-1         vctrs_0.6.5       \n[13] reshape2_1.4.4     quadprog_1.5-8     pkgconfig_2.0.3    fastmap_1.2.0     \n[17] backports_1.5.0    labeling_0.4.3     pbivnorm_0.6.0     rmarkdown_2.29    \n[21] tzdb_0.5.0         haven_2.5.4        nloptr_2.2.1       xfun_0.51         \n[25] jsonlite_1.9.1     psych_2.5.3        cluster_2.1.8      R6_2.6.1          \n[29] stringi_1.8.4      RColorBrewer_1.1-3 boot_1.3-31        rpart_4.1.24      \n[33] cellranger_1.1.0   Rcpp_1.0.14        knitr_1.50         base64enc_0.1-3   \n[37] splines_4.4.3      nnet_7.3-20        timechange_0.3.0   tidyselect_1.2.1  \n[41] rstudioapi_0.17.1  yaml_2.3.10        lattice_0.22-6     plyr_1.8.9        \n[45] withr_3.0.2        evaluate_1.0.3     foreign_0.8-88     proxy_0.4-27      \n[49] pillar_1.10.1      checkmate_2.3.2    stats4_4.4.3       reformulas_0.4.0  \n[53] generics_0.1.3     mix_1.0-13         hms_1.1.3          munsell_0.5.1     \n[57] scales_1.3.0       rootSolve_1.8.2.4  minqa_1.2.8        xtable_1.8-4      \n[61] class_7.3-23       glue_1.8.0         Hmisc_5.2-3        lmom_3.2          \n[65] tools_4.4.3        data.table_1.17.0  Exact_3.3          mvtnorm_1.3-3     \n[69] mitools_2.4        rbibutils_2.3      colorspace_2.1-1   nlme_3.1-167      \n[73] htmlTable_2.4.3    Formula_1.2-5      cli_3.6.4          expm_1.0-0        \n[77] viridisLite_0.4.2  gtable_0.3.6       digest_0.6.37      htmlwidgets_1.6.4 \n[81] farver_2.1.2       htmltools_0.5.8.1  lifecycle_1.0.4    httr_1.4.7        \n\n\n\n\n\n\nAkinshin, A. (2023). Weighted quantile estimators. arXiv. https://doi.org/10.48550/arXiv.2304.07265\n\n\nAtaneka, A., Kelcey, B., Dong, N., Bulus, M., & Bai, F. (2023). PowerUp R Shiny app (v. 0.9) manual. https://www.causalevaluation.org/uploads/7/3/3/6/73366257/r_shinnyapp_manual_0.9.pdf\n\n\nBulus, M. (2023). pwrss: Statistical power and sample size calculation tools. https://CRAN.R-project.org/package=pwrss\n\n\nBulus, M., Dong, N., Kelcey, B., & Spybrook, J. (2021). PowerUpR: Power analysis tools for multilevel randomized experiments. https://doi.org/10.32614/CRAN.package.PowerUpR\n\n\nChampely, S. (2020). pwr: Basic functions for power analysis. https://github.com/heliosdrm/pwr\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates, Publishers. https://doi.org/10.4324/9780203771587\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGoodman, S. (2008). A dirty dozen: Twelve p-value misconceptions. Seminars in Hematology, 45(3), 135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nJackson-Wood, M. (2017). statistical test flowchart. https://www.statsflowchart.co.uk\n\n\nJak, S., Jorgensen, T. D., Verdam, M. G. E., Oort, F. J., & Elffers, L. (2020). Analytical power calculations for structural equation modeling: A tutorial and shiny app. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01479-0\n\n\nMagnusson, K. (2013). Creating a typical textbook illustration of statistical power using either ggplot or base graphics. https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics\n\n\nMagnusson, K. (2014). Understanding statistical power and significance testing. https://rpsychologist.com/d3/nhst/\n\n\nMagnusson, K. (2015). Distribution of p-values when comparing two groups. https://rpsychologist.com/d3/pdist\n\n\nMagnusson, K. (2021). Understanding p-values through simulations. https://rpsychologist.com/pvalue\n\n\nMathieu, J. E., Aguinis, H., Culpepper, S. A., & Chen, G. (2012). Understanding and estimating the power to detect cross-level interaction effects in multilevel modeling. Journal of Applied Psychology, 97(5), 951–966. https://doi.org/10.1037/a0028380\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree: The case of r and d. Psychological Methods, 11(4), 386–401. https://doi.org/10.1037/1082-989X.11.4.386\n\n\nMoshagen, M., & Bader, M. (2024). semPower: General power analysis for structural equation models. Behavior Research Methods, 56(4), 2901–2922. https://doi.org/10.3758/s13428-023-02254-7\n\n\nMurayama, K., Usami, S., & Sakaki, M. (2022). Summary-statistics-based power analysis: A new and practical method to determine sample size for mixed-effects modeling. Psychological Methods, 27(6), 1014–1038. https://doi.org/10.1037/met0000330\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nSchoemann, A. M., Boulton, A. J., & Short, S. D. (2017). Determining power and sample size for simple and complex mediation models. Social Psychological and Personality Science, 8(4), 379–386. https://doi.org/10.1177/1948550617715068\n\n\nSignorell, A. (2025). DescTools: Tools for descriptive statistics. https://andrisignorell.github.io/DescTools/\n\n\nWang, Y. A., & Rhemtulla, M. (2021). Power analysis for parameter estimation in structural equation modeling: A discussion and tutorial. Advances in Methods and Practices in Psychological Science, 4(1), 1–17. https://doi.org/10.1177/2515245920918253\n\n\nZhang, Z., & Yuan, K.-H. (2018). Practical statistical power analysis using WebPower and R. ISDSA Press. https://doi.org/10.35566/power",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "10  Correlation Analysis",
    "section": "",
    "text": "10.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationGettingStarted",
    "href": "correlation.html#sec-correlationGettingStarted",
    "title": "10  Correlation Analysis",
    "section": "",
    "text": "10.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"XICOR\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationOverview",
    "href": "correlation.html#sec-correlationOverview",
    "title": "10  Correlation Analysis",
    "section": "\n10.2 Overview of Correlation",
    "text": "10.2 Overview of Correlation\nCorrelation is an index of the association between variables. Covariance is the association between variables and in an unstandardized metric that differs for variables with different scales. By contrast, correlation is in a standarized metric that does not differ for variables with different scales. When examining the association between variables that are interval or ratio levels of measurement, Pearson correlation is used. When examining the association between variables that are ordinal in level of measurement, Spearman correlation is used. Pearson correlation is an index of the linear association between variables. If a nonlinear association is present, other indices like xi [\\(\\xi\\); Chatterjee (2021)] and distance correlation coefficients are better suited to detect the association.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-correlation-coefficient-r",
    "href": "correlation.html#the-correlation-coefficient-r",
    "title": "10  Correlation Analysis",
    "section": "\n10.3 The Correlation Coefficient (\\(r\\))",
    "text": "10.3 The Correlation Coefficient (\\(r\\))\nThe formula for the correlation coefficient is in Equation 9.22.\nThe correlation coefficient ranges from −1.0 to +1.0. The correlation coefficient (\\(r\\)) tells you two things: (1) the direction (sign) of the association (positive or negative) and (2) the magnitude of the association. If the correlation coefficient is positive, the association is positive. If the correlation coefficient is negative, the association is negative. If the association is positive, as X increases, Y increases (or conversely, as X decreases, Y decreases). If the association is negative, as X increases, Y decreases (or conversely, as X decreases, Y increases). The smaller the absolute value of the correlation coefficient (i.e., the closer the \\(r\\) value is to zero), the weaker the association and the flatter the slope of the best-fit line in a scatterplot. The larger the absolute value of the correlation coefficient (i.e., the closer the absolute value of the \\(r\\) value is to one), the stronger the association and the steeper the slope of the best-fit line in a scatterplot. See Figure 10.1 for a range of different correlation coefficients and what some example data may look like for each direction and strength of association.\n\nCodeset.seed(52242)\ncorrelations &lt;- data.frame(criterion = rnorm(1000))\n\ncorrelations$v1 &lt;- complement(correlations$criterion, -1)\ncorrelations$v2 &lt;- complement(correlations$criterion, -.9)\ncorrelations$v3 &lt;- complement(correlations$criterion, -.8)\ncorrelations$v4 &lt;- complement(correlations$criterion, -.7)\ncorrelations$v5 &lt;- complement(correlations$criterion, -.6)\ncorrelations$v6 &lt;- complement(correlations$criterion, -.5)\ncorrelations$v7 &lt;- complement(correlations$criterion, -.4)\ncorrelations$v8 &lt;- complement(correlations$criterion, -.3)\ncorrelations$v9 &lt;- complement(correlations$criterion, -.2)\ncorrelations$v10 &lt;-complement(correlations$criterion, -.1)\ncorrelations$v11 &lt;-complement(correlations$criterion, 0)\ncorrelations$v12 &lt;-complement(correlations$criterion, .1)\ncorrelations$v13 &lt;-complement(correlations$criterion, .2)\ncorrelations$v14 &lt;-complement(correlations$criterion, .3)\ncorrelations$v15 &lt;-complement(correlations$criterion, .4)\ncorrelations$v16 &lt;-complement(correlations$criterion, .5)\ncorrelations$v17 &lt;-complement(correlations$criterion, .6)\ncorrelations$v18 &lt;-complement(correlations$criterion, .7)\ncorrelations$v19 &lt;-complement(correlations$criterion, .8)\ncorrelations$v20 &lt;-complement(correlations$criterion, .9)\ncorrelations$v21 &lt;-complement(correlations$criterion, 1)\n\npar(mfrow = c(7,3), mar = c(1, 0, 1, 0))\n\n# -1.0\nplot(correlations$criterion, correlations$v1, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v1)$estimate, 2))))\nabline(lm(v1 ~ criterion, data = correlations), col = \"black\")\n\n# -.9\nplot(correlations$criterion, correlations$v2, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v2)$estimate, 2))))\nabline(lm(v2 ~ criterion, data = correlations), col = \"black\")\n\n# -.8\nplot(correlations$criterion, correlations$v3, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v3)$estimate, 2))))\nabline(lm(v3 ~ criterion, data = correlations), col = \"black\")\n\n# -.7\nplot(correlations$criterion, correlations$v4, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v4)$estimate, 2))))\nabline(lm(v4 ~ criterion, data = correlations), col = \"black\")\n\n# -.6\nplot(correlations$criterion, correlations$v5, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v5)$estimate, 2))))\nabline(lm(v5 ~ criterion, data = correlations), col = \"black\")\n\n# -.5\nplot(correlations$criterion, correlations$v6, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v6)$estimate, 2))))\nabline(lm(v6 ~ criterion, data = correlations), col = \"black\")\n\n# -.4\nplot(correlations$criterion, correlations$v7, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v7)$estimate, 2))))\nabline(lm(v7 ~ criterion, data = correlations), col = \"black\")\n\n# -.3\nplot(correlations$criterion, correlations$v8, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v8)$estimate, 2))))\nabline(lm(v8 ~ criterion, data = correlations), col = \"black\")\n\n# -.2\nplot(correlations$criterion, correlations$v9, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v9)$estimate, 2))))\nabline(lm(v9 ~ criterion, data = correlations), col = \"black\")\n\n# -.1\nplot(correlations$criterion, correlations$v10, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v10)$estimate, 2))))\nabline(lm(v10 ~ criterion, data = correlations), col = \"black\")\n\n# 0.0\nplot(correlations$criterion, correlations$v11, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v11)$estimate, 2))))\nabline(lm(v11 ~ criterion, data = correlations), col = \"black\")\n\n# 0.1\nplot(correlations$criterion, correlations$v12, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v12)$estimate, 2))))\nabline(lm(v12 ~ criterion, data = correlations), col = \"black\")\n\n# 0.2\nplot(correlations$criterion, correlations$v13, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v13)$estimate, 2))))\nabline(lm(v13 ~ criterion, data = correlations), col = \"black\")\n\n# 0.3\nplot(correlations$criterion, correlations$v14, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v14)$estimate, 2))))\nabline(lm(v14 ~ criterion, data = correlations), col = \"black\")\n\n# 0.4\nplot(correlations$criterion, correlations$v15, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v15)$estimate, 2))))\nabline(lm(v15 ~ criterion, data = correlations), col = \"black\")\n\n# 0.5\nplot(correlations$criterion, correlations$v16, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v16)$estimate, 2))))\nabline(lm(v16 ~ criterion, data = correlations), col = \"black\")\n\n# 0.6\nplot(correlations$criterion, correlations$v17, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v17)$estimate, 2))))\nabline(lm(v17 ~ criterion, data = correlations), col = \"black\")\n\n# 0.7\nplot(correlations$criterion, correlations$v18, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v18)$estimate, 2))))\nabline(lm(v18 ~ criterion, data = correlations), col = \"black\")\n\n# 0.8\nplot(correlations$criterion, correlations$v19, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v19)$estimate, 2))))\nabline(lm(v19 ~ criterion, data = correlations), col = \"black\")\n\n# 0.9\nplot(correlations$criterion, correlations$v20, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v20)$estimate, 2))))\nabline(lm(v20 ~ criterion, data = correlations), col = \"black\")\n\n# 1.0\nplot(correlations$criterion, correlations$v21, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v21)$estimate, 2))))\nabline(lm(v21 ~ criterion, data = correlations), col = \"black\")\n\ninvisible(dev.off()) #par(mfrow = c(1,1))\n\n\n\n\n\n\nFigure 10.1: Correlation Coefficients.\n\n\n\n\nSee Figure 10.2 for the interpretation of the magnitude and direction (sign) of various correlation coefficients.\n\nCodelibrary(\"patchwork\")\n\nset.seed(52242)\ncorrelations2 &lt;- data.frame(criterion = rnorm(15))\n\ncorrelations2$v1 &lt;- complement(correlations2$criterion, -1)\ncorrelations2$v2 &lt;- complement(correlations2$criterion, -.9)\ncorrelations2$v3 &lt;- complement(correlations2$criterion, -.8)\ncorrelations2$v4 &lt;- complement(correlations2$criterion, -.7)\ncorrelations2$v5 &lt;- complement(correlations2$criterion, -.6)\ncorrelations2$v6 &lt;- complement(correlations2$criterion, -.5)\ncorrelations2$v7 &lt;- complement(correlations2$criterion, -.4)\ncorrelations2$v8 &lt;- complement(correlations2$criterion, -.3)\ncorrelations2$v9 &lt;- complement(correlations2$criterion, -.2)\ncorrelations2$v10 &lt;-complement(correlations2$criterion, -.1)\ncorrelations2$v11 &lt;-complement(correlations2$criterion, 0)\ncorrelations2$v12 &lt;-complement(correlations2$criterion, .1)\ncorrelations2$v13 &lt;-complement(correlations2$criterion, .2)\ncorrelations2$v14 &lt;-complement(correlations2$criterion, .3)\ncorrelations2$v15 &lt;-complement(correlations2$criterion, .4)\ncorrelations2$v16 &lt;-complement(correlations2$criterion, .5)\ncorrelations2$v17 &lt;-complement(correlations2$criterion, .6)\ncorrelations2$v18 &lt;-complement(correlations2$criterion, .7)\ncorrelations2$v19 &lt;-complement(correlations2$criterion, .8)\ncorrelations2$v20 &lt;-complement(correlations2$criterion, .9)\ncorrelations2$v21 &lt;-complement(correlations2$criterion, 1)\n\n# -1.0\np1 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v1\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Perfect Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−1.0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.9\np2 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v2\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Strong Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.9\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.5\np3 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v6\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Moderate Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.5\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.2\np4 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v9\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Weak Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.2\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.0\np5 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v11\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"No Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.2\np6 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v13\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Weak Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".2\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.5\np7 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v16\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Moderate Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".5\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.9\np8 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v20\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Strong Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".9\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 1.0\np9 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v21\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Perfect Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"1.0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n  plot_layout(\n    ncol = 3,\n    heights = 1,\n    widths = 1)\n\n\n\n\n\n\nFigure 10.2: Interpretation of the Magnitude and Direction (Sign) of Correlation Coefficients.\n\n\n\n\nAn interactive visualization by Magnusson (2020) on interpreting correlations is at the following link: https://rpsychologist.com/correlation/ (archived at https://perma.cc/G8YR-VCM4)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationExamples",
    "href": "correlation.html#sec-correlationExamples",
    "title": "10  Correlation Analysis",
    "section": "\n10.4 Examples",
    "text": "10.4 Examples\n\n10.4.1 Covariance\n\n10.4.2 Pearson Correlation\n\n10.4.3 Spearman Correlation\n\n10.4.4 Nonlinear Correlation\nA nonlinear correlation can be estimated using the XICOR package (Chatterjee & Holmes, 2023; Holmes & Chatterjee, 2023):\n\n10.4.5 Correlation Matrix\n\n10.4.6 Correlogram",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationOutliers",
    "href": "correlation.html#sec-correlationOutliers",
    "title": "10  Correlation Analysis",
    "section": "\n10.5 Impact of Outliers",
    "text": "10.5 Impact of Outliers",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlation-correlationAndCausation",
    "href": "correlation.html#sec-correlation-correlationAndCausation",
    "title": "10  Correlation Analysis",
    "section": "\n10.6 Correlation Does Not Imply Causation",
    "text": "10.6 Correlation Does Not Imply Causation\nAs described in Section 8.4.2.1, correlation does not imply causation. There are several reasons (described in Section 8.4.2.1) that, just because X is correlated with Y does not necessarily mean that X causes Y. However, correlation can still be useful. In order for two processes to be causally related, they must be associated. That is, association is necessary but insufficient for causality.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationConclusion",
    "href": "correlation.html#sec-correlationConclusion",
    "title": "10  Correlation Analysis",
    "section": "\n10.7 Conclusion",
    "text": "10.7 Conclusion\nCorrelation is an index of the association between variables. The correlation coefficient (\\(r\\)) ranges from −1 to +1, and indicates the sign and magnitude of the association. Although correlation does not imply causation, identifying associations between variables can still be useful because association is a necessary (but insufficient) condition for causality.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationSessionInfo",
    "href": "correlation.html#sec-correlationSessionInfo",
    "title": "10  Correlation Analysis",
    "section": "\n10.8 Session Info",
    "text": "10.8 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] patchwork_1.3.0   lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n [5] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n [9] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   XICOR_0.4.1      \n[13] petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   psych_2.5.3        viridisLite_0.4.2  farver_2.1.2      \n [5] fastmap_1.2.0      digest_0.6.37      rpart_4.1.24       timechange_0.3.0  \n [9] lifecycle_1.0.4    cluster_2.1.8      magrittr_2.0.3     compiler_4.4.3    \n[13] rlang_1.1.5        Hmisc_5.2-3        tools_4.4.3        yaml_2.3.10       \n[17] data.table_1.17.0  knitr_1.50         labeling_0.4.3     htmlwidgets_1.6.4 \n[21] mnormt_2.1.1       plyr_1.8.9         RColorBrewer_1.1-3 foreign_0.8-88    \n[25] withr_3.0.2        R.oo_1.27.0        nnet_7.3-20        grid_4.4.3        \n[29] stats4_4.4.3       lavaan_0.6-19      xtable_1.8-4       colorspace_2.1-1  \n[33] scales_1.3.0       cli_3.6.4          mvtnorm_1.3-3      rmarkdown_2.29    \n[37] generics_0.1.3     rstudioapi_0.17.1  reshape2_1.4.4     tzdb_0.5.0        \n[41] DBI_1.2.3          rtf_0.4-14.1       splines_4.4.3      parallel_4.4.3    \n[45] base64enc_0.1-3    mitools_2.4        vctrs_0.6.5        Matrix_1.7-2      \n[49] jsonlite_1.9.1     hms_1.1.3          Formula_1.2-5      htmlTable_2.4.3   \n[53] glue_1.8.0         stringi_1.8.4      gtable_0.3.6       quadprog_1.5-8    \n[57] munsell_0.5.1      pillar_1.10.1      psychTools_2.4.3   htmltools_0.5.8.1 \n[61] R6_2.6.1           mix_1.0-13         evaluate_1.0.3     pbivnorm_0.6.0    \n[65] lattice_0.22-6     R.methodsS3_1.8.2  backports_1.5.0    Rcpp_1.0.14       \n[69] gridExtra_2.3      nlme_3.1-167       checkmate_2.3.2    mgcv_1.9-1        \n[73] xfun_0.51          pkgconfig_2.0.3   \n\n\n\n\n\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009–2022. https://doi.org/10.1080/01621459.2020.1758115\n\n\nChatterjee, S., & Holmes, S. (2023). XICOR: Robust and generalized correlation coefficients. https://CRAN.R-project.org/package=XICOR\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nHolmes, S., & Chatterjee, S. (2023). XICOR: Association measurement through cross rank increments. https://CRAN.R-project.org/package=XICOR\n\n\nMagnusson, K. (2020). Interpreting correlations. https://rpsychologist.com/correlation",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html",
    "href": "multiple-regression.html",
    "title": "11  Multiple Regression",
    "section": "",
    "text": "11.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionGettingStarted",
    "href": "multiple-regression.html#sec-multipleRegressionGettingStarted",
    "title": "11  Multiple Regression",
    "section": "",
    "text": "11.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"tidyverse\")\nlibrary(\"knitr\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionOverview",
    "href": "multiple-regression.html#sec-multipleRegressionOverview",
    "title": "11  Multiple Regression",
    "section": "\n11.2 Overview of Multiple Regression",
    "text": "11.2 Overview of Multiple Regression\nMultiple regression examines the association between multiple predictor variables and one outcome variable. It allows obtaining a more accurate estimate of the unique contribution of a given predictor variable, by controlling for other variables (covariates).\nRegression with one predictor variable takes the form of Equation 11.1:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\epsilon\n\\tag{11.1}\\]\nwhere \\(y\\) is the outcome variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, \\(x_1\\) is the predictor variable, and \\(\\epsilon\\) is the error term.\nA regression line is depicted in Figure 11.4.\n\n\n\n\n\nFigure 11.1: A Regression Best-Fit Line.\n\n\nRegression with multiple predictors—i.e., multiple regression—takes the form of Equation 11.2:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon\n\\tag{11.2}\\]\nwhere \\(p\\) is the number of predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionComponents",
    "href": "multiple-regression.html#sec-multipleRegressionComponents",
    "title": "11  Multiple Regression",
    "section": "\n11.3 Components",
    "text": "11.3 Components\n\n\n\\(B\\) = unstandardized coefficient: direction and magnitude of the estimate (original scale)\n\n\\(\\beta\\) (beta) = standardized coefficient: direction and magnitude of the estimate (standard deviation scale)\n\n\\(SE\\) = standard error: uncertainty of unstandardized estimate\n\nThe unstandardized regression coefficient (\\(B\\)) is interpreted such that, for every unit change in the predictor variable, there is a __ unit change in the outcome variable. For instance, when examining the association between age and fantasy points, if the unstandardized regression coefficient is 2.3, players score on average 2.3 more points for each additional year of age. (In reality, we might expect a nonlinear, inverted-U-shaped association between age and fantasy points such that players tend to reach their peak in the middle of their careers.) Unstandardized regression coefficients are tied to the metric of the raw data. Thus, a large unstandardized regression coefficient for two variables may mean completely different things. Holding the strength of the association constant, you tend to see larger unstandardized regression coefficients for variables with smaller units and smaller unstandardized regression coefficients for variables with larger units.\nStandardized regression coefficients can be obtained by standardizing the variables to z-scores so they all have a mean of zero and standard deviation of one. The standardized regression coefficient (\\(\\beta\\)) is interpreted such that, for every standard deviation change in the predictor variable, there is a __ standard deviation change in the outcome variable. For instance, when examining the association between age and fantasy points, if the standardized regression coefficient is 0.1, players score on average 0.1 standard deviation more points for each additional standard deviation of their year of age. Standardized regression coefficients—though not the case in all instances—tend to fall between [−1, 1]. Thus, standardized regression coefficients tend to be more comparable across variables and models compared to unstandardized regression coefficients. In this way, standardized regression coefficients provide a meaningful index of effect size.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-assumptionsRegression",
    "href": "multiple-regression.html#sec-assumptionsRegression",
    "title": "11  Multiple Regression",
    "section": "\n11.4 Assumptions of Multiple Regression",
    "text": "11.4 Assumptions of Multiple Regression\nLinear regression models make the following assumptions:\n\nthere is a linear association between the predictor variables and the outcome variable\nthere is homoscedasticity of the residuals; the residuals do not differ as a function of the predictor variables or as a function of the outcome variable\nthe residuals are independent; they are uncorrelated with each other\nthe residuals are normally distributed",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionRSquared",
    "href": "multiple-regression.html#sec-multipleRegressionRSquared",
    "title": "11  Multiple Regression",
    "section": "\n11.5 Coefficient of Determination (\\(R^2\\))",
    "text": "11.5 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions: \\(R^2 = \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\\). Various formulas for \\(R^2\\) are in Equation 9.19. Larger \\(R^2\\) values indicate greater accuracy. Multiple regression can be conceptualized with overlapping circles (similar to a venn diagram), where the non-overlapping portions of the circles reflect nonshared variance and the overlapping portions of the circles reflect shared variance, as in Figure 11.4.\n\n\n\n\n\nFigure 11.2: Conceptual Depiction of Proportion of Variance Explained (\\(R^2\\)) in an Outcome Variable (\\(Y\\)) by Multiple Predictors (\\(X1\\) and \\(X2\\)) in Multiple Regression. The size of each circle represents the variable’s variance. The proportion of variance in \\(Y\\) that is explained by the predictors is depicted by the areas in orange. The dark orange space (\\(G\\)) is where multiple predictors explain overlapping variance in the outcome. Overlapping variance that is explained in the outcome (\\(G\\)) will not be recovered in the regression coefficients when both predictors are included in the regression model. From Petersen (2024) and Petersen (2025).\n\n\nOne issue with \\(R^2\\) is that it increases as the number of predictors increases, which can lead to overfitting if using \\(R^2\\) as an index to compare models for purposes of selecting the “best-fitting” model. Consider the following example (adapted from Petersen (2025)) in which you have one predictor variable and one outcome variable, as shown in Table 11.1.\n\n\n\nTable 11.1: Example Data of Predictor (x1) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\n\n\n\n7\n1\n\n\n13\n2\n\n\n29\n7\n\n\n10\n2\n\n\n\n\n\n\n\n\nUsing the data, the best fitting regression model is: \\(y =\\) 3.98 \\(+\\) 3.59 \\(\\cdot x_1\\). In this example, the \\(R^2\\) is 0.98. The equation is not a perfect prediction, but with a single predictor variable, it captures the majority of the variance in the outcome.\nNow consider the following example where you add a second predictor variable to the data above, as shown in Table 11.2.\n\n\n\nTable 11.2: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\nx2\n\n\n\n7\n1\n3\n\n\n13\n2\n5\n\n\n29\n7\n1\n\n\n10\n2\n2\n\n\n\n\n\n\n\n\nWith the second predictor variable, the best fitting regression model is: \\(y =\\) 0.00 + 4.00 \\(\\cdot x_1 +\\) 1.00 \\(\\cdot x_2\\). In this example, the \\(R^2\\) is 1.00. The equation with the second predictor variable provides a perfect prediction of the outcome.\nProviding perfect prediction with the right set of predictor variables is the dream of multiple regression. So, using multiple regression, we often add predictor variables to incrementally improve prediction. Knowing how much variance would be accounted for by random chance follows Equation 11.3:\n\\[\nE(R^2) = \\frac{K}{n-1}\n\\tag{11.3}\\]\nwhere \\(E(R^2)\\) is the expected value of \\(R^2\\) (the proportion of variance explained), \\(K\\) is the number of predictor variables, and \\(n\\) is the sample size. The formula demonstrates that the more predictor variables in the regression model, the more variance will be accounted for by chance. With many predictor variables and a small sample, you can account for a large share of the variance merely by chance.\nAs an example, consider that we have 13 predictor variables to predict fantasy performance for 43 players. Assume that, with 13 predictor variables, we explain 38% of the variance (\\(R^2 = .38; r = .62\\)). We explained a lot of the variance in the outcome, but it is important to consider how much variance could have been explained by random chance: \\(E(R^2) = \\frac{K}{n-1} = \\frac{13}{43 - 1} = .31\\). We expect to explain 31% of the variance, by chance, in the outcome. So, 82% of the variance explained was likely spurious (i.e., \\(\\frac{.31}{.38} = .82\\)). As the sample size increases, the spuriousness decreases.\nTo account for the number of predictor variables in the model, we can use a modified version of \\(R^2\\) called adjusted \\(R^2\\) (\\(R^2_{adj}\\)). Adjusted \\(R^2\\) (\\(R^2_{adj}\\)) accounts for the number of predictor variables in the model, based on how much would be expected to be accounted for by chance to penalize overfitting. Adjusted \\(R^2\\) (\\(R^2_{adj}\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictor variables in the model. The formula for adjusted \\(R^2\\) (\\(R^2_{adj}\\)) is in Equation 11.4:\n\\[\nR^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1}\n\\tag{11.4}\\]\nwhere \\(p\\) is the number of predictor variables in the model, and \\(n\\) is the sample size.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-overfitting",
    "href": "multiple-regression.html#sec-overfitting",
    "title": "11  Multiple Regression",
    "section": "\n11.6 Overfitting",
    "text": "11.6 Overfitting\nStatistical models applied to big data (e.g., data with many predictor variables) can overfit the data, which means that the statistical model accounts for error variance, which will not generalize to future samples. So, even though an overfitting statistical model appears to be accurate because it is accounting for more variance, it is not actually that accurate—it will predict new data less accurately than how accurately it accounts for the data with which the model was built. In the case of fantasy football analytics, this is especially relevant because there are hundreds if not thousands of variables we could consider for inclusion and many, many players when considering historical data.\nConsider an example where you develop an algorithm to predict players’ fantasy performance based on 2023 data using hundreds of predictor variables. To some extent, these predictor variables will likely account for true variance (i.e., signal) and error variance (i.e., noise). If we were to apply the same algorithm based on the 2023 prediction model to 2024 data, the prediction model would likely predict less accurately than with 2023 data. The regression coefficients in the [FILL IN]\nIn Figure 11.3, the blue line represents the true distribution of the data, and the red line is an overfitting model:\n\nCodeset.seed(52242)\n\nsampleSize &lt;- 200\nquadraticX &lt;- rnorm(sampleSize)\nquadraticY &lt;- quadraticX ^ 2 + rnorm(sampleSize)\nquadraticData &lt;- cbind(quadraticX, quadraticY) %&gt;%\n  data.frame %&gt;%\n  arrange(quadraticX)\n\nquadraticModel &lt;- lm(\n  quadraticY ~ quadraticX + I(quadraticX ^ 2),\n  data = quadraticData)\n\nquadraticNewData &lt;- data.frame(\n  quadraticX = seq(\n    from = min(quadraticData$quadraticX),\n    to = max(quadraticData$quadraticY),\n    length.out = sampleSize))\n\nquadraticNewData$quadraticY &lt;- predict(\n  quadraticModel,\n  newdata = quadraticNewData)\n\nloessFit &lt;- loess(\n  quadraticY ~ quadraticX,\n  data = quadraticData,\n  span = 0.01,\n  degree = 1)\n\nloessNewData &lt;- data.frame(\n  quadraticX = seq(\n    from = min(quadraticData$quadraticX),\n    to = max(quadraticData$quadraticY),\n    length.out = sampleSize))\n\nquadraticNewData$loessY &lt;- predict(\n  loessFit,\n  newdata = quadraticNewData)\n\nplot(\n  x = quadraticData$quadraticX,\n  y = quadraticData$quadraticY,\n  xlab = \"\",\n  ylab = \"\")\n\nlines(\n  quadraticNewData$quadraticY ~ quadraticNewData$quadraticX,\n  lwd = 2,\n  col = \"blue\")\n\nlines(\n  quadraticNewData$loessY ~ quadraticNewData$quadraticX,\n  lwd = 2,\n  col = \"red\")\n\n\n\n\n\n\nFigure 11.3: Over-fitting Model in Red Relative to the True Distribution of the Data in Blue. From Petersen (2024) and Petersen (2025).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-covariates",
    "href": "multiple-regression.html#sec-covariates",
    "title": "11  Multiple Regression",
    "section": "\n11.7 Covariates",
    "text": "11.7 Covariates\nCovariates are variables that you include in the statistical model to try to control for them so you can better isolate the unique contribution of the predictor variable(s) in relation to the outcome variable. Use of covariates examines the association between the predictor variable and the outcome variable when holding people’s level constant on the covariates. Inclusion of confounds as covariates allows potentially gaining a more accurate estimate of the causal effect of the predictor variable on the outcome variable. Ideally, you want to include any and all confounds as covariates. As described in Section 8.4.2.1, confounds are third variables that influence both the predictor variable and the outcome variable and explain their association. Covariates are potentially (but not necessarily) confounds. For instance, you might include the player’s age as a covariate in a model that examines whether a player’s 40-yard dash time at the NFL Combine predicts their fantasy points in their rookie year, but it may not be a confound.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionMulticollinearity",
    "href": "multiple-regression.html#sec-multipleRegressionMulticollinearity",
    "title": "11  Multiple Regression",
    "section": "\n11.8 Multicollinearity",
    "text": "11.8 Multicollinearity\nMulticollinearity occurs when two or more predictor variables in a regression model are highly correlated. The problem of having multiple predictor variables that are highly correlated is that it makes it challenging to estimate the regression coefficients accurately.\nMulticollinearity in multiple regression is depicted conceptually in Figure 11.4.\n\n\n\n\n\nFigure 11.4: Conceptual Depiction of Multicollinearity in Multiple Regression. From Petersen (2024) and Petersen (2025).\n\n\nConsider the following example adapted from Petersen (2025) where you have two predictor variables and one outcome variable, as shown in Table 11.3.\n\n\n\nTable 11.3: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\nx2\n\n\n\n9\n2.0\n4\n\n\n11\n3.0\n6\n\n\n17\n4.0\n8\n\n\n3\n1.0\n2\n\n\n21\n5.0\n10\n\n\n13\n3.5\n7\n\n\n\n\n\n\n\n\nThe second predictor variable is not very good—it is exactly twice the value of the first predictor variable; thus, the two predictor variables are perfectly correlated (i.e., \\(r = 1.0\\)). This means that there are different prediction equation possibilities that are equally good—see Equations in Equation 11.5:\n\\[\n\\begin{aligned}\n  2x_2 &= y \\\\\n  0x_1 + 2x_2 &= y \\\\\n  4x_1 &= y \\\\\n  4x_1 + 0x_2 &= y \\\\\n  2x_1 + 1x_2 &= y \\\\\n  5x_1 - 0.5x_2 &= y \\\\\n  ...\n&= y\n\\end{aligned}\n\\tag{11.5}\\]\nThen, what are the regression coefficients? We do not know what are the correct regression coefficients because each of the possibilities fits the data equally well. Thus, when estimating the regression model, we could obtain arbitrary estimates of the regression coefficients with an enormous standard error around each estimate. In general, multicollinearity increases the uncertainty (i.e., standard errors and confidence intervals) around the parameter estimates. Any predictor variables that have a correlation above ~ \\(r = .30\\) with each other could have an impact on the confidence interval of the regression coefficient. As the correlations among the predictor variables increase, the chance of getting an arbitrary answer increases, sometimes called “bouncing betas.” So, it is important to examine a correlation matrix of the predictor variables before putting them in the same regression model. You can also examine indices such as variance inflation factor (VIF).\nTo address multicollinearity, you can drop a redundant predictor or you can also use principal component analysis or factor analysis of the predictors to reduce the predictors down to a smaller number of meaningful predictors. For a meaningful answer in a regression framework that is precise and confident, you need a low level of intercorrelation among predictors, unless you have a very large sample size.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionOutliers",
    "href": "multiple-regression.html#sec-multipleRegressionOutliers",
    "title": "11  Multiple Regression",
    "section": "\n11.9 Impact of Oultiers",
    "text": "11.9 Impact of Oultiers\nAs with correlation, multiple regression can be strongly impacted by outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-moderatedMultipleRegression",
    "href": "multiple-regression.html#sec-moderatedMultipleRegression",
    "title": "11  Multiple Regression",
    "section": "\n11.10 Moderated Multiple Regression",
    "text": "11.10 Moderated Multiple Regression\nWhen examining moderation in multiple regression, several steps are important:\n\nWhen computing the interaction term, first mean-center the predictor variables. Calculate the interaction term as the multiplication of the mean-centered predictor variables. Mean-centering the predictor variables when computing the interaction term is important for addressing issues regarding multicollinearity (Iacobucci et al., 2016).\nWhen including an interaction term in the model, make sure also to include the main effects.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionMediation",
    "href": "multiple-regression.html#sec-multipleRegressionMediation",
    "title": "11  Multiple Regression",
    "section": "\n11.11 Mediation",
    "text": "11.11 Mediation",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionBayesian",
    "href": "multiple-regression.html#sec-multipleRegressionBayesian",
    "title": "11  Multiple Regression",
    "section": "\n11.12 Bayesian Multiple Regression",
    "text": "11.12 Bayesian Multiple Regression",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionConclusion",
    "href": "multiple-regression.html#sec-multipleRegressionConclusion",
    "title": "11  Multiple Regression",
    "section": "\n11.13 Conclusion",
    "text": "11.13 Conclusion\nMultiple regression allows examining the association between multiple predictor variables and one outcome variable. Inclusion of multiple predictors in the model allows for potentially greater predictive accuracy and identification of the extent to which each variable uniquely contributes to the outcome variable. As with correlation, an association does not imply causation. However, identifying associations is important because associations are a necessary (but insufficient) condition for causality. When developing a multiple regression model, it is important to pay attention for potential multicollinearity—it may become difficult to detect a given predictor variable as statistically significant due to the greater uncertainty around the parameter estimates.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionSessionInfo",
    "href": "multiple-regression.html#sec-multipleRegressionSessionInfo",
    "title": "11  Multiple Regression",
    "section": "\n11.14 Session Info",
    "text": "11.14 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] knitr_1.50        lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n [5] dplyr_1.1.4       purrr_1.0.4       readr_2.1.5       tidyr_1.3.1      \n [9] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       xfun_0.51          htmlwidgets_1.6.4  psych_2.5.3       \n [5] lattice_0.22-6     tzdb_0.5.0         quadprog_1.5-8     vctrs_0.6.5       \n [9] tools_4.4.3        generics_0.1.3     stats4_4.4.3       parallel_4.4.3    \n[13] cluster_2.1.8      pkgconfig_2.0.3    data.table_1.17.0  checkmate_2.3.2   \n[17] RColorBrewer_1.1-3 lifecycle_1.0.4    compiler_4.4.3     munsell_0.5.1     \n[21] mnormt_2.1.1       mitools_2.4        htmltools_0.5.8.1  yaml_2.3.10       \n[25] htmlTable_2.4.3    Formula_1.2-5      pillar_1.10.1      Hmisc_5.2-3       \n[29] rpart_4.1.24       nlme_3.1-167       lavaan_0.6-19      tidyselect_1.2.1  \n[33] digest_0.6.37      mvtnorm_1.3-3      stringi_1.8.4      reshape2_1.4.4    \n[37] fastmap_1.2.0      grid_4.4.3         colorspace_2.1-1   cli_3.6.4         \n[41] magrittr_2.0.3     base64enc_0.1-3    pbivnorm_0.6.0     foreign_0.8-88    \n[45] withr_3.0.2        scales_1.3.0       backports_1.5.0    timechange_0.3.0  \n[49] rmarkdown_2.29     nnet_7.3-20        gridExtra_2.3      hms_1.1.3         \n[53] evaluate_1.0.3     mix_1.0-13         viridisLite_0.4.2  rlang_1.1.5       \n[57] Rcpp_1.0.14        xtable_1.8-4       glue_1.8.0         DBI_1.2.3         \n[61] rstudioapi_0.17.1  jsonlite_1.9.1     R6_2.6.1           plyr_1.8.9        \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nIacobucci, D., Schneider, M. J., Popovich, D. L., & Bakamitsos, G. A. (2016). Mean centering helps alleviate “micro” but not “macro” multicollinearity. Behavior Research Methods, 48(4), 1308–1317. https://doi.org/10.3758/s13428-015-0624-x\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "mixed-models.html",
    "href": "mixed-models.html",
    "title": "12  Mixed Models",
    "section": "",
    "text": "12.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsGettingStarted",
    "href": "mixed-models.html#sec-mixedModelsGettingStarted",
    "title": "12  Mixed Models",
    "section": "",
    "text": "12.1.1 Load Packages\n\nCodelibrary(\"lme4\")\nlibrary(\"lmerTest\")\nlibrary(\"MuMIn\")\nlibrary(\"emmeans\")\nlibrary(\"sjstats\")\nlibrary(\"mgcv\")\nlibrary(\"AICcmodavg\")\nlibrary(\"bbmle\")\nlibrary(\"rstan\")\nlibrary(\"brms\")\nlibrary(\"cmdstanr\") # todo: install.packages(\"cmdstanr\", repos = c(\"https://stan-dev.r-universe.dev\", getOption(\"repos\"))); check_cmdstan_toolchain(); cmdstanr::install_cmdstan()\nlibrary(\"fitdistrplus\")\nlibrary(\"parallel\")\nlibrary(\"parallelly\")\nlibrary(\"plotly\")\nlibrary(\"viridis\")\nlibrary(\"tidyverse\")\n\n\n\n12.1.2 Specify Package Options\n\nCodeemm_options(lmerTest.limit = 100000)\nemm_options(pbkrtest.limit = 100000)\n\n\n\n12.1.3 Load Data\n\nCodeload(file = \"./data/nfl_depthCharts.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsOverview",
    "href": "mixed-models.html#sec-mixedModelsOverview",
    "title": "12  Mixed Models",
    "section": "\n12.2 Overview of Mixed Models",
    "text": "12.2 Overview of Mixed Models\nWe will discuss a modeling framework that goes by many terms, including mixed models, mixed-effects models, multilevel models, hierarchical linear models. They are sometimes called multilevel models and hierarchical linear models, whose name emphasizes the hierarchical structure of the data because the data are nonindependent. When observations (i.e., data points) are collected from multiple lower-level units (e.g., people) in an upper-level unit (e.g., married couple, family, classroom, school, neighborhood, team), the data from the lower-level units are considered “nested” within the upper-level unit. In this context, nested data refers to multiple observations from the same upper-level unit. For instance, longitudinal data are nested within the same participant. Students are nested within classrooms, which are nested within schools. Players are nested within teams.\nWhen data are nested, the data from the lower-level unit are likely to be correlated, to some degree, because they come from the same upper-level unit. For example, multiple players may come from the same team, and the players’ performance on that team is likely interrelated because they share common experiences and influence one another. Thus, data from multiple players on a given team are considered nested within that team. Longitudinal data can also be considered nested data, in which time points are nested within the person (i.e., the same player provides an observation across multiple time points). As we will discuss, it is important to account for levels of nesting when the observations are nonindependent.\nThese models are also sometimes called mixed models or mixed-effects models because the models can include a mix of fixed and random effects. Fixed effects are effects that are constant across individuals (i.e., upper-level units). Random effects are effects that vary across individuals (i.e., upper-level units). For instance, consider a longitudinal study of fantasy performance as a function of age. If we have longitudinal data for multiple players, the time points are nested within players. Examining the association between age as a fixed effect in relation to fantasy performace would examine the assocication between a player’s age and their fantasy performance while holding the association between age and fantasy performance constant across all players. That is, it would assume that all players show the same trajectory such as increase for 4 years then decrease. Examining the association between age as a random effect in relation to fantasy performace would examine the assocication between a player’s age and their fantasy performance while allowing the association between age and fantasy performance to vary across players. That is, it would allow the possibility that some players improve with age, whereas other players decline in performance with age.\nWhen including random effects of a variable (e.g., age) in a mixed model, it is also important to include fixed effects of that variable in the model. This is because random effects have a mean of zero. Fixed effects allow the mean to differ from zero. Thus, inclusion of random effects without the corresponding fixed effect can lead to bias in estimation of the association between the predictor variables and the outcome variable.\n\n12.2.1 Ecological Fallacy\nAs described in Section 14.5.9, the ecological fallacy is the error of drawing inferences about an individual from group-level data. An type of ecological fallacy is Simpson’s paradox.\n\n12.2.2 Simpson’s Paradox\nExamples of Simpson’s Paradox are depicted in Figures 13.5 and 12.1. In the example below, there is a positive association between the predictor variable and outcome variable for each group. However, when the groups are combined, there is a negative association between predictor variable and outcome variable. That is, the sign of an association can differ at different levels of analysis (e.g., group level versus person level).\n\n\n\n\n\nFigure 12.1: An Example of Simpson’s Paradox. In this example, there is a positive association between x and y within each group (i.e., red or blue), but there is a negative association between x and y when the groups are combined. (Figure retrieved from https://en.wikipedia.org/wiki/File:Simpson%27s_paradox_continuous.svg)\n\n\nConsider that we observe a between-person association between a predictor, for example, how much sports drink a player drinks and their performance, such that players who drink more sports drink before games tend to perform better during games. However, based on this association, if we draw the inference that sports drink consumption leads to better performance, this could be a faulty inference. It is possible that, at the within-person level, there is no association or even a negative association between sports drink consumption and performance. For helping to approximate causality, a much stronger test than relying on the between-person association would be to examine the association within the individual. That is, examining the association within the individual would examine: when a player consumes more sports drink, whether they perform better than when the same player consumes less sports drink. We describe within-person analyses to approximate causal inferences in Section 13.4.2.2.\nIn short, we often need to account for the groups or “levels” within which people are nested. When multiple observations are nested within the same upper-level unit, they are often correlated. That is, some variance is attributable to the lower-level unit (e.g., the player) and some variance is attributable to the upper-level unit (e.g., the team). It is important to evaluate how much variance is attributable to the upper-level unit. If substantial variance is attributable to the upper-level unit, it is important to account for the nested structure of the data. Mixed models are a useful approach to account for data with a nested structure so you can avoid committing the ecological fallacy.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-fantasyPointsByAgeExperience",
    "href": "mixed-models.html#sec-fantasyPointsByAgeExperience",
    "title": "12  Mixed Models",
    "section": "\n12.3 Fantasy Points Per Season by Position, Age, and Experience",
    "text": "12.3 Fantasy Points Per Season by Position, Age, and Experience\nIn this chapter, we use mixed models to demonstrate how to model trajectories (growth curves) of players’ fantasy points as a function of age. One of the challenges when modeling players’ fantasy points as a function of age is that fantasy points are a function of both ability and opportunity. With age, the player’s ability and opportunity may both decline, but it is difficult to disentangle the extent to which a player’s decline is related to declines in ability versus opportunity. As players get older, they may be supplanted by younger, more talented players. With age, despite still having latent (unobserved) ability, players will get fewer and, eventually, no opportunity. And we do not know the counterfactual—how many fantasy points they would have scored had they been given the full opportunity each season. Thus, players may go from scoring 100+ points in a season to all of a sudden scoring way fewer points, with little in the way of intermediate steps. Thus, this should inform how you interpret the resulting trajectories. For instance, the steep declines can make it look like the model-implied fantasy points go well below zero at later ages, even though it is obviously not likely for players to obtain such a magnitude of negative fantasy points. The negative model-implied values of fantasy points at later ages are likely an artifact of the decreasing opportunity that players tend to get as they age. The crossover point, where the positive values becomes negative might indicate, for instance, that players at that age tend to have retired and thus have no opportunity. Regardless, the form/shape/steepness of the decline is still potentially meaningful even if the precise negative number at a given age is not.\n\nCodeplayer_stats_seasonal_offense_subset &lt;- player_stats_seasonal %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\") | position %in% c(\"K\"))\n\nplayer_stats_seasonal_offense_subset$position[which(player_stats_seasonal_offense_subset$position == \"HB\")] &lt;- \"RB\"\n\nplayer_stats_seasonal_offense_subset$player_idFactor &lt;- factor(player_stats_seasonal_offense_subset$player_id)\nplayer_stats_seasonal_offense_subset$positionFactor &lt;- factor(player_stats_seasonal_offense_subset$position)\n\n\n\nCodeseasons17week &lt;- 2001:2020\nseasons18week &lt;- 2021:max(nfl_depthCharts$season, na.rm = TRUE)\n\nendOfSeasonDepthCharts &lt;- nfl_depthCharts %&gt;% \n  filter((season %in% seasons17week & week == 18) | (season %in% seasons18week & week == 19)) # get end-of-season depth charts\n\nqb1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"QB\", depth_team == 1)\n\nfb1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"FB\", depth_team == 1)\n\nk1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"K\", depth_team == 1)\n\nrb1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"RB\", depth_team == 1)\n\nwr1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"WR\", depth_team == 1)\n\nte1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"TE\", depth_team == 1)\n\nplayer_stats_seasonal_offense_subsetDepth &lt;- player_stats_seasonal_offense_subset %&gt;% \n  filter(player_id %in% c(\n    qb1s$gsis_id,\n    fb1s$gsis_id,\n    k1s$gsis_id,\n    rb1s$gsis_id,\n    wr1s$gsis_id,\n    te1s$gsis_id\n    ))\n\n\nCreate a newdata object for generating the plots of model-implied fantasy points by age and position:\n\nCodepointsPerSeason_positionAge_newData &lt;- expand.grid(\n  positionFactor = factor(c(\"FB\",\"QB\",\"RB\",\"TE\",\"WR\")), #,\"K\"\n  age = seq(from = 20, to = 40, length.out = 10000)\n)\n\npointsPerSeason_positionAge_newData$ageCentered20 &lt;- pointsPerSeason_positionAge_newData$age - 20\npointsPerSeason_positionAge_newData$ageCentered20Quadratic &lt;- pointsPerSeason_positionAge_newData$ageCentered20 ^ 2\npointsPerSeason_positionAge_newData$years_of_experience &lt;- floor(pointsPerSeason_positionAge_newData$age - 22) # assuming that most players start at age 22 (i.e., rookie year) and thus have 1 year of experience at age 23\npointsPerSeason_positionAge_newData$years_of_experience[which(pointsPerSeason_positionAge_newData$years_of_experience &lt; 0)] &lt;- 0\n\n\nCreate an object with complete cases for generating the plots of individuals’ model-implied fantasy points by age and position:\n\nCodeplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subset %&gt;%\n  filter(\n    !is.na(player_idFactor),\n    !is.na(fantasyPoints),\n    !is.na(positionFactor),\n    !is.na(ageCentered20),\n    !is.na(ageCentered20Quadratic),\n    !is.na(years_of_experience))\n\n\n\n12.3.1 Scatterplots of Fantasy Points by Age and Position\nScatterplots are a helpful tool for quickly examining the association between two variables. However, scatterplots—as well as correlation and multiple regression—can hide meaningful associations that differ across units of analysis.\n\n12.3.1.1 Quarterbacks\nA scatterplot of Quarterbacks’ fantasy points by age is in Figure 12.2.\n\nCodeplot_scatterplotFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"QB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_scatterplotFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.2: Scatterplot of Fantasy Points by Age for Quarterbacks.\n\n\n\nBased on the scatterplot (and the bivariate association below), Quarterbacks’ fantasy points appear to increase with age.\n\nCodecor.test(\n  formula = ~ age + fantasyPoints,\n  data = player_stats_seasonal_offense_subset %&gt;% filter(position == \"QB\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  age and fantasyPoints\nt = 2.4333, df = 1922, p-value = 0.01505\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.01075572 0.09985864\nsample estimates:\n       cor \n0.05541751 \n\n\n\n12.3.1.2 Fullbacks\nA scatterplot of Fullbacks’ fantasy points by age is in Figure 12.3.\n\nCodeplot_scatterplotFantasyPointsByAgeFB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"FB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Fullbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_scatterplotFantasyPointsByAgeFB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.3: Scatterplot of Fantasy Points by Age for Fullbacks.\n\n\n\nBased on the scatterplot, Fullbacks’ fantasy points appear to be relatively stable across ages.\n\n12.3.1.3 Running Backs\nA scatterplot of Running Backs’ fantasy points by age is in Figure 12.4.\n\nCodeplot_scatterplotFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"RB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_scatterplotFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.4: Scatterplot of Fantasy Points by Age for Running Backs.\n\n\n\nBased on the scatterplot, Running Backs’ fantasy points appear to be relatively stable across ages.\n\n12.3.1.4 Wide Receivers\nA scatterplot of Wide Receivers’ fantasy points by age is in Figure 12.5.\n\nCodeplot_scatterplotFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"WR\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_scatterplotFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.5: Scatterplot of Fantasy Points by Age for Wide Receivers.\n\n\n\nBased on the scatterplot, Wide Receivers’ fantasy points appear to be relatively stable across ages.\n\n12.3.1.5 Tight Ends\nA scatterplot of Tight Ends’ fantasy points by age is in Figure 12.6.\n\nCodeplot_scatterplotFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"TE\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_scatterplotFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.6: Scatterplot of Fantasy Points by Age for Tight Ends.\n\n\n\nBased on the scatterplot (and the bivariate association below), Tight Ends’ fantasy points appear to increase with age.\n\nCodecor.test(\n  formula = ~ age + fantasyPoints,\n  data = player_stats_seasonal_offense_subset %&gt;% filter(position == \"TE\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  age and fantasyPoints\nt = 5.6935, df = 2874, p-value = 1.37e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.06932659 0.14161227\nsample estimates:\n      cor \n0.1056089 \n\n\n\n12.3.2 Plots of Raw Trajectories of Fantasy Points By Age and Player\nScatterplots can be helpful for quickly visualizing the association between two variables. However, as mentioned earlier, scatterplots can hide the association between variables at different units of analysis. For instance, consider that we are trying to predict how a player will perform based on their age. We are interested not only in what the association is between age and fantasy points between players (i.e., a between-person association). We are also interested in what the association is between age and fantasy points within a given player (and within each player; i.e., a within-individual association). Arguably, the within-individual association between age and fantasy points is more relevant to the prediction of performance than the association between age and fantasy points between players. Assuming that the between-player association between age and fantasy points is the same as the within-player association when it is not is an example of the ecological fallacy.\nBelow, we depict players’ raw trajectories of fantasy points as a function of age. These are known as spaghetti plots. By examining the trajectory for each player, we can get a better understanding of hor performance changes (within an individual) as a function of age.\n\n12.3.2.1 Quarterbacks\nA plot of Quarterbacks’ raw fantasy points data by age is in Figure 12.7.\n\nCodeplot_rawFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"QB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_rawFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.7: Plot of Raw Trajectories of Fantasy Points by Age for Quarterbacks.\n\n\n\n\n12.3.2.2 Fullbacks\nA plot of Fullbacks’ raw fantasy points data by age is in Figure 12.8.\n\nCodeplot_rawFantasyPointsByAgeFB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"FB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Fullbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_rawFantasyPointsByAgeFB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.8: Plot of Raw Trajectories of Fantasy Points by Age for Fullbacks.\n\n\n\n\n12.3.2.3 Running Backs\nA plot of Running Backs’ raw fantasy points data by age is in Figure 12.9.\n\nCodeplot_rawFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"RB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_rawFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.9: Plot of Raw Trajectories of Fantasy Points by Age for Running Backs.\n\n\n\n\n12.3.2.4 Wide Receivers\nA plot of Wide Receivers’ raw fantasy points data by age is in Figure 12.10.\n\nCodeplot_rawFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"WR\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_rawFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.10: Plot of Raw Trajectories of Fantasy Points by Age for Wide Receivers.\n\n\n\n\n12.3.2.5 Tight Ends\nA plot of Tight Ends’ raw fantasy points data by age is in Figure 12.11.\n\nCodeplot_rawFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"TE\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nggplotly(\n  plot_rawFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.11: Plot of Raw Trajectories of Fantasy Points by Age for Tight Ends.\n\n\n\n\n12.3.3 Linear Regression Models\n\n12.3.3.1 Null Model\nWe can estimate model fit using the MuMIn package (Bartoń, 2024):\n\nCodepointsPerSeason_nullModel &lt;- lm(\n  fantasyPoints ~ 1,\n  data = player_stats_seasonal_offense_subset,\n  na.action = \"na.exclude\"\n)\n\nsummary(pointsPerSeason_nullModel)\n\n\nCall:\nlm(formula = fantasyPoints ~ 1, data = player_stats_seasonal_offense_subset, \n    na.action = \"na.exclude\")\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-70.13 -50.15 -28.25  27.32 371.25 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  57.8550     0.5802   99.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 69.28 on 14259 degrees of freedom\n  (1038 observations deleted due to missingness)\n\nCodesummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodeAIC(pointsPerSeason_nullModel)\n\n[1] 161343.8\n\nCodeMuMIn::AICc(pointsPerSeason_nullModel)\n\n[1] 161343.8\n\n\nA plot of the model-implied trajectories of fantasy points by age from the null model is in Figure 12.12.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_nullModel &lt;- predict(\n  object = pointsPerSeason_nullModel,\n  newdata = pointsPerSeason_positionAge_newData\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_nullModel\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Null Model\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.12: Plot of Model-Implied Trajectories of Fantasy Points by Age in Null Model.\n\n\n\n\n\n12.3.3.2 Linear Model\n\nCodepointsPerSeason_linearRegression &lt;- lm(\n  fantasyPoints ~ positionFactor + ageCentered20 + positionFactor:ageCentered20,\n  data = player_stats_seasonal_offense_subset,\n  na.action = \"na.exclude\"\n)\n\nsummary(pointsPerSeason_linearRegression)\n\n\nCall:\nlm(formula = fantasyPoints ~ positionFactor + ageCentered20 + \n    positionFactor:ageCentered20, data = player_stats_seasonal_offense_subset, \n    na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-125.62  -45.07  -17.15   28.24  361.49 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     12.1856     6.4287   1.896   0.0580 .  \npositionFactorQB                76.3279     7.2075  10.590  &lt; 2e-16 ***\npositionFactorRB                50.1549     6.9163   7.252 4.33e-13 ***\npositionFactorTE                11.7881     7.0514   1.672   0.0946 .  \npositionFactorWR                26.3865     6.7391   3.915 9.07e-05 ***\nageCentered20                    0.9980     0.8244   1.211   0.2261    \npositionFactorQB:ageCentered20   0.3193     0.8899   0.359   0.7198    \npositionFactorRB:ageCentered20  -0.2670     0.9119  -0.293   0.7697    \npositionFactorTE:ageCentered20   0.2666     0.9139   0.292   0.7705    \npositionFactorWR:ageCentered20   1.6862     0.8734   1.931   0.0535 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.61 on 14250 degrees of freedom\n  (1038 observations deleted due to missingness)\nMultiple R-squared:  0.1039,    Adjusted R-squared:  0.1033 \nF-statistic: 183.5 on 9 and 14250 DF,  p-value: &lt; 2.2e-16\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.1038626\n\nCodeAIC(pointsPerSeason_linearRegression)\n\n[1] 159798.1\n\nCodeMuMIn::AICc(pointsPerSeason_linearRegression)\n\n[1] 159798.1\n\n\nA plot of the model-implied trajectories of fantasy points by age from the linear regression model is in Figure 12.13.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_linearRegression &lt;- predict(\n  object = pointsPerSeason_linearRegression,\n  newdata = pointsPerSeason_positionAge_newData\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_linearRegression,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Linear Regression Model\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.13: Plot of Model-Implied Trajectories of Fantasy Points by Age in Linear Regression Model.\n\n\n\n\n\n12.3.3.3 Quadratic Model\n\nCodepointsPerSeason_quadraticRegression &lt;- lm(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic,\n  data = player_stats_seasonal_offense_subset,\n  na.action = \"na.exclude\"\n)\n\nsummary(pointsPerSeason_quadraticRegression)\n\n\nCall:\nlm(formula = fantasyPoints ~ positionFactor + ageCentered20 + \n    ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic, \n    data = player_stats_seasonal_offense_subset, na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-128.72  -44.87  -17.22   28.36  360.36 \n\nCoefficients:\n                                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              0.93927   13.81204   0.068 0.945784\npositionFactorQB                        92.65990   15.12690   6.126 9.28e-10\npositionFactorRB                        56.36453   14.68364   3.839 0.000124\npositionFactorTE                        19.99959   14.93471   1.339 0.180548\npositionFactorWR                        33.26215   14.32988   2.321 0.020291\nageCentered20                            4.16191    3.53667   1.177 0.239299\nageCentered20Quadratic                  -0.18902    0.20546  -0.920 0.357610\npositionFactorQB:ageCentered20          -4.10358    3.78168  -1.085 0.277886\npositionFactorRB:ageCentered20          -1.68525    3.85511  -0.437 0.662011\npositionFactorTE:ageCentered20          -1.97485    3.85615  -0.512 0.608568\npositionFactorWR:ageCentered20          -0.07568    3.69732  -0.020 0.983669\npositionFactorQB:ageCentered20Quadratic  0.25025    0.21492   1.164 0.244290\npositionFactorRB:ageCentered20Quadratic  0.06553    0.23073   0.284 0.776413\npositionFactorTE:ageCentered20Quadratic  0.13143    0.22542   0.583 0.559888\npositionFactorWR:ageCentered20Quadratic  0.09954    0.21589   0.461 0.644766\n                                           \n(Intercept)                                \npositionFactorQB                        ***\npositionFactorRB                        ***\npositionFactorTE                           \npositionFactorWR                        *  \nageCentered20                              \nageCentered20Quadratic                     \npositionFactorQB:ageCentered20             \npositionFactorRB:ageCentered20             \npositionFactorTE:ageCentered20             \npositionFactorWR:ageCentered20             \npositionFactorQB:ageCentered20Quadratic    \npositionFactorRB:ageCentered20Quadratic    \npositionFactorTE:ageCentered20Quadratic    \npositionFactorWR:ageCentered20Quadratic    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.6 on 14245 degrees of freedom\n  (1038 observations deleted due to missingness)\nMultiple R-squared:  0.1042,    Adjusted R-squared:  0.1033 \nF-statistic: 118.4 on 14 and 14245 DF,  p-value: &lt; 2.2e-16\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.1042009\n\nCodeAIC(pointsPerSeason_quadraticRegression)\n\n[1] 159802.7\n\nCodeMuMIn::AICc(pointsPerSeason_quadraticRegression)\n\n[1] 159802.7\n\n\nA plot of the model-implied trajectories of fantasy points by age from the regression model with a quadratic term for age is in Figure 12.14.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_quadraticRegression &lt;- predict(\n  object = pointsPerSeason_quadraticRegression,\n  newdata = pointsPerSeason_positionAge_newData\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_quadraticRegression,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Quadratic Regression Model\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.14: Plot of Model-Implied Trajectories of Fantasy Points by Age in Quadratic Regression Model.\n\n\n\n\n\n12.3.3.4 Compare Models\nWe compare models using the bbmle (Bolker & R Development Core Team, 2023) and MuMIn (Bartoń, 2024) packages.\n\nCodeanova(\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression\n)\n\n\n  \n\n\nCodeAIC(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression\n  )\n\n\n  \n\n\nCodelmModels &lt;- list(\n  \"nullModel\" = pointsPerSeason_nullModel,\n  \"linearRegression\" = pointsPerSeason_linearRegression,\n  \"quadraticRegression\" = pointsPerSeason_quadraticRegression\n)\n\nbbmle::AICtab(lmModels)\n\n                    dAIC   df\nlinearRegression       0.0 11\nquadraticRegression    4.6 16\nnullModel           1545.8 2 \n\nCodeMuMIn::AICc(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression\n)\n\n\n  \n\n\nCodesummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.1038626\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.1042009\n\nCodedeviance(pointsPerSeason_nullModel)\n\n[1] 68441485\n\nCodedeviance(pointsPerSeason_linearRegression)\n\n[1] 61332977\n\nCodedeviance(pointsPerSeason_quadraticRegression)\n\n[1] 61309819\n\nCodelogLik(pointsPerSeason_nullModel)\n\n'log Lik.' -80669.91 (df=2)\n\nCodelogLik(pointsPerSeason_linearRegression)\n\n'log Lik.' -79888.03 (df=11)\n\nCodelogLik(pointsPerSeason_quadraticRegression)\n\n'log Lik.' -79885.33 (df=16)\n\n\n\n12.3.4 Mixed Models\nBy accounting for which player each observation comes from using mixed models, we can examine the association between age and fantasy points in a more meaningful way, without violating the assumption in multiple regression that the observations are independent (i.e., that the residuals are uncorrelated).\n\n12.3.4.1 Random Intercepts Model\nWe estimate the multilevel models using the lmerTest package (Kuznetsova et al., 2020), which is an extension of the lme4 package (Bates et al., 2015, 2025):\n\nCodepointsPerSeason_randomIntercepts &lt;- lmerTest::lmer(\n  fantasyPoints ~ 1 + (1 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_randomIntercepts)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ 1 + (1 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n154267.1 154289.8 -77130.5 154261.1    14257 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.7750 -0.4300 -0.1629  0.3480  5.0752 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n player_idFactor (Intercept) 1979     44.49   \n Residual                    2060     45.38   \nNumber of obs: 14260, groups:  player_idFactor, 3524\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   42.7908     0.8858 4009.9557   48.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_randomIntercepts)\n\n     R2m       R2c\n[1,]   0 0.4900142\n\nCodeperformance::icc(pointsPerSeason_randomIntercepts)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_randomIntercepts)\n\n[1] 154267.1\n\nCodeAICc(pointsPerSeason_randomIntercepts)\n\nnumeric(0)\n\n\nA plot of the model-implied trajectories of fantasy points by age from the mixed model with random intercepts is in Figure 12.15.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomIntercepts &lt;- predict(\n  object = pointsPerSeason_randomIntercepts,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomIntercepts\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age\",\n    subtitle = \"Random Intercepts Model\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.15: Plot of Model-Implied Trajectories of Fantasy Points by Age in Random Intercepts Mixed Model.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age from the mixed model with random intercepts is in Figure 12.16.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomIntercepts &lt;- predict(\n  object = pointsPerSeason_randomIntercepts,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomIntercepts &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomIntercepts = round(fantasyPoints_randomIntercepts, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomIntercepts,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomIntercepts,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomIntercepts\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints = round(fantasyPoints_randomIntercepts, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    color = \"#3366FF\",\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position: Random Intercepts Model\",\n    #color = \"Position\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsRandomIntercepts,\n  tooltip = c(\"age\",\"fantasyPoints_randomIntercepts\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.16: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Mixed Model with Random Intercepts. Overlaid with the Model-Implied Trajectory.\n\n\n\n\n12.3.4.2 Random Intercepts Model with Position as Fixed-Effect Predictor\nWe use the emmeans package (Lenth, 2025) to determine the model-implied number of points per season by position.\n\nCodepointsPerSeason_position &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + (1 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_position)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ positionFactor + (1 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n153972.8 154025.7 -76979.4 153958.8    14253 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.7860 -0.4425 -0.1374  0.3626  4.9947 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n player_idFactor (Intercept) 1747     41.8    \n Residual                    2061     45.4    \nNumber of obs: 14260, groups:  player_idFactor, 3524\n\nFixed effects:\n                 Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)        16.780      3.981 3669.655   4.215 2.56e-05 ***\npositionFactorQB   57.079      4.709 3679.865  12.120  &lt; 2e-16 ***\npositionFactorRB   35.172      4.305 3716.687   8.170 4.19e-16 ***\npositionFactorTE    7.868      4.408 3712.096   1.785   0.0744 .  \npositionFactorWR   24.000      4.213 3708.861   5.696 1.32e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstFQB pstFRB pstFTE\npostnFctrQB -0.845                     \npostnFctrRB -0.925  0.782              \npostnFctrTE -0.903  0.763  0.835       \npostnFctrWR -0.945  0.799  0.874  0.853\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_position)\n\n            R2m     R2c\n[1,] 0.06193692 0.49239\n\nCodeemmeans::emmeans(pointsPerSeason_position, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               16.8 3.98 3100     8.97     24.6\n QB               73.9 2.52 3132    68.92     78.8\n RB               52.0 1.64 3404    48.73     55.2\n TE               24.6 1.90 3312    20.93     28.4\n WR               40.8 1.38 3446    38.07     43.5\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_position)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_position)\n\n[1] 153972.8\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and a fixed effect of position is in Figure 12.17.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_position &lt;- predict(\n  object = pointsPerSeason_position,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_position,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Random Intercepts Model With Position as Fixed-Effect Predictor\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.17: Plot of Model-Implied Trajectories of Fantasy Points by Age in Random Intercepts Mixed Model With Position as a Fixed-Effect Predictor.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and a fixed effect of position is in Figure 12.18.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_position &lt;- predict(\n  object = pointsPerSeason_position,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsPosition &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_position = round(fantasyPoints_position, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_position,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_position,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_position,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_position = round(fantasyPoints_position, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nRandom Intercepts Model With Position As Predictor\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsPosition,\n  tooltip = c(\"age\",\"fantasyPoints_position\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.18: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Mixed Model With Random Intercepts and a Fixed-Effect of Position. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.3.4.3 Identify the Best-Fitting Functional Form of Age\n\n12.3.4.3.1 Linear Models\n\n12.3.4.3.1.1 Random Intercepts, Fixed Linear Slopes\n\nCodepointsPerSeason_positionAgeFixedLinearSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + (1 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeFixedLinearSlopes)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ positionFactor + ageCentered20 + (1 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n153842.4 153902.9 -76913.2 153826.4    14252 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.0420 -0.4418 -0.1344  0.3603  4.9221 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n player_idFactor (Intercept) 1857     43.09   \n Residual                    2012     44.86   \nNumber of obs: 14260, groups:  player_idFactor, 3524\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)         28.6721     4.1869  3932.9414   6.848 8.65e-12 ***\npositionFactorQB    58.5427     4.8085  3571.0834  12.175  &lt; 2e-16 ***\npositionFactorRB    32.6953     4.3966  3608.3713   7.437 1.28e-13 ***\npositionFactorTE     6.6769     4.4988  3595.3608   1.484    0.138    \npositionFactorWR    21.8545     4.3020  3597.8279   5.080 3.97e-07 ***\nageCentered20       -1.8016     0.1528 14084.4134 -11.794  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstFQB pstFRB pstFTE pstFWR\npostnFctrQB -0.812                            \npostnFctrRB -0.907  0.779                     \npostnFctrTE -0.881  0.762  0.835              \npostnFctrWR -0.925  0.797  0.874  0.853       \nageCentrd20 -0.242 -0.033  0.043  0.021  0.038\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n            R2m       R2c\n[1,] 0.06722691 0.5148777\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeFixedLinearSlopes, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               16.7 4.07 3125     8.78     24.7\n QB               75.3 2.57 3193    70.24     80.3\n RB               49.4 1.68 3465    46.15     52.7\n TE               23.4 1.93 3332    19.63     27.2\n WR               38.6 1.42 3496    35.83     41.4\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeFixedLinearSlopes, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.62   40.7 1.13 3197     38.5     42.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n[1] 153842.4\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and fixed linear slopes is in Figure 12.19.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_fixedLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeFixedLinearSlopes,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_fixedLinearSlopes,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts and Fixed Linear Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.19: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts and Fixed Linear Slopes.\n\n\n\n\nA plot of individuals model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and fixed linear slopes is in Figure 12.20.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_fixedLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeFixedLinearSlopes,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsFixedLinearSlopes &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_fixedLinearSlopes = round(fantasyPoints_fixedLinearSlopes, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_fixedLinearSlopes,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_fixedLinearSlopes,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_fixedLinearSlopes,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData,\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts and Fixed Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsFixedLinearSlopes,\n  tooltip = c(\"age\",\"fantasyPoints_fixedLinearSlopes\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.20: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts and Fixed Slopes. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.3.4.3.1.2 Random Intercepts, Random Linear Slopes\n\nCodepointsPerSeason_positionAgeRandomLinearSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearSlopes)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ positionFactor + ageCentered20 + (1 + ageCentered20 |  \n    player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n153481.2 153556.9 -76730.6 153461.2    14250 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.7151 -0.4289 -0.1332  0.3487  4.9944 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   3072.54  55.431        \n                 ageCentered20   19.21   4.383   -0.65\n Residual                      1843.32  42.934        \nNumber of obs: 14260, groups:  player_idFactor, 3524\n\nFixed effects:\n                  Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)        26.5645     4.2417 3921.6784   6.263 4.19e-10 ***\npositionFactorQB   58.3340     4.7928 3390.8225  12.171  &lt; 2e-16 ***\npositionFactorRB   32.6527     4.3806 3474.5600   7.454 1.14e-13 ***\npositionFactorTE    6.9364     4.4778 3446.1603   1.549    0.121    \npositionFactorWR   22.2693     4.2840 3453.0992   5.198 2.13e-07 ***\nageCentered20      -1.4258     0.1928 1894.5867  -7.396 2.09e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstFQB pstFRB pstFTE pstFWR\npostnFctrQB -0.795                            \npostnFctrRB -0.892  0.775                     \npostnFctrTE -0.866  0.759  0.832              \npostnFctrWR -0.911  0.793  0.871  0.851       \nageCentrd20 -0.306 -0.023  0.047  0.023  0.043\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n            R2m       R2c\n[1,] 0.06417996 0.5548786\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearSlopes, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               17.1 4.04 2879      9.2     25.1\n QB               75.5 2.59 2895     70.4     80.5\n RB               49.8 1.70 3440     46.5     53.1\n TE               24.1 1.94 3214     20.3     27.9\n WR               39.4 1.43 3454     36.6     42.2\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearSlopes, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.62   41.2 1.13 2952       39     43.4\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n[1] 153481.2\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and random linear slopes is in Figure 12.21.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearSlopes,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearSlopes,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts and Random Linear Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.21: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts and Random Linear Slopes.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and random linear slopes is in Figure 12.21.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearSlopes,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomLinearSlopes &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomLinearSlopes = round(fantasyPoints_randomLinearSlopes, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearSlopes,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomLinearSlopes,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomLinearSlopes,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_randomLinearSlopes = round(fantasyPoints_randomLinearSlopes, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts and Random Linear Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsRandomLinearSlopes,\n  tooltip = c(\"age\",\"fantasyPoints_randomLinearSlopes\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.22: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts and Random Linear Slopes. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.3.4.3.2 Quadratic Models\n\n12.3.4.3.2.1 Random Intercepts, Random Linear Slopes, Fixed Quadratic Slopes\n\nCodepointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n152901.3 152984.5 -76439.6 152879.3    14249 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.4294 -0.4322 -0.1260  0.3460  5.0212 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   3310.43  57.536        \n                 ageCentered20   38.12   6.174   -0.65\n Residual                      1676.66  40.947        \nNumber of obs: 14260, groups:  player_idFactor, 3524\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             -15.40649    4.62920 4975.01100  -3.328 0.000881 ***\npositionFactorQB         61.96134    4.89486 3453.60328  12.658  &lt; 2e-16 ***\npositionFactorRB         35.79492    4.44957 3483.59848   8.045 1.18e-15 ***\npositionFactorTE          8.23374    4.55105 3461.28492   1.809 0.070507 .  \npositionFactorWR         24.87243    4.35546 3470.61766   5.711 1.22e-08 ***\nageCentered20            11.89481    0.55856 8050.02798  21.295  &lt; 2e-16 ***\nageCentered20Quadratic   -0.96155    0.03584 5725.52193 -26.830  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstFQB pstFRB pstFTE pstFWR agCn20\npostnFctrQB -0.748                                   \npostnFctrRB -0.841  0.776                            \npostnFctrTE -0.813  0.758  0.836                     \npostnFctrWR -0.859  0.792  0.874  0.854              \nageCentrd20 -0.441  0.009  0.039  0.021  0.039       \nagCntrd20Qd  0.354 -0.014 -0.024 -0.014 -0.025 -0.918\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n           R2m       R2c\n[1,] 0.1349671 0.6392304\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               10.2 4.12 3014     2.12     18.3\n QB               72.2 2.67 3163    66.94     77.4\n RB               46.0 1.72 3543    42.63     49.4\n TE               18.4 1.97 3341    14.58     22.3\n WR               35.1 1.46 3608    32.22     37.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.62   36.4 1.17 3076     34.1     38.7\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   55.2   36.4 1.17 3076     34.1     38.7\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n[1] 152901.3\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes is in Figure 12.23.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomLinearFixedQuadraticSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.23: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes is in Figure 12.24.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomLinearFixedQuadraticSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomLinearFixedQuadracticSlopes &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomLinearFixedQuadraticSlopes = round(fantasyPoints_randomLinearFixedQuadraticSlopes, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_randomLinearFixedQuadraticSlopes = round(fantasyPoints_randomLinearFixedQuadraticSlopes, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts, Random Linear Slopes, and\\nFixed Quadratic Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsRandomLinearFixedQuadracticSlopes,\n  tooltip = c(\"age\",\"fantasyPoints_randomLinearFixedQuadraticSlopes\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.24: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.3.4.3.2.2 Random Intercepts, Random Linear Slopes, Random Quadratic Slopes\n\nCodepointsPerSeason_positionAgeRandomLinearRandomQuadraticSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + (1 + ageCentered20 + ageCentered20Quadratic | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\n\n\n12.3.4.3.2.3 Random Intercepts, Random Linear Slopes, Fixed Quadratic Slopes in Interaction With Position\n\nCodepointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic +  \n    (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n152825.4 152969.1 -76393.7 152787.4    14241 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.5816 -0.4323 -0.1221  0.3449  5.0167 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   3218.06  56.728        \n                 ageCentered20   37.45   6.119   -0.63\n Residual                      1658.97  40.730        \nNumber of obs: 14260, groups:  player_idFactor, 3524\n\nFixed effects:\n                                          Estimate Std. Error         df\n(Intercept)                               -0.05775   12.26894 5946.13845\npositionFactorQB                          29.29382   13.65985 5972.26697\npositionFactorRB                          23.85573   13.06073 6066.05480\npositionFactorTE                          -1.08757   13.31800 6140.76958\npositionFactorWR                           4.80544   12.78116 5982.06534\nageCentered20                              5.73818    3.11284 6429.94040\nageCentered20Quadratic                    -0.43001    0.19636 4962.78508\npositionFactorQB:ageCentered20             9.77696    3.35128 6738.88013\npositionFactorRB:ageCentered20             7.44900    3.38596 6803.87955\npositionFactorTE:ageCentered20             2.97685    3.40175 6849.17035\npositionFactorWR:ageCentered20             7.92225    3.26705 6579.28345\npositionFactorQB:ageCentered20Quadratic   -0.65405    0.20602 5207.40766\npositionFactorRB:ageCentered20Quadratic   -0.84712    0.21934 5316.28856\npositionFactorTE:ageCentered20Quadratic   -0.21070    0.21603 5315.56492\npositionFactorWR:ageCentered20Quadratic   -0.67470    0.20762 5043.58381\n                                        t value Pr(&gt;|t|)    \n(Intercept)                              -0.005 0.996244    \npositionFactorQB                          2.145 0.032032 *  \npositionFactorRB                          1.827 0.067820 .  \npositionFactorTE                         -0.082 0.934918    \npositionFactorWR                          0.376 0.706946    \nageCentered20                             1.843 0.065318 .  \nageCentered20Quadratic                   -2.190 0.028577 *  \npositionFactorQB:ageCentered20            2.917 0.003541 ** \npositionFactorRB:ageCentered20            2.200 0.027843 *  \npositionFactorTE:ageCentered20            0.875 0.381554    \npositionFactorWR:ageCentered20            2.425 0.015340 *  \npositionFactorQB:ageCentered20Quadratic  -3.175 0.001508 ** \npositionFactorRB:ageCentered20Quadratic  -3.862 0.000114 ***\npositionFactorTE:ageCentered20Quadratic  -0.975 0.329428    \npositionFactorWR:ageCentered20Quadratic  -3.250 0.001163 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n           R2m       R2c\n[1,] 0.1339151 0.6447082\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               14.2 4.34 3300     5.67     22.7\n QB               72.0 2.69 3132    66.76     77.3\n RB               40.5 1.88 3442    36.84     44.2\n TE               21.1 2.07 3242    17.09     25.2\n WR               34.1 1.54 3277    31.11     37.2\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction, \"ageCentered20\")\n\n ageCentered20 emmean  SE   df lower.CL upper.CL\n          6.62   36.4 1.2 3296       34     38.8\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   55.2   36.4 1.17 3076     34.1     38.7\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n[1] 152825.4\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes in interaction with position is in Figure 12.25.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomLinearFixedQuadraticSlopesInteraction &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes\\nin Interaction With Position\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.25: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes in Interaction With Position.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes in interaction with position is in Figure 12.26.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomLinearFixedQuadraticSlopesInteraction &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomLinearFixedQuadraticSlopesInteraction &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomLinearFixedQuadraticSlopesInteraction = round(fantasyPoints_randomLinearFixedQuadraticSlopesInteraction, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_randomLinearFixedQuadraticSlopesInteraction = round(fantasyPoints_randomLinearFixedQuadraticSlopesInteraction, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts, Random Linear Slopes, and\\nFixed Quadratic Slopes in Interaction With Position\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsRandomLinearFixedQuadraticSlopesInteraction,\n  tooltip = c(\"age\",\"fantasyPoints_randomLinearFixedQuadraticSlopesInteraction\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.26: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes in Interaction With Position. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.3.4.3.2.4 Adding Fixed-Effect Predictor of Experience\n\nCodepointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n152424.2 152575.5 -76192.1 152384.2    14224 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.5470 -0.4274 -0.1141  0.3425  5.0245 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   3093.36  55.618        \n                 ageCentered20   38.17   6.178   -0.66\n Residual                      1660.95  40.755        \nNumber of obs: 14244, groups:  player_idFactor, 3511\n\nFixed effects:\n                                         Estimate Std. Error        df t value\n(Intercept)                               13.8454    12.3258 6111.1264   1.123\npositionFactorQB                          18.7958    13.6770 6124.9213   1.374\npositionFactorRB                          17.7284    13.0840 6207.1460   1.355\npositionFactorTE                          -7.2636    13.3361 6279.6654  -0.545\npositionFactorWR                          -2.0098    12.8054 6128.0578  -0.157\nageCentered20                             -1.3607     3.1564 6802.5402  -0.431\nageCentered20Quadratic                    -0.4892     0.1970 5101.7031  -2.483\nyears_of_experience                        8.6927     0.5679 4914.2930  15.307\npositionFactorQB:ageCentered20            10.1244     3.3584 6907.1815   3.015\npositionFactorRB:ageCentered20             8.0954     3.3939 6959.4658   2.385\npositionFactorTE:ageCentered20             3.6031     3.4097 7008.6376   1.057\npositionFactorWR:ageCentered20             8.5093     3.2752 6742.3155   2.598\npositionFactorQB:ageCentered20Quadratic   -0.6305     0.2066 5346.3427  -3.051\npositionFactorRB:ageCentered20Quadratic   -0.8764     0.2199 5439.8418  -3.985\npositionFactorTE:ageCentered20Quadratic   -0.2356     0.2167 5444.9948  -1.087\npositionFactorWR:ageCentered20Quadratic   -0.6933     0.2083 5174.0295  -3.329\n                                        Pr(&gt;|t|)    \n(Intercept)                             0.261360    \npositionFactorQB                        0.169411    \npositionFactorRB                        0.175477    \npositionFactorTE                        0.586006    \npositionFactorWR                        0.875288    \nageCentered20                           0.666415    \nageCentered20Quadratic                  0.013068 *  \nyears_of_experience                      &lt; 2e-16 ***\npositionFactorQB:ageCentered20          0.002582 ** \npositionFactorRB:ageCentered20          0.017094 *  \npositionFactorTE:ageCentered20          0.290668    \npositionFactorWR:ageCentered20          0.009394 ** \npositionFactorQB:ageCentered20Quadratic 0.002291 ** \npositionFactorRB:ageCentered20Quadratic 6.85e-05 ***\npositionFactorTE:ageCentered20Quadratic 0.276960    \npositionFactorWR:ageCentered20Quadratic 0.000877 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n          R2m       R2c\n[1,] 0.164644 0.6401657\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               19.9 4.22 3293     11.7     28.2\n QB               70.9 2.59 3107     65.8     76.0\n RB               42.8 1.83 3395     39.2     46.4\n TE               23.5 2.00 3211     19.6     27.4\n WR               35.9 1.50 3226     33.0     38.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.62   38.6 1.17 3266     36.3     40.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   55.3   38.6 1.17 3266     36.3     40.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"years_of_experience\")\n\n years_of_experience emmean   SE   df lower.CL upper.CL\n                4.85   38.6 1.17 3266     36.3     40.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n[1] 152424.2\n\n\n\n12.3.4.3.3 Compare Models\nAfter fitting several models, we now must compare their fit to determine which model fits “best” while also considering parsimony. Parsimonious models are more likely to be true and more likely to generalize to other samples, because more complex models are more likely to overfit the data. Thus, more complex models will almost always fit better than simpler models. Thus, we are not just interested in whether a more complex model fits better than the simpler model; we also care about whether the more complex model fits significantly better than the simpler model given its additional complexity. For evaluating and comparing models, we examine the likelihood ratio test, the Akaike Information Criterion (AIC), the corrected AIC (AICc), the Bayesian Information Criterion (BIC), \\(R^2\\), deviance, and log likelihood.\nThe BIC penalizes model complexity more than the AIC does. The BIC is preferable when there is a “true” model, and one intends to identify the true model. The AIC is preferable when we are concerned more about predictive accuracy and when overfitting is less of a concern. Because we are more concerned about predictive accuracy and we do not believe one of these models is the “true” model per se of age-related changes in fantasy performance, we will give more weight to AIC than BIC.\nBelow, we specify various groups of models for the model fit comparisons:\n\nCodelmVsMixedModel &lt;- list(\n  \"nullModel\" = pointsPerSeason_nullModel,\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts\n)\n\nlmAndMixedModels &lt;- list(\n  \"nullModel\" = pointsPerSeason_nullModel,\n  \"linearRegression\" = pointsPerSeason_linearRegression,\n  \"quadraticRegression\" = pointsPerSeason_quadraticRegression,\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts,\n  \"position\" = pointsPerSeason_position,\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\nmixedModels &lt;- list(\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts,\n  \"position\" = pointsPerSeason_position,\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\nmixedModels1 &lt;- list(\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts,\n  \"position\" = pointsPerSeason_position\n)\n\nmixedModels2 &lt;- list(\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\n\n\n12.3.4.3.3.1 Likelihood Ratio Test\n\nCodeanova(\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_position\n)\n\n\n  \n\n\nCodeanova(\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\n\n  \n\n\n\n\n12.3.4.3.3.2 Akaike Information Criterion (AIC)\n\nCodeAIC(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n  )\n\n\n  \n\n\nCodebbmle::AICtab(lmAndMixedModels)\n\n                                      dAIC   df\nrandomLinearFixedQuadraticInteraction    0.0 19\nrandomLinearFixedQuadratic              75.9 11\nrandomLinear                           655.9 10\nfixedLinear                           1017.0 8 \nposition                              1147.4 7 \nrandomIntercepts                      1441.7 3 \nlinearRegression                      6972.7 11\nquadraticRegression                   6977.3 16\nnullModel                             8518.5 2 \n\n\n\n12.3.4.3.3.3 Corrected Akaike Information Criterion (AICc)\nWe compute model fit using the using the AICcmodavg (Mazerolle, 2025), bbmle (Bolker & R Development Core Team, 2023), and MuMIn (Bartoń, 2024) packages\n\nCode#AICcmodavg::aictab(lmVsMixedModel) # throws error (can't mix lm with lmer)\nbbmle::AICctab(lmVsMixedModel)\n\n                 dAICc  df\nrandomIntercepts    0.0 3 \nnullModel        7076.7 2 \n\nCodeAICcmodavg::aictab(mixedModels) # throws error (can't mix lm with lmer)\n\n\n  \n\n\nCode#bbmle::AICctab(mixedModels) # throws error (different numbers of observations)\n\nAICcmodavg::aictab(mixedModels1)\n\n\n  \n\n\nCodebbmle::AICctab(mixedModels1)\n\n                 dAICc df\nposition           0.0 7 \nrandomIntercepts 294.3 3 \n\nCodeAICcmodavg::aictab(mixedModels2)\n\n\n  \n\n\nCodebbmle::AICctab(mixedModels2)\n\n                                      dAICc  df\nrandomLinearFixedQuadraticInteraction    0.0 19\nrandomLinearFixedQuadratic              75.9 11\nrandomLinear                           655.8 10\nfixedLinear                           1017.0 8 \n\nCodeMuMIn::AICc(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\n\n  \n\n\n\n\n12.3.4.3.3.4 Bayesian Information Criterion (BIC)\n\nCodeBIC(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n  )\n\n\n  \n\n\nCode#AICcmodavg::bictab(lmAndMixedModels) # throws error (can't mix lm with lmer)\nbbmle::BICtab(lmVsMixedModel)\n\n                 dBIC   df\nrandomIntercepts    0.0 3 \nnullModel        7069.2 2 \n\nCodeAICcmodavg::bictab(mixedModels)\n\n\n  \n\n\nCode#bbmle::AICctab(mixedModels) # throws error (different numbers of observations)\n\nAICcmodavg::bictab(mixedModels1)\n\n\n  \n\n\nCodebbmle::BICtab(mixedModels1)\n\n                 dBIC  df\nposition           0.0 7 \nrandomIntercepts 264.1 3 \n\nCodeAICcmodavg::bictab(mixedModels2)\n\n\n  \n\n\nCodebbmle::BICtab(mixedModels2)\n\n                                      dBIC  df\nrandomLinearFixedQuadraticInteraction   0.0 19\nrandomLinearFixedQuadratic             15.4 11\nrandomLinear                          587.8 10\nfixedLinear                           933.8 8 \n\n\n\n12.3.4.3.3.5 \\(R^2\\)\n\n\nCodesummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.1038626\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.1042009\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_randomIntercepts)\n\n     R2m       R2c\n[1,]   0 0.4900142\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n            R2m       R2c\n[1,] 0.06722691 0.5148777\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n            R2m       R2c\n[1,] 0.06417996 0.5548786\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n           R2m       R2c\n[1,] 0.1349671 0.6392304\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n           R2m       R2c\n[1,] 0.1339151 0.6447082\n\n\n\n12.3.4.3.3.6 Deviance\n\nCodedeviance(pointsPerSeason_nullModel)\n\n[1] 68441485\n\nCodedeviance(pointsPerSeason_linearRegression)\n\n[1] 61332977\n\nCodedeviance(pointsPerSeason_quadraticRegression)\n\n[1] 61309819\n\nCodedeviance(pointsPerSeason_randomIntercepts)\n\n[1] 154261.1\n\nCodedeviance(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n[1] 153826.4\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n[1] 153461.2\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n[1] 152879.3\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n[1] 152787.4\n\n\n\n12.3.4.3.3.7 Log Likelihood\n\nCodelogLik(pointsPerSeason_nullModel)\n\n'log Lik.' -80669.91 (df=2)\n\nCodelogLik(pointsPerSeason_linearRegression)\n\n'log Lik.' -79888.03 (df=11)\n\nCodelogLik(pointsPerSeason_quadraticRegression)\n\n'log Lik.' -79885.33 (df=16)\n\nCodelogLik(pointsPerSeason_randomIntercepts)\n\n'log Lik.' -77130.54 (df=3)\n\nCodelogLik(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n'log Lik.' -76913.2 (df=8)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n'log Lik.' -76730.62 (df=10)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n'log Lik.' -76439.64 (df=11)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n'log Lik.' -76393.68 (df=19)\n\n\n\n12.3.4.3.4 Generalized Additive Model\n\nCodenum_cores &lt;- detectCores()\nnum_cores\n\n[1] 4\n\nCodepointsPerSeason_gam &lt;- bam( # using bam() instead of gam() for faster estimation due to large size of data\n  fantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\"),\n  data = player_stats_seasonal_offense_subset,\n  nthreads = num_cores\n)\n\npointsPerSeason_gamSummary &lt;- summary(pointsPerSeason_gam)\n\npointsPerSeason_gamSummary\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nfantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + \n    years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\")\n\nParametric coefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -21.690      4.703  -4.612 4.03e-06 ***\npositionFactorQB      56.420      4.958  11.380  &lt; 2e-16 ***\npositionFactorRB      21.039      4.609   4.565 5.04e-06 ***\npositionFactorTE       3.139      4.667   0.673  0.50126    \npositionFactorWR      14.002      4.476   3.128  0.00176 ** \nyears_of_experience    8.889      0.532  16.709  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                       edf   Ref.df       F p-value    \ns(ageCentered20):positionFactorFB    2.869    3.636  18.734  &lt;2e-16 ***\ns(ageCentered20):positionFactorQB    5.805    6.979 103.554  &lt;2e-16 ***\ns(ageCentered20):positionFactorRB    6.040    6.986 111.726  &lt;2e-16 ***\ns(ageCentered20):positionFactorTE    4.886    5.964  49.183  &lt;2e-16 ***\ns(ageCentered20):positionFactorWR    6.458    7.550 106.842  &lt;2e-16 ***\ns(player_idFactor,ageCentered20)  2063.651 3506.000   3.493  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.542   Deviance explained = 60.9%\nfREML =  77134  Scale est. = 2199.3    n = 14244\n\nCodepointsPerSeason_gamSummary$r.sq\n\n[1] 0.5418733\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_gam)\n\n           R2m       R2c\n[1,] 0.5091151 0.5091151\n\nCodeAIC(pointsPerSeason_gam)\n\n[1] 151973.1\n\n\n\n12.3.4.3.4.1 Compare Models\n\nCodelinearMixedModelsVsGAM &lt;- list(\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  \"gam\" = pointsPerSeason_gam\n)\n\nAIC(\n  #pointsPerSeason_nullModel,\n  #pointsPerSeason_linearRegression,\n  #pointsPerSeason_quadraticRegression,\n  #pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  pointsPerSeason_gam\n)\n\n\n  \n\n\nCode#AICcmodavg::aictab(linearMixedModelsVsGAM) # throws error (can't mix bam with lmer)\n#bbmle::AICctab(linearMixedModelsVsGAM) # different numbers of observations\n\nMuMIn::AICc(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  pointsPerSeason_gam\n)\n\n\n  \n\n\nCodeBIC(\n  #pointsPerSeason_nullModel,\n  #pointsPerSeason_linearRegression,\n  #pointsPerSeason_quadraticRegression,\n  #pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  pointsPerSeason_gam\n)\n\n\n  \n\n\nCode#AICcmodavg::bictab(linearMixedModelsVsGAM) # throws error (can't mix bam with lmer)\n#bbmle::AICctab(linearMixedModelsVsGAM) # different numbers of observations\n\nsummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.1038626\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.1042009\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_randomIntercepts)\n\n     R2m       R2c\n[1,]   0 0.4900142\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n            R2m       R2c\n[1,] 0.06722691 0.5148777\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n            R2m       R2c\n[1,] 0.06417996 0.5548786\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n           R2m       R2c\n[1,] 0.1349671 0.6392304\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n           R2m       R2c\n[1,] 0.1339151 0.6447082\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeason_gam)\n\n           R2m       R2c\n[1,] 0.5091151 0.5091151\n\nCodedeviance(pointsPerSeason_nullModel)\n\n[1] 68441485\n\nCodedeviance(pointsPerSeason_linearRegression)\n\n[1] 61332977\n\nCodedeviance(pointsPerSeason_quadraticRegression)\n\n[1] 61309819\n\nCodedeviance(pointsPerSeason_randomIntercepts)\n\n[1] 154261.1\n\nCodedeviance(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n[1] 153826.4\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n[1] 153461.2\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n[1] 152879.3\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n[1] 152787.4\n\nCodedeviance(pointsPerSeason_gam)\n\n[1] 26718092\n\nCodelogLik(pointsPerSeason_nullModel)\n\n'log Lik.' -80669.91 (df=2)\n\nCodelogLik(pointsPerSeason_linearRegression)\n\n'log Lik.' -79888.03 (df=11)\n\nCodelogLik(pointsPerSeason_quadraticRegression)\n\n'log Lik.' -79885.33 (df=16)\n\nCodelogLik(pointsPerSeason_randomIntercepts)\n\n'log Lik.' -77130.54 (df=3)\n\nCodelogLik(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n'log Lik.' -76913.2 (df=8)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n'log Lik.' -76730.62 (df=10)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n'log Lik.' -76439.64 (df=11)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n'log Lik.' -76393.68 (df=19)\n\nCodelogLik(pointsPerSeason_gam)\n\n'log Lik.' -73888.17 (df=2098.373)\n\n\n\n12.3.4.3.5 Players Who Were (at Least Once) at the Top of the End-of-Season Depth Chart\n\nCodepointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 152378.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.5482 -0.4271 -0.1138  0.3423  5.0231 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   3106.75  55.738        \n                 ageCentered20   38.52   6.206   -0.66\n Residual                      1661.44  40.761        \nNumber of obs: 14244, groups:  player_idFactor, 3511\n\nFixed effects:\n                                         Estimate Std. Error        df t value\n(Intercept)                               13.8126    12.3391 6103.9006   1.119\npositionFactorQB                          18.7444    13.6917 6116.6942   1.369\npositionFactorRB                          17.6807    13.0980 6199.0400   1.350\npositionFactorTE                          -7.2658    13.3502 6271.6365  -0.544\npositionFactorWR                          -2.0563    12.8192 6120.5858  -0.160\nageCentered20                             -1.3503     3.1599 6806.7944  -0.427\nageCentered20Quadratic                    -0.4901     0.1973 5111.8386  -2.484\nyears_of_experience                        8.6933     0.5685 4905.1739  15.291\npositionFactorQB:ageCentered20            10.1409     3.3621 6910.8877   3.016\npositionFactorRB:ageCentered20             8.1157     3.3976 6962.8329   2.389\npositionFactorTE:ageCentered20             3.6049     3.4133 7012.9492   1.056\npositionFactorWR:ageCentered20             8.5289     3.2788 6746.8146   2.601\npositionFactorQB:ageCentered20Quadratic   -0.6319     0.2069 5357.1133  -3.054\npositionFactorRB:ageCentered20Quadratic   -0.8785     0.2202 5449.3923  -3.989\npositionFactorTE:ageCentered20Quadratic   -0.2358     0.2169 5456.0663  -1.087\npositionFactorWR:ageCentered20Quadratic   -0.6952     0.2085 5184.5856  -3.334\n                                        Pr(&gt;|t|)    \n(Intercept)                             0.263007    \npositionFactorQB                        0.171038    \npositionFactorRB                        0.177104    \npositionFactorTE                        0.586293    \npositionFactorWR                        0.872568    \nageCentered20                           0.669163    \nageCentered20Quadratic                  0.013018 *  \nyears_of_experience                      &lt; 2e-16 ***\npositionFactorQB:ageCentered20          0.002568 ** \npositionFactorRB:ageCentered20          0.016937 *  \npositionFactorTE:ageCentered20          0.290943    \npositionFactorWR:ageCentered20          0.009309 ** \npositionFactorQB:ageCentered20Quadratic 0.002269 ** \npositionFactorRB:ageCentered20Quadratic 6.72e-05 ***\npositionFactorTE:ageCentered20Quadratic 0.277031    \npositionFactorWR:ageCentered20Quadratic 0.000862 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n           R2m       R2c\n[1,] 0.1646887 0.6409286\n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               19.9 4.22 3274     11.7     28.2\n QB               70.9 2.59 3088     65.8     76.0\n RB               42.8 1.83 3371     39.2     46.4\n TE               23.5 2.00 3191     19.6     27.4\n WR               35.9 1.50 3203     33.0     38.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.62   38.6 1.17 3245     36.3     40.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   55.3   38.6 1.17 3245     36.3     40.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"years_of_experience\")\n\n years_of_experience emmean   SE   df lower.CL upper.CL\n                4.85   38.6 1.17 3245     36.3     40.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n\n  \n\n\nCodeAIC(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n[1] 152418.2\n\nCodepointsPerSeasonDepth_gam &lt;- bam( # using bam() instead of gam() for faster estimation due to large size of data\n  fantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\"),\n  data = player_stats_seasonal_offense_subsetDepth,\n  nthreads = num_cores\n)\n\npointsPerSeasonDepth_gamSummary &lt;- summary(pointsPerSeasonDepth_gam)\n\npointsPerSeasonDepth_gamSummary\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nfantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + \n    years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\")\n\nParametric coefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -17.2863     6.8878  -2.510   0.0121 *  \npositionFactorQB     88.5217     7.0417  12.571  &lt; 2e-16 ***\npositionFactorRB     38.0908     6.5764   5.792 7.19e-09 ***\npositionFactorTE     16.4866     6.7519   2.442   0.0146 *  \npositionFactorWR     29.9955     6.3300   4.739 2.19e-06 ***\nyears_of_experience   6.7191     0.7833   8.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                       edf   Ref.df      F  p-value    \ns(ageCentered20):positionFactorFB    2.441    3.112  9.738 1.94e-06 ***\ns(ageCentered20):positionFactorQB    5.347    6.532 47.785  &lt; 2e-16 ***\ns(ageCentered20):positionFactorRB    5.178    6.125 59.978  &lt; 2e-16 ***\ns(ageCentered20):positionFactorTE    4.139    5.145 23.221  &lt; 2e-16 ***\ns(ageCentered20):positionFactorWR    5.765    6.893 57.223  &lt; 2e-16 ***\ns(player_idFactor,ageCentered20)  1180.358 1624.000  4.996  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.531   Deviance explained = 58.9%\nfREML =  54241  Scale est. = 2650.5    n = 9866\n\nCodepointsPerSeasonDepth_gamSummary$r.sq\n\n[1] 0.5314977\n\nCodeMuMIn::r.squaredGLMM(pointsPerSeasonDepth_gam)\n\n           R2m       R2c\n[1,] 0.5034788 0.5034788\n\nCodeAIC(pointsPerSeasonDepth_gam)\n\n[1] 106900.8\n\n\n\n12.3.5 Bayesian Mixed Models\n\n12.3.5.1 Determine Response Distribution\n\nCodeplayer_stats_seasonal_offense_subset$fantasyPoints_posOnlyNoZeros &lt;- player_stats_seasonal_offense_subset$fantasyPoints_posOnly &lt;- player_stats_seasonal_offense_subset$fantasyPoints\nplayer_stats_seasonal_offense_subset$fantasyPoints_posOnly[player_stats_seasonal_offense_subset$fantasyPoints &lt; 0] &lt;- 0\nplayer_stats_seasonal_offense_subset$fantasyPoints_posOnlyNoZeros[player_stats_seasonal_offense_subset$fantasyPoints &lt;= 0] &lt;- 0.01\n\nfantasyPointsVector &lt;- player_stats_seasonal_offense_subset$fantasyPoints %&gt;% \n    na.omit() %&gt;% \n    as.vector()\n\nfantasyPointsVector_posOnlyNoZeros &lt;- fantasyPointsVector_posOnly &lt;- fantasyPointsVector\nfantasyPointsVector_posOnly[fantasyPointsVector &lt; 0] &lt;- 0\nfantasyPointsVector_posOnlyNoZeros[fantasyPointsVector &lt;= 0] &lt;- 0.01\n\n\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_subset,\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.27: Histogram of Fantasy Points with Overlaid Density Plot and Rug Plot.\n\n\n\n\nWe can use the fitdistrplus package (Delignette-Muller & Dutang, 2015; Delignette-Muller et al., 2025):\n\nCodefitdistrplus::descdist(\n  fantasyPointsVector,\n  boot = 1000)\n\nsummary statistics\n------\nmin:  -12.28   max:  429.1 \nmedian:  29.6 \nmean:  57.85498 \nestimated sd:  69.28116 \nestimated skewness:  1.715222 \nestimated kurtosis:  5.913723 \n\n\n\n\n\n\n\nFigure 12.28: Cullen and Frey Graph.\n\n\n\n\n\nCode# all values\nfit.norm &lt;- fitdistrplus::fitdist(fantasyPointsVector, \"norm\")\n\n# positive-only\nfit.exp &lt;- fitdist(fantasyPointsVector_posOnly, \"exp\")\n\n# positive and no zeros\nfit.gamma &lt;- fitdistrplus::fitdist(fantasyPointsVector_posOnlyNoZeros, \"gamma\")\nfit.lognormal &lt;- fitdistrplus::fitdist(fantasyPointsVector_posOnlyNoZeros, \"lnorm\")\nfit.weibull &lt;- fitdistrplus::fitdist(fantasyPointsVector_posOnlyNoZeros, \"weibull\")\n\n# Model fit\nAIC(fit.norm)\n\n[1] 161343.8\n\nCodeAIC(fit.exp)\n\n[1] 144261.5\n\nCodeAIC(fit.gamma) # fits best\n\n[1] 139803.9\n\nCodeAIC(fit.lognormal)\n\n[1] 146160.6\n\nCodeAIC(fit.weibull)\n\n[1] 140414.7\n\n\n\nCodeplot(fit.norm)\n\n\n\n\n\n\nFigure 12.29: Fit of normal distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.lognormal)\n\n\n\n\n\n\nFigure 12.30: Fit of log normal distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.gamma)\n\n\n\n\n\n\nFigure 12.31: Fit of gamma distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.exp)\n\n\n\n\n\n\nFigure 12.32: Fit of exponential distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.weibull)\n\n\n\n\n\n\nFigure 12.33: Fit of Weibull distribution to fantasy points.\n\n\n\n\n\nCodemodel_normal &lt;- glm(\n  formula = fantasyPoints ~ 1,\n  family = gaussian(),\n  data = player_stats_seasonal_offense_subset,\n  maxit = 100000)\n\nmodel_gamma &lt;- glm(\n  formula = fantasyPoints_posOnlyNoZeros ~ 1,\n  family = Gamma(),\n  data = player_stats_seasonal_offense_subset,\n  maxit = 100000)\n\nsum(resid(model_normal)^2)\n\n[1] 68441485\n\nCodesum(resid(model_gamma)^2)\n\n[1] 32652.7\n\nCodeAIC(model_normal)\n\n[1] 161343.8\n\nCodeAIC(model_gamma)\n\n[1] 140296.7\n\n\nBecause the response distribution (i.e., fantasy points) is positively skewed, we will use a gamma response distribution.\n\n12.3.5.2 Specify Model Formula\nInformation about smooth terms in the mgcv package (Wood, 2017, 2023) is provided at the following link: https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/smooth.terms.html. We estimate the model using the brms package (Bürkner, 2017, 2018, 2024), which provides an interface to the rstan package (Guo et al., 2025).\nSpecify model formula:\n\nCodebayesianMixedModelFormula &lt;- brms::bf(\n  fantasyPoints_posOnly ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(ageCentered20, player_idFactor, bs = \"re\") + (1 | player_idFactor)\n)\n\n\n\n12.3.5.3 Run Model\nNow, we can run the model.\n\n\n\n\n\n\nNote 12.1: Bayesian mixed model\n\n\n\nNote: the following code that runs the model takes a while. If you just want to save time and load the model object instead of running the model, you can load the model object (which has already been fit) using this code:\n\nCodeload(url(\"https://osf.io/download/q6rjf/\"))\n\n\n\n\nWe use the cmdstanr backend (Gabry et al., 2024) and threading using the parellely package (Bengtsson, 2025) for parallel (faster) processing:\n\nCodebayesianMixedModelFit &lt;- brms::brm(\n  formula = bayesianMixedModelFormula,\n  data = player_stats_seasonal_offense_subset,\n  family = hurdle_gamma(),\n  cores = 4,\n  save_pars = save_pars(latent = FALSE, all = FALSE),\n  threads = threading(parallelly::availableCores()),\n  backend = \"cmdstanr\",\n  seed = 52242,\n  silent = 0\n)\n\n\n\n12.3.5.4 Model Summary\n\nCodesummary(bayesianMixedModelFit)\n\n Family: hurdle_gamma \n  Links: mu = log; shape = identity; hu = identity \nFormula: fantasyPoints_posOnly ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(ageCentered20, player_idFactor, bs = \"re\") + (1 | player_idFactor) \n   Data: player_stats_seasonal_offense_subset (Number of observations: 14244) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nsds(sageCentered20positionFactorFB_1)     2.59      1.17     0.93     5.50 1.00\nsds(sageCentered20positionFactorQB_1)     1.16      0.56     0.46     2.59 1.00\nsds(sageCentered20positionFactorRB_1)     1.91      0.80     0.84     3.91 1.00\nsds(sageCentered20positionFactorTE_1)     2.32      0.92     1.11     4.67 1.00\nsds(sageCentered20positionFactorWR_1)     1.85      0.68     0.90     3.52 1.00\nsds(sageCentered20player_idFactor_1)      1.40      0.14     1.12     1.67 1.00\n                                      Bulk_ESS Tail_ESS\nsds(sageCentered20positionFactorFB_1)     2944     3092\nsds(sageCentered20positionFactorQB_1)     3009     3171\nsds(sageCentered20positionFactorRB_1)     2695     2946\nsds(sageCentered20positionFactorTE_1)     2333     2622\nsds(sageCentered20positionFactorWR_1)     2786     2858\nsds(sageCentered20player_idFactor_1)       305      734\n\nMultilevel Hyperparameters:\n~player_idFactor (Number of levels: 3511) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.95      0.02     0.91     0.99 1.00     1128     2110\n\nRegression Coefficients:\n                                  Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                             1.40      0.11     1.17     1.62 1.00\npositionFactorQB                      1.22      0.11     0.99     1.44 1.00\npositionFactorRB                      0.74      0.11     0.52     0.95 1.00\npositionFactorTE                      0.15      0.11    -0.08     0.36 1.00\npositionFactorWR                      0.62      0.11     0.40     0.82 1.00\nyears_of_experience                   0.25      0.01     0.22     0.28 1.00\nsageCentered20:positionFactorFB_1   -10.98      6.13   -23.84     1.14 1.00\nsageCentered20:positionFactorQB_1   -11.14      1.96   -14.81    -6.88 1.00\nsageCentered20:positionFactorRB_1   -12.00      4.38   -20.49    -3.07 1.00\nsageCentered20:positionFactorTE_1   -12.70      4.78   -23.43    -4.25 1.00\nsageCentered20:positionFactorWR_1   -12.26      3.51   -19.64    -5.44 1.00\n                                  Bulk_ESS Tail_ESS\nIntercept                             1779     2593\npositionFactorQB                      1836     2376\npositionFactorRB                      1692     2097\npositionFactorTE                      1636     2032\npositionFactorWR                      1607     2210\nyears_of_experience                   2533     3128\nsageCentered20:positionFactorFB_1     4256     2767\nsageCentered20:positionFactorQB_1     3727     2691\nsageCentered20:positionFactorRB_1     4145     2565\nsageCentered20:positionFactorTE_1     3387     2332\nsageCentered20:positionFactorWR_1     3725     2764\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.37      0.02     1.33     1.40 1.00     2120     3090\nhu        0.04      0.00     0.04     0.04 1.00     8134     2905\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodebrms::prior_summary(bayesianMixedModelFit)\n\n\n  \n\n\n\n\n12.3.5.5 Trace Plots\n\nCodeplot(bayesianMixedModelFit, ask = FALSE)\n\n\n\n\n\n\nFigure 12.34: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n\n\n\n\nFigure 12.35: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n\n\n\n\nFigure 12.36: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n\n\n\n\nFigure 12.37: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n12.3.5.6 Posterior Predictive Check\n\nCodepp_check(bayesianMixedModelFit) + \n  ggplot2::xlim(0, 600)\n\n\n\n\n\n\nFigure 12.38: Posterior Predictive Check from Bayesian Mixed Model.\n\n\n\n\n\n12.3.6 Plots of Model-Implied Fantasy Points by Position and Age\n\nCode# From Quadratic Model: All Players\npointsPerSeason_positionAge_newData$fantasyPoints_quadratic &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\n# From Quadratic Model: Players at Top of End-of-Season Depth Chart\npointsPerSeason_positionAge_newData$fantasyPoints_depthQuadratic &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\n# From GAM Model: All Players\npointsPerSeason_positionAge_newData$fantasyPoints_gam &lt;- predict(\n  object = pointsPerSeason_gam,\n  newdata = pointsPerSeason_positionAge_newData,\n  newdata.guaranteed = TRUE,\n  exclude = \"s(player_idFactor,ageCentered20)\"\n)\n\n# From GAM Model: Players at Top of End-of-Season Depth Chart\npointsPerSeason_positionAge_newData$fantasyPoints_depthGAM &lt;- predict(\n  object = pointsPerSeasonDepth_gam,\n  newdata = pointsPerSeason_positionAge_newData,\n  newdata.guaranteed = TRUE,\n  exclude = \"s(player_idFactor,ageCentered20)\"\n)\n\n\nPlots of model-implied fantasy points by position and age are in Figures 12.39–12.42.\n\n12.3.6.1 Quadratic Model\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_quadratic,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Quadratic Model with All Players\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.39: Plot of Model-Implied Quadratic Trajectories of Fantasy Points by Age.\n\n\n\n\n\n12.3.6.2 Quadratic Model: Top of Depth Chart\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_depthQuadratic,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Quadratic Model with Players Who Were Once at Top of End-of-Season Depth Chart\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.40: Plot of Model-Implied Quadratic Trajectories of Fantasy Points by Age For Players Who Were Once at the Top of the End-of-Season Depth Chart.\n\n\n\n\n\n12.3.6.3 Generalized Additive Model\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Generalized Additive Model with All Players\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.41: Plot of Implied Trajectories of Fantasy Points by Age from a Generalized Additive Model.\n\n\n\n\n\n12.3.6.4 Generalized Additive Model: Top of Depth Chart\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_depthGAM,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Generalized Additive Model with Players Who Were Once at Top of End-of-Season Depth Chart\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.42: Plot of Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, For Players Who Were Once at the Top of the End-of-Season Depth Chart.\n\n\n\n\n\n12.3.7 Plots of Individuals’ Model-Implied Fantasy Points by Age\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_quadratic &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplayer_stats_seasonal_offense_subsetCC$fantasyPoints_gam &lt;- predict(\n  object = pointsPerSeason_gam,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\n\n\nCodezeroAge &lt;- pointsPerSeason_positionAge_newData %&gt;% \n  group_by(positionFactor) %&gt;% \n  filter(fantasyPoints_gam &lt; 0) %&gt;% \n  slice(which.min(age))\n\npeakAge &lt;- pointsPerSeason_positionAge_newData %&gt;% \n  group_by(positionFactor) %&gt;% \n  slice(which.max(fantasyPoints_gam))\n\npeakAge2 &lt;- pointsPerSeason_positionAge_newData %&gt;% \n  filter(age &gt; 22) %&gt;% \n  group_by(positionFactor) %&gt;% \n  slice(which.max(fantasyPoints_gam))\n\nqbPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"QB\")], 0)\nfbPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"FB\")], 0)\nrbPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"RB\")], 0)\nwrPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"WR\")], 0)\nwrPeakAge2 &lt;- round(peakAge2$age[which(peakAge$positionFactor == \"WR\")], 0)\ntePeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"TE\")], 0)\n\nqbZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"QB\")], 0)\nfbZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"FB\")], 0)\nrbZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"RB\")], 0)\nwrZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"WR\")], 0)\nteZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"TE\")], 0)\n\n\n\n12.3.7.1 Quarterbacks\nA plot of Quarterbacks’ model-implied fantasy points by age is in Figure 12.43. The model-implied peak of Quarterbacks’ fantasy points is at age 20. The model-predicted value of zero fantasy points for Quarterbacks is at age 35.\n\nCodeplot_individualFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"QB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"QB\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.43: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Quarterbacks. Overlaid with the Model-Implied Trajectory for Quarterbacks in Blue.\n\n\n\n\n12.3.7.2 Fullbacks\nA plot of Fullbacks’ model-implied fantasy points by age is in Figure 12.44. The model-implied peak of Fullbacks’ fantasy points is at age 27. The model-predicted value of zero fantasy points for Fullbacks is at age 32.\n\nCodeplot_individualFantasyPointsByAgeFB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"FB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"FB\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Fullbacks\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeFB,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.44: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Fullbacks. Overlaid with the Model-Implied Trajectory for Fullbacks in Blue.\n\n\n\n\n12.3.7.3 Running Backs\nA plot of Running Backs’ model-implied fantasy points by age is in Figure 12.45. The model-implied peak of Running Backs’ fantasy points is at age 20. The model-predicted value of zero fantasy points for Running Backs is at age 30.\n\nCodeplot_individualFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"RB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"RB\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.45: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Running Backs. Overlaid with the Model-Implied Trajectory for Running Backs in Blue.\n\n\n\n\n12.3.7.4 Wide Receivers\nA plot of Wide Receivers’ model-implied fantasy points by age is in Figure 12.46. The model-implied peaks of Wide Receivers’ fantasy points are at ages 20 and 26. The model-predicted value of zero fantasy points for Wide Receivers is at age 31.\n\nCodeplot_individualFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"WR\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"WR\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.46: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Wide Receivers. Overlaid with the Model-Implied Trajectory for Wide Receivers in Blue.\n\n\n\n\n12.3.7.5 Tight Ends\nA plot of Tight Ends’ model-implied fantasy points by age is in Figure 12.47. The model-implied peak of Tight Ends’ fantasy points is at age 20. The model-predicted value of zero fantasy points for Tight Ends is at age 32.\n\nCodeplot_individualFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"TE\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"TE\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.47: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Tight Ends. Overlaid with the Model-Implied Trajectory for Wide Tight Ends in Blue.\n\n\n\n\n12.3.8 Summary of Findings\nWe applied mixed models with random intercepts and random slopes to allow our model estimates to account for the fact that different players have different starting points (intercepts) and different changes over time (slopes) in fantasy points. A quadratic, inverted-U-shaped form as a function of age fit better than a linear form as a function of age in predicting players’ fantasy points. A generalized additive model that allowed further nonlinearity fit even better than the quadratic model.\nBased on the bivariate scatterplots between age and fantasy points, we might conclude that players tend to stay stable or even increase in fantasy points with age. However, this conclusion would be wrong. When we account for the longitudinal data (i.e., multiple observations over time for the same player) using mixed models, we observe that fantasy points tend to decrease with age, with the timing and rate of decline differing for each position. In other words, the association between age and fantasy points differs at the person level versus the group level. This is an example of Simpson’s paradox.\nThe discrepancy between the positive or null association between age and players’ fantasy points at the group level, and the negative association at the person level may be due, in part, to the selective attrition of players with age. The players who play the longest will tend to be the highest performing players, whereas the poorest performing players will retire or get dropped from the team at younger ages. Thus, the selective attrition of weaker players may make it seem that there is no association between age and performance (or even a positive one!), when in fact, players’ performance tends to decrease with age after age 26 or so (with the timing differing from position to position), until the player eventually retires or is dropped from the team. Selective attrition is common in longitudinal studies (such as this one) and intervention studies. For instance, attrition may be more likely for individuals from lower socioeconomic status backgrounds because they may face more challenges in continuing in longitudinal studies such as fewer financial resources, greater life stressors, etc. In addition, people who experience side effects or lack of improvement may be more likely to drop out of intervention studies. Examining only those who completed treatment (an example of selection bias) would make the intervention look more effective than it actually was because the people who stay in the study are those who experience the greatest improvement. Thus, it is important to use approaches such as mixed models or other approaches that account for the multiple observations from the same person, that use all available information, and that do not exclude people who do not complete all portions of the study.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsConclusion",
    "href": "mixed-models.html#sec-mixedModelsConclusion",
    "title": "12  Mixed Models",
    "section": "\n12.4 Conclusion",
    "text": "12.4 Conclusion\nMixed models allow accounting for multiple levels or units of analysis and to include both fixed and random effects. Inclusion of random effects allows the association between the predictor variables (the intercept and age) and the outcome variable (fantasy points) to differ for each individual in the grouping level (in this case, each player). This allows for more accurately predicting phenomena. Based on the bivariate scatterplots between age and fantasy points, we might conclude that players tend to stay stable or even increase in fantasy points with age. However, this conclusion would be wrong. When we account for the longitudinal data using mixed models, we observe that players’ fantasy points tend to decrease with age, with the timing and rate of decline differing for each position. In other words, the association between age and fantasy points differs at the person level versus the group level, which is an example of Simpson’s paradox. In sum, mixed models are valuable for examining associations between variables when there are multiple levels of data (i.e., multiple observations within the same unit, known as clustering or nesting). It is important not to confuse the association at one level (e.g., group level) with the association at another level (e.g., person level), which is an example of the ecological fallacy.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsSessionInfo",
    "href": "mixed-models.html#sec-mixedModelsSessionInfo",
    "title": "12  Mixed Models",
    "section": "\n12.5 Session Info",
    "text": "12.5 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] parallel  stats4    stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n [4] dplyr_1.1.4         purrr_1.0.4         readr_2.1.5        \n [7] tidyr_1.3.1         tibble_3.2.1        tidyverse_2.0.0    \n[10] viridis_0.6.5       viridisLite_0.4.2   plotly_4.10.4      \n[13] ggplot2_3.5.1       parallelly_1.42.0   fitdistrplus_1.2-2 \n[16] survival_3.8-3      MASS_7.3-64         cmdstanr_0.8.1.9000\n[19] brms_2.22.0         Rcpp_1.0.14         rstan_2.32.7       \n[22] StanHeaders_2.32.10 bbmle_1.0.25.1      AICcmodavg_2.3-4   \n[25] mgcv_1.9-1          nlme_3.1-167        sjstats_0.19.0     \n[28] emmeans_1.11.0      MuMIn_1.48.4        lmerTest_3.1-3     \n[31] lme4_1.1-36         Matrix_1.7-2       \n\nloaded via a namespace (and not attached):\n [1] Rdpack_2.6.3         gridExtra_2.3        inline_0.3.21       \n [4] rlang_1.1.5          magrittr_2.0.3       matrixStats_1.5.0   \n [7] compiler_4.4.3       loo_2.8.0            reshape2_1.4.4      \n[10] vctrs_0.6.5          pkgconfig_2.0.3      fastmap_1.2.0       \n[13] backports_1.5.0      labeling_0.4.3       rmarkdown_2.29      \n[16] tzdb_0.5.0           ps_1.9.0             nloptr_2.2.1        \n[19] xfun_0.51            jsonlite_1.9.1       VGAM_1.1-13         \n[22] broom_1.0.7          R6_2.6.1             stringi_1.8.4       \n[25] boot_1.3-31          numDeriv_2016.8-1.1  estimability_1.5.1  \n[28] knitr_1.50           bayesplot_1.11.1     timechange_0.3.0    \n[31] splines_4.4.3        tidyselect_1.2.1     abind_1.4-8         \n[34] yaml_2.3.10          codetools_0.2-20     curl_6.2.1          \n[37] processx_3.8.6       pkgbuild_1.4.6       plyr_1.8.9          \n[40] lattice_0.22-6       withr_3.0.2          bridgesampling_1.1-2\n[43] posterior_1.6.1      coda_0.19-4.1        evaluate_1.0.3      \n[46] RcppParallel_5.1.10  pillar_1.10.1        tensorA_0.36.2.1    \n[49] checkmate_2.3.2      reformulas_0.4.0     insight_1.1.0       \n[52] distributional_0.5.0 generics_0.1.3       hms_1.1.3           \n[55] rstantools_2.4.0     munsell_0.5.1        scales_1.3.0        \n[58] minqa_1.2.8          xtable_1.8-4         glue_1.8.0          \n[61] unmarked_1.5.0       lazyeval_0.2.2       tools_4.4.3         \n[64] data.table_1.17.0    mvtnorm_1.3-3        grid_4.4.3          \n[67] crosstalk_1.2.1      rbibutils_2.3        QuickJSR_1.6.0      \n[70] bdsmatrix_1.3-7      datawizard_1.0.1     colorspace_2.1-1    \n[73] performance_0.13.0   cli_3.6.4            Brobdingnag_1.2-9   \n[76] V8_6.0.2             gtable_0.3.6         digest_0.6.37       \n[79] pbkrtest_0.5.3       farver_2.1.2         htmlwidgets_1.6.4   \n[82] htmltools_0.5.8.1    lifecycle_1.0.4      httr_1.4.7          \n\n\n\n\n\n\nBartoń, K. (2024). MuMIn: Multi-model inference. https://CRAN.R-project.org/package=MuMIn\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2025). lme4: Linear mixed-effects models using Eigen and S4. https://github.com/lme4/lme4/\n\n\nBengtsson, H. (2025). parallelly: Enhancing the parallel package. https://parallelly.futureverse.org\n\n\nBolker, B., & R Development Core Team. (2023). bbmle: Tools for general maximum likelihood estimation. https://github.com/bbolker/bbmle\n\n\nBürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2024). brms: Bayesian regression models using Stan. https://github.com/paul-buerkner/brms\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDelignette-Muller, M. L., & Dutang, C. (2015). fitdistrplus: An R package for fitting distributions. Journal of Statistical Software, 64(4), 1–34. https://doi.org/10.18637/jss.v064.i04\n\n\nDelignette-Muller, M.-L., Dutang, C., & Siberchicot, A. (2025). fitdistrplus: Help to fit of a parametric distribution to non-censored or censored data. https://lbbe-software.github.io/fitdistrplus/\n\n\nGabry, J., Češnovar, R., & Johnson, A. (2024). cmdstanr: R interface to CmdStan. https://mc-stan.org/cmdstanr/\n\n\nGuo, J., Gabry, J., Goodrich, B., Johnson, A., Weber, S., & Badr, H. S. (2025). rstan: R interface to Stan. https://mc-stan.org/rstan/\n\n\nKuznetsova, A., Bruun Brockhoff, P., & Haubo Bojesen Christensen, R. (2020). lmerTest: Tests in linear mixed effects models. https://github.com/runehaubo/lmerTestR\n\n\nLenth, R. V. (2025). emmeans: Estimated marginal means, aka least-squares means. https://rvlenth.github.io/emmeans/\n\n\nMazerolle, M. J. (2025). AICcmodavg: Model selection and multimodel inference based on (Q)AIC(c). https://CRAN.R-project.org/package=AICcmodavg\n\n\nWood, S. N. (2017). Generalized additive models: An introduction with R (2nd ed.). CRC press. https://doi.org/10.1201/9781315370279\n\n\nWood, S. N. (2023). mgcv: Mixed GAM computation vehicle with automatic smoothness estimation. https://doi.org/10.32614/CRAN.package.mgcv",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "causal-inference.html",
    "href": "causal-inference.html",
    "title": "13  Causal Inference",
    "section": "",
    "text": "13.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceGettingStarted",
    "href": "causal-inference.html#sec-causalInferenceGettingStarted",
    "title": "13  Causal Inference",
    "section": "",
    "text": "13.1.1 Load Packages\n\nCodelibrary(\"dagitty\")\nlibrary(\"ggdag\")\n\n\n\n13.1.2 Causation\nA causal effect is an effect in which the level of one input variable results in different levels on the outcome variable. For instance, for a given player, the extent to which consumption of sports drink influences their performance reflects the difference between how well the player would perform if they consume sports drink compared to how well they would perform if they do not consume sports drink.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-correlationCausality",
    "href": "causal-inference.html#sec-correlationCausality",
    "title": "13  Causal Inference",
    "section": "\n13.2 Correlation Does Not Imply Causation",
    "text": "13.2 Correlation Does Not Imply Causation\nAs described in Section 8.4.2.1, there are several reasons why two variables, X and Y, might be correlated:\n\n\nX causes Y\n\n\nY causes X\n\n\nX and Y are bidirectional: X causes Y and Y causes X\n\na third variable (i.e., confound), Z, influences both X and Y\n\nthe association between X and Y is spurious",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-conditionsForCausality",
    "href": "causal-inference.html#sec-conditionsForCausality",
    "title": "13  Causal Inference",
    "section": "\n13.3 Criteria for Causality",
    "text": "13.3 Criteria for Causality\nHow do we know whether two processes are causally related? There are three criteria for establishing causality (Shadish et al., 2002):\n\nThe cause (e.g., the independent or predictor variable) temporally precedes the effect (i.e., the dependent or outcome variable).\nThe cause is related to (i.e., associated with) the effect.\nThere are no other alternative explanations for the effect apart from the cause.\n\nThe first criterion for establishing causality involves temporal precedence. In order for a cause to influence an effect, the cause must occur before the effect. For instance, if sports drink consumption influences player performance, the sports drink consumption (that is presumed to influence performance) must occur prior to the performance improvement. Establishing the first criterion eliminates the possibility that the association between the purported cause and effect reflects reverse causation. Reverse causation occurs when the purported effect is actually the cause of the purported cause, rather than the other way around. For instance, if sports drink consumption occurs only once, and it occurs only before and not after performance, then we have ruled out the possibility of reverse causation (i.e., that better performance causes players to consume sports drink).\nThe second criterion involves association. The purported cause must be associated with the purported effect. Nevertheless, as the maxim goes, “correlation does not imply causation.” Just because two variables are correlated does not necessarily mean that they are causally related. However, correlation is useful because causality requires that the two processes be correlated. That is, correlation is a necessary but insufficient condition for causality. For instance, if sports drink consumption influences player performance, sports drink consumption must be associated with performance improvement.\nThe third criterion involves ruling out alternative reasons why the purported cause and effect may be related. As noted in Section 13.2, there are four reasons why X may be correlated with Y. If we meet the first criterion of causality, we have removed the possibility that Y causes X (i.e., reverse causality). To meet the third criterion of causality, we need to remove the possibility that the association reflects a third variable (confound) that influences both the cause and effect, and we need to remove the possibility that the association is spurious—the possibility that the association between the purported cause and effect is due to random chance.\nThere are multiple approaches to meeting the third criterion of causality, such as by use of experiments, longitudinal designs, control variables, within-subject designs, and genetically informed designs, as described in Section 13.4.\nIn general, to meet the third criterion of causality, one must consider the counterfactual—what would have happened “counter to the fact”. A counterfactual is what would have happened in the hypothetical scenario that the cause did not occur [i.e., what would have happened in the absence of the cause; Shadish et al. (2002)]. When engaging in causal inference, it is important to consider what would have happened if the hypothetical cause had actually not occurred. It is like an alternate universe where the cause did not happen. For instance, consider that we conduct an experiment to randomly assign some players to consume a sports drink before a game and other players to drink only water. In this case, our treatment/intervention group is the group of players that consumed a sports drink. The control group is the group players that drank only water. Now, consider that the players in the treatment group outperform the players in the the control group in their football game. In such a study, we observe what did happen when players received a treatment. The counterfactual is knowledge of what would have happened to those same players if they simultaneously had not received treatment (Shadish et al., 2002). The true causal effect, then, is the difference between what did happen and what would have happened. However, we cannot observe a counterfactual. This is the fundamental problem of causal inference—we only observe one potential world/outcome/universe. That is, we do not know for sure what would have happened to the players who received treatment if those same players had actually not received treatment. We have a control group, but the control group does not have the same players as the intervention group, and it is impossible for a person to simultaneously receive and not receive treatment.\nSo, our goal in working toward causal inference as scientists is to create reasonable approximations to this impossible counterfactual (Shadish et al., 2002). For instance, if using a between-subject design, we want the two groups to be equivalent in every possible way except whether or not they receive the treatment, so we might stratify each group to be equivalent in terms of age, weight, position, experience, skill, etc. Or, we might test the same people using an A-B-A-B within-subject design. In an A-B-A-B within-subject design, players receive no treatment at baseline (timepoint 1: game 1), receive the treatment at timepoint 2 (game 2), receive no treatment at timepoint 3 (game 3), and receive the treatment at timepoint 4 (game 4). Neither of these approximations is a true counterfactual. In the between-subject design, the players differ between the two groups, so we cannot know how the individuals who received the treatment would have performed if they had actually not received the treatment. In the A-B-A-B within-subject design, the players are the same, but they timepoints that they receive or do not receive the treatment differ, and there can be carryover effects from one condition to the next. For instance, consuming sports drinks before game 2 might also help them be better hydrated in general, including, for subsequent games. Thus, we cannot know how a player would have performed in game 1 with treatment or in game 2 without treatment, etc. Nevertheless, it is important to be aware of the counterfactual and to engage in counterfactual reasoning to consider what would have happened if the supposed cause had not occurred. Considering the counterfactual is important for designing closer approximations to the counterfactual in studies for stronger research designs and stronger causal inference.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-approachesCausalInference",
    "href": "causal-inference.html#sec-approachesCausalInference",
    "title": "13  Causal Inference",
    "section": "\n13.4 Approaches for Causal Inference",
    "text": "13.4 Approaches for Causal Inference\n\n13.4.1 Experimental Designs\nAs described in Section 8.4.1, experimental designs are designs in which participants are randomly assigned to one or more levels of the independent variable to observe its effects on the dependent variable. Experimental designs provide the strongest tests of causality because they can rule out reverse causation and third variables. For instance, by manipulating sports drink consumption before the player performs, they can eliminate the possibility that reverse causation explains the effect of the independent variable on the dependent variable. Second, through randomly assigning players to consume or not consume sports drink, this holds everything else constant (so long as the groups are evenly distributed according to other factors, such as their age, weight, etc.) and thus removes the possibility that third variable confounds explain the effect of the independent variable on the dependent variable.\n\n13.4.2 Quasi-Experimental Designs\nAlthough experimental designs provide the strongest tests of causality, manytimes they are impossible, unethical, or impractical to conduct. For instance, it would likely not be practical to randomly assign National Football League (NFL) players to either consume or not consume sports drink before their games. Players have their pregame rituals and routines and many would likely not agree to participate in such a study. Thus, we often rely on quasi-experimental designs such as natural experiments and observational/correlational designs.\nWe cannot directly test or establish causality from a non-experimental research design. Nevertheless, we can leverage various design features that, in combination with other studies using different research methods, collectively strengthen our ability to make causal inferences. For instance, there are are no experiments in humans showing that smoking causes cancer—randomly assigning people to smoke or not smoke would not be ethical. The causal inference that smoking causes cancer was derived from a combination of experimental studies in rodents and observational studies in humans.\n\n13.4.2.1 Longitudinal Designs\nResearch designs can be compared in terms of their internal validity—the extent to which we can be confident about causal inferences. A cross-sectional association is depicted in Figure 13.1:\n\n\n\n\n\nFigure 13.1: Cross-Sectional Association. T1 = Timepoint 1. From Petersen (2024) and Petersen (2025).\n\n\nFor instance, we might observe that sports drink consumptions is concurrently associated with better player performance. Among observational/correlational research designs, cross-sectional designs tend to have the weakest internal validity. For the reasons described in Section 13.2, if we observe a cross-sectional association between X (e.g., sports drink consumption) and Y (e.g., player performance), we have little confidence that X causes Y. As a result, longitudinal designs can be valuable for more closely approximating causality if an experimental designs is not possible. Consider a lagged association that might be observed in a longitudinal design, as in Figure 13.2, which is a slightly better approach than relying on cross-sectional associations:\n\n\n\n\n\nFigure 13.2: Lagged Association. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024) and Petersen (2025).\n\n\nFor instance, we might observe that sports drink performance before the game is associated with better player performance during the game. A lagged association has somewhat better internal validity than a cross-sectional association because we have greater evidence of temporal precedence—that the influence of the predictor precedes the outcome because the predictor was assessed before the outcome and it shows a predictive association. However, part of the association between the predictor with later levels of the outcome could be due to prior levels of the outcome that are stable across time. That is, it could be that better player performance leads players to consume more sports drink and that player performance is relatively stable across time. In such a case, it may be observed that sports drink consumption predicts later player performance even though player performance influences sports drink consumption, rather than the other way around Thus, consider an even stronger alternative—a lagged association that controls for prior levels of the outcome, as in Figure 13.3:\n\n\n\n\n\nFigure 13.3: Lagged Association, Controlling for Prior Levels of the Outcome. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024) and Petersen (2025).\n\n\nFor instance, we might observe that sports drink performance before the game is associated with better player performance during the game, while controlling for prior player performance. A lagged association controlling for prior levels of the outcome has better internal validity than a lagged association that does not control for prior levels of the outcome. A lagged association that controls for prior levels further reduces the likelihood that the association owes to the reverse direction of effect, because earlier levels of the outcome are controlled. However, consider an even stronger alternative—lagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect, as depicted in Figure 13.4:\n\n\n\n\n\nFigure 13.4: Lagged Association, Controlling for Prior Levels of the Outcome, Simultaneously Testing Both Directions Of Effect. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024) and Petersen (2025).\n\n\nLagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect provide the strongest internal validity among observational/correlational designs. Such a design can help better clarify which among the variables is the chicken and the egg—which variable is more likely to be the cause and which is more likely to be the effect. If there are bidirectional effects, such a design can also help clarify the magnitude of each direction of effect. For instance, we can simultaneously evaluate the extent to which sports drink predicts later player performance (while controlling for prior performance) and the reverse—player performance predicting later sports drink consumption (while contorlling for prior sports drink consumption).\n\n13.4.2.2 Within-Subject Analyses\nAnother design feature of longitudinal designs that can lead to greater internal validity is the use of within-subject analyses. Between-subject analyses, might examine, for instanc, whether players who consume more sports drink perform better on average compared to players who consume less sports drink. However, there are other between-person differences that could explain any observed between-subject associations between sports drink consumption and players performance. Another approach could be to apply within-subject analyses. For instance, you could examining whether, within the same individual, if a player consumes a sports drink, do they perform better compared to games in which they did not consume a sports drink. When we control for prior levels of the outcome in the prediction, we are evaluating whether the predictor is associated with witin-person change in the outcome. Predicting within-person change provides stronger evidence consistent with causality because it uses the individual as their own control and controls for many time-invariant confounds (i.e., confounds that do not change across time). However, predicting within-person change does not, by itself, control for time-varying confounds. So, it can also be useful to control for time-varying confounds, such as by use of control variables.\n\n13.4.2.3 Control Variables\nOne of the plausible alternatives to the inference that X causes Y is that there are third variable confounds that influence both X and Y, thus explaining why X and Y are associated, as depicted in Figures 8.3 and 13.10. Thus, another approach that can help increase internal validity is to include plausible confounds as control variables. For instance, if a third variable such as education level might be a confound that influences both sports drink consumption and player performance, you could control for a player’s education level.\nThere are several ways to control for a variable:\n\nrandomization (when possible)\nrestriction\nmatching\ncovariate\nstratification\n\nOne way to control for a variable—and what is typically considered the most rigorous control—when possible, is to randomize (which would make it an experiment). For example, you could randomly assign players to consume sports drink during a game versus water. By randomly assigning players to each group, it is expected that, if the groups are large enough, the differences between the groups on all variables besides the independent and dependent variables will be approximately equal across both groups; thus, such an approach controls for both observed and unobserved confounds, and any difference in the dependent variable is thought to be attributable to the manipulation of the independent variable. However, sometimes a variable is not able to be manipulated for practical or ethical reasons. For instance, if you wanted to control for education level when examining the association between sports drink consumption and performance, you cannot randomly assign players to have lower educational levels (though you could possibly provide an intervention to increase the education levels of a subset of players). Thus, observational studies are also necessary.\nHowever, in observational studies, the groups may differ on the confounding factors; people may choose the level of the they receive. Thus, we need to attempt to control for the confounds; however, in an observational study, we can only potentially adjust for the observed confounds, unlike in an experiment in which we can control for both observed and unobserved confounds.\nA second way to control for a variable is restriction—e.g., you can select your sample so that it only includes portions/subgroups of a variable. For example, you could recruit—as participants—only those players who are college graduates.\nMatching means that you match people based on the control variable. If some people consume sports drink whereas others do not, matching would mean finding—for every player included in the study who consumes sports drink—an equivalent player in terms of education who does not consume sports drink.\nAnother way to control for a variable is to include the variable as a covariate in the model. Inclusion of a covariate attempts to control for the variable by examining the association between the predictor variable and the outcome variable while holding the covariate variables constant. For example, if you want to control for a player’s education level in examining the association between sports drink consumption and performance, you could statistically control for player education by including the player’s education level as a covariate in predicting performance. Such a model would examine whether, when accounting for education level, there is an association between sports drink consumption and player performance.\nAnother way to control for a variable is to stratify based on the variable—e.g., you can examine the association between the variables of interest within subgroups of the control variable. For instance, taking the example of controlling for a player’s education in examining the association between sports drink consumption and performance, you could examine the association between sports drink consumption and performance separately for various education levels: a) high school or lower, b) some college, c) college degree, d) graduate degree.\nIn each of these examples, you are controlling for player education for purposes of examining the effect of sports drink consumption on performance. Failure to control for important third variables can lead to erroneous conclusions, as evidenced by the association depicted in Figure 13.5. In the example, if we did not control for gender, we would infer that there is a positive association between dosage and recovery probability. However, when we examine each men and women separately, we learn that the association between dosage and recovery probability is actually negative within each gender group. Thus, in this case, failure to control for gender would lead to false inferences about the association between dosage and recovery probability.\n\n\n\n\n\nFigure 13.5: Example Where Failing to Control for a Variable (In This Case, Gender) Would Lead to False Inferences. In this example, the association between dosage and recovery probability is positive at the population level, but the association is negative among men and women separately. (Figure reprinted from Kievit et al. (2013), Figure 1, p. 2. Kievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513)\n\n\nHowever, it can be problematic to control for variables indiscriminantly (Spector & Brannick, 2010; Wysocki et al., 2022). The use of causal diagrams can inform which variables are important to be included as control variables, and—just as important—which variables not to include as control variables, as described in Section 13.5. Another approach that is sometimes used to control for variables—either by using matching, or as a sample weight, or with stratification—is the use of propensity scores.\n\n13.4.2.4 Genetically Informed Designs\nAnother approach to control for variables is to use genetically informed designs. Genetically informed designs allow controlling for potential genetic effects in order to more closely approximate the contributions of various environmental effects. Genetically informed designs exploit differing degrees of genetic relatedness among participants to capture the extent to which genetic factors may contribute to an outcome. The average percent of DNA shared between people of varying relationships is provided by International Society of Genetic Genealogy (2022) in Table 13.1 (archived at https://perma.cc/MK3D-DST8):\n\n\nTable 13.1: Average Percent of Autosomal DNA Shared by Pairs of Relatives by Relationship Type.\n\n\n\n\n\n\n\nRelationship\nAverage Percent of Autosomal DNA Shared by Pairs of Relatives\n\n\n\nMonozygotic (“identical”) twins\n100%\n\n\nDizygotic (“fraternal”) twins\n50%\n\n\nParent/child\n50%\n\n\nFull siblings\n50%\n\n\nGrandparent/grandchild\n25%\n\n\nAunt-or-uncle/niece-or-nephew\n25%\n\n\nHalf-siblings\n25%\n\n\nFirst cousin\n12.5%\n\n\nGreat-grandparent/great-grandchild\n12.5%\n\n\n\n\n\n\nFor instance, researchers may compare monozygotic twins versus dizygotic twins in some outcome—a so-called “twin study”. It is assumed that the trait/outcome is attributable to genetic factors to the extent that the monozygotic twins (who share 100% of their DNA) are more similar in the trait or outcome compared to the dizygotic twins (who share on average 50% of their DNA). Alternatively, researchers could compare full siblings versus half-siblings, or they could compare full siblings versus first cousins.\nGenetically informed designs are not as relevant for fantasy football analytics, but they are useful to present as one of various design features that researchers can draw upon to strengthen their ability to make causal inferences.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalDiagrams",
    "href": "causal-inference.html#sec-causalDiagrams",
    "title": "13  Causal Inference",
    "section": "\n13.5 Causal Diagrams",
    "text": "13.5 Causal Diagrams\n\n13.5.1 Overview\nA key tool when describing a research question or hypothesis is to create a conceptual depiction of the hypothesized causal processes. A causal diagram depicts the hypothesized causal processes that link two or more variables. A common form of causal diagrams is the directed acyclic graph (DAG). DAGs provide a helpful tool to communicate about causal questions and help identify how to avoid bias (i.e., overestimation) in associations between variables due to confounding (i.e., common causes) (Digitale et al., 2022). For instance, from a DAG, it is possible to determine what variables it is important to control for (and how)—and just as importantly, what not to control for—in order to get unbiased estimates of the effect of one variable on another variable or of the association between two variables of interest. To create DAGs, you can use the R package dagitty (Textor et al., 2016; R-daggity?) or the associated browser-based extension, DAGitty: https://dagitty.net (archived at https://perma.cc/U9BY-VZE2). Examples of various causal diagrams that could explain why X is associated with Y are in Figures 13.6, 13.8 and 13.10.\n\nCodeXCausesY &lt;- dagitty::dagitty(\"dag{\n  X -&gt; Y\n}\")\n\nplot(dagitty::graphLayout(XCausesY))\n\ndagitty::impliedConditionalIndependencies(XCausesY)\n\n\n\n\n\n\nFigure 13.6: Causal Diagram (Directed Acyclic Graph) Depicting X Causing Y.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax) using the ggdag package (Barrett, 2024):\n\nCodeXCausesY_alt &lt;- ggdag::dagify(\n  Y ~ X\n)\n\n#plot(XCausesY_alt) # this creates the same plot as above\nggdag::ggdag(XCausesY_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.7: Causal Diagram (Directed Acyclic Graph) Depicting X Causing Y.\n\n\n\n\n\nCodeYCausesX &lt;- dagitty::dagitty(\"dag{\n  Y -&gt; X\n}\")\n\nplot(dagitty::graphLayout(YCausesX))\n\ndagitty::impliedConditionalIndependencies(YCausesX)\n\n\n\n\n\n\nFigure 13.8: Causal Diagram (Directed Acyclic Graph) Depicting Reverse Causation: Y Causing X.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodeYCausesX_alt &lt;- ggdag::dagify(\n  X ~ Y\n)\n\n#plot(YCausesX_alt) # this creates the same plot as above\nggdag::ggdag(YCausesX_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.9: Causal Diagram (Directed Acyclic Graph) Depicting Reverse Causation: Y Causing X.\n\n\n\n\n\nCodeZCausesXandY &lt;- dagitty::dagitty(\"dag{\n  Z -&gt; Y\n  Z -&gt; X\n  X &lt;-&gt; Y\n}\")\n\nplot(dagitty::graphLayout(ZCausesXandY))\n\n\n\n\n\n\nFigure 13.10: Causal Diagram (Directed Acyclic Graph) Depicting a Third Variable Confound, Z, Causing X and Y, Thus Explaining Why X and Y are associated.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodeZCausesXandY_alt &lt;- ggdag::dagify(\n  X ~ Z,\n  Y ~ Z,\n  X ~~ Y\n)\n\n#plot(ZCausesXandY_alt) # this creates the same plot as above\nggdag::ggdag(ZCausesXandY_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.11: Causal Diagram (Directed Acyclic Graph) Depicting a Third Variable Confound, Z, Causing X and Y, Thus Explaining Why X and Y are associated.\n\n\n\n\nConsider another example in Figure 13.12:\n\nCodemediationDag &lt;- dagitty::dagitty(\"dag{\n  X -&gt; M1\n  X -&gt; M2\n  M1 -&gt; Y\n  M2 -&gt; Y\n  M1 &lt;-&gt; M2\n}\")\n\nplot(dagitty::graphLayout(mediationDag))\n\n\n\n\n\n\nFigure 13.12: Causal Diagram (Directed Acyclic Graph).\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(mediationDag)\n\nX _||_ Y | M1, M2\n\nCodedagitty::adjustmentSets(\n  mediationDag,\n  exposure = \"M1\",\n  outcome = \"Y\",\n  effect = \"total\")\n\n{ M2 }\n\n\nIn this example, X influences Y via M1 and M2 (i.e., multiple mediators), and M1 is also associated with M2. The dagitty::impliedConditionalIndependencies() function identifies variables in the causal diagram that are conditionally independent (i.e., uncorrelated) after controlling for other variables in the model. For this causal diagram, X is conditionally independent with Y because X is independent with Y when controlling for M1 and M2.\nThe dagitty::adjustmentSets() function identifies variables that would be necessary to control for in order to identify an unbiased estimate of the association (whether the total effect, i.e., effect = \"total\"; or the direct effect, i.e., effect = \"direct\") between two variables (exposure and outcome). In this case, to identify the unbiased association between M1 and Y, it is important to control for M2.\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodemediationDag_alt &lt;- ggdag::dagify(\n  M1 ~ X,\n  M2 ~ X,\n  Y ~ M1,\n  Y ~ M2,\n  M1 ~~ M2\n)\n\n#plot(mediationDag_alt) # this creates the same plot as above\nggdag::ggdag(mediationDag_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.13: Causal Diagram (Directed Acyclic Graph).\n\n\n\n\n\n13.5.2 Confounding\nConfounding occurs when two variables—that are both caused by another variable(s)—have a spurious or noncausal association (D’Onofrio et al., 2020). That is, two variables share a common cause, and the common cause leads the variables to be associated even though they are not causally related. The common cause—i.e., the variable that influences the two variables—is known as a confound (or confounder). An example of confounding is depicted in Figure 13.14:\n\nCodeconfounding &lt;- ggdag::confounder_triangle(\n  x = \"Player Endurance\",\n  y = \"Field Goals Made\",\n  z = \"Stadium Altitude\")\n\nconfounding %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.14: Causal Diagram (Directed Acyclic Graph) Example of Confounding.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(confounding)\n\nx _||_ y | z\n\n\nThe output indicates that player endurance (X) and field goals made (Y) are conditionally independent when accounting for stadium altitude (Z). Conditional independence refers to two variables being unassociated when controlling for other variables.\n\nCodedagitty::adjustmentSets(\n  confounding,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n{ z }\n\n\nThe output indicates that, to obtain an unbiased estimate of the causal association between two variables, it is necessary to control for any confounding (Lederer et al., 2019). That is, to obtain an unbiased estimate of the causal association between player endurance (X) and field goals made (Y), it is necessary to control for stadium altitude (Z).\n\n13.5.3 Mediation\nMediation can be divided into two types: full and partial. In full mediation, the mediator(s) fully account for the effect of the predictor variable on the outcome variable. In partial mediation, the mediator(s) partially but do not fully account for the effect of the predictor variable on the outcome variable.\n\n13.5.3.1 Full Mediation\nAn example of full mediation is depicted in Figure 13.15:\n\nCodefull_mediation &lt;- ggdag::mediation_triangle(\n  x = \"Coaching Quality\",\n  y = \"Fantasy Points\",\n  m = \"Player Preparation\")\n\nfull_mediation %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.15: Causal Diagram (Directed Acyclic Graph) Example of Full Mediation.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(full_mediation)\n\nx _||_ y | m\n\n\nIn full mediation, X and Y are conditionally independent when accounting for the mediator (Z). In this case, coaching quality (X) and fantasy points (Y) are conditionally independent when accounting for player preparation (M). In other words, in this example, player preparation is the mechanism that fully (i.e., 100%) accounts for the effect of coaching quality on players’ fantasy points.\n\nCodedagitty::adjustmentSets(\n  full_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"direct\")\n\n{ m }\n\n\nThe output indicates that, to obtain an unbiased estimate of the direct causal association between coaching quality (X) and fantasy points (Y) (i.e., the effect that is not mediated through intermediate processes), it is necessary to control for player preparation (M).\n\nCodedagitty::adjustmentSets(\n  full_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nHowever, to obtain an unbiased estimate of the total causal association between coaching quality (X) and fantasy points (Y) (i.e., including the portion of the effect that is mediated through intermediate processes), it is important not to control for player preparation (M). When the goal is to understand the (total) causal effect of coaching quality (X) and fantasy points (Y), controlling for the mediator (player preparation; M) would be inappropriate because doing so would remove the causal effect, thus artificially reducing the estimate of the association between coaching quality (X) and fantasy points (Y) (Lederer et al., 2019).\n\n13.5.3.2 Partial Mediation\nAn example of partial mediation is depicted in Figure 13.16:\n\nCodepartial_mediation &lt;- ggdag::mediation_triangle(\n  x = \"Coaching Quality\",\n  y = \"Fantasy Points\",\n  m = \"Player Preparation\",\n  x_y_associated = TRUE)\n\npartial_mediation %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.16: Causal Diagram (Directed Acyclic Graph) Example of Partial Mediation.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(partial_mediation)\n\n\nIn partial mediation, X and Y are not conditionally independent when accounting for the mediator (Z). In this case, coaching quality (X) and fantasy points (Y) are still associated when accounting for player preparation (M). In other words, in this example, player preparation is a mechanism that partially but does not fully account for the effect of coaching quality on players’ fantasy points. That is, there are likely other mechanisms, in addition to player preparation, that collectively account for the effect of coaching quality on fantasy points. For instance, coaching quality could also influence player fantasy points through better play-calling.\n\nCodedagitty::adjustmentSets(\n  partial_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"direct\")\n\n{ m }\n\n\nAs with full mediation, the output indicates that, to obtain an unbiased estimate of the direct causal association between coaching quality (X) and fantasy points (Y) (i.e., the effect that is not mediated through intermediate processes), it is necessary to control for player preparation (M).\n\nCodedagitty::adjustmentSets(\n  partial_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nHowever, as with full mediation, to obtain an unbiased estimate of the total causal association between coaching quality (X) and fantasy points (Y) (i.e., including the portion of the effect that is mediated through intermediate processes), it is important not to control for player preparation (M). When the goal is to understand the (total) causal effect of coaching quality (X) and fantasy points (Y), controlling for a mediator (player preparation; M) would be inappropriate because doing so would remove the causal effect, thus artificially reducing the estimate of the association between coaching quality (X) and fantasy points (Y) (Lederer et al., 2019).\n\n13.5.4 Ancestors and Descendants\nIn a causal model, an ancestor is a variable that influences another variable, either directly or indirectly via another variable (Rohrer, 2018). A descendant is a variable that is influenced by another variable (Rohrer, 2018). In general, one should not control for descendants of the outcome variable, because doing so could eliminate the apparent effect of a legitimate cause on the outcome variable (Digitale et al., 2022). Moreover, as described above, when trying to understand the total causal effect of a predictor variable on an outcome variable, one should not control for descendants of the predictor variable that are also antecedents of the outcome variable (i.e., mediators of the effect of the predictor variable on the outcome variable) (Digitale et al., 2022).\nConsider the example in Figure 13.17:\n\nCodedescendentDag &lt;- dagitty::dagitty(\"dag{\n  X -&gt; M\n  M -&gt; Y\n  X -&gt; Y\n  Y -&gt; Z\n}\")\n\n#plot(dagitty::graphLayout(descendentDag))\nggdag::ggdag(descendentDag) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.17: Causal Diagram (Directed Acyclic Graph) Example of Ancestor (Mediation) and Descendant of Y.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(descendentDag)\n\nM _||_ Z | Y\nX _||_ Z | Y\n\n\nIn this example, X and M are conditionally independent with Z when accounting for the mediator (Y).\n\nCodedagitty::adjustmentSets(\n  descendentDag,\n  exposure = \"X\",\n  outcome = \"Y\",\n  effect = \"direct\")\n\n{ M }\n\nCodedagitty::adjustmentSets(\n  descendentDag,\n  exposure = \"X\",\n  outcome = \"Y\",\n  effect = \"total\")\n\n {}\n\n\nAs indicated above, one should not control for the descendant of the outcome variable; thus, one should not control for Z when examining the association between X or M and Y.\n\n13.5.5 Collider Bias\nCollision occurs when two variables influence a third variable, the collider (D’Onofrio et al., 2020). That is, a collider is a variable that is caused by two other variables (i.e., a common effect). An example collision is depicted in Figures 13.18 and 13.19:\n\nCodecolliderBias1 &lt;- ggdag::collider_triangle(\n  x = \"Diet\",\n  y = \"Coaching Strategy\",\n  m = \"Injury Status\")\n\ncolliderBias1 %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.18: Causal Diagram (Directed Acyclic Graph) Example of a Collision with a Collider (Injury Status).\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(colliderBias1)\n\nx _||_ y\n\n\nIn this example collision, diet (X) and coaching strategy (Y) are independent.\n\nCodedagitty::adjustmentSets(\n  colliderBias1,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nAs the output indicates, we should not control for the collider when examining the association between the two causes of the collider. That is, we should not control for injury status (M) when examining the association between diet (X) and coaching strategy. Controlling for the collider leads to confounding and can artificially induce an association between the two causes of the collider despite no causal association between them (Lederer et al., 2019). Obtaining a distorted or artificial association between two variables due to inappropriately controlling for a collider is known as collider bias.\nConsider another example:\n\nCodecolliderBias2 &lt;- ggdag::collider_triangle(\n  x = \"Coaching Quality\",\n  y = \"Player Preparation\",\n  m = \"Fantasy Points\",\n  x_y_associated = TRUE)\n\ncolliderBias2 %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.19: Causal Diagram (Directed Acyclic Graph) Example of Collider Bias.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(colliderBias2)\n\n\nIn this example of collider bias, there are no conditional independencies.\n\nCodedagitty::adjustmentSets(\n  colliderBias2,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nAgain, it would be important not to control for the collider, fantasy points (M), when examining the association between coaching quality (X) and player preparation (Y). In this case, controlling for the collider would remove some of the causal effect of coaching quality on player preparation and could lead to an artificially smaller estimate of the causal effect between coaching quality and player preparation.\n\n13.5.5.1 M-Bias\nCollider bias may also occur when neither variable of interest is a direct cause of the collider (Lederer et al., 2019). M-bias is a form of collider bias that occurs when two variables that are not causally related, A and B, both influence a collider, M, and each (A and B) also influences a separate variable—e.g., A influences X and B influences Y. M-bias is so-named from the M-shape of the DAG. An example of M-bias is depicted in Figure 13.20:\n\nCodemBias &lt;- ggdag::m_bias(\n  x = \"Number of Media Articles About the Team\",\n  y = \"Fantasy Points\",\n  a = \"Team Record\",\n  b = \"Coaching Quality\",\n  m = \"Fan Attendance at Game\")\n\nmBias %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.20: Causal Diagram (Directed Acyclic Graph) Example of M-Bias.\n\n\n\n\nIn this example, fan attendance is the collider that is influenced separately by the team record and the coaching quality. This is a fictitious example for purposes of demonstration; in reality, coaching quality influences the team’s record.\n\nCodedagitty::impliedConditionalIndependencies(mBias)\n\na _||_ b\na _||_ y\nb _||_ x\nm _||_ x | a\nm _||_ y | b\nx _||_ y\n\n\nAs the output indicates, there are several conditional independencies.\n\nCodedagitty::adjustmentSets(\n  mBias,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nIt is important not to control for the collider (fan attendance). If you control for the collider, you can induce an artificial association between team record and coaching quality. Moreover, because doing so induces an artificial association between team record and coaching quality, it can also induce an artificial association between the effects of team record and coaching quality: number of media articles about the team and fantasy points, respectively. That is, controlling for the collider can lead to an artificial association between X and Y that does not reflect a causal process.\n\n13.5.5.2 Butterfly Bias\nButterfly bias occurs when both confounding and M-bias are present. Butterfly bias (aka bow-tie bias) is so-named from the butterfly shape of the DAG. In butterfly bias, the following criteria are met:\n\nTwo variables (A and B) influence a collider (M).\nThe collider influences two variables, X and Y.\n\nA also influences X.\n\nB also influences Y.\n\nA and B are not causally related.\n\nX and Y are not causally related.\n\nOr, more succinctly:\n\n\nA influences M and X.\n\nB influences M and Y.\n\nM influences X and Y.\n\nIn butterfly bias, the collider (M) is also a confound. That is, a variable is both influenced by two variables and influences two variables. An example of butterfly bias is depicted in Figure 13.21:\n\nCodebutterflyBias &lt;- ggdag::butterfly_bias(\n  x = \"Off-field Behavior\",\n  y = \"Fantasy Points\",\n  a = \"Diet\",\n  b = \"Coaching Quality\",\n  m = \"Mental Health\")\n\nbutterflyBias %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.21: Causal Diagram (Directed Acyclic Graph) Example of Butterfly Bias.\n\n\n\n\nIn this case, players’ mental health is a collider of their diet and the quality of the coaching they receive. In addition, players’ mental health is a confound of their off-field behavior and fantasy points.\n\nCodedagitty::impliedConditionalIndependencies(butterflyBias)\n\na _||_ b\na _||_ y | b, m\nb _||_ x | a, m\nx _||_ y | b, m\nx _||_ y | a, m\n\n\nAs the output indicates, there are several conditional independencies.\n\nCodedagitty::adjustmentSets(\n  butterflyBias,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n{ b, m }\n{ a, m }\n\n\nWhen dealing with a collider that is also a confound, controlling for either set, B and M or A and M, will provide an unbiased estimate of the association between X and Y. In this case, controlling for either a) coaching quality and mental health or b) diet and mental health—but not both sets—will yield an unbiased estimate of the association between off-field behavior and fantasy points.\n\n13.5.6 Selection Bias\nSelection bias occurs when the selection of participants or their inclusion in analyses depends on the variables being studied (or on their colliders). For instance, if you are conducting a study on the extent to which sports drink consumption influences fantasy points, there would be selection bias if players are less likely to participate in the study if they score fewer fantasy points.\nNow, consider a study in which you conduct a randomized controlled trial (RCT; i.e., an experiment) to evaluate the effect of a new medication on player performance. You randomly assign some players to take the medication and other players to take a placebo. Assume the new medication has side effects and leads many of the players who take it to drop out of the study. This is an example of attrition bias (i.e., systematic attrition). If you were to exclude these individuals from your analysis, it may make it appear that the medication led to better performance, because the players who experienced the side effect (and worse performance) dropped out of the study. Hence, conducting an analysis that excludes these players from the analysis would involve selection bias.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceConclusion",
    "href": "causal-inference.html#sec-causalInferenceConclusion",
    "title": "13  Causal Inference",
    "section": "\n13.6 Conclusion",
    "text": "13.6 Conclusion\nThere are three criteria for establishing causality: 1) the cause precedes the effect. 2) The cause is related to the effect. 3) There are no other alternative explanations for the effect apart from the cause. In general, it is important to be aware of the counterfactual and to consider what would have happened if the supposed cause had not occurred. Various experimental and quasi-experimental designs and approaches can be leveraged to more closely approximate causal inferences. Longitudinal designs, within-subject analyses, inclusion of control variables, and genetically informed designs are all quasi-experimental designs that afford the researcher greater control over some possible third variable confounds. Causal diagrams can be a useful tool for identifying the proper variables to control for (and those not to control for). When confounding exists, it is important to control for the confound(s). It is important not to control for mediators when interested in the total effect of the predictor variable on the outcome variable. In addition, it is important not to control for descendants of the outcome variable. When there is a collision, it is important not to control for the collider (unless the collider is also a confound).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceSessionInfo",
    "href": "causal-inference.html#sec-causalInferenceSessionInfo",
    "title": "13  Causal Inference",
    "section": "\n13.7 Session Info",
    "text": "13.7 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggdag_0.2.13  dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] viridis_0.6.5      generics_0.1.3     tidyr_1.3.1        stringi_1.8.4     \n [5] digest_0.6.37      magrittr_2.0.3     evaluate_1.0.3     grid_4.4.3        \n [9] fastmap_1.2.0      jsonlite_1.9.1     ggrepel_0.9.6      gridExtra_2.3     \n[13] purrr_1.0.4        viridisLite_0.4.2  scales_1.3.0       tweenr_2.0.3      \n[17] cli_3.6.4          rlang_1.1.5        graphlayouts_1.2.2 polyclip_1.10-7   \n[21] tidygraph_1.3.1    munsell_0.5.1      withr_3.0.2        cachem_1.1.0      \n[25] yaml_2.3.10        tools_4.4.3        memoise_2.0.1      dplyr_1.1.4       \n[29] colorspace_2.1-1   ggplot2_3.5.1      boot_1.3-31        curl_6.2.1        \n[33] vctrs_0.6.5        R6_2.6.1           lifecycle_1.0.4    stringr_1.5.1     \n[37] V8_6.0.2           htmlwidgets_1.6.4  MASS_7.3-64        ggraph_2.2.1      \n[41] pkgconfig_2.0.3    pillar_1.10.1      gtable_0.3.6       glue_1.8.0        \n[45] Rcpp_1.0.14        ggforce_0.4.2      xfun_0.51          tibble_3.2.1      \n[49] tidyselect_1.2.1   knitr_1.50         farver_2.1.2       htmltools_0.5.8.1 \n[53] igraph_2.1.4       labeling_0.4.3     rmarkdown_2.29     compiler_4.4.3    \n\n\n\n\n\n\nBarrett, M. (2024). ggdag: Analyze and create elegant directed acyclic graphs. https://github.com/r-causal/ggdag\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nD’Onofrio, B. M., Sjölander, A., Lahey, B. B., Lichtenstein, P., & Öberg, A. S. (2020). Accounting for confounding in observational studies. Annual Review of Clinical Psychology, 16(1), 25–48. https://doi.org/10.1146/annurev-clinpsy-032816-045030\n\n\nDigitale, J. C., Martin, J. N., & Glymour, M. M. (2022). Tutorial on directed acyclic graphs. Journal of Clinical Epidemiology, 142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001\n\n\nInternational Society of Genetic Genealogy. (2022). Autosomal DNA statistics. https://isogg.org/wiki/Autosomal_DNA_statistics\n\n\nKievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513\n\n\nLederer, D. J., Bell, S. C., Branson, R. D., Chalmers, J. D., Marshall, R., Maslove, D. M., Ost, D. E., Punjabi, N. M., Schatz, M., Smyth, A. R., Stewart, P. W., Suissa, S., Adjei, A. A., Akdis, C. A., Azoulay, É., Bakker, J., Ballas, Z. K., Bardin, P. G., Barreiro, E., … Vincent, J.-L. (2019). Control of confounding and reporting of results in causal inference studies. Guidance for authors from editors of respiratory, sleep, and critical care journals. Annals of the American Thoracic Society, 16(1), 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42. https://doi.org/10.1177/2515245917745629\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin.\n\n\nSpector, P. E., & Brannick, M. T. (2010). Methodological urban legends: The misuse of statistical control variables. Organizational Research Methods, 14(2), 287–305. https://doi.org/10.1177/1094428110369842\n\n\nTextor, J., van der Zander, B., Gilthorpe, M. S., Liśkiewicz, M., & Ellison, G. T. (2016). Robust causal inference using directed acyclic graphs: The r package ’dagitty’. International Journal of Epidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical control requires causal justification. Advances in Methods and Practices in Psychological Science, 5(2), 25152459221095823. https://doi.org/10.1177/25152459221095823",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html",
    "href": "cognitive-bias.html",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "",
    "text": "14.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasGettingStarted",
    "href": "cognitive-bias.html#sec-cognitiveBiasGettingStarted",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "",
    "text": "14.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasOverview",
    "href": "cognitive-bias.html#sec-cognitiveBiasOverview",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.2 Overview",
    "text": "14.2 Overview\nWhen considering judgment and prediction, it is important to consider psychological concepts, including heuristics and cognitive biases. In the modern world of big data, research and society need people who know how to make sense of the information around us. Given humans’ cognitive biases, it is valuable to leverage more objective approaches than relying on our “gut” and intutition. Statistical approaches can be a more objective way to identify systematic patterns.\nStatistical analysis—and science more generally—is a process to the pursuit of knowledge. An epistemology is an approach to knowledge. Science is perhaps the best approach (epistemology) that society has to approximate truth. Unlike other approaches to knowledge, science relies on empirical evidence and does not give undue weight to anecdotal evidence, intuition, tradition, or authority.\nPer Petersen (2025), here are the characteristics of science that distinguish it from pseudoscience:\n\n\nRisky hypotheses are posed that are falsifiable. The hypotheses can be shown to be wrong.\nFindings can be replicated independently by different research groups and different methods. Evidence converges across studies and methods.\nPotential alternative explanations for findings are specified and examined empirically (with data).\nSteps are taken to guard against the undue influence of personal beliefs and biases.\nThe strength of claims reflects the strength of evidence. Findings and the ability to make judgments or predictions are not overstated. For instance, it is important to present the degree of uncertainty from assessments with error bars or confidence intervals.\nScientifically supported measurement strategies are used based on their psychometrics, including reliability and validity.\n\n\nNevertheless, statistical analysis is not purely objective and is not a panacea. Science is a human enterprise—it is performed by humans each of whom has their own biases. For instance, cognitive biases can influence how people interpret statistics. As a result, the findings from any given study may be incorrect. Thus, it would be imprudent to make decisions based solely on the results of one study. That is why we wait for findings to be independently replicated by different groups of researchers using different methods.\nIf a research team publishes flashy new and exciting findings, other researchers have an incentive to disprove the prior findings. Thus, we have more confidence if findings stand up to scrutiny from independent groups of researchers. We also draw upon meta-analyses—studies of many studies, to summarize the results of many studies and not just the findings from any single study that may not replicate. In this way, we can identify which findings are robust and most likely true versus the findings that fail to replicate. Thus, despite its flaws like any other human enterprise, science is a self-correcting process in which the long arc bends toward truth.\nIn our everyday lives, humans are presented with overwhelming amounts of information. Because human minds cannot parse every piece of information equally, we tend to take mental shortcuts, called heuristics. These mental shortcuts can be helpful. They reduce our mental load and can help us make quick judgments to stay alive or to make complex decisions in the face of uncertainty. For instance, snap judgments can help us rapidly differentiate between a friend and a foe, for survival. However, these mental shortcuts can also lead us astray and to make systematic errors in our judgments and predictions. For instance, heuristics can lead us to identify patterns out of randomness. Cognitive biases are systematic errors in thinking. Fallacies are forms of flawed reasoning.\nBelow, we provide examples of heuristics, cognitive biases, and fallacies. These are not exhaustive lists of all the heuristics, cognitive biases, and fallacies. They are some examples of key ones that are relevant fantasy football.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-heuristics",
    "href": "cognitive-bias.html#sec-heuristics",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.3 Examples of Heuristics",
    "text": "14.3 Examples of Heuristics\nAs described above, heuristics are mental shortcuts that people use to handle the overwhelming amount of information to process. Important heuristics used in judgment and prediction in the face of uncertainty include (Critcher & Rosenzweig, 2014; Kahneman, 2011; Tversky & Kahneman, 1974):\n\navailability heuristic\nrepresentativeness heuristic\nanchoring and adjustment heuristic\naffect heuristic\nperformance heuristic\n“What You See Is All There Is” (WYSIATI) heuristic\n\nUse of heuristics has been observed among fantasy sports managers (B. Smith et al., 2006).\n\n14.3.1 Availability Heuristic\nThe availability heuristic refers to the tendency for a person to make judgments or predictions about the frequency or probability of something based on how readily instances can be brought to mind. For instance, when making fantasy predictions about a player, more recent big performance games (or games that receive more news coverage) may more easily come to mind compared to lower-scoring games and games that occurred longer ago. Thus, a manager may be more inclined to pick players to start who had more recent, stronger performances rather than players who have higher long-term averages.\n\n14.3.2 Representativeness Heuristic\nThe representativess heuristic refers to the tendency for a person’s judgments or predictions about individuals to be made based on how similar the individual is to (i.e., how closely the individual resembles) the person’s existing mental prototypes. In this way, the representativeness heuristic involves stereotyping, which is not a statistically optimal approach to predictions (Kahneman, 2011). Stereotypes are statements about groups or categories that are at least tentatively accepted as facts about every member of that group or category (Kahneman, 2011). That is, a (perhaps common) characteristic of a group is attributed to each member of that group. In addition, the representativness heuristic does not adequately consider the quality of the evidence (Kahneman, 2011). It is thus important to question the predictive accuracy of the evidence for the particular prediction question.\nConsider the following example of the representativeness heuristic. When coming out of college, Tight End Kyle Pitts drew comparisons to the “LeBron James” of Tight Ends [Nivison (2021); archived at https://perma.cc/JQB5-XPVL]. The idea that his athletic profile leads him to be similar to the prototype of LeBron James may have led him to be too highly drafted by fantasy managers in his first seasons.\nHere is a video from Get Up ESPN (2021) of Kyle Pitts drawing comparisons to the LeBron James of Tight Ends:\nVideo of Kyle Pitts drawing comparisons to the LeBron James of Tight Ends. From: https://x.com/GetUpESPN/status/1380165126108672001 (archived at https://perma.cc/JW8E-KV2C)\nThe representativeness heuristic has been observed in gambling markets for predicting team wins in the National Football League (NFL) (Woodland & Woodland, 2015) and in decision making in fantasy soccer (Kotrba, 2020).\n\n14.3.3 Anchoring and Adjustment Heuristic\nThe anchoring and adjustment heuristic refers to the tendency for a person’s judgments or predictions to be made with a reference point—an anchor—as a starting point from which they adjust their estimates upward or downward. The anchor provides a suggestion to the person—even without their paying attention, leading to a priming effect which evokes information that is compatible with the suggestion (i.e., a suggestive influence). We are more suggestible than we would like to think we are. The anchor is often inaccurate and given too much weight in the person’s calculation, and too little adjustment is made away from the anchor. As an example, insufficient adjustment from an anchor may lead someone to drive too fast after exiting a highway (Kahneman, 2011). The anchoring effect occurs even when the anchor (i.e., suggestion) is a random number and is completely uninformative of the correct answer (Kahneman, 2011)!\nApplied to fantasy football, a manager is trying to predict how many fantasy points a top Running Back may score. The player scored 300 fantasy points last season, but the team added a stronger backup Running Back and changed the Offensive Coordinator to be a more pass-heavy offense. The manager may use 300 fantasy points as an anchor (based on the player’s performance last season), and may adjust downward 15 points to account for the offseason changes. However, it is possible that this downward adjustment is insufficient to account not only for the offseasons changes but also for potential regression effects. Regression effects are discussed further in Section 14.5.2.\nTo counter the effect of the anchoring heuristic, Kahneman (2011) suggests deliberately thinking the opposite of the anchor. And, if a person you are negotiating with makes an outrageous initial offer, instead of making an equally outrageous counter-offer, Kahneman (2011) suggests storming out or threatening to leave, and indicating that you are unwilling to negotiate with that number on the table. This technique could be helpful during trade negotiations for players in fantasy football. Other techniques include, instead of focusing on the anchor, focusing on the minimal offer that the opponent would accept, or on the costs to the opponent of failing to reach an agreement (Kahneman, 2011).\n\n14.3.4 Affect Heuristic\nThe affect heuristic refers to the tendency for a person to make judgments or predictions based on their emotions and feelings—such as whether they like or dislike something—with little deliberation or reasoning. That is, the affect heuristic involves letting one’s likes and dislikes determine one’s beliefs about the world (Kahneman, 2011). For instance, if a manager’s favorite team is the Dallas Cowboys, they may hold a distaste for players on teams that are rivals of the Cowboys, such as the Philadelphia Eagles. The affect heuristic may lead them to believe that Eagles players are overrated and to avoid drafting Eagles players even though some Eagles players may perform well.\n\n14.3.5 Performance Heuristic\nThe performance heuristic refers to the tendency for people to predict their improvement in a task based on their previous performance (Critcher & Rosenzweig, 2014). Although past performance tends to be correlated with future performance, people tend to show regression to the mean, as described in Section 14.5.2. Thus, people who show strong performance in their first few games may in many cases show worse performance in subsequent games. Moreover, initial performance does not necessarily indicate the extent of one’s future improvement. For instance, if a manager has a successful first season, the performance heuristic may lead them to overestimate the extent to which they will improve in their skill or performance in the future.\n\n14.3.6 What You See Is All There Is (WYSIATI) Heuristic\nThe “What You See Is All There Is” (WYSIATI) heuristic refers to the tendency for a person to make a decision or judgment based solely on the information that is immediately available to them (i.e., “right in front of them”), without considering the possibility that there may be additional important information that they do not have (Enke, 2020). The WYSIATI heuristic involves jumping to conclusions on the basis of limited information (Kahneman, 2011). For instance, a manager might start a player based on the fact that the player scored lots of points in their most recent game. However, the manager may not consider other important factors that they did not know about, such as the fact that the player filled in for an injured player, and will get fewer touches in the upcoming game now that the original starter is back to full health.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiases",
    "href": "cognitive-bias.html#sec-cognitiveBiases",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.4 Examples of Cognitive Biases",
    "text": "14.4 Examples of Cognitive Biases\nAs described above, cognitive biases are systematic errors in thinking. They lead us to be less accurate in our judgments and predictions. Inaccuracy may not always be bad; several cognitive biases may serve to help people feel better about themselves (R. M. Miller, 2013), including confirmation bias, overconfidence bias, optimism bias, and self-serving bias; or to prevent feelings of regret (e.g., omission bias)—or to prevent having to admit failure (e.g., commitment bias). In fantasy football, however, cognitive biases and inaccurate judgments can lead to worse performance. Cognitive biases are often due to the use of heuristics.\nMiller (2013) describes cognitive biases in fantasy sports. Examples of cognitive biases relevant to fantasy football that result from one or more heuristics include:\n\noverconfidence bias\noptimism bias\nconfirmation bias\nin-group bias\nhindsight bias\noutcome bias\nself-serving bias\nomission bias\nloss aversion bias\nrisk aversion bias\nprimacy effect bias\nrecency effect bias\nframing effect bias\nendowment effect bias\nbandwagon effect bias\nDunning–Kruger effect bias\n\nIn general, when faced with a situation in which bias is likely, Kahneman (2011) advises slowing down, being thoughtful and deliberate, questioning your intuitions, and imposing orderly procedures, such as checklists, reference-class forecasting, and a “premortem”. A premortem is when you consider, in advance of the decision, what likely went wrong if the decision were to work out terribly. Reference-class forecasting involves examining a reference class of similar projects to identify the most likely range of outcomes for the project.\n\n14.4.1 Overconfidence Bias\nIn general, people tend to be overconfident in their judgments and predictions. Overconfidence bias is the tendency for a person to have greater confidence in their abilities (including judgments and predictions) than is objectively warranted. There are three general ways that overconfidence has been identified (Moore & Healy, 2008):\n\n\noverestimation of one’s actual performance\n\noverplacement of one’s performance relative to others\n\noverprecision in one’s beliefs/jugments/predictions\n\nOverestimation involves believing that one will perform better than one actually performs. Overestimation can be identified with a calibration plot of the predicted performance versus actual performance, where the person’s predicted performance is systematically higher (in at least some cases) than their actual performance. Overestimation corresponds to the “overprediction” form of miscalibration.\nOverplacement involves believing that one is better than others or will perform better than others, even when they do not. For instance, it is a common finding that more than half of people believe they are “above average” (i.e., above the median), even though that is statistically impossible. This calls to mind the fictitious Lake Wobegon in the radio show A Praririe Home Companion, “where all the women are strong, all the men are good-looking, and all the children are above average.” As an example, 90% of drivers believe they are better than average (Kahneman, 2011).\nOverprecision involves expressing excessive certainty regarding the accuracy of one’s beliefs/judgments/predictions. For instance, if when a given meteorologist says it will rain 80% of the time, it actually rains 30% of the time, the meteorologist’s predictions are overprecise. Likewise, if the weather forecast says it will rain 10% of the time and it actually rains 30% of the time, the predictions are also overprecise because the forecaster is expressing stronger confidence than is warranted that it will not rain. Overprecision can be identified with a calibration plot of the predicted probabilities versus the actual probabilities. Overprecision corresponds to the “overextremity” form of miscalibration.\nOverestimation and overprecision are studied in various ways. Typically, people are asked about (a) whether an event will occur or (b) the likelihood that the event will occur, across many events. For the former (approach “a”), people may be asked to make a dichotomous judgment or prediction, by responding to the question: e.g., “Will it rain tomorrow? [YES/NO]”. They will then rate their confidence (as a percentage) in their answer (0–100%). They would make each of these two ratings for each of many events. Then, we can evaluate, for a given respondent, the degree to which the probabilistic estimate of an event reflects the true underlying probability of the event. For instance, for a given respondent (and for respondents in general), for the events when the respondent says they are 80% confident an event (e.g., rain) will occur, does the event actually occur around 80% of the time? For the latter (approach “b”), people may indicate the likelihood that the event will occur, by responding to the question: “How likely is it that it will rain tomorrow? (0–100%)”. Then, we can evaluate, for instance, for the events when the respondent says that an event (e.g., rain) is 80% likely to occur, does the event actually occur around 80% of the time?\nThe confidence that people tend to have in their predictions depends less and the quantity and quality of the evidence for their predictions and more on the quality of the narrative they can tell about the (little) information they have immediate access to (Kahneman, 2011). For instance, people tend not to consider other relevant, important factors that they do not have immediate access to, consistent with the WYSIATI heuristic. Instead, people tend to focus on the coherence and plausibility of the story, which differs from its probability (Kahneman, 2011). That is, one’s confidence is not necessarily related to the likelihood that the judgment is correct—it often reflects the “illusion of validity” (Kahneman, 2011). As one gains more knowledge in a domain, the person also tends to become unrealistically overconfident in their ability to make accurate judgments and predictions (Kahneman, 2011). In general, people tend to focus on what they know and to neglect what they do not know leading to overconfidence in one’s beliefs (Kahneman, 2011).\nA fantasy manager may be even more likely to exhibit overconfidence if they previously performed well or won their league, for which luck and random chance plays an important role. Indeed, it is estimated that nearly half (~45%) of the variability in fantasy football performance is estimated to be luck [and around 55% due to skill; Getty et al. (2018)]. A manager who won their league in the prior season may believe they will perform better than they actually will (overestimation), will perform better than average (overplacement), and may hold excessive confidence regarding the accuracy of their predictions about which players will perform well or poorly (overprecision). These various types of overconfidence may lead them to draft high-risk players based on gut feeling, neglecting statistical analysis and expert consensus.\nPeople tend to focus on the role of skill and to neglect the role of luck when explaining the past and predicting the future, giving people an illusion of control (Kahneman, 2011). Players’ performance in fantasy football, and human behavior more generally, is complex and multiply determined (i.e., is influenced by many factors). Despite the bluster of so-called experts who pretend to know more than they can know, no one can consistently and accurately predict how all players will perform. Remain humble in your predictions; do not be more confident than is warranted. If you approach the task of prediction with humility, you may be more able to be flexible and more willing to consider other players who you can draft for good value.\n\n14.4.2 Optimism Bias\nOptimism bias is the tendency for people to overestimate one’s likelihood of experiencing positive events and underestimate one’s likelihood of experiencing negative events. For instance a manager might overestimate the likelihood that their team will win the championship and may underestimate the likelihood that they may lose a player to injury. In terms of its key consequences for decisions, optimism bias may be considered the most important cognitive bias (Kahneman, 2011). In general, people show an optimism bias. However, some people (“optimists”) are even more optimistic than others (Kahneman, 2011). The optimism bias—in particular a delusional optimism—can lead to overconfidence (Kahneman, 2011). Although optimism is associated with many advantages in terms of well-being, it can also lead to excessive risk-taking and to costly persistence (Kahneman, 2011).\n\n14.4.3 Confirmation Bias\nConfirmation bias is the tendency for people to search for, interpret, and remember information that confirms one’s beliefs, as opposed to information that might disconfirm one’s beliefs. The search for confirming evidence is called “positive test strategy”; it runs counter to advice by philosophers of science who advise testing hypotheses by trying to refute them [i.e., by searching for disconfirming evidence; Kahneman (2011)]. The result of confirmation bias is that people are unlikely to change their minds about something that they have a pre-existing belief about, because they tend to look only for information that supports their pre-existing beliefs. For instance, if you believe that a particular player is a strong breakout candidate to be a sleeper, you may be more likely to pay attention to evidence that supports that the player will breakout and may be less likely to pay attention to evidence that indicates the player may struggle. That is, people tend to look for confirmation that the players they want to draft (or start) are preferred by the experts (R. M. Miller, 2013). Confirmation bias may lead to “going with your gut” (i.e., making a decision based on your intuition).\nAs a budding empiricist, you should actively seek out information that challenges or disconfirms your beliefs—particularly from trustworthy, reputable sources—and work to incorporate the information into your beliefs. Do your best to go into observation, data analysis, and data interpretation with an open mind.\n\n14.4.4 In-Group Bias\nIn-group bias is the tendency for people to prefer others who are members of their same (in-)group. For instance, a manager whose favorite team is the Dallas Cowboys might try to draft mostly Cowboys players, regardless of their historical performance.\n\n14.4.5 Hindsight Bias\n\n“Hindsight is 20/20.” – Idiom\n\nHindsight bias is the tendency to perceive that past events were more predictable than they were (i.e., the “I-knew-it-all-along” effect). Everything seems to make sense in hindsight (Kahneman, 2011). People tend to remember the succeess of their predictions and forget the failures of their predictions. For instance, if a third-string Quarterback has a breakout game, a fantasy manager may claim that they “knew it all along” that the player was going to breakout, despite not having picked up the player. That same manager may forget the many other predictions they had that did not come true.\n\n14.4.6 Outcome Bias\nOutcome bias is the tendency to evaluate the quality of a decision based on the eventual outcome of that decision, rather than based on the quality of the decision based on the information that was known at the time of the decision. For instance, consider that a manager starts a lower-ranked player over a higher-ranked player because the lower-ranked player has a more favorable matchup. Now, consider that the lower-ranked player scores fewer points than the higher-ranked player because the lower-ranked player got injured. If the manager concludes that their decision-making process for who to start was flawed and thus not pay attention to matchups in subsequent decisions, this would reflect outcome bias. Likewise, if the manager makes a poor-quality decision but gets lucky, and they attribute their success to a sound decision-making process, this would also reflect outcome bias. In general, people tend to blame decision makers for good decisions that turned out badly, and to give them too little credit for good decisions that turned out well [because the decisions appear obvious but only after the fact; Kahneman (2011)]. Hindsight bias and outcome bias tend to lead to risk aversion (Kahneman, 2011).\nFocus on the decision-making process when evaluating the quality of decisions. Yes, also evaluate the outcomes of decisions, but do not give yourself too much credit for lucky decisions or too much blame for unlucky decisions (like injuries). That said, one thing you can count on is that players get injured. So, it is to your benefit to draft and compose your team to have some bench depth so you can handle injuries when they inevitably occur.\n\n14.4.7 Self-Serving Bias\nSelf-serving bias is the tendency to perceive oneself in an overly positive manner. Doing so may function to bolster people’s self-esteem. One way that self-serving bias can commonly manifest is the tendency people to attribute successes to internal factors—such as one’s character, ability, effort, or actions—but to attribute failures to external factors beyond one’s control—such as others, unfairness, bad luck, etc. That is, self-serving bias involves claiming more reponsibility for one’s success than for one’s failure (R. M. Miller, 2013). For instance, a manager who wins the league may attribute their success to their fantasy football skill and smart decisions (rather than luck). However, if they get last place, they may attribute their failure not to themselves but instead to factors outside of their control, such as bad luck, player injuries, and poor performance by players due to circumstances out of their control (e.g., bad weather conditions, coaching decisions).\nAlthough the self-serving bias may help us feel better about ourselves, it can lead us to overestimate our abilities when things go well and to fail to learn from mistakes when things go poorly. Remember that nearly half of the variability in fantasy football performance is estimated to be luck, and the other half likely due to skill (Getty et al., 2018). Thus, successes in fantasy football likely involve a good amount of luck; likewise, failures in fantasy football can also reflect mistakes and poor decision making that arise from lower skill.\n\n14.4.8 Omission Bias\nOmission bias is the tendency to prefer inaction over action, or to prefer the default option over changes from the default option (Kahneman, 2011). For instance, consider a manager who is on the fence to drop an underperforming player and to pick up someone else to start. If the manager does not drop the player and the player scores few points, the manager is likely to be less critical and regretful of their decision than if they had picked up another player who then scored the same (low) number of points. Thus, a person’s decisions may be influenced by the anticipation or fear of regret so as to keep the status quo, avoid losses, and minimize the expectation of regret—such as choosing the same order each time at a restaurant to avoid regret (Kahneman, 2011). However, people may anticipate more regret than they actually experience, because people tend to underestimate the protective impact of the psychological defenses they will deploy against regret (Kahneman, 2011). In general, Kahneman (2011) recommends not to put too much weight on potential future regret when making decisions: “even if you have some [regret], it will hurt less than you now think.” (p. 352).\nConsider another example adapted from Miller (2013): two players scored the same number of points in a given week. A Wide Receiver scored 1.6 points from 16 receiving yards on 1 target. A Running Back scored 1.6 points from 56 receiving yards and two dropped fumbles. The manager may harbor more blame for the player who performed a harmful action (losing two fumbles) than the player who received only one target (harmful inaction).\nPay attention to players’ usage and opportunities, not just how many points a player scored and whether they had more fumbles or interceptions than other players. The more times a player touches the ball, the more points they are likely to score. And do not let the anticipation of regret prevent you from making important decisions.\n\n14.4.9 Loss Aversion Bias\nLoss aversion bias is the tendency to avoid losses rather than acquiring equivalent gains. That is, loss aversion (similar to punishment sensitivity, as opposed to reward sensitivity) is the tendency to weigh losses (or punishment) more heavily than gains (or rewards). Thus, the target state matters less than the change (i.e., gains and/or losses) from one’s reference point [e.g., their current roster; Kahneman (2011)]. Loss aversion is a key aspect of prospect theory and suggests that individuals tend to be biased in favor of their reference situation and toward making smaller rather than larger changes (Kahneman, 2011). Loss aversion explains why people tend to favor the favoring the status quo; the negative consequences of a change often loom larger than the positive consequences of a change (Kahneman, 2011). Aversion to losses and potential threats may be related to humans’ survival instinct (Kahneman, 2011). With par as a reference point, this may explain why professional golfers tend to putt more accurately for par than for birdie, even for putts of equivalent distance (Kahneman, 2011).\nPrinciples of prospect theory and loss aversion bias are depicted in Figure 14.1.\n\n\n\n\n\nFigure 14.1: Prospect Theory and Loss Aversion: People Tend to Weigh Losses More Heavily Than Gains. That is, the utility function is steeper in the loss (red) region than in the gain (blue) region. Adapted from https://commons.wikimedia.org/wiki/File:Graphique_th%C3%A9orie_des_perspectives.svg.\n\n\nThe loss aversion ratio indicates the extent to which a person weighs losses greater than gains, and for many people is around 2 (typically between 1.5 to 2.5; (Kahneman, 2011). That is, people tend to give as much twice the weight to losses as to gains. The late baseball manager Sparky Anderson once said, “Losing hurts twice as bad as winning feels good.” Loss aversion is different from risk aversion. Loss aversion is exemplified when teams play conservatively so as “not to lose” instead of “to win.” In fantasy football, loss aversion may lead managers to start or hold onto players for too long who were highly drafted yet are underperforming instead of starting a more promising player out of fear of losing potential value from their initial investment. Loss aversion can also influence trade negotiations.\nAn example of loss aversion bias in coaching basketball is removing a star player from a game so you can keep the player for the end of the game (Moskowitz & Wertheim, 2011). As described in Section 25.3.1, an example of loss aversion bias in football is punting on fourth down when, in many cases, it would be advantageous to go for it (Moskowitz & Wertheim, 2011).\n\n14.4.10 Risk Aversion Bias\nRisk aversion bias is the tendency to prefer outcomes with low uncertainty (i.e., risk), even if they offer lower potential rewards compared to outcomes with greater uncertainty but potentially greater rewards. Risk aversion leads people to select safer options but may lead them to miss out on higher-gain opportunities. For instance, risk aversion may lead a fantasy manager to start players who are more steady (i.e., show greater game-to-game consistency) over players who are more volatile (i.e., show greater game-to-game variability) but have higher upside potential.\nIn mixed gambles, in which it is possible for a person to experience either a gain or a loss, loss aversion tends to lead to risk-averse choices (Kahneman, 2011). By contrast, when all of a person’s options are poor, people tend to engage in risk seeking, as has been observed in entrepreneurs and in generals (Kahneman, 2011). In fantasy football, risk seeking may be more likely when a manager has a team full of underperforming players and a weak record.\n\n14.4.11 Primacy Effect Bias\nPrimacy effect bias is the tendency to recall the first information that one encounters better than information presented later on. For instance, primacy effect bias might lead a manager to draft a player based on the first information they encountered about the player.\n\n14.4.12 Recency Effect Bias\nRecency effect bias is the tendency to weigh recent events more than earlier ones. For instance, a manager might observe that a Running Back on the waiver wire performed well in the last two games. Recency effect bias may lead the manager to pick up the player, overvaluing their recent performance. For instance, the manager may not have adequately weighed the player’s overall season performance and the fact that the starting Running Back is returning to the lineup from injury, and that is why the player received more carries in the past two games (i.e., in place of the injured starter).\nRecency effect bias seems a bit at odds with primacy effect bias; however, they can both coexist. People tend to remember the earliest information they encounter in addition to the most recent information, and they tend to forget the information encountered in between.\n\n14.4.13 Framing Effect Bias\nFraming effect bias is the tendency to make decisions based on how information is framed (e.g., based on positive or negative connotations), rather than just the information itself. For instance, consider a weekly column written by two different fantasy football expert commentators. Expert A may write that a Running Back has scored at least 15 fantasy points in 70% of games this season (i.e., positive framing). Expert B may write that the same Running Back has scored fewer than 15 fantasy points in 30% of games this season (i.e., negative framing). Both statements convey the same statistical information; however, a manager might be more inclined to start the player when reading the statement from Expert A than from Expert B because of how the information was framed (i.e., presented). People prefer to make decisions to avoid loss, consistent with loss aversion bias, so the negative framing might lead a person to be less likely to start the player.\nPay attention to how information is framed, knowing that the framing itself could lead you astray. Thus, try to focus on the information itself rather than the framing when making decisions.\n\n14.4.14 Endowment Effect Bias\nEndowment effect bias is the tendency to overvalue merely because one owns it—owning it increases its value to the owner [especially for goods that are not regularly traded; Kahneman (2011)]. That is, a manager tends to value the same player more when they play for their team rather than an opponent’s team. It is related to loss aversion bias and prospect theory; when owning a good (i.e., a person’s reference point), they focus on the loss of the good if they were they to sell or trade it away rather than on the gains from the sale or trade. When trading, the endowment effect bias might lead a manager to demand more to trade away a player than to acquire the same player (R. M. Miller, 2013). For instance, a manager might overvalue a player they drafted in the first round, refusing to trade them even if they could get a better-performing player in return.\nA player’s trade value is not fixed. It makes sense to adjust a player’s trade value based on many factors such as your team’s needs, the needs of your trade partner’s team, the player’s remaining strength of schedule, etc. However, you want to avoid adjusting a player’s value based merely on the fact that he is on your team. Evaluate a variety of trade analyzers to more objectively evaluate players’ worth.\n\n14.4.15 Bandwagon Effect Bias\nThe bandwagon effect bias is the tendency to do or believe things because other people are. It involves social conformity. For instance, consider if a rookie Wide Receiver has a breakout game and he is picked up in many fantasy leagues. A given manager might pick up the player because the player is frequently being picked up in many fantasy leagues, without evaluating whether the player’s success is sustainable.\n\n14.4.16 Dunning–Kruger Effect Bias\n\n“The more you know, the more you know you don’t know.” – Anonymous\n\nThe Dunning–Kruger effect bias is the tendency for people with low ability/competency in a task to overestimate their ability. The Dunning–Kruger effect is depicted in Figures 14.2 and 14.3. For instance, consider a new fantasy manager who experiences some initial wins (often called “beginner’s luck”). They may attribute their successes to their skill rather than to luck. Their overconfidence may lead them to believe they can win the league without much preparation.\n\n\n\n\n\nFigure 14.2: Dunning–Krueger Effect: Confidence as a Function of Competence. People with low competency in a task tend to overestimate their ability. Adapted from https://commons.wikimedia.org/wiki/File:Effet_Dunning-Kruger.svg.\n\n\n\n\n\n\n\nFigure 14.3: Dunning–Krueger Effect: Perceived Performance as a Function of Actual Performance. People who perform poorly in a task tend to overestimate their performance. Adapted from https://commons.wikimedia.org/wiki/File:Dunning-kruger_effect_-_percentile.svg.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-fallacies",
    "href": "cognitive-bias.html#sec-fallacies",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.5 Examples of Fallacies",
    "text": "14.5 Examples of Fallacies\nAs described above, fallacies are mistaken beliefs and flawed reasoning. Fallacies are often due to the use of heuristics and to cognitive biases. Examples of fallacies include:\n\n\nbase rate fallacy (aka base rate neglect)\nregression fallacy\nhot hand fallacy\nsunk cost fallacy\ngambler’s fallacy\nanecdotal fallacy\nnarrative fallacy\nconditional probability fallacy\necological fallacy\n\n\n14.5.1 Base Rate Fallacy\nThe base rate fallacy (aka base rate neglect) is the tendency to ignore information about the general probability of an event in favor of specific information about the event. The base rate is a marginal probability, which is the general probability of an event irrespective of other things. The, a “low base-rate” event is an event with a low probability of occurring (i.e., an unlikely event). Focusing on the specific information about the event (and thus underweighting the base rate) involves the representativeness heuristic (Kahneman, 2011)—making judgments based on how similar the event is to a mental prototype regardless of how frequent the event the event actually is. As noted by Kahneman (2011), “people who have information about an individual case rarely feel the need to know the statistics of the class which the case belongs.” (p. 249). In general, people tend to a) overestimate the likelihood of low base-rate events and b) overweight low base-rate events in their decisions (Kahneman, 2011). This is especially true if the unlikely event attracts attention, because of the confirmatory bias of memory (Kahneman, 2011).\nFor example, the base rate of work-related injury is the general probability of experiencing a work-related injury, irrespective of other factors (e.g., the type of job, the person’s age, the person’s sex). Among the working population in the U.S., the lifetime prevalence of work-related injuries (i.e., the percent of people who will experience a work-related injury at some point in their lives), is ~35% [Free et al. (2020); archived at https://perma.cc/A2L6-WPEH]. Thus, the base rate of work-related injuries in the U.S. is ~35%. The probability of work-related injuries is higher for some occupations (e.g., construction) and for some groups (e.g., men, 55–64-year-olds, Black or Multiracial, who are self-employed and have less than high school education) than others. Nevertheless, if we ignore all of the interacting factors, the general probability of work-related injuries is 35%. If we made a prediction that someone would be highly likely (&gt; 90%) to experience a work-related injury because they are male and self-employed, this would be ignoring the relatively lower base rate of work-related injury. Indeed, even men (36.7%) and self-employed individuals (41.2%) have less than a 50% chance of experiencing a work-related injury.\nThe framing of an event can also alter its weighting. For instance, concrete representations tend to be weighted more heavily than abstract representations (Kahneman, 2011). Abstract representations are framed in terms of “how likely” the event will occur in in terms of chance, risk, probability, or likelihood. Concrete representations are framed in terms of “how many” people will be affected by the event. An example abstract representation is that there is a 0.001% chance of dying from this particular disease. When framed in this way, the risk appears small. Now, consider the same information framed in terms of a concrete representation: 1 in 100,000 people will die from this disease. Due to a phenomenon called denominator neglect, low base-rate events are more heavily weighted when they are framed using a concrete representation than an abstract representation (Kahneman, 2011).\nThere are low base-rate events that tend to be underweighted when making decisions, such as whether to make anticipatory preparations for a future earthquake, housing bubble, or financial crisis (Kahneman, 2011). One major cause of underweighting is if a person has never experienced the rare event (i.e., the earthquake or finanical crisis) (Kahneman, 2011). However, for low base-rate events that people have experienced, the availability and representativeness heuristics tend to lead people to overestimate the likelihood that the unlikely event occurs. In general, many factors—including obsessive concerns about the event, vivid images of the event, concrete representations of the consequences of the event (e.g., “a disease kills 1 person out of every 1,000”), and explicit reminders of the event—contribute to the overweighting of unlikely events (Kahneman, 2011).\nAs applied to fantasy football, consider that you read about a potential sleeper Wide Receiver who had a stellar performance in a preseason game. If you select this player early on in the draft based on this information, this would be ignoring the general probability that most players who have a strong performance in the preseason do not perform as well in the regular season (i.e., base rate neglect). Performance in the preseason is not strongly predictive of performance in the regular season [Schalter (2022); archived at https://perma.cc/FSG2-6AXE].\nMore information on base rates and how to counteract the base rate fallacy in described in Chapter 16.\n\n14.5.2 Regression Fallacy\nThe regression fallacy is the failure to account for the fact that things tend to naturally fluctuate around their mean and that, after an extreme fluctuation, subsequent scores tend to regress (or reverse) to the mean. Regression to the mean occurs because performance—both success and failure—is influenced by talent and by luck (Kahneman, 2011). And, unlike talent (or one’s level one the construct of interest), luck is fickle. Regression to the mean can explain why those with depression tend to improve over time without doing anything, and can explain why highly intelligent women tend to marry men who are less intelligent than they are (Kahneman, 2011). Despite peoples’ tendency to ascribe such changes to causal phenomena, regression-related changes do not need a causal explanation—they are a mathematically inevitable consequence that arises from the fact that an outcome is influenced, to some degree, both by one’s level on the construct and by error (Kahneman, 2011).\nAn example of the regression fallacy is the so-called Sports Illustrated or Madden cover jinx curse. The Sports Illustrated or Madden cover jinx curse is the urban legend that players who appear on the cover of Sports Illustrated (the magazine) or Madden (the video game) will perform poorly. But, such a phenomenon can be more simply explained by regression to the mean [Kahneman (2011); G. Smith (2016); archived at https://perma.cc/CZM9-TVFN]. When a player has a superb season, they likely benefited from some degree to good luck, and it is unlikely that they will repeat such a stellar season the following year. Instead, they are likely—at least based on random fluctuation—to regress to their long-term mean.\nApplied to fantasy football, consider that a Quarterback had a 5-touchdown game in Week 1. You are in need of a strong Quarterback, so you drop a solid player to pick him up. However, it is possible that the Quarterback benefited from playing against a week defense in the first game of the season. Future matchups may prove more difficult, and the player is unlikely to sustain such a solid performance consistently throughout the season (i.e., they are likely to regress toward their mean).\n\n14.5.3 Hot Hand Fallacy\nThe “hot hand” is the idea that a player who experiences a successful outcome will have greater chance of success in subsequent attempts. For instance, in basketball, it is widely claimed by coaches, players, and commentators that players who have the hot hand (i.e., who are “on fire”) are more likely to make shots because they made previous shots and are gaining confidence. The hot hand is a supposed example of momentum. However, there is not strong evidence of momentum in sports either at the player or team level; rather, there may be evidence of reversals, the opposite of momentum (Moskowitz & Wertheim, 2011). Streaks certainly exist, but momentum suggests that if a person performs well (or poorly) in a given instance, their performance will influence (and thus predict) their future performance. In general, streaks appear to be largely random variation (i.e., luck/chance) around a player’s mean performance ability.\nEvidence on the hot hand is mixed. Considerable evidence historically has suggested that there is no such thing as a “hot hand” even though many players and fans believe in it (Avugos et al., 2013; Bar-Eli et al., 2006; Gilovich et al., 1985; Moskowitz & Wertheim, 2011; Wetzels et al., 2016). Even though streaks may occur, making a shot may not increase the likelihood of making the next shot; that is, the likelihood of making a field goal seems to be independent of past success (or failure) on recent attempts. Defenses may adjust to guard a “hot” player more closely. However, the findings suggesting no hot hand tend to hold even whether there is no defense (Gilovich et al., 1985). Some recent research, however, has suggested that there may be a small hot hand effect in some contexts (Bocskocsky et al., 2014; Miller & Sanjurjo, 2014; Miller & Sanjurjo, 2024). However, if any such effect exists, the hot hand may be limited to a small subset of players and the effect size of any hot hand effect appears to be small (Pelechrinis & Winston, 2022). Indeed, the slight potential increase in shooting ability tends to be offset by the tendency that “hot” players have to increase their shot difficulty after making a previous shot, so the net hot hand effect appears to be “vanishingly small” (Partnow, 2021).\nIn football, when trying how to distribute the ball among multiple Running Backs, it is not uncommon to hear that a coach wants to give the ball to the Running Back with the “hot hand.” In fantasy football, consider that a player just had a multiple touchdown game. Due to the hot hand fallacy, a manager might continue to start the player because they believe the player is “on fire” and is likely to continue to score at an unsustainable rate.\nIt is important consider whether such a string of strong performances are outliers and if the player may, in future games, regress to the mean. When considering whether strong performances are outliers and may regress to the mean, it is valuable to consider whether the player’s health, skill, or situation has appreciably changed (compared to the player’s earlier, weaker performances). Is the player finally fully healthy? Has the player appreciably improved in some skill that will benefit them in future games? Has the player’s long-term situation improved, such as moving up the depth chart, or receiving more carries/targets that is not tied to a specific opponent or game script? Or, alternatively, do the improvements appear to be driven by transient, game-specific factors, such as a the health of a teammate, the opponents they played, or the game script that ensued? If long-term outlook of the player has appreciably changed due to changes in the fundamentals of a player’s value, such as their health, skill, or situation, it is less likely that such performance improvements will regress over the long run.\n\n14.5.4 Sunk Cost Fallacy\nA sunk cost is a cost (e.g., in money, time, or effort) that has already been incurred and cannot be recovered. For instance, if a person orders an expensive meal at a restaurant, the order is a sunk cost. The sunk cost fallacy is the tendency to continue an endeavor when there is a sunk cost. For instance, when ordering the expensive meal at the restaurant, a person may over-eat so that they feel that they eat their money’s worth of food. The sunk cost fallacy is an example of the commitment bias, an escalation of commitment by which a person continues to commit additional time, effort, or resources to a failing project. People tend to refuse to cut their losses when doing so would admit failure (Kahneman, 2011). It is important to be willing to identify and terminate failing projects to cut one’s losses.\nApplied to fantasy football, consider a situation in which you invest a lot of salary cap or a high draft pick to draft a promising player, but they repeatedly underperform. If you continue to start the player to justify your large investment, instead of benching him in favor of a higher-performing player, you are committing the sunk cost fallacy. It is important to own up to our mistakes; when you make a mistake, the sooner you realize it, own it—for instance, by either benching or dropping the weak-performing player—and move on, the sooner you will be able to replace him with a better-performing player. At the same time, you probably do not want to give too much weight to a player’s performance in any given week, due to random chance that may lead a player to greatly under- or overperform; and whose future peformance is likely to regress (whether positively or negatively) to their mean.\n\n14.5.5 Gambler’s Fallacy\nThe gambler’s fallacy occurs due to an erroneous belief in the law of small numbers. The law of large numbers is a mathematical theorem that the average of a sufficiently large number of independent observations converges to the true value. For instance, if you flip a fair coin 1 million times, it is likely to land heads-up ~50% of the time. The law of small numbers (aka hasty generalization), by contrast, is an erroneous belief that small samples are representative of the populations from which they were drawn. In general, large samples yield more precise results than small samples, whereas extreme results (in either direction—e.g., much higher or much lower than 50% of coins that flip heads-up) are more likely to be obtained from small samples (Kahneman, 2011). For instance, if you flip a coin 10 times, belief in the law of small numbers would lead one to believe that the coin will flip heads-up exactly 5 times out of 10. However, in reality, the chance is less than 1 in 4 (24.6%) that exactly 5 of 10 coin flips turn up heads, as calculated below and as depicted in Figure 14.4:\n\nCodedbinom(\n  x = 5,     # number of coins that flip heads-up\n  size = 10, # how many times you flip a coin\n  prob = 0.5 # probability of a coin flipping heads-up (i.e., fair coin = 50%)\n)\n\n[1] 0.2460938\n\n\nThe dbinom() function in R provides the density of a binomial distribution. A binomial distribution is the probability of a particular number of successes (e.g., coins flipping heads-up) given a certain number of independent trials.\n\nCodeset.seed(52242)\n\nsmallNumFlips &lt;- 10\nlargeNumFlips &lt;- 1000000\nnumSimulations &lt;- 100000\n\nnumHeadsSmallNumFlips &lt;- rbinom(\n  n = numSimulations,\n  size = smallNumFlips,\n  prob = .5\n)\n\nnumHeadsLargeNumFlips &lt;- rbinom(\n  n = numSimulations,\n  size = largeNumFlips,\n  prob = .5\n)\n\nsimulationOfFlippingCoins &lt;- data.frame(\n  numHeadsSmallNumFlips = numHeadsSmallNumFlips,\n  numHeadsLargeNumFlips = numHeadsLargeNumFlips\n)\n\nsimulationOfFlippingCoins &lt;- simulationOfFlippingCoins %&gt;% \n  mutate(\n    proportionHeadsSmallNumFlips = numHeadsSmallNumFlips / smallNumFlips,\n    proportionHeadsLargeNumFlips = numHeadsLargeNumFlips / largeNumFlips,\n    highlight = ifelse(numHeadsSmallNumFlips == 5, \"yes\", \"no\"),\n    extremeSmallNumFlips = ifelse(proportionHeadsSmallNumFlips &lt;= 0.2 | proportionHeadsSmallNumFlips &gt;= 0.8, TRUE, FALSE),\n    extremeLargeNumFlips = ifelse(proportionHeadsLargeNumFlips &lt;= 0.2 | proportionHeadsLargeNumFlips &gt;= 0.8, TRUE, FALSE),\n  )\n\nggplot2::ggplot(\n  data = simulationOfFlippingCoins,\n  mapping = aes(\n    x = numHeadsSmallNumFlips,\n    fill = highlight)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    bins = 11\n  ) +\n  scale_x_continuous(\n    breaks = 0:10\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"yes\" = \"tomato\",\n      \"no\" = \"gray\")\n  ) +\n  labs(\n    x = \"Number of Coins Flipped Heads (out of 10 Coin Flips)\",\n    y = \"Frequency\",\n    title = \"Histogram of Number of Coins that Flip Heads-Up\\nin a Simulation of 10 Coin Flips\\n(with 100,000 Replications).\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 14.4: Histogram of Number of Coins that Flip Heads-Up in a Simulation of 10 Coin Flips (with 100,000 Replications).\n\n\n\n\nAlthough 5 is the modal count of coins that flip heads-up out of 10 flips (i.e., it was more common than any other number), it is less common than the aggregate probability of flipping any number of heads besides 5. The probability of getting any other number of coin flips turning up heads (other than 5) is:\n\nCodedbinom(\n  x = c(0:4, 6:10),\n  size = 10,\n  prob = 0.5\n) %&gt;% sum()\n\n[1] 0.7539063\n\n\nMoreover, among the simulations with only 10 coin flips, nearly 11% of sets had extreme results (≤ 20% heads or ≥ 80% heads):\n\nCodeprop.table(table(simulationOfFlippingCoins$extremeSmallNumFlips))\n\n\n  FALSE    TRUE \n0.89004 0.10996 \n\n\nBy contrast, among the simulations with 1 million coin flips, none of the sets had extreme results:\n\nCodeprop.table(table(simulationOfFlippingCoins$extremeLargeNumFlips))\n\n\nFALSE \n    1 \n\n\nThe gambler’s fallacy is the erroneous belief that future probabilities are influenced by past events, even when the events are independent. For example, a gambler may pay close attention to a particular slot machine. If the slot machine has not paid out in a while, the gambler may believe that the slot machine is about to pay out soon, and may start putting coins in the slots. Or, a gambler on a losing streak may (wrongly) believe that they “are due” and keeps gambling as a result.\nApplied to fantasy football, consider that a Quarterback has had several lousy games in a row. The gambler’s fallacy might lead a manager to start the player under the belief that the player “is due” for a big game, expecting a strong performance from the player merely because they player has not had a good game in a while.\n\n14.5.6 Anecdotal Fallacy\nThe anecdotal fallacy is the error of drawing inferences based on personal experience or singular examples rather than based on stronger forms of evidence. Although people tend to find stories and anecdotes compelling, they are among the weakest forms of evidence, as described in Section 8.5.3. Consider that a fantasy football commentator writes an article about how they won their league by drafting a Quarterback and Wide Receiver by drafting with their first two picks, respectively. The anecdotal fallacy might lead a manager who reads the article to believe that they are more likely to win their league if they follow a similar strategy, despite stronger evidence otherwise.\n\n14.5.7 Narrative Fallacy\nThe narrative fallacy is the error of creating a story with cause-and-effect explanations from random events. According to Kahneman (2011), people find stories more compelling if they a) are simple (as opposed to complex), b) are concrete (as opposed to abstract), c) provide a greater attribution to talent, stupidity, and intentions (compared to luck), and d) focus on a few salient events that occured (rather than the many events that failed to occur). For instance, consider that a player undergoes a challenging start to the season; however, they have higher-performing games in the middle of the season. A fantasy football manager may construct a “causal narrative” that the player overcame obstacles and is now trending toward strong performance for the rest of the season, even though the stronger performance could reflect some natural and random fluctuation.\n\n14.5.8 Conditional Probability Fallacy\nWe describe the conditional probability fallacy in Section 16.3.2 after introducing conditional probability in Section 16.3.1.3.\n\n14.5.9 Ecological Fallacy\nThe ecological fallacy is the error of drawing inferences about an individual from group-level data. For instance, a manager is interested in how age is associated with fantasy performance. They examine the correlation between age and fantasy points and find that age is positively correlated with fantasy points among Quarterbacks. The ecological fallacy would then take this finding to infer that an individual Quarterback is likely to increase in their performance with age. We describe the ecological fallacy more in Section 12.2.1.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasConclusion",
    "href": "cognitive-bias.html#sec-cognitiveBiasConclusion",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.6 Conclusion",
    "text": "14.6 Conclusion\nIn conclusion, there are many heuristics, cognitive bias, and fallacies that people engage in when making judgments and predictions. It is prudent to be aware of these common biases and to work to counteract them. For instance, look for information that challenges or disconfirms your beliefs, and work to incorporate this information into your beliefs. Do your best to pursue observation, data analysis, and data interpretation with an open mind. You never know what important information you might discover if you go in with an open mind. Pay attention to fundamentals of a player’s value, such as their health, skill, or situation when considering whether a player’s performance may regress to the mean. If the strong performances appear to be driven by transient, game-specific factors, such as a the health of a teammate, the opponents they played, or the game script that ensued, future performances may be more likely to regress to the mean. In general, people tend to be overconfident in their predictions. There is considerable luck in fantasy football. Approach the task of prediction with humility; no one is consistently able to accurately predict how well players will perform.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasSessionInfo",
    "href": "cognitive-bias.html#sec-cognitiveBiasSessionInfo",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.7 Session Info",
    "text": "14.7 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_1.9.1    compiler_4.4.3    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.6.1         \n [9] labeling_0.4.3    generics_0.1.3    knitr_1.50        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.10.1     tzdb_0.5.0        rlang_1.1.5      \n[17] stringi_1.8.4     xfun_0.51         timechange_0.3.0  cli_3.6.4        \n[21] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.4.3       \n[25] hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5       evaluate_1.0.3   \n[29] glue_1.8.0        farver_2.1.2      colorspace_2.1-1  rmarkdown_2.29   \n[33] tools_4.4.3       pkgconfig_2.0.3   htmltools_0.5.8.1\n\n\n\n\n\n\nAvugos, S., Köppen, J., Czienskowski, U., Raab, M., & Bar-Eli, M. (2013). The “hot hand” reconsidered: A meta-analytic approach. Psychology of Sport and Exercise, 14(1), 21–27. https://doi.org/10.1016/j.psychsport.2012.07.005\n\n\nBar-Eli, M., Avugos, S., & Raab, M. (2006). Twenty years of “hot hand” research: Review and critique. Psychology of Sport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001\n\n\nBocskocsky, A., Ezekowitz, J., & Stein, C. (2014). The hot hand: A new approach to an old “fallacy.” MIT Sloan Sports Analytics Conference. https://www.sloansportsconference.com/research-papers/the-hot-hand-a-new-approach-to-an-old-fallacy\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nCritcher, C. R., & Rosenzweig, E. L. (2014). The performance heuristic: A misguided reliance on past success when predicting prospects for improvement. Journal of Experimental Psychology: General, 143(2), 480–485. https://doi.org/10.1037/a0034129\n\n\nEnke, B. (2020). What you see is all there is. The Quarterly Journal of Economics, 135(3), 1363–1398. https://doi.org/10.1093/qje/qjaa012\n\n\nFree, H., Groenewold, M. R., & Luckhaupt, S. E. (2020). Lifetime prevalence of self-reported work-related health problems among US workers—United States, 2018. MMWR. Morbidity and Mortality Weekly Report, 69(13), 361–365. https://doi.org/10.15585/mmwr.mm6913a1\n\n\nGet Up ESPN. (2021). @nfldraftscout on the Cowboys’ interest in drafting Kyle Pitts. https://x.com/GetUpESPN/status/1380165126108672001\n\n\nGetty, D., Li, H., Yano, M., Gao, C., & Hosoi, A. E. (2018). Luck and the law: Quantifying chance in fantasy sports and other contests. SIAM Review, 60(4), 869–887. https://doi.org/10.1137/16m1102094\n\n\nGilovich, T., Vallone, R., & Tversky, A. (1985). The hot hand in basketball: On the misperception of random sequences. Cognitive Psychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nKotrba, V. (2020). Heuristics in fantasy sports: Is it profitable to strategize based on favourite of the match? Mind & Society, 19(1), 195–206. https://doi.org/10.1007/s11299-020-00231-7\n\n\nMiller, J. B., & Sanjurjo, A. (2014). A cold shower for the hot hand fallacy. Innocenzo Gasparini Institute for Economic Research. https://repec.unibocconi.it/igier/igi/wp/2014/518.pdf\n\n\nMiller, J. B., & Sanjurjo, A. (2024). A cold shower for the hot hand fallacy: Robust evidence from controlled settings. The Review of Economics and Statistics, 106(6), 1607–1619. https://doi.org/10.1162/rest_a_01280\n\n\nMiller, R. M. (2013). Cognitive bias in fantasy sports: Is your brain sabotaging your team? Xlibris Press.\n\n\nMoore, D. A., & Healy, P. J. (2008). The trouble with overconfidence. Psychological Review, 115(2), 502–517. https://doi.org/10.1037/0033-295X.115.2.502\n\n\nMoskowitz, T. J., & Wertheim, L. J. (2011). Scorecasting: The hidden influences behind how sports are played and games are won. Three Rivers Press.\n\n\nNivison, A. (2021). Florida TE Kyle Pitts draws comparison to Lebron James. https://247sports.com/article/kyle-pitts-lebron-james-2021-nfl-draft-florida-gators-football-163882176\n\n\nPartnow, S. (2021). The midrange theory: Basketball’s evolution in the age of analytics. Triumph Books.\n\n\nPelechrinis, K., & Winston, W. (2022). The hot hand in the wild. PLOS ONE, 17(1), e0261890. https://doi.org/10.1371/journal.pone.0261890\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nSchalter, T. (2022). The NFL preseason is not predictive — but it can often seem that way. https://fivethirtyeight.com/features/the-nfl-preseason-is-not-predictive-but-it-can-often-seem-that-way\n\n\nSmith, B., Sharma, P., & Hooper, P. (2006). Decision making in online fantasy sports communities. Interactive Technology and Smart Education, 3(4), 347–360. https://doi.org/10.1108/17415650680000072\n\n\nSmith, G. (2016). The Sports Illustrated cover jinx. https://www.psychologytoday.com/us/blog/what-the-luck/201610/the-sports-illustrated-cover-jinx\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131. https://doi.org/10.1126/science.185.4157.1124\n\n\nWetzels, R., Tutschkow, D., Dolan, C., Sluis, S. van der, Dutilh, G., & Wagenmakers, E.-J. (2016). A Bayesian test for the hot hand phenomenon. Journal of Mathematical Psychology, 72, 200–209. https://doi.org/10.1016/j.jmp.2015.12.003\n\n\nWoodland, L. M., & Woodland, B. M. (2015). The National Football League season wins total betting market: The impact of heuristics on behavior. Southern Economic Journal, 82(1), 38–54. https://doi.org/10.4284/0038-4038-2013.145",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html",
    "href": "actuarial.html",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "",
    "text": "15.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-judgmentVsActuarialGettingStarted",
    "href": "actuarial.html#sec-judgmentVsActuarialGettingStarted",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "",
    "text": "15.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-approachesToPrediction",
    "href": "actuarial.html#sec-approachesToPrediction",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.2 Approaches to Prediction",
    "text": "15.2 Approaches to Prediction\nThere are two primary approaches to prediction: human judgment and the actuarial (i.e., statistical) method.\n\n15.2.1 Human Judgment\nUsing the judgment method of prediction, all gathered information is collected and formulated into a prediction in the person’s mind. The person selects, measures, and combines information and produces projections solely according to their experience and judgment. For instance, a proclaimed “fantasy expert” might use their experience, expertise, and judgment to make a prediction about how each player will perform by using whatever information and data they deem to be important, aggregating all of this information in their mind to make the prediction for each player. Professional scouts and coaches use judgment when making predictions or selecting players based on their impressions of the players (Den Hartigh et al., 2018). As an example in popular media, in the movie, “Trouble with the Curve”, a professional scout makes a judgment about how well a baseball hitter will do in the major leagues from his impressions of the hitter’s ability based on the sound of the ball off the player’s bat.\n\n15.2.2 Actuarial/Statistical Method\nIn the actuarial or statistical method of prediction, information is gathered and combined systematically in an evidence-based statistical prediction formula. The method is based on equations and data, so both are needed.\nAn example of a statistical method of prediction is the Violence Risk Appraisal Guide (Rice et al., 2013). The Violence Risk Appraisal Guide is used in an attempt to predict violence and is used for parole decisions. For instance, the equation might be something like Equation 15.1:\n\\[\n\\scriptsize\n\\text{violence risk} = \\beta \\cdot \\text{conduct disorder} + \\beta \\cdot \\text{substance use} + \\beta \\cdot \\text{suspended from school} + \\beta \\cdot \\text{childhood aggression} + ...\n\\tag{15.1}\\]\nThen, based on their score and the established cutoffs, a person is given a “low risk”, “medium risk”, or “high risk” designation.\nAn actuarial formula for projecting a Running Back’s rushing yards might be something like Equation 15.2:\n\\[\n\\scriptsize\n\\text{rushing yards} = \\beta \\cdot \\text{rushing yards last season} + \\beta \\cdot \\text{age} + \\beta \\cdot \\text{injury history} + \\beta \\cdot \\text{strength of offensive line} + ...\n\\tag{15.2}\\]\nThe beta weights in the actuarial model reflect the relative weight to assign each predictor. For instance, in predicting rushing yards, a player’s historical performance is likely the strongest predictor, whereas injury history might be a relatively weaker predictor. Thus, we might give historical performance a beta of 3 and injury history a beta of 1 to give a player’s historical performance three times more weight than the player’s injury history in predicting their rushing yards. For generating the actuarial model, you could obtain the beta weights for each predictor from multiple regression, from machine learning, or from prior research on the relative importance of each predictor.\nAs an example of using the actuarial approach, Billy Beane, who was the general manager of the Oakland Athletics at the time, wanted to find ways for his team—which had less finanical resources than its competitors—to compete with teams that hard more money to sign players. Because the team did not have the resources to sign the best players, they had to find to find other ways to find the optimal players that they could afford. So, he used statistical formulas that weight variables, such as on-base percentage and slugging percentage, according to their value for winning games, for the use of selecting players. His approach became well-known based on the Michael Lewis book, “Moneyball: The Art of Winning an Unfair Game”, and the eventual movie, “Moneyball”.\n\n15.2.3 Combining Human Judgment and Statistical Algorithms\nThere are numerous ways in which humans and statistical algorithms could be involved. On one extreme, humans make all judgments. On the other extreme, although humans may be involved in data collection, a statistical formula makes all decisions based on the input data, consistent with an actuarial approach. However, the human judgment and actuarial approaches can be combined in a hybrid way (Dana & Thomas, 2006). For example, to save time and money, a clinical psychologist might use an actuarial approach in all cases, but might only use a judgment approach when the actuarial approach gives a “positive” test. Or, the clinical psychologist might use both human judgment and an actuarial approach independently to see whether they agree. That is, the clinician may make a prediction based on their judgment and might also generate a prediction from an actuarial approach.\nThe challenge is what to do when the human and the algorithm disagree. Hypothetically, humans reviewing and adjusting the results from the statistical algorithm could lead to more accurate prediction. However, human input also could lead to the possibility or exacerbation of biased predictions. In general, with very few exceptions, actuarial approaches are as accurate or more accurate than “expert” judgment (Ægisdóttir et al., 2006; Baird & Wagner, 2000; Dawes et al., 1989; Grove et al., 2000; Grove & Meehl, 1996). This is also likely true with respect to predicting player performance in sports (Den Hartigh et al., 2018). Moreover, the superiority of actuarial approaches to human judgment tends to hold even when the expert is given more information than the actuarial approach (Dawes et al., 1989). In addition, actuarial predictions outperform human judgment even when the human is given the result of the actuarial prediction (Kahneman, 2011). Allowing experts to override actuarial predictions consistently leads to lower predictive accuracy (Garb & Wood, 2019).\nThere is sometimes a misconception that formulas cannot account for qualitative information. However, that is not true. Qualitative information can be scored or coded to be quantified so that it can be included in statistical formulas. For instance, if an expert scout is able to meaningfully assess a player’s cognitive and motivational factors (i.e., the “X factor” or “intangibles”), the scout can score this across multiple players and include these data in the actuarial prediction formula. For instance, the scout could use a rating scale (e.g., 1 = “poor”; 2 = “fair”; 3 = “good”; 4 = “very good”; 5 = “excellent”) to code (i.e., translate) their qualitative judgment into a quantifiable rating that can be integrated with other information in the actuarial formula. That said, the quality of predictions rests on the quality and relevance of the assessment information for the particular prediction decision. If the assessment data are lousy, it is unlikely that a statistical algorithm (or a human for that matter) will make an accurate prediction: “Garbage in, garbage out”. A statistical formula cannot rescue inaccurate assessment data.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-errorsInHumanJudgment",
    "href": "actuarial.html#sec-errorsInHumanJudgment",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.3 Errors in Human Judgment",
    "text": "15.3 Errors in Human Judgment\nHuman judgment is naturally subject to errors. Common heuristics, cognitive biases, and fallacies are described in Chapter 14. Below, I describe a few errors to which human judgment seems particularly prone.\nWhen operating freely, clinicians and medical experts (and humans more generally) tend to overestimate exceptions to the established rules (i.e., the broken leg syndrome). Meehl (1957) acknowledged that there may be some situations where it is glaringly obvious that the statistical formula would be incorrect because it fails to account for an important factor. He called these special cases “broken leg” cases, in which the human should deviate from the formula (i.e., broken leg countervailing). The example goes like this:\n\nIf a sociologist were predicting whether Professor X would go to the movies on a certain night, he might have an equation involving age, academic specialty, and introversion score. The equation might yield a probability of .90 that Professor X goes to the movie tonight. But if the family doctor announced that Professor X had just broken his leg, no sensible sociologist would stick with the equation. Why didn’t the factor of ‘broken leg’ appear in the formula? Because broken legs are very rare, and in the sociologist’s entire sample of 500 criterion cases plus 250 cross-validating cases, he did not come upon a single instance of it. He uses the broken leg datum confidently, because ‘broken leg’ is a subclass of a larger class we may crudely denote as ‘relatively immobilizing illness or injury,’ and movie-attending is a subclass of a larger class of ‘actions requiring moderate mobility.’\n— Meehl (1957, pp. 269–270)\n\nHowever, people too often think that cases where they disagree with the statistical algorithm are broken leg cases. People too often think their case is an exception to the rule. As a result, they too often change the result of the statistical algorithm and are more likely to be wrong than right in doing so. Because actuarial methods are based on actual population levels (i.e., base rates), unique exceptions are not overestimated.\nActuarial predictions are perfectly reliable—they will always return the same conclusion given an identical set of data. The human judge is likely to both disagree with others and with themselves given the same set of symptoms.\nThe decision by an expert (all by all humans) is likely to be influenced by past experiences. Actuarial methods are based on objective algorithms, and past personal experience and personal biases do not factor into any decisions. Humans give weight to less relevant information, and often give too much weight to singular variables. Actuarial formulas do a better job of focusing on relevant variables. Computers are good at factoring in base rates. Humans ignore base rates (base rate neglect).\nComputers are better at accurately weighing predictors and calculating unbiased risk estimates. In an actuarial formula, the relevant predictors are weighted according to their predictive power.\nHumans are typically given no feedback on their judgments. To improve accuracy of judgments, it is important for feedback to be clear, consistent, and timely. Intuition is a form of recognition-based judgment (i.e., recognizing cues that provide access to information in memory). Development of strong intuition depends on the quality and speed of feedback, in addition to having adequate opportunities to practice [i.e., sufficient opportunities to learn the cues; Kahneman (2011)]. The quality and speed of the feedback tend to benefit anesthesiologists who often quickly learn the results of their actions. By contrast, radiologists tend not to receive quality feedback about the accuracy of their diagnoses, including their false-positive and false-negative decisions (Kahneman, 2011).\nIn general, many so-called experts are “pseudo-experts” who do not know the boundaries of their competence—that is, they do not know what they do not know; they have the illusion of validity of their predictions and are overconfident about their predictions (Kahneman, 2011). Yet, many people arrogantly proclaim to have predictive powers, including in low-validity environments such as fantasy football. Indeed, pundits are more likely to be television guests if they are opinionated, clear, and (overly) confident and make big, bold predictions, because they are more entertaining and their predictions seem more compelling [even though they tend to be less accurate than individuals whose thinking is more complex and less decisive; Kahneman (2011); Silver (2012)]. Consider sports pundits like Stephen A. Smith and Skip Bayless who make bold predictions with uber confidence. Optimism and (over)confidence are valued by society (Kahneman, 2011). Nevertheless, true experts know their limits in terms of knowledge and ability to predict.\nTotalProSports.com (2017) provides a video of sports pundits, Stephen A. Smith and Skip Bayless, making bold statements and incorrect predictions:\nVideo of Sports Pundits, Stephen A. Smith and Skip Bayless, Making Bold Statements and Incorrect Predictions. From: https://www.youtube.com/watch?v=lTjBuEPcLlc.\nIntuitions tend to be skilled when a) the environment is regular and predictable, and b) there is opportunity to learn the regularities, cues, and contingencies through extensive practice (Kahneman, 2011). Example domains that meet these conditions supporting intuition include activities such as chess, bridge, and poker, and occupations such as medical providers, athletes, and firefighters. By contrast, fantasy football and other domains such as stock-picking, clinical psychology, and other long-terms forecasts are low-validity environments that are irregular and unpredictable. In environments that do not have stable regularities, intuition cannot be trusted (Kahneman, 2011).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-humansVsComputers",
    "href": "actuarial.html#sec-humansVsComputers",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.4 Humans Versus Computers",
    "text": "15.4 Humans Versus Computers\n\n15.4.1 Advantages of Computers\nHere are some advantages of computers over humans, including “experts”:\n\nComputers can process lots of information simultaneously. So can humans. But computers can to an even greater degree.\nComputers are faster at making calculations.\nGiven the same input, a formula will give the exact same result everytime. Humans’ judgment tends to be inconsistent both across raters and within rater across time, when trying to make judgments or predictions from complex information (Kahneman, 2011). As noted in Section 8.8.3, reliability sets the upper bound for validity, so unreliable judgments cannot be accurate (i.e., valid).\nComputations by computers are error-free (as long as the computations are programmed correctly).\nComputers’ judgments will not be biased by fatigue or emotional responses.\nComputers’ judgments will tend not to be biased in the way that humans’ cognitive biases are. Computers are less likely to be overconfident in their judgments.\nComputers can more accurately weight the set of predictors based on large data sets. Humans tend to give too much weight to singular predictors. Experts may attempt to be clever and to consider complex combinations of predictors, but doing so often reduces validity (Kahneman, 2011). Simple combinations of predictions often outperform more complex combinations (Kahneman, 2011).\n\n15.4.2 Advantages of Humans\nComputers are bad at some things too. Here are some advantages of humans over computers (as of now):\n\nHumans can be better at identifying patterns in data (but also can mistakenly identify patterns where there are none—i.e., illusory correlation).\nHumans can be flexible and take a different approach if a given approach is not working.\nHumans are better at tasks requiring creativity and imagination, such as developing theories that explain phenomena.\nHumans have the ability to reason, which is especially important when dealing with complex, abstract, or open-ended problems, or problems that have not been faced before (or for which we have insufficient data).\nHumans are better able to learn.\nHumans are better at holistic, gestalt processing, including facial and linguistic processing.\n\nThere may be situations in which a human judgment would do better than an actuarial judgment. One situation where human judgment would be important is when no actuarial method exists for the judgment or prediction. For instance, when no actuarial method exists for the diagnosis or disorder (e.g., suicide), it is up to the clinician. However, we could collect data on the outcomes or on clinicians’ judgments to develop an actuarial method that will be more reliable than the clinicians’ judgments. That is, an actuarial method developed based on clinicians’ judgments will be more accurate than clinicians’ judgments. In other words, we do not necessarily need outcome data to develop an actuarial method. We could use the client’s data as predictors of the clinicians’ judgments to develop a structured approach to prediction that weighs factors similarly to clinicians, but with more reliable predictions.\nAnother situation in which human judgment could outperform a statistical algorithm is in true “broken leg” cases, e.g., important and rare events (edge cases) that are not yet accounted for by the algorithm.\nAnother situation in which human judgment could be preferable is if advanced, complex theories exist. Computers have a difficult time adhering to complex theories, so clinicians may be better suited. However, we do not have any of these complex theories in psychology that are accurate. We would need strong theory informed by data regarding causal influences, and accurate measures to assess them. However, no theories in psychology are that good. Nevertheless, predictive accuracy can be improved when considering theory (Garb & Wood, 2019; Silver, 2012).\nIf the prediction requires complex configural relations that a computer will have a difficult time replicating, a clinician’s judgment may be preferred. Although the likelihood that a person can accurately work through these complex relations is theoretically possible, it is highly unlikely. Holistic pattern recognition (such as language and faces) tends to be better by humans than computers. But computers are getting better with holistic pattern recognition through machine learning.\nIn sum, the human seeks to integrate information to make a decision, but is biased.\n\n15.4.3 Comparison of Evidence\nHundreds of studies have examined clinical versus actuarial prediction methods across many disciplines. Findings consistently show that actuarial methods are as accurate or more accurate than human judgment/prediction methods. “There is no controversy in social science that shows such a large body of qualitatively diverse studies coming out so uniformly…as this one” (Meehl, 1986, pp. 373–374).\nActuarial methods are particularly valuable for criterion-referenced assessment tasks, in which the aim is to predict specific events or outcomes (Garb & Wood, 2019). For instance, actuarial methods have shown promise in predicting violence, criminal recidivism, psychosis onset, course of mental disorders, treatment selection, treatment failure, suicide attempts, and suicide (Garb & Wood, 2019). Actuarial methods are especially important to use in low-validity environments (like fantasy football) in which there is considerable uncertainty and unpredictability (Kahneman, 2011).\nMoreover, actuarial methods are explicit; they can be transparent and lead to informed scientific criticism to improve them. By contrast, human judgment methods are not typically transparent; human judgment relies on mental processes that are often difficult to specify.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-whyHumanJudgmentIsMoreWidelyUsed",
    "href": "actuarial.html#sec-whyHumanJudgmentIsMoreWidelyUsed",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.5 Why Judgment is More Widely Used Than Statistical Formulas",
    "text": "15.5 Why Judgment is More Widely Used Than Statistical Formulas\nDespite actuarial methods being generally more accurate than human judgment, judgment is much more widely used by clinicians. There are several reasons why actuarial methods have not caught on; one reason is professional traditions. Experts in any field do not like to think that a computer could outperform them. Some practitioners argue that judgment/prediction is an “art form” and that using a statistical formula is treating people like a number. However, using an approach (i.e., human judgment) that systematically leads to less accurate decisions and predictions is an ethical problem.\nSome clinicians do not think that group averages (e.g., in terms of which treatment is most effective) apply to an individual client. This invokes the distinction between nomothetic (group-level) inferences and idiographic (individual-level) inferences. However, the scientific evidence and probability theory strongly indicate that it is better to generalize from group-level evidence than throwing out all the evidence and taking the approach of “anything goes.” Clinicians frequently believe the broken leg fallacy, i.e., thinking that your client is an exception to the algorithmic prediction. In most cases, deviating from the statistical formula will result in less accurate predictions. People tend to overestimate the probability of low base rate conditions and events.\nAnother reason why actuarial methods have not caught on is the belief that receiving a treatment is the only thing that matters. But it is an empirical question which treatment is most effective for whom. What if we could do better? For example, we could potentially use a formula to identify the most effective treatment for a client. Some treatments are no better than placebo; other treatments are actually harmful (Lilienfeld, 2007; Williams et al., 2021).\nAnother reason why judgment methods are more widely used than actuarial methods is that so-called “experts” (and people in general) show overconfidence in their predictions—clinicians, experts, and humans in general think they are more accurate than they actually are. We see this when examining their calibration; their predictions tend to be miscalibrated. For example, things they report with 80% confidence occur less than 80% of the time, an example of overprecision in their predictions. Humans will sometimes be correct by chance, and they tend to mis-attribute that to their skill; humans tend to remember the successes and forget the failures.\nAnother argument against using actuarial methods is that “no methods exist”. In some cases, that is true—actuarial methods do not yet exist for some prediction problems. However, one can always create an algorithm of the experts’ judgments, even if one does not have access to the outcome information. A model of clinicians’ responses tends to be more accurate than clinicians’ judgments themselves because the model gives the same outcome with the same input data—i.e., it is perfectly reliable.\nAnother argument from some clinicians is that, “My job is to understand, not to predict”. But what kind of understanding does not involve predictions? Accurate predictions help in understanding. Knowing how people would perform in different conditions is the same thing as good understanding.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-stepsActuarial",
    "href": "actuarial.html#sec-stepsActuarial",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.6 Steps to Conduct Actuarial Approaches",
    "text": "15.6 Steps to Conduct Actuarial Approaches\nHere are several steps to conduct actuarial approaches (Den Hartigh et al., 2018; Kahneman, 2011):\n\nDetermine a set of relevant variables to measure\nDetermine how you will combine the variables\n\nDo some variables have more weight than other variables?\n\n\nDetermine how the variables will be scored\n\ne.g., a 7-point likert scale\n\n\nCombine the scores based on the pre-specified formula\nUse the final score to make your prediction (for selecting players)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-challengesDataDrivenApproaches",
    "href": "actuarial.html#sec-challengesDataDrivenApproaches",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.7 Challenges of Data-Driven Approaches and How to Address",
    "text": "15.7 Challenges of Data-Driven Approaches and How to Address\nThere are various challenges of data-driven approaches. First, they are sometimes not interpretable or consistent with theory. Second, they tend to overfit the data. Overfitting is described in Section 11.6. Third, as a result of overfitting the data, they tend to show shrinkage.\n\n15.7.1 Shrinkage\nIn general, there is often shrinkage of estimates from training data set to a test data set. Shrinkage is when variables with stronger predictive power in the original data set tend to show somewhat smaller predictive power (smaller regression coefficients) when applied to new groups. Shrinkage reflects a model overfitting—i.e., when the model explains error variance by capitalizing on chance. Shrinkage is especially likely when the original sample is small and/or unrepresentative and the number of variables considered for inclusion is large. To help minimize the extent of shrinkage, it is recommended to apply cross-validation.\n\n15.7.2 Cross-Validation\nCross-validation with large, representative samples can help evaluate the amount of shrinkage of estimates, particularly for more complex models such as machine learning models (Ursenbach et al., 2019). Ideally, cross-validation would be conducted with a separate sample (external cross-validation) to see the generalizability of estimates. However, you can also do internal cross-validation. For example, you can perform k-fold cross-validation, where you:\n\nsplit the data set into k groups\nfor each unique group:\n\ntake the group as a hold-out data set (also called a test data set)\ntake the remaining groups as a training data set\nfit a model on the training data set and evaluate it on the test data set\nafter all k-folds have been used as the test data set, and all models have been fit, you average the estimates across the models, which presumably yields more robust, generalizable estimates",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-bestActuarialApproaches",
    "href": "actuarial.html#sec-bestActuarialApproaches",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.8 Best Actuarial Approaches to Prediction",
    "text": "15.8 Best Actuarial Approaches to Prediction\nThe best actuarial models tend to be relatively simple (parsimonious), that can account for one or several of the most important predictors and their optimal weightings, and that account for the base rate of the phenomenon. Multiple regression and/or prior literature can be used to identify the weights of various predictors. Even unit-weighted formulas (formulas whose predictor variables are equally weighted with a weight of one) can sometimes generalize better to other samples than complex weightings (Garb & Wood, 2019; Kahneman, 2011). Differential weightings sometimes capture random variance and over-fit the model, thus leading to predictive accuracy shrinkage in cross-validation samples (Garb & Wood, 2019), as described below. The choice of predictor variables often matters more than their weighting.\nAn emerging technique that holds promise for increasing predictive accuracy of actuarial methods is machine learning (Garb & Wood, 2019). However, one challenge of some machine learning techniques is that they are like a “black box” and are not transparent, which raises ethical concerns (Garb & Wood, 2019). Moreover, machine learning also tends to lead to overfitting and shrinkage. machine learning may be most valuable when the data available are complex and there are many predictor variables (Garb & Wood, 2019), and when the model is validated with cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-conclusion-actuarial",
    "href": "actuarial.html#sec-conclusion-actuarial",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.9 Conclusion",
    "text": "15.9 Conclusion\nIn general, it is better to develop and use structured, actuarial approaches than informal approaches that rely on human judgment or judgment by “so-called” experts. Actuarial approaches to prediction tend to be as accurate or more accurate than expert judgment. Nevertheless, in many domains, human judgment tends to be much more widely used than actuarial approaches.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-judgmentVsActuarialSessionInfo",
    "href": "actuarial.html#sec-judgmentVsActuarialSessionInfo",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.10 Session Info",
    "text": "15.10 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.3    fastmap_1.2.0     cli_3.6.4        \n [5] tools_4.4.3       htmltools_0.5.8.1 rmarkdown_2.29    knitr_1.50       \n [9] jsonlite_1.9.1    xfun_0.51         digest_0.6.37     rlang_1.1.5      \n[13] evaluate_1.0.3   \n\n\n\n\n\n\nÆgisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S., Anderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K., Walker, B. S., Cohen, G., & Rush, J. D. (2006). The meta-analysis of clinical judgment project: Fifty-six years of accumulated research on clinical versus statistical prediction. The Counseling Psychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875\n\n\nBaird, C., & Wagner, D. (2000). The relative validity of actuarial- and consensus-based risk assessment systems. Children and Youth Services Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDana, J., & Thomas, R. (2006). In defense of clinical judgment … and mechanical prediction. Journal of Behavioral Decision Making, 19(5), 413–428. https://doi.org/10.1002/bdm.537\n\n\nDawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus actuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nGarb, H. N., & Wood, J. M. (2019). Methodological advances in statistical prediction. Psychological Assessment, 31(12), 1456–1466. https://doi.org/10.1037/pas0000673\n\n\nGrove, W. M., & Meehl, P. E. (1996). Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical–statistical controversy. Psychology, Public Policy, and Law, 2(2), 293–323. https://doi.org/10.1037/1076-8971.2.2.293\n\n\nGrove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C. (2000). Clinical versus mechanical prediction: A meta-analysis. Psychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nLilienfeld, S. O. (2007). Psychological treatments that cause harm. Perspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1986). Causes and effects of my disturbing little book. Journal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6\n\n\nRice, M. E., Harris, G. T., & Lang, C. (2013). Validation of and revision to the VRAG and SORAG: The Violence Risk Appraisal Guide—Revised (VRAG-R). Psychological Assessment, 25(3), 951–965. https://doi.org/10.1037/a0032878\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.\n\n\nTotalProSports.com. (2017). 10 most ridiculous things ever said by Stephen A. Smith or Skip Bayless. https://www.youtube.com/watch?v=lTjBuEPcLlc\n\n\nUrsenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D., Kosteniuk, J., & Spiteri, R. J. (2019). Scoring algorithms for a computer-based cognitive screening tool: An illustrative example of overfitting machine learning approaches and the impact on estimates of classification accuracy. Psychological Assessment, 31(11), 1377–1382. https://doi.org/10.1037/pas0000764\n\n\nWilliams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., & Sakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific review of evidential value. Clinical Psychology: Science and Practice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "base-rates.html",
    "href": "base-rates.html",
    "title": "16  Base Rates",
    "section": "",
    "text": "16.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesGettingStarted",
    "href": "base-rates.html#sec-baseRatesGettingStarted",
    "title": "16  Base Rates",
    "section": "",
    "text": "16.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesOverview",
    "href": "base-rates.html#sec-baseRatesOverview",
    "title": "16  Base Rates",
    "section": "\n16.2 Overview",
    "text": "16.2 Overview\nPredicting player performance is a complex prediction task. Performance is probabilistically influenced by many processes, including processes internal to the player in addition to external processes. Moreover, people’s performance occurs in the context of a dynamic system with nonlinear, probabilistic, and cascading influences that change across time. The ever-changing system makes behavior challenging to predict. And, similar to chaos theory, one small change in the system can lead to large differences later on. Moreover, there are important factors to keep in mind when making predictions.\nLet’s consider a prediction example, assuming the following probabilities:\n\nThe probability of contracting HIV is .3%\nThe probability of a positive test for HIV is 1%\nThe probability of a positive test if you have HIV is 95%\n\nWhat is the probability of HIV if you have a positive test?\nAs we will see, the probability is: \\(\\frac{95\\% \\times .3\\%}{1\\%} = 28.5\\%\\). So based on the above probabilities, if you have a positive test, the probability that you have HIV is 28.5%. Most people tend to vastly overestimate the likelihood that the person has HIV in this example. Why? Because they do not pay enough attention to the base rate (in this example, the base rate of HIV is .3%).\nIn general, people tend to overestimate the likelihood of low base-rate events. That is, if the base rate of an event or condition—such as schizophrenia—is low (e.g., ~0.5%), people overestimate the likelihood that a person has schizophrenia when given specific information about the person such as their symptoms and history.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-probability",
    "href": "base-rates.html#sec-probability",
    "title": "16  Base Rates",
    "section": "\n16.3 Issues Around Probability",
    "text": "16.3 Issues Around Probability\n\n16.3.1 Types of Probabilities\nIt is important to distinguish between different types of probabilities: marginal probabilities, joint probabilities, and conditional probabilities.\n\n16.3.1.1 Base Rate (Marginal Probability)\nThe base rate is a marginal probability, which is the general probability of an event irrespective of other things. For instance, the base rate of HIV is the probability of developing HIV. In the U.S., the prevalence rate of HIV is ~0.4% of the adult population [AIDSVu (2022); archived at https://perma.cc/8GE6-GAPC].\nFor instance, we can consider the following marginal probabilities:\n\\(P(C_i)\\) is the probability (i.e., base rate) of a classification, \\(C\\), independent of other things. A base rate is often used as the “prior probability” in a Bayesian model. In our example above, \\(P(C_i)\\) is the base rate (i.e., prevalence) of HIV in the population: \\(P(\\text{HIV}) = .3\\%\\). \\(P(R_i)\\) is the probability (base rate) of a response, \\(R\\), independent of other things. In the example above, \\(P(R_i)\\) is the base rate of a positive test for HIV: \\(P(\\text{positive test}) = 1\\%\\). The base rate of a positive test is known as the positivity rate or selection ratio.\n\n16.3.1.2 Joint Probability\nA joint probability is the probability of two (or more) events occurring simultaneously. For instance, the probability of events \\(A\\) and \\(B\\) both occurring together is \\(P(A, B)\\). A joint probability can be calculated using the marginal probability of each event, as in Equation 16.1:\n\\[\nP(A, B) = P(A) \\cdot P(B)\n\\tag{16.1}\\]\nConversely (and rearranging the terms for the calculation of conditional probability), a joint probability can also be calculated using the conditional probability and marginal probability, as in Equation 16.2:\n\\[\nP(A, B) = P(A | B) \\cdot P(B)\n\\tag{16.2}\\]\n\n16.3.1.3 Conditional Probability\nA conditional probability is the probability of one event occurring given the occurrence of another event. Conditional probabilities are written as: \\(P(A | B)\\). This is read as the probability that event \\(A\\) occurs given that event \\(B\\) occurred. For instance, we can consider the following conditional probabilities:\n\\(P(C | R)\\) is the probability of a classification, \\(C\\), given a response, \\(R\\). In other words, \\(P(C | R)\\) is the probability of having HIV given a positive test: \\(P(\\text{HIV} | \\text{positive test})\\). \\(P(R | C)\\) is the probability of a response, \\(R\\), given a classification, \\(C\\). In the example above, \\(P(R | C)\\) is the probability of having a positive test given that a person has HIV: \\(P(\\text{positive test} | \\text{HIV}) = 95\\%\\).\nA conditional probability can be calculated using the joint probability and marginal probability (base rate), as in Equation 16.3:\n\\[\nP(A, B) = P(A | B) \\cdot P(B)\n\\tag{16.3}\\]\n\n16.3.2 Confusion of the Inverse\nA conditional probability is not the same thing as its reverse (or inverse) conditional probability. Unless the base rate of the two events (\\(C\\) and \\(R\\)) are the same, \\(P(C | R) \\neq P(R | C)\\). However, people frequently make the mistake of thinking that two inverse conditional probabilities are the same. This mistake is known as the “confusion of the inverse”, or the “inverse fallacy”, or the “conditional probability fallacy”. The confusion of inverse probabilities is the logical error of representative thinking that leads people to assume that the probability of \\(C\\) given \\(R\\) is the same as the probability of \\(R\\) given C, even though this is not true. As a few examples to demonstrate the logical fallacy, if 93% of breast cancers occur in high-risk women, this does not mean that 93% of high-risk women will eventually get breast cancer. As another example, if 77% of car accidents take place within 15 miles of a driver’s home, this does not mean that you will get in an accident 77% of times you drive within 15 miles of your home.\nWhich car is the most frequently stolen? It is often the Honda Accord or Honda Civic—probably because they are among the most popular/commonly available cars. The probability that the car is a Honda Accord given that a car was stolen (\\(p(\\text{Honda Accord } | \\text{ Stolen})\\)) is what the media reports and what the police care about. However, that is not what buyers and car insurance companies should care about. Instead, they care about the probability that the car will be stolen given that it is a Honda Accord (\\(p(\\text{Stolen } | \\text{ Honda Accord})\\)).\nApplied to fantasy football, the probability that a given player will be injured given that he is a Running Back (\\(p(\\text{Injured } | \\text{ RB})\\)) is not the same as the probability that a given player is a Running Back given that he is injured (\\(p(\\text{RB } | \\text{ Injured})\\)).\n\n16.3.3 Bayes’ Theorem\n\n16.3.3.1 Standard Formulation\nAn alternative way of calculating a conditional probability is using the inverse conditional probability (instead of the joint probability). This is known as Bayes’ theorem. Bayes’ theorem can help us calculate a conditional probability of some classification, \\(C\\), given some response, \\(R\\), if we know the inverse conditional probability and the base rate (marginal probability) of each. Bayes’ theorem is in Equation 16.4:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}\n\\end{aligned}\n\\tag{16.4}\\]\nOr, equivalently (rearranging the terms):\n\\[\n\\begin{aligned}\n  \\frac{P(C | R)}{P(R | C)} = \\frac{P(C_i)}{P(R_i)}\n\\end{aligned}\n\\tag{16.5}\\]\nOr, equivalently (rearranging the terms):\n\\[\n\\begin{aligned}\n  \\frac{P(C | R)}{P(C_i)} = \\frac{P(R | C)}{P(R_i)}\n\\end{aligned}\n\\tag{16.6}\\]\nMore generally, Bayes’ theorem has been described as:\n\\[\n\\begin{aligned}\n  P(H | E) &= \\frac{P(E | H) \\cdot P(H)}{P(E)} \\\\\n  \\text{posterior probability} &= \\frac{\\text{likelihood} \\times \\text{prior probability}}{\\text{model evidence}}\n\\end{aligned}\n\\tag{16.7}\\]\nwhere \\(H\\) is the hypothesis, and \\(E\\) is the evidence—the new information that was not used in computing the prior probability.\nIn Bayesian terms, the posterior probability is the conditional probability of one event occurring given another event—it is the updated probability after the evidence is considered. In this case, the posterior probability is the probability of the classification occurring (\\(C\\)) given the response (\\(R\\)). The likelihood is the inverse conditional probability—the probability of the response (\\(R\\)) occurring given the classification (\\(C\\)). The prior probability is the marginal probability of the event (i.e., the classification) occurring, before we take into account any new information. The model evidence is the marginal probability of the other event occurring—i.e., the marginal probability of seeing the evidence.\nBayes’ theorem provides the foundation for a paradigm of statistics called Bayesian statistics, which (unlike frequentist statistics) does not use p-values.\nIn the HIV example above, we can calculate the conditional probability of HIV given a positive test using three terms: the conditional probability of a positive test given HIV (i.e., the sensitivity of the test), the base rate of HIV, and the base rate of a positive test for HIV. The conditional probability of HIV given a positive test is in Equation 16.8:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{HIV} | \\text{positive test}) &= \\frac{P(\\text{positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{positive test})} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{base rate of HIV}}{\\text{base rate of positive test}} \\\\\n  &= \\frac{95\\% \\times .3\\%}{1\\%} = \\frac{.95 \\times .003}{.01}\\\\\n  &= 28.5\\%\n\\end{aligned}\n\\tag{16.8}\\]\nThe petersenlab package (Petersen, 2025a) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .95,\n  pA = .003,\n  pB = .01)\n\n[1] 0.285\n\n\nThus, assuming the probabilities in the example above, the conditional probability of having HIV if a person has a positive test is 28.5%. Given a positive test, chances are higher than not that the person does not have HIV.\nNow let’s see what happens if the person tests positive a second time. We would revise our “prior probability” for HIV from the general prevalence in the population (0.3%) to be the “posterior probability” of HIV given a first positive test (28.5%). This is known as Bayesian updating. We would also update the “evidence” to be the marginal probability of getting a second positive test.\nIf we do not know a marginal probability (i.e., base rate) of an event (e.g., getting a second positive test), we can calculate a marginal probability with the law of total probability using conditional probabilities and the marginal probability of another event (e.g., having HIV). According to the law of total probability, the probability of getting a positive test is the probability that a person with HIV gets a positive test (i.e., sensitivity) times the base rate of HIV plus the probability that a person without HIV gets a positive test (i.e., false positive rate) times the base rate of not having HIV, as in Equation 16.9:\n\\[\n\\begin{aligned}\nP(\\text{not } C_i) &= 1 - P(C_i) \\\\\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\\n  1\\% &= 95\\% \\times .3\\% + P(R | \\text{not } C) \\times 99.7\\% \\\\\n\\end{aligned}\n\\tag{16.9}\\]\nIn this case, we know the marginal probability (\\(P(R_i)\\)), and we can use that to solve for the unknown conditional probability that reflects the false positive rate (\\(P(R | \\text{not } C)\\)), as in Equation 16.10:\n\\[\n\\scriptsize\n\\begin{aligned}\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) && \\\\\n  P(R_i) - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &= P(R | C) \\cdot P(C_i) && \\text{Move } P(R | \\text{not } C) \\text{ to the left side} \\\\\n  - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &= P(R | C) \\cdot P(C_i) - P(R_i) && \\text{Move } P(R_i) \\text{ to the right side} \\\\\n  P(R | \\text{not } C) \\cdot P(\\text{not } C_i) &= P(R_i) - [P(R | C) \\cdot P(C_i)] && \\text{Multiply by } -1 \\\\\n  P(R | \\text{not } C) &= \\frac{P(R_i) - [P(R | C) \\cdot P(C_i)]}{P(\\text{not } C_i)} && \\text{Divide by } P(R | \\text{not } C) \\\\\n  &= \\frac{1\\% - [95\\% \\times .3\\%]}{99.7\\%} = \\frac{.01 - [.95 \\times .003]}{.997}\\\\\n  &= .7171515\\% \\\\\n\\end{aligned}\n\\tag{16.10}\\]\nWe can then estimate the marginal probability of the event, substititing in \\(P(R | \\text{not } C)\\), using the law of total probability. The petersenlab package (Petersen, 2025a) contains the pA() function that estimates the marginal probability of one event, \\(A\\).\n\nCodepetersenlab::pA(\n  pAgivenB = .95,\n  pB = .003,\n  pAgivenNotB = .007171515)\n\n[1] 0.01\n\n\nThe petersenlab package (Petersen, 2025a) contains the pBgivenNotA() function that estimates the probability of one event, \\(B\\), given that another event, \\(A\\), did not occur.\n\nCodepetersenlab::pBgivenNotA(\n  pBgivenA = .95,\n  pA = .003,\n  pB = .01)\n\n[1] 0.007171515\n\n\nWith this conditional probability (\\(P(R | \\text{not } C)\\)), the updated marginal probability of having HIV (\\(P(C_i)\\)), and the updated marginal probability of not having HIV (\\(P(\\text{not } C_i)\\)), we can now calculate an updated estimate of the marginal probability of getting a second positive test. The probability of getting a second positive test is the probability that a person with HIV gets a second positive test (i.e., sensitivity) times the updated probability of HIV plus the probability that a person without HIV gets a second positive test (i.e., false positive rate) times the updated probability of not having HIV, as in Equation 16.11:\n\\[\n\\begin{aligned}\n  P(R_{i}) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\\n  &= 95\\% \\times 28.5\\% + .7171515\\% \\times 71.5\\% = .95 \\times .285 + .007171515 \\times .715 \\\\\n  &= 27.58776\\%\n\\end{aligned}\n\\tag{16.11}\\]\nThe petersenlab package (Petersen, 2025a) contains the pB() function that estimates the marginal probability of one event, \\(B\\).\n\nCodepetersenlab::pB(\n  pBgivenA = .95,\n  pA = .285,\n  pBgivenNotA = .007171515)\n\n[1] 0.2758776\n\n\nWe then substitute the updated marginal probability of HIV (\\(P(C_i)\\)) and the updated marginal probability of getting a second positive test (\\(P(R_i)\\)) into Bayes’ theorem to get the probability that the person has HIV if they have a second positive test (assuming the errors of each test are independent, i.e., uncorrelated), as in Equation 16.12:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{HIV} | \\text{a second positive test}) &= \\frac{P(\\text{a second positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{a second positive test})} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{updated base rate of HIV}}{\\text{updated base rate of positive test}} \\\\\n  &= \\frac{95\\% \\times 28.5\\%}{27.58776\\%} \\\\\n  &= 98.14\\%\n\\end{aligned}\n\\tag{16.12}\\]\nThe petersenlab package (Petersen, 2025a) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .95,\n  pA = .285,\n  pB = .2758776)\n\n[1] 0.9814135\n\n\nThus, a second positive test greatly increases the posterior probability that the person has HIV from 28.5% to over 98%.\nAs seen in the rearranged formula in Equation 16.5, the ratio of the conditional probabilities is equal to the ratio of the base rates. Thus, it is important to consider base rates. People have a strong tendency to ignore (or give insufficient weight to) base rates when making predictions. The failure to consider the base rate when making predictions when given specific information about a case is known as the base rate fallacy or as base rate neglect. For example, people tend to say that the probability of a rare event is more likely than it actually is given specific information.\nAs seen in the rearranged formula in Equation 16.6, the inverse conditional probabilities (\\(P(C | R)\\) and \\(P(R | C)\\)) are not equal unless the base rates of \\(C\\) and \\(R\\) are the same. If the base rates are not equal, we are making at least some prediction errors. If \\(P(C_i) &gt; P(R_i)\\), our predictions must include some false negatives. If \\(P(R_i) &gt; P(C_i)\\), our predictions must include some false positives.\n\n16.3.3.2 Alternative Formulation\nUsing the law of total probability, we can substitute the calculation of the marginal probability (\\(P(R_i)\\)) into Bayes’ theorem to get an alternative formulation of Bayes’ theorem, as in Equation 16.13:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i)} \\\\\n  &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]}\n\\end{aligned}\n\\tag{16.13}\\]\nInstead of using marginal probability (base rate) of \\(R\\), as in the original formulation of Bayes’ theorem, it uses the conditional probability, \\(P(R|\\text{not } C)\\). Thus, it uses three terms: two conditional probabilities—\\(P(R|C)\\) and \\(P(R|\\text{not } C)\\)—and one marginal probability, \\(P(C_i)\\).\nLet us see how the alternative formulation of Bayes’ theorem applies to the HIV example above. We can calculate the probability of HIV given a positive test using three terms: the conditional probability that a person with HIV gets a positive test (i.e., sensitivity), the conditional probability that a person without HIV gets a positive test (i.e., false positive rate), and the base rate of HIV. Using the \\(P(R|\\text{not } C)\\) calculated in Equation 16.10, the conditional probability of HIV given a single positive test is in Equation 16.14:\n\\[\n\\small\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{base rate of HIV}}{\\text{sensitivity of test} \\times \\text{base rate of HIV} + \\text{false positive rate of test} \\times (1 - \\text{base rate of HIV})} \\\\\n  &= \\frac{95\\% \\times .3\\%}{95\\% \\times .3\\% + .7171515\\% \\times (1 - .3\\%)} = \\frac{.95 \\times .003}{.95 \\times .003 + .007171515 \\times (1 - .003)}\\\\\n  &= 28.5\\%\n\\end{aligned}\n\\tag{16.14}\\]\nThe petersenlab package (Petersen, 2025a) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .003,\n  pBgivenNotA = .007171515)\n\n[1] 0.285\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .003,\n  pBgivenNotA = pBgivenNotA(\n    pBgivenA = .95,\n    pA = .003,\n    pB = .01))\n\n[1] 0.285\n\n\nTo calculate the conditional probability of HIV given a second positive test, we update our priors because the person has now tested positive for HIV. We update the prior probability of HIV (\\(P(C_i)\\)) based on the posterior probability of HIV after a positive test (\\(P(C | R)\\)) that we calculated above. We can calculate the conditional probability of HIV given a second positive test using three terms: the conditional probability that a person with HIV gets a positive test (i.e., sensitivity; which stays the same), the conditional probability that a person without HIV gets a positive test (i.e., false positive rate; which stays the same), and the updated marginal probability of HIV. The conditional probability of HIV given a second positive test is in Equation 16.15:\n\\[\n\\scriptsize\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{updated base rate of HIV}}{\\text{sensitivity of test} \\times \\text{updated base rate of HIV} + \\text{false positive rate of test} \\times (1 - \\text{updated base rate of HIV})} \\\\\n  &= \\frac{95\\% \\times 28.5\\%}{95\\% \\times 28.5\\% + .7171515\\% \\times (1 - 28.5\\%)} = \\frac{.95 \\times .285}{.95 \\times .285 + .007171515 \\times (1 - .285)}\\\\\n  &= 98.14\\%\n\\end{aligned}\n\\tag{16.15}\\]\nThe petersenlab package (Petersen, 2025a) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .285,\n  pBgivenNotA = .007171515)\n\n[1] 0.9814134\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .285,\n  pBgivenNotA = pBgivenNotA(\n    pBgivenA = .95,\n    pA = .003,\n    pB = .01))\n\n[1] 0.9814134\n\n\n\n16.3.3.3 Interim Summary\nIn sum, the marginal probability, including the prior probability or base rate, should be weighed heavily in predictions unless there are sufficient data to indicate otherwise, i.e., to update the posterior probability based on new evidence. Bayes’ theorem specifies how prior beliefs (i.e., base rate informations) should be integrated with the predictive accuracy of the evidence to make predictions. It thus provides a powerful tool to anchor predictions to the base rate unless sufficient evidence changes the posterior probability (by updating the evidence and prior probability). In general, you should anchor your predictions to the base rate and adjust from there. As noted by Kahneman (2011), if you have doubts about the quality of the evidence for a particular prediction question, keep your predictions close to the base rate, and modify them only modifying them based on the new information.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-cabExample",
    "href": "base-rates.html#sec-cabExample",
    "title": "16  Base Rates",
    "section": "\n16.4 Cab Example",
    "text": "16.4 Cab Example\nBelow is an example:\n\nA cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data:\n\n85% of the cabs in the city are Green and 15% are Blue.\nA witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time.\n\nWhat is the probability that the cab involved in the accident was Blue rather than Green?\n— Kahneman (2011, p. 166)\n\nThus, we know the following:\n\\[\n\\begin{aligned}\n  P(\\text{Blue}) &= .15 && \\text{prior probability of a Blue cab}\\\\\n  P(\\text{Green}) &= .85 && \\text{prior probability of a Green cab}\\\\\n  P(\\text{Correct}|\\text{Blue}) &= .80 && \\text{probability the witness correctly identifies a Blue cab}\\\\\n  P(\\text{Correct}|\\text{Green}) &= .80 && \\text{probability the witness correctly identifies a Green cab}\\\\\n  P(\\text{Incorrect}|\\text{Blue}) &= .20 && \\text{probability the witness incorrectly identifies a Blue cab}\\\\\n  P(\\text{Incorrect}|\\text{Green}) &= .20 && \\text{probability the witness incorrectly identifies a Green cab}\\\\\n\\end{aligned}\n\\tag{16.16}\\]\nWe want to know the probability that the cab involved in the accident was Blue, given that the witness identified it as Blue (\\(P(\\text{Blue}|\\text{Identified as Blue})\\)). To estimate this probability, we can apply Bayes’ theorem to estimate the posterior probability:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}\\\\\n  P(\\text{Blue}|\\text{Identified as Blue}) &= \\frac{P(\\text{Identified as Blue}|\\text{Blue}) \\cdot P(\\text{Blue})}{P(\\text{Identified as Blue})}\n\\end{aligned}\n\\tag{16.17}\\]\nWe can compute the term in the denominator (\\(P(\\text{Identified as Blue})\\)) using the law of total probability (described in Section 16.3.3).\n\\[\n\\begin{aligned}\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i)\\\\\n  P(R_i) &= P(\\text{Identified as Blue}|\\text{Blue}) \\cdot P(\\text{Blue}) + P(\\text{Identified as Blue}|\\text{Green}) \\cdot P(\\text{Green})\\\\\n  0.29 &= (.80 \\times .15) + (.20 \\times .85) \\\\\n\\end{aligned}\n\\tag{16.18}\\]\n\nCodepetersenlab::pA(\n  pAgivenB = .80,\n  pB = .15,\n  pAgivenNotB = .20)\n\n[1] 0.29\n\n\nWe can now substitute this value into the denominator of Bayes’ theorem to estimate the posterior probability:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}\\\\\n  P(\\text{Blue}|\\text{Identified as Blue}) &= \\frac{P(\\text{Identified as Blue}|\\text{Blue}) \\cdot P(\\text{Blue})}{P(\\text{Identified as Blue})}\\\\\n  0.414 &= \\frac{0.80 \\times 0.15}{0.29}\n\\end{aligned}\n\\tag{16.19}\\]\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .80,\n  pA = .15,\n  pB = .29)\n\n[1] 0.4137931\n\n\nThus, there was a 41.4% probability that the car involved in the accident was Blue rather than Green. However, when faced with this problem, people tend to ignore the base rate and go with the witness (Kahneman, 2011). According to Kahneman (2011), the most frequent response to this question regarding is that there is an 80% that the car was Blue.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-nateSilverExamples",
    "href": "base-rates.html#sec-nateSilverExamples",
    "title": "16  Base Rates",
    "section": "\n16.5 Nate Silver Examples",
    "text": "16.5 Nate Silver Examples\nSilver (2012) provides several examples that leverage the alternative formulation of Bayes’ theorem provided in Equation 16.13 and summarized below:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]}\n\\end{aligned}\n\\tag{16.20}\\]\nIn each example, the formula uses three elements to calculate the probability that the hypothesis is true:\n\nthe conditional probability the likelihood of observing the evidence, \\(R\\), given that the hypothesis, \\(C\\), is true (i.e., \\(P(R|C)\\); true positive rate)\nthe conditional probability the likelihood of observing the evidence, \\(R\\), given that the hypothesis, \\(C\\), is false (i.e., \\(P(R | \\text{not } C)\\); false positive rate)\nthe marginal probability (base rate) of the event occurring (i.e., the prior probability of the hypothesis, \\(C\\), being true; \\(P(C_i)\\))\n\nThus, the formula uses the base rate, the true positive rate (sensitivity), and the false positive rate. The ratio of the true positive rate to the false positive rate is called the positive likelihood ratio, and is used in Bayesian updating.\n\n16.5.1 Example 1: Is Your Partner Cheating on You?\nExample 1: You came home and found a strange pair of underwear in your underwear drawer. What is the probability that your partner is cheating on you?\n\nthe prior probability that your partner is cheating on you: 4%\nthe conditional probability of underwear appearing given that your partner is cheating on you: 50%\nthe conditional probability of underwear appearing given that your partner is not cheating on you: 5%\n\n\nCodepAgivenB(\n  pBgivenA = .50,\n  pA = .04,\n  pBgivenNotA = .05)\n\n[1] 0.2941176\n\n\n\n16.5.2 Example 2: Does a Person Have Breast Cancer?\nExample 2: What is the probability that a woman in her 40s has breast cancer if she tested positive on a mammogram?\n\nthe prior probability that she has breast cancer: 1.4%\nthe conditional probability that she has a positive test given that she has breast cancer: 75%\nthe conditional probability that she has a positive test given that she does not have breast cancer: 10%\n\n\nCodepAgivenB(\n  pBgivenA = .75,\n  pA = .014,\n  pBgivenNotA = .10)\n\n[1] 0.09624198\n\n\n\n16.5.3 Example 3: Was it a Terrorist Attack?\n\n16.5.3.1 Example 3A: The First Plane Hit the World Trade Center\nExample 3A: Consider the information we had on 9/11 when the first plane hit the World Trade Center. What is the probability that a terror attack occurred given that the first plane hit the World Trade Center?\n\nthe prior probability that terrorists crash a plane into a Manhattan skyscraper: 0.005%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are attacking Manhattan skyscrapers: 100%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are not attacking Manhattan skyscrapers (i.e., it is an accident): 0.008%\n\n\nCodepAgivenB(\n  pBgivenA = 1,\n  pA = .00005,\n  pBgivenNotA = .00008)\n\n[1] 0.3846272\n\n\n\n16.5.3.2 Example 3B: The Second Plane Hit the World Trade Center\nExample 3B: Now, consider that a second plane just hit the World Trade Center. What is the probability that a terror attack occurred given that a second plane hit the World Trade Center?\n\nthe revised prior probability that terrorists crash a plane into a Manhattan skyscraper (from Example 3A): 38.46272%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are attacking Manhattan skyscrapers: 100%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are not attacking Manhattan skyscrapers (i.e., it is an accident): 0.008%\n\n\nCodepAgivenB(\n  pBgivenA = 1,\n  pA = .3846272,\n  pBgivenNotA = .00008)\n\n[1] 0.999872",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRateFantasyFootball",
    "href": "base-rates.html#sec-baseRateFantasyFootball",
    "title": "16  Base Rates",
    "section": "\n16.6 Base Rates Applied to Fantasy Football",
    "text": "16.6 Base Rates Applied to Fantasy Football\nBase rates are also relevant to fantasy football. Unlike yardage (e.g., passing yards, rushing yards, receiving yards), touchdowns occur relatively less frequently. Whereas a solid Wide Receiver may log 100+ receptions and 1,200+ yards in a season, they may have “only” 8–14 receiving touchdowns in a given season. As noted in Section 17.6.5, lower base-rate events—including touchdowns—are harder to predict accurately. As noted by Harris (2012): “NFL statistical projections are basically impossible to get right. (Take it from someone who helps create them for a living.) Yes, we can do a passable job with yardage totals for players who don’t suffer unexpected injuries or depth-chart pratfalls. But so much of fantasy football hinges on touchdowns, and touchdowns are impossibly difficult to predict from season to season (let alone week to week).” (archived at https://perma.cc/4QNH-J2LD). Thus, it is important not to lend too much credence to predictions of touchdowns. Focus on other things that may be more predictable (and that may be indirectly prognostic of touchdowns) such as yards, carries/targets, receptions, depth of targets, red zone carries/targets, short distance carries/targets, etc.\n[PROVIDE ACCURACY OF PROJECTIONS OF TOUCHDOWNS VS YARDAGE]\nWhen dealing with numeric predictions (rather than categorical outcomes), the equivalent of the base rate is the average value. For instance, the “base rate” of fantasy points for a given position is the average number of fantasy points for that position. We could subdivide even further to identify, for instance, the “base rate” of fantasy points for the Wide Receiver at the top of the depth chart on a team.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRateRookiePerformance",
    "href": "base-rates.html#sec-baseRateRookiePerformance",
    "title": "16  Base Rates",
    "section": "\n16.7 Base Rate of Rookie Performance",
    "text": "16.7 Base Rate of Rookie Performance\n\n16.7.1 Quarterbacks\n\n16.7.2 Running Backs",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-accountForBaseRates",
    "href": "base-rates.html#sec-accountForBaseRates",
    "title": "16  Base Rates",
    "section": "\n16.8 How to Account for Base Rates",
    "text": "16.8 How to Account for Base Rates\nThere are various ways to account for base rates, including the use of actuarial formulas and the use of Bayesian updating.\n\n16.8.1 Actuarial Formula\nOne approach to account for base rates is to use actuarial formulas (rather than human judgment) to make the predictions. Actuarial formulas based on multiple regression or machine learning can account for the base rate of the event.\n\n16.8.2 Bayesian Updating\nAnother approach to account for base rates is to leverage Bayes’ theorem, using Bayesian updating and the probability nomogram. Bayesian updating is a form of anchoring and adjustment; however, unlike the anchoring and adjustment heuristic, it is a systematic approach to anchoring and adjustment that anchors one’s predictions to the base rate, and then adjusts according to new information. That is, we start with a pretest probability (i.e., base rate) and update our predictions based on the extent of new information (i.e., the likelihood ratio).\nTo perform Bayesian updating involves comparing the relative probability of two outcomes, \\(P(C | R)\\) versus \\(P(\\text{not } C | R)\\). If we want to compare the relative probability of two outcomes, we can use the odds form of Bayes’ theorem, as in Equation 16.21:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{not } C | R) &= \\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)} \\\\\n  \\frac{P(C | R)}{P(\\text{not } C | R)} &= \\frac{\\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}}{\\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)}} \\\\\n  &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)} \\\\\n  &= \\frac{P(C_i)}{P(\\text{not } C_i)} \\times \\frac{P(R | C)}{P(R | \\text{not } C)} \\\\\n  \\text{posterior odds} &= \\text{prior odds} \\times \\text{likelihood ratio}\n\\end{aligned}\n\\tag{16.21}\\]\nAs presented in Equation 16.21, the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. Below, we describe the likelihood ratio.\n\n16.8.2.1 Diagnostic Likelihood Ratio\nA likelihood ratio is the ratio of two probabilities. It can be used to compare the likelihood of two possibilities. The diagnostic likelihood ratio is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect. The diagnostic likelihood ratio is also called the risk ratio. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio.\n\n16.8.2.1.1 Positive Likelihood Ratio (LR+)\nThe positive likelihood ratio (LR+) compares the true positive rate to the false positive rate. Positive likelihood ratio values range from 1 to infinity. Higher values reflect greater accuracy, because it indicates the degree to which a true positive is more likely than a false positive. The formula for calculating the positive likelihood ratio is in Equation 16.22.\n\\[\n\\begin{aligned}\n  \\text{positive likelihood ratio (LR+)} &= \\frac{\\text{TPR}}{\\text{FPR}} \\\\\n  &= \\frac{P(R|C)}{P(R|\\text{not } C)} \\\\\n  &= \\frac{P(R|C)}{1 - P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n\\end{aligned}\n\\tag{16.22}\\]\n\n16.8.2.1.2 Negative Likelihood Ratio (LR−)\nThe negative likelihood ratio (LR−) compares the false negative rate to the true negative rate. Negative likelihood ratio values range from 0 to 1. Smaller values reflect greater accuracy, because it indicates that a false negative is less likely than a true negative. The formula for calculating the negative likelihood ratio is in Equation 16.23.\n\\[\n\\begin{aligned}\n  \\text{negative likelihood ratio } (\\text{LR}-) &= \\frac{\\text{FNR}}{\\text{TNR}} \\\\\n  &= \\frac{P(\\text{not } R|C)}{P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{1 - P(R|C)}{P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{1 - \\text{sensitivity}}{\\text{specificity}}\n\\end{aligned}\n\\tag{16.23}\\]\n\n16.8.2.2 Probability Nomogram\nUsing Bayes’ theorem (described in Section 16.3.3), solving for posttest odds (based on pretest odds and the likelihood ratio, as in Equation 16.21), and converting odds to probabilities, we can use a Fagan probability nomogram to determine the posttest probability following a test result. The calculation of posttest probability is described in INSERT. A probability nomogram (aka Fagan nomogram) is a way of visually applying Bayes’ theorem to determine the posttest probability of having a condition based on the pretest (or prior) probability and likelihood ratio, as depicted in Figure 16.1. To use a probability nomogram, connect the dots from the starting probability (left line) with the likelihood ratio (middle line) to see the updated probability. The updated (posttest) probability is where the connecting line crosses the third, right line.\n\n\n\n\n\nFigure 16.1: Probability Nomogram. (Figure retrieved from https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Fagan_nomogram.svg/945px-Fagan_nomogram.svg.png).\n\n\nFor instance, if the starting probability is 0.5% and the likelihood ratio is 10 (e.g., sensitivity = .90, specificity = .91: \\(\\text{likelihood ratio} = \\frac{\\text{sensitivity}}{1 - \\text{specificity}} = \\frac{.9}{1-.91} = 10\\)) from a positive test (i.e., positive likelihood ratio), the updated probability is less than 5%, as depicted in Figure 16.2.\nAn interactive probability nomogram is available from Altarejos & Hayward (2025) at the following link: https://jamaevidence.mhmedical.com/data/calculators/LR_nomogram.html (archived at https://perma.cc/Z3SW-QMJ3).\nThe petersenlab package (Petersen, 2025a) contains the posttestProbability() function that estimates the posttest probability of an event, given the pretest probability and the likelihood ratio, or given the pretest probability and the sensitivity (SN) and specificity (SP) of the test.\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  likelihoodRatio = 10)\n\n[1] 0.04784689\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  SN = .90,\n  SP = .91)\n\n[1] 0.04784689\n\n\nThe function can also estimate the posttest probability of an event given the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):\n\nCodepetersenlab::posttestProbability(\n  TP = 450,\n  TN = 90545,\n  FP = 8955,\n  FN = 50)\n\n[1] 0.04784689\n\n\nWe discuss true positives (TP), true negatives (TN), false positives (FP), false negatives (FN), sensitivity (SN), and specificity (SP) in Section 17.6 (Section 17.6.1 and Section 17.6.6).\nIf the starting probability is 0.5% and the likelihood ratio is 0.11 from a negative test (i.e., negative likelihood ratio), the updated probability is nearly indistinguishable from zero (0.05%).\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  likelihoodRatio = 0.11)\n\n[1] 0.0005524584\n\n\n\n\n\n\n\nFigure 16.2: Probability Nomogram Example. (Figure adapted from https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Fagan_nomogram.svg/945px-Fagan_nomogram.svg.png. Also provided in: Petersen (2024) and Petersen (2025b).)\n\n\nA probability nomogram calculator is available from Schwartz (2006) at the following link: http://araw.mede.uic.edu/cgi-bin/testcalc.pl (archived at https://perma.cc/X8TF-7YBX). The petersenlab package (Petersen, 2025a) contains the nomogrammer() function that creates a nomogram plot using the positive and negative likelihood ratio or using the sensitivity (SN) and specificity (SP) of the test, as adapted from Chekroud (2017):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .005,\n  SN = 0.90,\n  SP = 0.91)\n\n\n\n\n\n\n\nThe blue line indicates the posterior probability of the condition given a positive test. The pink line indicates the posterior probability of the condition given a negative test.\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .005,\n  PLR = 10,\n  NLR = 0.11)\n\n\n\n\n\n\n\nThe function can also create a nomogram plot using the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):\n\nCodepetersenlab::nomogrammer(\n  TP = 450,\n  TN = 90545,\n  FP = 8955,\n  FN = 50)\n\n\n\n\n\n\n\nThe function can also create a nomogram plot using the sensitivity (SN) and selection rate of the test. Here is a nomogram plot from the HIV example:\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .003,\n  SN = .95,\n  selectionRate = .01\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from the cab example (Kahneman, 2011):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .15,\n  SN = .80,\n  SP = .80\n)\n\n\n\n\n\n\n\nThe function can also create a nomogram plot using the sensitivity (SN) and false positive rate of the test. Here is a nomogram plot from Example 1 from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .04,\n  SN = .50,\n  FPR = .05\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from Example 2 from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .014,\n  SN = .75,\n  FPR = .10\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from Example 3A from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .00005,\n  SN = 1,\n  FPR = .00008\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from Example 3B from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .3846272,\n  SN = 1,\n  FPR = .00008\n)\n\n\n\n\n\n\n\n\n16.8.2.3 Informal Updating\nKahneman (2011) provides the following guidance for an informal approach to updating that anchors predictions to the base rate:\n\nStart with the “baseline prediction” (i.e., base rate or average outcome).\nGenerate or identify your “intuitive prediction”—the number that matches your impression of the evidence.\nYour posterior prediction should fall somewhere between the baseline prediction and the intuitive prediction. “In the default case of no useful evidence, you stay with the baseline [prediction]. At the other extreme, you also stay with your initial [i.e., intuitive] prediction. This will happen, of course, only if you remain completely confident in your initial prediction after a critical review of the evidence that supports it. In most cases you will find some reasons to doubt that the correlation between your intuitive judgment and the truth is perfect, and you will end up somewhere between the two poles.” (pp. 191–192). Base the extent of adjustment (from the baseline prediction) on the magnitude of the correlation between your prediction/evidence and the truth, which acts similar to the likelihood ratio. For instance, if the correlation between your prediction/evidence and the truth is .5, move 50% of the difference from the baseline prediction to the intuitive prediction.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesConclusion",
    "href": "base-rates.html#sec-baseRatesConclusion",
    "title": "16  Base Rates",
    "section": "\n16.9 Conclusion",
    "text": "16.9 Conclusion\nFantasy performance—and behavior more generally—is challenging to predict. People commonly demonstrate biases and fallacies when making predictions. People tend to ignore base rates (base rate fallacy) when making predictions. They also tend to confuse inverse conditional probabilities (conditional probability fallacy). Bayes’ theorem provides a way to convert from one conditional probability to its inverse conditional probability using the base rate of each event. There are various ways to account for base rates for more accurate predictions, including through the use of actuarial formulas, Bayesian updating, and more informal approaches. Bayesian updating uses Bayes’ theorem to calculate a posttest probability from a pretest probability and a test result (likelihood ratio). The probability nomogram is a visual approach to Bayesian updating.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesSessionInfo",
    "href": "base-rates.html#sec-baseRatesSessionInfo",
    "title": "16  Base Rates",
    "section": "\n16.10 Session Info",
    "text": "16.10 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] generics_0.1.3     lattice_0.22-6     stringi_1.8.4      digest_0.6.37     \n [5] magrittr_2.0.3     evaluate_1.0.3     grid_4.4.3         RColorBrewer_1.1-3\n [9] mvtnorm_1.3-3      fastmap_1.2.0      plyr_1.8.9         jsonlite_1.9.1    \n[13] nnet_7.3-20        backports_1.5.0    DBI_1.2.3          Formula_1.2-5     \n[17] gridExtra_2.3      purrr_1.0.4        viridisLite_0.4.2  scales_1.3.0      \n[21] pbivnorm_0.6.0     mnormt_2.1.1       cli_3.6.4          mitools_2.4       \n[25] rlang_1.1.5        munsell_0.5.1      Hmisc_5.2-3        withr_3.0.2       \n[29] base64enc_0.1-3    mix_1.0-13         parallel_4.4.3     tools_4.4.3       \n[33] reshape2_1.4.4     checkmate_2.3.2    htmlTable_2.4.3    dplyr_1.1.4       \n[37] colorspace_2.1-1   ggplot2_3.5.1      vctrs_0.6.5        R6_2.6.1          \n[41] rpart_4.1.24       stats4_4.4.3       lifecycle_1.0.4    stringr_1.5.1     \n[45] htmlwidgets_1.6.4  psych_2.5.3        foreign_0.8-88     cluster_2.1.8     \n[49] pkgconfig_2.0.3    pillar_1.10.1      gtable_0.3.6       Rcpp_1.0.14       \n[53] glue_1.8.0         data.table_1.17.0  xfun_0.51          tibble_3.2.1      \n[57] tidyselect_1.2.1   rstudioapi_0.17.1  knitr_1.50         farver_2.1.2      \n[61] xtable_1.8-4       nlme_3.1-167       htmltools_0.5.8.1  labeling_0.4.3    \n[65] rmarkdown_2.29     lavaan_0.6-19      compiler_4.4.3     quadprog_1.5-8    \n\n\n\n\n\n\nAIDSVu. (2022). Understanding the current HIV epidemic in the United States. https://map.aidsvu.org/profiles/nation/usa/overview\n\n\nAltarejos, J., & Hayward, R. (2025). Likelihood ratio nomogram. Centre for Health Evidence. https://jamaevidence.mhmedical.com/data/calculators/LR_nomogram.html\n\n\nChekroud, A. (2017). nomogrammer: Fagan’s nomograms with ggplot2. https://github.com/achekroud/nomogrammer\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nHarris, C. (2012). How to make VBD work for you. https://www.espn.com/fantasy/football/ffl/story?page=nfldk2k12_vbdwork\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nSchwartz, A. (2006). Diagnostic test calculator. http://araw.mede.uic.edu/cgi-bin/testcalc.pl\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html",
    "href": "evaluating-prediction-accuracy.html",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "",
    "text": "17.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyGettingStarted",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyGettingStarted",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "",
    "text": "17.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"tidyverse\")\nlibrary(\"pROC\")\nlibrary(\"magrittr\")\nlibrary(\"viridis\")",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyOverview",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyOverview",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.2 Overview",
    "text": "17.2 Overview\nPredictions can come in different types. Some predictions involve categorical data, whereas other predictions involve continuous data. When dealing with a dichotomous (nominal data that are binary) predictor and outcome variable (or continuous data that have been dichotomized using a cutoff), we can evaluate predictions using a 2x2 table known as a confusion matrix (see INSERT), or with logistic regression models. When dealing with a continuous outcome variable (e.g., ordinal, interval, or ratio data), we can evaluate predictions using multiple regression or similar variants such as structural equation modeling and mixed models.\nIn fantasy football, we most commonly predict continuous outcome variables (e.g., fantasy points, rushing yards). Nevertheless, it is also important to understand principles in the prediction of categorical outcomes variables.\nIn any domain, it is important to evaluate the accuracy of predictions, so we can know how (in)accurate we are, and we can strive to continually improve our predictions. Fantasy performance—and human behavior more general—is incredibly challenging to predict. Indeed, many things in the world, in particular long-term trends, are unpredictable (Kahneman, 2011). In fantasy football, there is considerable luck/chance/randomness. There are relatively few (i.e. 17) games, and there is a sizeable injury risk for each player in a given game. These and other factors combine to render fantasy football predictions not highly accurate. Domains with high uncertainty and unpredictability are considered “low-validity environments” (Kahneman, 2011, p. 223). But, first, let’s learn about the various ways we can evaluate the accuracy of predictions.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-accuracyTypes",
    "href": "evaluating-prediction-accuracy.html#sec-accuracyTypes",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.3 Types of Accuracy",
    "text": "17.3 Types of Accuracy\nThere are two primary dimensions of accuracy: (1) discrimination and (2) calibration. Discrimination and calibration are distinct forms of accuracy. Just because predictions are high in one form of accuracy does not mean that they will be high in the other form of accuracy. As described by Lindhiem et al. (2020), predictions can follow any of the following configurations (and anywhere in between):\n\nhigh discrimination, high calibration\n\nhigh discrimination, low calibration\n\nlow discrimination, high calibration\n\nlow discrimination, low calibration\n\n\nSome general indexes of accuracy combine discrimination and calibration, as described in Section 17.3.3.\nIn addition, accuracy indices can be threshold-dependent or -independent and can be scale-dependent or -independent. Threshold-dependent accuracy indices differ based on the cutoff (i.e., threshold), whereas threshold-independent accuracy indices do not. Thus, raising or lowering the cutoff will change threshold-dependent accuracy indices. Scale-dependent accuracy indices depend on the metric/scale of the data, whereas scale-independent accuracy indices do not. Thus, scale-dependent accuracy indices cannot be directly compared when using measures of differing scales, whereas scale-independent accuracy indices can be compared across data of differing scales.\n\n17.3.1 Discrimination\nWhen dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity, area under the ROC curve) are described in INSERT.\n\n17.3.2 Calibration\nWhen dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by Lindhiem et al. (2020).\nCalibration is relevant to all kinds of predictions, including weather forecasts. For instance, on the days that the meteorologist says there is a 60% chance of rain, it should rain about 60% of the time. Calibration is also important for fantasy football predictions. When projections state that a group of players is each expected to score 200 points, their projections would be miscalibrated if those players scored only 150 points on average.\nThere are four general patterns of miscalibration: overextremity, underextremity, overprediction, and underprediction (see Figure 17.7). Overextremity exists when the predicted probabilites are too close to the extremes (zero or one). Underextremity exists when the predicted probabilities are too far away from the extremes. Overprediction exists when the predicted probabilities are consistently greater than the observed probabilities. Underprediction exists when the predicted probabilities are consistently less than the observed probabilities. For a more thorough description of these types of miscalibration, see Lindhiem et al. (2020).\nIndices for evaluating calibration are described in Section 17.7.3.\n\n17.3.3 General Accuracy\nGeneral accuracy indices combine estimates of discrimination and calibration.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionCategorical",
    "href": "evaluating-prediction-accuracy.html#sec-predictionCategorical",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.4 Prediction of Categorical Outcomes",
    "text": "17.4 Prediction of Categorical Outcomes\nTo evaluate the accuracy of our predictions for categorical outcome variables (e.g., binary, dichotomous, or nominal data), we can use either threshold-dependent or threshold-independent accuracy indices.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionContinuous",
    "href": "evaluating-prediction-accuracy.html#sec-predictionContinuous",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.5 Prediction of Continuous Outcomes",
    "text": "17.5 Prediction of Continuous Outcomes\nTo evaluate the accuracy of our predictions for continuous outcome variables (e.g., ordinal, interval, or ratio data), the outcome variable does not have cutoffs, so we would use threshold-independent accuracy indices.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-thresholdDependentAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-thresholdDependentAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.6 Threshold-Dependent Accuracy Indices",
    "text": "17.6 Threshold-Dependent Accuracy Indices\n\n17.6.1 Decision Outcomes\nTo consider how we can evaluate the accuracy of predictions for a categorical outcome, consider an example adapted from Meehl & Rosen (1955). The military conducts a test of its prospective members to screen out applicants who would likely fail basic training. To evaluate the accuracy of our predictions using the test, we can examine a confusion matrix. A confusion matrix is a matrix that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes): true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\nWhen discussing the four decision outcomes, “true” means an accurate judgment, whereas “false” means an inaccurate judgment. “Positive” means that the judgment was that the person has the characteristic of interest, whereas “negative” means that the judgment was that the person does not have the characteristic of interest. A true positive is a correct judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually have (or will have) the characteristic. A true negative is a correct judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false positive is an incorrect judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false negative is an incorrect judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do have (or will have) the characteristic.\nAn example of a confusion matrix is in INSERT.\nWith the information in the confusion matrix, we can calculate the marginal sums and the proportion of people in each cell (in parentheses), as depicted in INSERT.\nThat is, we can sum across the rows and columns to identify how many people actually showed poor adjustment (\\(n = 100\\)) versus good adjustment (\\(n = 1,900\\)), and how many people were selected to reject (\\(n = 508\\)) versus retain (\\(n = 1,492\\)). If we sum the column of predicted marginal sums (\\(508 + 1,492\\)) or the row of actual marginal sums (\\(100 + 1,900\\)), we get the total number of people (\\(N = 2,000\\)).\nBased on the marginal sums, we can compute the marginal probabilities, as depicted in INSERT.\nThe marginal probability of the person having the characteristic of interest (i.e., showing poor adjustment) is called the base rate (BR). That is, the base rate is the proportion of people who have the characteristic. It is calculated by dividing the number of people with poor adjustment (\\(n = 100\\)) by the total number of people (\\(N = 2,000\\)): \\(BR = \\frac{FN + TP}{N}\\). Here, the base rate reflects the prevalence of poor adjustment. In this case, the base rate is .05, so there is a 5% chance that an applicant will be poorly adjusted. The marginal probability of good adjustment is equal to 1 minus the base rate of poor adjustment.\nThe marginal probability of predicting that a person has the characteristic (i.e., rejecting a person) is called the selection ratio (SR). The selection ratio is the proportion of people who will be selected (in this case, rejected rather than retained); i.e., the proportion of people who are identified as having the characteristic. The selection ratio is calculated by dividing the number of people selected to reject (\\(n = 508\\)) by the total number of people (\\(N = 2,000\\)): \\(SR = \\frac{TP + FP}{N}\\). In this case, the selection ratio is .25, so 25% of people are rejected. The marginal probability of not selecting someone to reject (i.e., the marginal probability of retaining) is equal to 1 minus the selection ratio.\nThe selection ratio might be something that the test dictates according to its cutoff score. Or, the selection ratio might be imposed by external factors that place limits on how many people you can assign a positive test value. For instance, when deciding whether to treat a client, the selection ratio may depend on how many therapists are available and how many cases can be treated.\n\n17.6.2 Percent Accuracy\nBased on the confusion matrix, we can calculate the prediction accuracy based on the percent accuracy of the predictions. The percent accuracy is the number of correct predictions divided by the total number of predictions, and multiplied by 100. In the context of a confusion matrix, this is calculated as: \\(100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\\). In this case, our percent accuracy was 78%—that is, 78% of our predictions were accurate, and 22% of our predictions were inaccurate.\n\n17.6.3 Percent Accuracy by Chance\n78% sounds pretty accurate. And it is much higher than 50%, so we are doing a pretty good job, right? Well, it is important to compare our accuracy to what accuracy we would expect to get by chance alone, if predictions were made by a random process rather than using a test’s scores. Our selection ratio was 25.4%. How accurate would we be if we randomly selected 25.4% of people to reject? To determine what accuracy we could get by chance alone given the selection ratio and the base rate, we can calculate the chance probability of true positives and the chance probability of true negatives. The probability of a given cell in the confusion matrix is a joint probability—the probability of two events occurring simultaneously. To calculate a joint probability, we multiply the probability of each event.\nSo, to get the chance expectancies of true positives, we would multiply the respective marginal probabilities, as in Equation 17.1:\n\\[\n\\begin{aligned}\n  P(TP) &= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\\n   &= BR \\times SR \\\\\n  &= .05 \\times .254 \\\\\n  &= .0127\n\\end{aligned}\n\\tag{17.1}\\]\nTo get the chance expectancies of true negatives, we would multiply the respective marginal probabilities, as in Equation 17.2:\n\\[\n\\begin{aligned}\n  P(TN) &= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\\n   &= (1 - BR) \\times (1 - SR) \\\\\n  &= .95 \\times .746 \\\\\n  &= .7087\n\\end{aligned}\n\\tag{17.2}\\]\nTo get the percent accuracy by chance, we sum the chance expectancies for the correct predictions (TP and TN): \\(.0127 + .7087 = .7214\\). Thus, the percent accuracy you can get by chance alone is 72%. This is because most of our predictions are to retain people, and the base rate of poor adjustment is quite low (.05). Our measure with 78% accuracy provides only a 6% increment in correct predictions. Thus, you cannot judge how good your judgment or prediction is until you know how you would do by random chance.\nThe chance expectancies for each cell of the confusion matrix are in INSERT\n\n17.6.4 Predicting from the Base Rate\nNow, let us consider how well you would do if you were to predict from the base rate. Predicting from the base rate is also called “betting from the base rate”, and it involves setting the selection ratio by taking advantage of the base rate so that you go with the most likely outcome in every prediction. Because the base rate is quite low (.05), we could predict from the base rate by selecting no one to reject (i.e., setting the selection ratio at zero). Our percent accuracy by chance if we predict from the base rate would be calculated by multiplying the marginal probabilities, as we did above, but with a new selection ratio, as in Equation 17.3:\n\\[\n\\begin{aligned}\n  P(TP) &= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\\n   &= BR \\times SR \\\\\n  &= .05 \\times 0 \\\\\n  &= 0 \\\\ \\\\\n  P(TN) &= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\\n   &= (1 - BR) \\times (1 - SR) \\\\\n  &= .95 \\times 1 \\\\\n  &= .95\n\\end{aligned}\n\\tag{17.3}\\]\nWe sum the chance expectancies for the correct predictions (TP and TN): \\(0 + .95 = .95\\). Thus, our percent accuracy by predicting from the base rate is 95%. This is damning to our measure because it is a much higher accuracy than the accuracy of our measure. That is, we can be much more accurate than our measure simply by predicting from the base rate and selecting no one to reject.\nGoing with the most likely outcome in every prediction (predicting from the base rate) can be highly accurate (in terms of percent accuracy) as noted by Meehl & Rosen (1955), especially when the base rate is very low or very high. This should serve as an important reminder that we need to compare the accuracy of our measures to the accuracy by (1) random chance and (2) predicting from the base rate. There are several important implications of the impact of base rates on prediction accuracy. One implication is that using the same test in different settings with different base rates will markedly change the accuracy of the test. Oftentimes, using a test will actually decrease the predictive accuracy when the base rate deviates greatly from .50. But percent accuracy is not everything. Percent accuracy treats different kinds of errors as if they are equally important. However, the value we place on different kinds of errors may be different, as described next.\n\n17.6.5 Different Kinds of Errors Have Different Costs\nSome errors have a high cost, and some errors have a low cost. Among the four decision outcomes, there are two types of errors: false positives and false negatives. The extent to which false positives and false negatives are costly depends on the prediction problem. So, even though you can often be most accurate by going with the base rate, it may be advantageous to use a screening instrument despite lower overall accuracy because of the huge difference in costs of false positives versus false negatives in some cases.\nConsider the example of a screening instrument for HIV. False positives would be cases where we said that someone is at high risk of HIV when they are not, whereas false negatives are cases where we said that someone is not at high risk when they actually are. The costs of false positives include a shortage of blood, some follow-up testing, and potentially some anxiety, but that is about it. The costs of false negatives may be people getting HIV. In this case, the costs of false negatives greatly outweigh the costs of false positives, so we use a screening instrument to try to identify the cases at high risk for HIV because of the important consequences of failing to do so, even though using the screening instrument will lower our overall accuracy level.\nAnother example is when the Central Intelligence Agency (CIA) used a screen for protective typists during wartime to try to detect spies. False positives would be cases where the CIA believes that a person is a spy when they are not, and the CIA does not hire them. False negatives would be cases where the CIA believes that a person is not a spy when they actually are, and the CIA hires them. In this case, a false positive would be fine, but a false negative would be really bad.\nHow you weigh the costs of different errors depends considerably on the domain and context. Possible costs of false positives to society include: unnecessary and costly treatment with side effects and sending an innocent person to jail (despite our presumption of innocence in the United States criminal justice system that a person is innocent until proven guilty). Possible costs of false negatives to society include: setting a guilty person free, failing to detect a bomb or tumor, and preventing someone from getting treatment who needs it.\nThe differential costs of different errors also depend on how much flexibility you have in the selection ratio in being able to set a stringent versus loose selection ratio. Consider if there is a high cost of getting rid of people during the selection process. For example, if you must hire 100 people and only 100 people apply for the position, you cannot lose people, so you need to hire even high-risk people. However, if you do not need to hire many people, then you can hire more conservatively.\nAny time the selection ratio differs from the base rate, you will make errors. For example, if you reject 25% of applicants, and the base rate of poor adjustment is 5%, then you are making errors of over-rejecting (false positives). By contrast, if you reject 1% of applicants and the base rate of poor adjustment is 5%, then you are making errors of under-rejecting or over-accepting (false negatives).\nA low base rate makes it harder to make predictions, and tends to lead to less accurate predictions. For instance, it is very challenging to predict low base rate behaviors, including suicide (Kessler et al., 2020). For this reason, it is likely much more challenging to predict touchdowns—which happen relatively less often—than it is to predict passing/rushing/receiving yards—which are more frequent and continuously distributed.\n[EVALUATE EMPIRICALLY]\n\n17.6.6 Sensitivity, Specificity, PPV, and NPV\nAs described earlier, percent accuracy is not the only important aspect of accuracy. Percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the condition (if the base rate is low) or that everyone has the condition (if the base rate is high). Thus, it is also important to consider other aspects of accuracy, including sensitivity (SN), specificity (SP), positive predictive value (PPV), and negative predictive value (NPV). We want our predictions to be sensitive to be able to detect the characteristic but also to be specific so that we classify only people actually with the characteristic as having the characteristic.\nLet us return to the confusion matrix in INSERT. If we know the frequency of each of the four predicted-actual combinations of the confusion matrix (TP, TN, FP, FN), we can calculate sensitivity, specificity, PPV, and NPV.\nSensitivity is the proportion of those with the characteristic (\\(\\text{TP} + \\text{FN}\\)) that we identified with our measure (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = .86\\). Specificity is the proportion of those who do not have the characteristic (\\(\\text{TN} + \\text{FP}\\)) that we correctly classify as not having the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = .78\\). PPV is the proportion of those who we classify as having the characteristic (\\(\\text{TP} + \\text{FP}\\)) who actually have the characteristic (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = .17\\). NPV is the proportion of those we classify as not having the characteristic (\\(\\text{TN} + \\text{FN}\\)) who actually do not have the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = .99\\).\nSensitivity, specificity, PPV, and NPV are proportions, and their values therefore range from 0 to 1, where higher values reflect greater accuracy. With sensitivity, specificity, PPV, and NPV, we have a good snapshot of how accurate the measure is at a given cutoff. In our case, our measure is good at finding whom to reject (high sensitivity), but it is rejecting too many people who do not need to be rejected (lower PPV due to many FPs). Most people whom we classify as having the characteristic do not actually have the characteristic. However, the fact that we are over-rejecting could be okay depending on our goals, for instance, if we do not care about over-dropping (i.e., the PPV being low).\n\n17.6.6.1 Some Accuracy Estimates Depend on the Cutoff\nSensitivity, specificity, PPV, and NPV differ based on the cutoff (i.e., threshold) for classification. Consider the following example. Aliens visit Earth, and they develop a test to determine whether a berry is edible or inedible.\nFigure 17.1 depicts the distributions of scores by berry type. Note how there are clearly two distinct distributions. However, the distributions overlap to some degree. Thus, any cutoff will have at least some inaccurate classifications. The extent of overlap of the distributions reflects the amount of measurement error of the measure with respect to the characteristic of interest.\n\nCode#No Cutoff\nsampleSize &lt;- 1000\n\nedibleScores &lt;- rnorm(sampleSize, 50, 15)\ninedibleScores &lt;- rnorm(sampleSize, 100, 15)\n\nedibleData &lt;- data.frame(\n  score = c(\n    edibleScores,\n    inedibleScores),\n  type = c(\n    rep(\"edible\", sampleSize),\n    rep(\"inedible\", sampleSize)))\n\ncutoff &lt;- 75\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = edibleData,\n  aes(\n    x = score,\n    ymin = 0,\n    fill = type)) +\n  geom_density(alpha = .5) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(2)[1],\n      viridis::viridis(2)[2])) +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.1: Distribution of Test Scores by Berry Type.\n\n\n\n\nFigure 17.2 depicts the distributions of scores by berry type with a cutoff. The red line indicates the cutoff—the level above which berries are classified by the test as inedible. There are errors on each side of the cutoff. Below the cutoff, there are some false negatives (blue): inedible berries that are inaccurately classified as edible. Above the cutoff, there are some false positives (green): edible berries that are inaccurately classified as inedible. Costs of false negatives could include sickness or death from eating the inedible berries. Costs of false positives could include taking longer to find food, finding insufficient food, and starvation.\n\nCode#Standard Cutoff\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.2: Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nBased on our assessment goals, we might use a different selection ratio by changing the cutoff. Figure 17.3 depicts the distributions of scores by berry type when we raise the cutoff. There are now more false negatives (blue) and fewer false positives (green). If we raise the cutoff (to be more conservative), the number of false negatives increases and the number of false positives decreases. Consequently, as the cutoff increases, sensitivity and NPV decrease (because we have more false negatives), whereas specificity and PPV increase (because we have fewer false positives). A higher cutoff could be optimal if the costs of false positives are considered greater than the costs of false negatives. For instance, if the aliens cannot risk eating the inedible berries because the berries are fatal, and there are sufficient edible berries that can be found to feed the alien colony.\n\nCode#Raise the cutoff\ncutoff &lt;- 85\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.3: Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nFigure 17.4 depicts the distributions of scores by berry type when we lower the cutoff. There are now fewer false negatives (blue) and more false positives (green). If we lower the cutoff (to be more liberal), the number of false negatives decreases and the number of false positives increases. Consequently, as the cutoff decreases, sensitivity and NPV increase (because we have fewer false negatives), whereas specificity and PPV decrease (because we have more false positives). A lower cutoff could be optimal if the costs of false negatives are considered greater than the costs of false positives. For instance, if the aliens cannot risk missing edible berries because they are in short supply relative to the size of the alien colony, and eating the inedible berries would, at worst, lead to minor, temporary discomfort.\n\nCode#Lower the cutoff\ncutoff &lt;- 65\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.4: Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nIn sum, sensitivity and specificity differ based on the cutoff for classification. If we raise the cutoff, sensitivity and PPV increase (due to fewer false positives), whereas sensitivity and NPV decrease (due to more false negatives). If we lower the cutoff, sensitivity and NPV increase (due to fewer false negatives), whereas specificity and PPV decrease (due to more false positives). Thus, the optimal cutoff depends on how costly each type of error is: false negatives and false positives. If false negatives are more costly than false positives, we would set a low cutoff. If false positives are more costly than false negatives, we would set a high cutoff.\n\n17.6.7 Signal Detection Theory\nSignal detection theory (SDT) is a probability-based theory for the detection of a given stimulus (signal) from a stimulus set that includes non-target stimuli (noise). SDT arose through the development of radar (RAdio Detection And Ranging) and sonar (SOund Navigation And Ranging) in World War II based on research on sensory-perception research. The military wanted to determine which objects on radar/sonar were enemy aircraft/submarines, and which were noise (e.g., different object in the environment or even just the weather itself). SDT allowed determining how many errors operators made (how accurate they were) and decomposing errors into different kinds of errors. SDT distinguishes between sensitivity and bias. In SDT, sensitivity (or discriminability) is how well an assessment distinguishes between a target stimulus and non-target stimuli (i.e., how well the assessment detects the target stimulus amid non-target stimuli). Bias is the extent to which the probability of a selection decision from the assessment is higher or lower than the true rate of the target stimulus.\nSome radar/sonar operators were not as sensitive to the differences between signal and noise, due to factors such as age, ability to distinguish gradations of a signal, etc. People who showed low sensitivity (i.e., who were not as successful at distinguishing between signal and noise) were screened out because the military perceived sensitivity as a skill that was not easily taught. By contrast, other operators could distinguish signal from noise, but their threshold was too low or high—they could take in information, but their decisions tended to be wrong due to systematic bias or poor calibration. That is, they systematically over-rejected or under-rejected stimuli. Over-rejecting leads to many false negatives (i.e., saying that a stimulus is safe when it is not). Under-rejecting leads to many false positives (i.e., saying that a stimulus is harmful when it is not). A person who showed good sensitivity but systematic bias was considered more teach-able than a person who showed low sensitivity. Thus, radar and sonar operators were selected based on their sensitivity to distinguish signal from noise, and then were trained to improve the calibration so they reduce their systematic bias and do not systematically over- or under-reject.\nAlthough SDT was originally developed for use in World War II, it now plays an important role in many areas of science and medicine. A medical application of SDT is tumor detection in radiology. Another application of SDT in society is using x-ray to detect bombs or other weapons. An example of applying SDT to fantasy football could be in the prediction (and evaluation) of whether or not a player scores a touchdown in a game.\nSDT metrics of sensitivity include \\(d'\\) (“\\(d\\)-prime”), \\(A\\) (or \\(A'\\)), and the area under the receiver operating characteristic (ROC) curve. SDT metrics of bias include \\(\\beta\\) (beta), \\(c\\), and \\(b\\).\n\n17.6.7.1 Receiver Operating Characteristic (ROC) Curve\nThe x-axis of the ROC curve is the false alarm rate or false positive rate (\\(1 -\\) specificity). The y-axis is the hit rate or true positive rate (sensitivity). We can trace the ROC curve as the combination between sensitivity and specificity at every possible cutoff. At a cutoff of zero (top right of ROC curve), we calculate sensitivity (1.0) and specificity (0) and plot it. At a cutoff of zero, the assessment tells us to make an action for every stimulus (i.e., it is the most liberal). We then gradually increase the cutoff, and plot sensitivity and specificity at each cutoff. As the cutoff increases, sensitivity decreases and specificity increases. We end at the highest possible cutoff, where the sensitivity is 0 and the specificity is 1.0 (i.e., we never make an action; i.e., it is the most conservative). Each point on the ROC curve corresponds to a pair of hit and false alarm rates (sensitivity and specificity) resulting from a specific cutoff value. Then, we can draw lines or a curve to connect the points.\nINSERT depicts an empirical ROC plot where lines are drawn to connect the hit and false alarm rates.\nINSERT depicts an ROC curve where a smoothed and fitted curve is drawn to connect the hit and false alarm rates.\n\n17.6.7.1.1 Area Under the ROC Curve\nROC methods can be used to compare and compute the discriminative power of measurement devices free from the influence of selection ratios, base rates, and costs and benefits. An ROC analysis yields a quantitative index of how well an index predicts a signal of interest or can discriminate between different signals. ROC analysis can help tell us how often our assessment would be correct. If we randomly pick two observations, and we were right once and wrong once, we were 50% accurate. But this would be a useless measure because it reflects chance responding.\nThe geometrical area under the ROC curve reflects the discriminative accuracy of the measure. The index is called the area under the curve (AUC) of an ROC curve. AUC quantifies the discriminative power of an assessment. AUC is the probability that a randomly selected target and a randomly selected non-target is ranked correctly by the assessment method. AUC values range from 0.0 to 1.0, where chance accuracy is 0.5 as indicated by diagonal line in the ROC curve. That is, a measure can be useful to the extent that its ROC curve is above the diagonal line (i.e., its discriminative accuracy is above chance).\nAUC is a threshold-independent accuracy index that applies across all possible cutoff values.\nFigure 17.5 depicts ROC curves with a range of AUC values.\n\nCodeset.seed(52242)\n\nauc60 &lt;- petersenlab::simulateAUC(.60, 50000)\nauc70 &lt;- petersenlab::simulateAUC(.70, 50000)\nauc80 &lt;- petersenlab::simulateAUC(.80, 50000)\nauc90 &lt;- petersenlab::simulateAUC(.90, 50000)\nauc95 &lt;- petersenlab::simulateAUC(.95, 50000)\nauc99 &lt;- petersenlab::simulateAUC(.99, 50000)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc60,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .52,\n  print.auc.y = .61,\n  print.auc.pattern = \"%.2f\")\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc70,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .6,\n  print.auc.y = .67,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc80,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .695,\n  print.auc.y = .735,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc90,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .805,\n  print.auc.y = .815,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc95,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .875,\n  print.auc.y = .865,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc99,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .94,\n  print.auc.y = .94,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\n\n\n\n\n\nFigure 17.5: Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures.\n\n\n\n\nAs an example, given an AUC of .75, this says that the overall score of an individual who has the characteristic in question will be higher 75% of the time than the overall score of an individual who does not have the characteristic. In lay terms, AUC provides the probability that we will classify correctly based on our instrument if we were to randomly pick one good and one bad outcome. AUC is a stronger index of accuracy than percent accuracy, because you can have high percent accuracy just by going with the base rate. AUC tells us how much better than chance a measure is at discriminating outcomes. AUC is useful as a measure of general discriminative accuracy, and it tells us how accurate a measure is at all possible cutoffs. Knowing the accuracy of a measure at all possible cutoffs can be helpful for selecting the optimal cutoff, given the goals of the assessment. In reality, however, we may not be interested in all cutoffs because not all errors are equal in their costs.\nIf we lower the base rate, we would need a larger sample to get enough people to classify into each group. SDT/ROC methods are traditionally about dichotomous decisions (yes/no), not graded judgments. SDT/ROC methods can get messy with ordinal data that are more graded because you would have an AUC curve for each ordinal grouping.\n\n17.6.8 Accuracy Indices\nThere are various accuracy indices we can use to evaluate the accuracy of predictions for categorical outcome variables. We have already described several accuracy indices, including percent accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and area under the ROC curve. We describe these and other indices in greater detail below.\nThe petersenlab package (Petersen, 2025a) contains the accuracyAtCutoff() function that computes many accuracy indices for the prediction of categorical outcome variables.\n\nCode#petersenlab::accuracyAtCutoff()\n\n\nThe petersenlab package (Petersen, 2025a) contains the accuracyAtEachCutoff() function that computes many accuracy indices for the prediction of categorical outcome variables at each possible cutoff.\n\nCode#petersenlab::accuracyAtEachCutoff()\n\n\nThere are also test calculators available online:\n\n\nhttp://araw.mede.uic.edu/cgi-bin/testcalc.pl [Schwartz (2006); archived at https://perma.cc/X8TF-7YBX]\n\nhttps://dlrs.shinyapps.io/shinyDLRs (Goodman et al., 2022)\n\n\n\n17.6.8.1 Confusion Matrix aka 2x2 Accuracy Table aka Cross-Tabulation aka Contingency Table\nA confusion matrix (aka 2x2 accuracy table, cross-tabulation table, or contigency table) is a matrix for categorical data that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes). In such a case, the confusion matrix provides a tabular count of each type of accurate cases (true positives and true negatives) versus the number of each type of error (false positives and false negatives), as shown in INSERT. An example of a confusion matrix is in INSERT.\n\n17.6.8.1.1 Number\n\nCode#table(mydata$diagnosisFactor, mydata$diseaseFactor)\n\n\n\n17.6.8.1.2 Number with margins added\n\nCode#addmargins(table(mydata$diagnosisFactor, mydata$diseaseFactor))\n\n\n\n17.6.8.1.3 Proportions\n\nCode#prop.table(table(mydata$diagnosisFactor, mydata$diseaseFactor))\n\n\n\n17.6.8.1.4 Proportions with margins added\n\nCode#addmargins(prop.table(table(mydata$diagnosisFactor, mydata$diseaseFactor)))\n\n\n\n17.6.8.2 True Positives (TP)\nTrue positives (TPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is correct—that is, the test says that a classification is present, and the classification is present. True positives are also called valid positives (VPs) or hits. Higher values reflect greater accuracy. The formula for true positives is in Equation 17.4:\n\\[\n\\begin{aligned}\n  \\text{TP} &= \\text{BR} \\times \\text{SR} \\times N\n\\end{aligned}\n\\tag{17.4}\\]\n\n17.6.8.3 True Negatives (TN)\nTrue negatives (TNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is correct—that is, the test says that a classification is not present, and the classification is actually not present. True negatives are also called valid negatives (VNs) or correct rejections. Higher values reflect greater accuracy. The formula for true negatives is in Equation 17.5:\n\\[\n\\begin{aligned}\n  \\text{TN} &= (1 - \\text{BR}) \\times (1 - \\text{SR}) \\times N\n\\end{aligned}\n\\tag{17.5}\\]\n\n17.6.8.4 False Positives (FP)\nFalse positives (FPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is incorrect—that is, the test says that a classification is present, and the classification is not present. False positives are also called false alarms (FAs). Lower values reflect greater accuracy. The formula for false positives is in Equation Equation 17.6:\n\\[\n\\begin{aligned}\n  \\text{FP} &= (1 - \\text{BR}) \\times \\text{SR} \\times N\n\\end{aligned}\n\\tag{17.6}\\]\n\n17.6.8.5 False Negatives (FN)\nFalse negatives (FNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is incorrect—that is, the test says that a classification is not present, and the classification is present. False negatives are also called misses. Lower values reflect greater accuracy. The formula for false negatives is in Equation 17.7:\n\\[\n\\begin{aligned}\n  \\text{FN} &= \\text{BR} \\times (1 - \\text{SR}) \\times N\n\\end{aligned}\n\\tag{17.7}\\]\n\n17.6.8.6 Selection Ratio (SR)\nThe selection ratio (SR) is the marginal probability of selection, independent of other things: \\(P(R_i)\\). It is not an index of accuracy, per se. In medicine, the selection ratio is the proportion of people who test positive for the disease. In fantasy football, the selection ratio is the proportion of players who you predict will show a given outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the selection ratio is the proportion of players who you predict will score a touchdown. The formula for calculating the selection ratio is in Equation 17.8.\n\\[\n\\begin{aligned}\n  \\text{SR} &= P(R_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FP}}{N}\n\\end{aligned}\n\\tag{17.8}\\]\n\n17.6.8.7 Base Rate (BR)\nThe base rate (BR) of a classification is its marginal probability, independent of other things: \\(P(C_i)\\). It is not an index of accuracy, per se. In medicine, the base rate of a disease is its prevalence in the population, as in Equation 17.9. Without additional information, the base rate is used as the initial pretest probability. In fantasy football, the base rate is the proportion of players who actually show the particular outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the base rate is the proportion of players who actually score a touchdown in the game. The formula for calculating the selection ratio is in Equation 17.9.\n\\[\n\\begin{aligned}\n  \\text{BR} &= P(C_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FN}}{N}\n\\end{aligned}\n\\tag{17.9}\\]\n\n17.6.8.8 Pretest Odds\nThe pretest odds of a classification can be estimated using the pretest probability (i.e., base rate). To convert a probability to odds, divide the probability by one minus that probability, as in Equation 17.10.\n\\[\n\\begin{aligned}\n  \\text{pretest odds} &= \\frac{\\text{pretest probability}}{1 - \\text{pretest probability}} \\\\\n\\end{aligned}\n\\tag{17.10}\\]\n\n17.6.8.9 Percent Accuracy\nPercent Accuracy is also called overall accuracy. Higher values reflect greater accuracy. The formula for percent accuracy is in Equation 17.11. Percent accuracy has several problems. First, it treats all errors (FP and FN) as equally important. However, in practice, it is rarely the case that false positives and false negatives are equally important. Second, percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the characteristic (if the base rate is low) or that everyone has the characteristic (if the base rate is high). Thus, it is also important to consider other aspects of accuracy.\n\\[\n\\text{Percent Accuracy} = 100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\n\\tag{17.11}\\]\n\n17.6.8.10 Percent Accuracy by Chance\nThe formula for calculating percent accuracy by chance is in Equation 17.12.\n\\[\n\\begin{aligned}\n  \\text{Percent Accuracy by Chance} &= 100\\% \\times [P(\\text{TP}) + P(\\text{TN})] \\\\\n  &= 100\\% \\times \\{(\\text{BR} \\times {\\text{SR}}) + [(1 - \\text{BR}) \\times (1 - \\text{SR})]\\}\n\\end{aligned}\n\\tag{17.12}\\]\n\n17.6.8.11 Percent Accuracy Predicting from the Base Rate\nPredicting from the base rate is going with the most likely outcome in every prediction. If the base rate is less than .50, it would involve predicting that the condition is absent for every case. If the base rate is .50 or above, it would involve predicting that the condition is present for every case. Predicting from the base rate is a special case of percent accuracy by chance when the selection ratio is set to either one (if the base rate \\(\\geq\\) .5) or zero (if the base rate &lt; .5).\n\n17.6.8.12 Relative Improvement Over Chance (RIOC)\nRelative improvement over chance (RIOC) is a prediction’s improvement over chance as a proportion of the maximum possible improvement over chance, as described by Farrington & Loeber (1989). Higher values reflect greater accuracy. The formula for calculating RIOC is in Equation 17.13.\n\\[\n\\begin{aligned}\n  \\text{relative improvement over chance (RIOC)} &= \\frac{\\text{total correct} - \\text{chance correct}}{\\text{maximum correct} - \\text{chance correct}} \\\\\n\\end{aligned}\n\\tag{17.13}\\]\n\n17.6.8.13 Relative Improvement Over Predicting from the Base Rate\nRelative improvement over predicting from the base rate is a prediction’s improvement over predicting from the base rate as a proportion of the maximum possible improvement over predicting from the base rate. Higher values reflect greater accuracy. The formula for calculating relative improvement over predicting from the base rate is in Equation 17.14.\n\\[\n\\scriptsize\n\\begin{aligned}\n  \\text{relative improvement over predicting from base rate} &= \\frac{\\text{total correct} - \\text{correct by predicting from base rate}}{\\text{maximum correct} - \\text{correct by predicting from base rate}} \\\\\n\\end{aligned}\n\\tag{17.14}\\]\n\n17.6.8.14 Sensitivity (SN)\nSensitivity (SN) is also called true positive rate (TPR), hit rate (HR), or recall. Sensitivity is the conditional probability of a positive test given that the person has the condition: \\(P(R|C)\\). Higher values reflect greater accuracy. The formula for calculating sensitivity is in Equation 17.15. As described in Section Section 17.6.6.1, as the cutoff increases (becomes more conservative), sensitivity decreases. As the cutoff decreases, sensitivity increases.\n\\[\n\\begin{aligned}\n  \\text{sensitivity (SN)} &= P(R|C) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{TP}}{N \\times \\text{BR}} = 1 - \\text{FNR}\n\\end{aligned}\n\\tag{17.15}\\]\n\n17.6.8.15 Specificity (SP)\nSpecificity (SP) is also called true negative rate (TNR) or selectivity. Specificity is the conditional probability of a negative test given that the person does not have the condition: \\(P(\\text{not } R|\\text{not } C)\\). Higher values reflect greater accuracy. The formula for calculating specificity is in Equation 17.16. As described in Section Section 17.6.6.1, as the cutoff increases (becomes more conservative), specificity increases. As the cutoff decreases, specificity decreases.\n\\[\n\\begin{aligned}\n  \\text{specificity (SP)} &= P(\\text{not } R|\\text{not } C) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{\\text{TN}}{N (1 - \\text{BR})} = 1 - \\text{FPR}\n\\end{aligned}\n\\tag{17.16}\\]\n\n17.6.8.16 False Negative Rate (FNR)\nThe false negative rate (FNR) is also called the miss rate. The false negative rate is the conditional probability of a negative test given that the person has the condition: \\(P(\\text{not } R|C)\\). Lower values reflect greater accuracy. The formula for calculating false negative rate is in Equation 17.17.\n\\[\n\\begin{aligned}\n  \\text{false negative rate (FNR)} &= P(\\text{not } R|C) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TP}} = \\frac{\\text{FN}}{N \\times \\text{BR}} = 1 - \\text{TPR}\n\\end{aligned}\n\\tag{17.17}\\]\n\n17.6.8.17 False Positive Rate (FPR)\nThe false positive rate (FPR) is also called the false alarm rate (FAR) or fall-out. The false positive rate is the conditional probability of a positive test given that the person does not have the condition: \\(P(R|\\text{not } C)\\). Lower values reflect greater accuracy. The formula for calculating false positive rate is in Equation 17.18:\n\\[\n\\begin{aligned}\n  \\text{false positive rate (FPR)} &= P(R|\\text{not } C) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} = \\frac{\\text{FP}}{N (1 - \\text{BR})} = 1 - \\text{TNR}\n\\end{aligned}\n\\tag{17.18}\\]\n\n17.6.8.18 Positive Predictive Value (PPV)\nThe positive predictive value (PPV) is also called the positive predictive power (PPP) or precision. Many people confuse sensitivity (\\(P(R|C)\\)) with its inverse conditional probability, PPV (\\(P(C|R)\\)). PPV is the conditional probability of having the condition given a positive test: \\(P(C|R)\\). Higher values reflect greater accuracy. The formula for calculating positive predictive value is in Equation 17.19.\nPPV can be low even when sensitivity is high because it depends not only on sensitivity, but also on specificity and the base rate. Because PPV depends on the base rate, PPV is not an intrinsic property of a measure. The same measure will have a different PPV in different contexts with different base rates (Treat & Viken, 2023). As described in Section Section 17.6.6.1, as the base rate increases, PPV increases. As the base rate decreases, PPV decreases. PPV also differs as a function of the cutoff. As described in Section Section 17.6.6.1, as the cutoff increases (becomes more conservative), PPV increases. As the cutoff decreases (becomes more liberal), PPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{positive predictive value (PPV)} &= P(C|R) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{\\text{TP}}{N \\times \\text{SR}}\\\\\n  &= \\frac{\\text{sensitivity} \\times {\\text{BR}}}{\\text{sensitivity} \\times {\\text{BR}} + [(1 - \\text{specificity}) \\times (1 - \\text{BR})]}\n\\end{aligned}\n\\tag{17.19}\\]\n\n17.6.8.19 Negative Predictive Value (NPV)\nThe negative predictive value (NPV) is also called the negative predictive power (NPP). Many people confuse specificity (\\(P(\\text{not } R|\\text{not } C)\\)) with its inverse conditional probability, NPV (\\(P(\\text{not } C| \\text{not } R)\\)). NPV is the conditional probability of not having the condition given a negative test: \\(P(\\text{not } C| \\text{not } R)\\). Higher values reflect greater accuracy. The formula for calculating negative predictive value is in Equation 17.20.\nNPV can be low even when specificity is high because it depends not only on specificity, but also on sensitivity and the base rate. Because NPV depends on the base rate, NPV is not an intrinsic property of a measure. The same measure will have a different NPV in different contexts with different base rates (Treat & Viken, 2023). As described in Section Section 17.6.6.1, as the base rate increases, NPV decreases. As the base rate decreases, NPV increases. NPV also differs as a function of the cutoff. As described in Section Section 17.6.6.1, as the cutoff increases (becomes more conservative), NPV decreases. As the cutoff decreases (becomes more liberal), NPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{negative predictive value (NPV)} &= P(\\text{not } C|\\text{not } R) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{\\text{TN}}{N(\\text{1 - SR})}\\\\\n  &= \\frac{\\text{specificity} \\times (1-{\\text{BR}})}{\\text{specificity} \\times (1-{\\text{BR}}) + [(1 - \\text{sensitivity}) \\times \\text{BR})]}\n\\end{aligned}\n\\tag{17.20}\\]\n\n17.6.8.20 False Discovery Rate (FDR)\nMany people confuse the false positive rate (\\(P(R|\\text{not } C)\\)) with its inverse conditional probability, the false discovery rate (\\(P(\\text{not } C| R)\\)). The false discovery rate (FDR) is the conditional probability of not having the condition given a positive test: \\(P(\\text{not } C| R)\\). Lower values reflect greater accuracy. The formula for calculating false discovery rate is in Equation 17.21.\n\\[\n\\begin{aligned}\n  \\text{false discovery rate (FDR)} &= P(\\text{not } C|R) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TP}} = 1 - \\text{PPV}\n\\end{aligned}\n\\tag{17.21}\\]\n\n17.6.8.21 False Omission Rate (FOR)\nMany people confuse the false negative rate (\\(P(\\text{not } R|C)\\)) with its inverse conditional probability, the false omission rate (\\(P(C|\\text{not } R)\\)). The false omission rate (FOR) is the conditional probability of having the condition given a negative test: \\(P(C|\\text{not } R)\\). Lower values reflect greater accuracy. The formula for calculating false omission rate is in Section 17.6.8.21.\n\\[\n\\begin{aligned}\n  \\text{false omission rate (FOR)} &= P(C|\\text{not } R) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TN}} = 1 - \\text{NPV}\n\\end{aligned}\n\\tag{17.22}\\]\n\n17.6.8.22 Youden’s J Statistic\nYouden’s J statistic is also called Youden’s Index or informedness. Youden’s J statistic is the sum of sensitivity and specificity (and subtracting one). Higher values reflect greater accuracy. The formula for calculating Youden’s J statistic is in Equation 17.23.\n\\[\n\\begin{aligned}\n  \\text{Youden's J statistic} &= \\text{sensitivity} + \\text{specificity} - 1\n\\end{aligned}\n\\tag{17.23}\\]\n\n17.6.8.23 Balanced Accuracy\nBalanced accuracy is the average of sensitivity and specificity. Higher values reflect greater accuracy. The formula for calculating balanced accuracy is in Equation 17.24.\n\\[\n\\begin{aligned}\n  \\text{balanced accuracy} &= \\frac{\\text{sensitivity} + \\text{specificity}}{2}\n\\end{aligned}\n\\tag{17.24}\\]\n\n17.6.8.24 F-Score\nThe F-score combines precision (positive predictive value) and recall (sensitivity), where \\(\\beta\\) indicates how many times more important sensitivity is than the positive predictive value. If sensitivity and the positive predictive value are equally important, \\(\\beta = 1\\), and the F-score is called the \\(F_1\\) score. Higher values reflect greater accuracy. The formula for calculating the F-score is in Equation 17.25.\n\\[\n\\begin{aligned}\n  F_\\beta &= (1 + \\beta^2) \\cdot \\frac{\\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\beta^2 \\cdot \\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{(1 + \\beta^2) \\cdot \\text{TP}}{(1 + \\beta^2) \\cdot \\text{TP} + \\beta^2 \\cdot \\text{FN} + \\text{FP}}\n\\end{aligned}\n\\tag{17.25}\\]\nThe formula for calculating the \\(F_1\\) score is in Equation 17.26.\n\\[\n\\begin{aligned}\n  F_1 &= \\frac{2 \\cdot \\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FN} + \\text{FP}}\n\\end{aligned}\n\\tag{17.26}\\]\n\n17.6.8.25 Matthews Correlation Coefficient (MCC)\nThe Matthews correlation coefficient (MCC) is also called the phi coefficient. It is a correlation coefficient between predicted and observed values from a binary classification. Higher values reflect greater accuracy. The formula for calculating the MCC is in Equation 17.27.\n\\[\n\\begin{aligned}\n  \\text{MCC} &= \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n\\end{aligned}\n\\tag{17.27}\\]\n\n17.6.8.26 Diagnostic Odds Ratio\nThe diagnostic odds ratio is the odds of a positive test among people with the condition relative to the odds of a positive test among people without the condition. Higher values reflect greater accuracy. The formula for calculating the diagnostic odds ratio is in Equation 17.28. If the predictor is bad, the diagnostic odds ratio could be less than one, and values can go up from there. If the diagnostic odds ratio is greater than 2, we take the odds ratio seriously because we are twice as likely to predict accurately than inaccurately. However, the diagnostic odds ratio ignores/hides base rates. When interpreting the diagnostic odds ratio, it is important to keep in mind the practical significance, because otherwise it is not very meaningful. Consider a risk factor that has a diagnostic odds ratio of 3 for tuberculosis, i.e., it puts you at 3 times as likely to develop tuberculosis. The prevalence of tuberculosis is relatively low. Assuming the prevalence of tuberculosis is less than 1/10th of 1%, your risk of developing tuberculosis is still very low even if the risk factor (with a diagnostic odds ratio of 3) is present.\n\\[\n\\begin{aligned}\n  \\text{diagnostic odds ratio} &= \\frac{\\text{TP} \\times \\text{TN}}{\\text{FP} \\times \\text{FN}} \\\\\n  &= \\frac{\\text{sensitivity} \\times \\text{specificity}}{(1 - \\text{sensitivity}) \\times (1 - \\text{specificity})} \\\\\n  &= \\frac{\\text{PPV} \\times \\text{NPV}}{(1 - \\text{PPV}) \\times (1 - \\text{NPV})} \\\\\n  &= \\frac{\\text{LR+}}{\\text{LR}-}\n\\end{aligned}\n\\tag{17.28}\\]\n\n17.6.8.27 Diagnostic Likelihood Ratio\nThe diagnostic likelihood ratio is described in Section 16.8.2.1. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio.\n\n17.6.8.27.1 Positive Likelihood Ratio (LR+)\nThe positive likelihood ratio (LR+) is described in Section 16.8.2.1.1. The formula for calculating the positive likelihood ratio is in Equation 16.22.\n\n17.6.8.27.2 Negative Likelihood Ratio (LR−)\nThe negative likelihood ratio (LR−) is described in Section 16.8.2.1.2. The formula for calculating the negative likelihood ratio is in Equation 16.22.\n\n17.6.8.28 Posttest Odds\nAs presented in Equation 16.21, the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. The posttest odds and posttest probability can be useful to calculate when the pretest probability is different from the pretest probability (or prevalence) of the classification. For instance, you might use a different pretest probability if a test result is already known and you want to know the updated posttest probability after conducting a second test. The formula for calculating posttest odds is in Equation 17.29.\n\\[\n\\begin{aligned}\n  \\text{posttest odds} &= \\text{pretest odds} \\times \\text{likelihood ratio} \\\\\n\\end{aligned}\n\\tag{17.29}\\]\nFor calculating the posttest odds of a true positive compared to a false positive, we use the positive likelihood ratio below. We would use the negative likelihood ratio if we wanted to calculate the posttest odds of a false negative compared to a true negative.\n\n17.6.8.29 Posttest Probability\nThe posttest probability is the probability of having the characteristic given a test result. When the base rate is used as the pretest probability, the posttest probability given a positive test is equal to positive predictive value. To convert odds to a probability, divide the odds by one plus the odds, as is in Equation 17.30.\n\\[\n\\begin{aligned}\n  \\text{posttest probability} &= \\frac{\\text{posttest odds}}{1 + \\text{posttest odds}}\n\\end{aligned}\n\\tag{17.30}\\]\n\n17.6.8.30 Mean Difference Between Predicted and Observed Values\nThe mean difference between predicted values versus observed values at a given cutoff is an index of miscalibration of predictions at that cutoff. It is called “calibration-in-the-small” (as opposed to calibration-in-the-large, which spans all cutoffs). Values closer to zero reflect greater accuracy. Values above zero indicate that the predicted values are, on average, greater than the observed values. Values below zero indicate that the observed values are, on average, greater than the predicted values.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-thresholdIndependentAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-thresholdIndependentAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.7 Threshold-Independent Accuracy Indices",
    "text": "17.7 Threshold-Independent Accuracy Indices\nThis section describes threshold-independent indexes of accuracy. That is, each index of accuracy described in this section provides a single numerical index of accuracy that aggregates the accuracy across all possible cutoffs. The petersenlab package (Petersen, 2025a) contains the accuracyOverall() function that computes many threshold-independent accuracy indices.\n\n17.7.1 General Prediction Accuracy\nThere are many metrics of general prediction accuracy. When thinking about which metric(s) may be best for a given problem, it is important to consider the purpose of the assessment. The estimates of general prediction accuracy are separated below into scale-dependent and scale-independent accuracy estimates.\n\n17.7.1.1 Scale-Dependent Accuracy Estimates\nThe estimates of prediction accuracy described in this section are scale-dependent. These accuracy estimates depend on the unit of measurement and therefore cannot be compared across measures with different scales or across data sets.\n\n17.7.1.1.1 Mean Error\nHere, “error” (\\(e\\)) is the difference between the predicted and observed value for a given individual (\\(i\\)). Mean error (ME; also known as bias) is the mean difference between the predicted and observed values across individuals (\\(i\\)), that is, the mean of the errors across individuals (\\(e_i\\)). Values closer to zero reflect greater accuracy. If mean error is above zero, it indicates that predicted values are, on average, greater than observed values (i.e., overestimating errors). If mean error is below zero, it indicates that predicted values are, on average, less than observed values (i.e., underestimating errors). If both over-estimating and under-estimating errors are present, however, they can cancel each other out. As a result, even with a mean error of zero, there can still be considerable error present. Thus, although mean error can be helpful for examining whether predictions systematically under- or over-estimate the actual scores, other forms of accuracy are necessary to examine the extent of error. The formula for mean error is in Equation 17.31:\n\\[\n\\begin{aligned}\n  \\text{mean error} &= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)}{n} \\\\\n  &= \\text{mean}(e_i)\n\\end{aligned}\n\\tag{17.31}\\]\n\n17.7.1.1.2 Mean Absolute Error (MAE)\nMean absolute error (MAE) is the mean of the absolute value of differences between the predicted and observed values across individuals, that is, the mean of the absolute value of errors. Smaller MAE values (closer to zero) reflect greater accuracy. MAE is preferred over root mean squared error (RMSE) when you want to give equal weight to all errors and when the outliers have considerable impact. The formula for MAE is in Equation 17.32:\n\\[\n\\begin{aligned}\n  \\text{mean absolute error (MAE)} &= \\frac{\\sum\\limits_{i = 1}^n|\\text{predicted}_i - \\text{observed}_i|}{n} \\\\\n  &= \\text{mean}(|e_i|)\n\\end{aligned}\n\\tag{17.32}\\]\n\n17.7.1.1.3 Mean Squared Error (MSE)\nMean squared error (MSE) is the mean of the square of the differences between the predicted and observed values across individuals, that is, the mean of the squared value of errors. Smaller MSE values (closer to zero) reflect greater accuracy. MSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, MSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for MSE is in Equation 17.33:\n\\[\n\\begin{aligned}\n  \\text{mean squared error (MSE)} &= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n} \\\\\n  &= \\text{mean}(e_i^2)\n\\end{aligned}\n\\tag{17.33}\\]\n\n17.7.1.1.4 Root Mean Squared Error (RMSE)\nRoot mean squared error (RMSE) is the square root of the mean of the square of the differences between the predicted and observed values across individuals, that is, the root mean squared value of errors. Smaller RMSE values (closer to zero) reflect greater accuracy. RMSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, RMSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for RMSE is in Equation 17.34:\n\\[\n\\begin{aligned}\n  \\text{root mean squared error (RMSE)} &= \\sqrt{\\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n}} \\\\\n  &= \\sqrt{\\text{mean}(e_i^2)}\n\\end{aligned}\n\\tag{17.34}\\]\n\n17.7.1.2 Scale-Independent Accuracy Estimates\nThe estimates of prediction accuracy described in this section are intended to be scale-independent (unit-free) so the accuracy estimates can be compared across measures with different scales or across data sets (Hyndman & Athanasopoulos, 2021).\n\n17.7.1.2.1 Mean Percentage Error (MPE)\nMean percentage error (MPE) values closer to zero reflect greater accuracy. The formula for percentage error is in Equation 17.35:\n\\[\n\\begin{aligned}\n  \\text{percentage error }(p_i) = \\frac{100\\% \\times (\\text{observed}_i - \\text{predicted}_i)}{\\text{observed}_i}\n\\end{aligned}\n\\tag{17.35}\\]\nWe then take the mean of the percentage errors to get MPE. The formula for MPE is in Equation 17.36:\n\\[\n\\begin{aligned}\n  \\text{mean percentage error (MPE)} &= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i} \\\\\n  &= \\text{mean(percentage error)} \\\\\n  &= \\text{mean}(p_i)\n\\end{aligned}\n\\tag{17.36}\\]\nNote: MPE is undefined when one or more of the observed values equals zero, due to division by zero. The accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.2 Mean Absolute Percentage Error (MAPE)\nSmaller mean absolute percentage error (MAPE) values (closer to zero) reflect greater accuracy. The formula for MAPE is in Equation 17.37. MAPE is asymmetric because it overweights underestimates and underweights overestimates. MAPE can be preferable to symmetric mean absolute percentage error (sMAPE) if there are no observed values of zero and if you want to emphasize the importance of underestimates (relative to overestimates).\n\\[\n\\begin{aligned}\n  \\text{mean absolute percentage error (MAPE)} &= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\Bigg|\\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i}\\Bigg| \\\\\n  &= \\text{mean(|percentage error|)} \\\\\n  &= \\text{mean}(|p_i|)\n\\end{aligned}\n\\tag{17.37}\\]\nNote: MAPE is undefined when one or more of the observed values equals zero, due to division by zero. The accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.3 Symmetric Mean Absolute Percentage Error (sMAPE)\nUnlike MAPE, symmetric mean absolute percentage error (sMAPE) is symmetric because it equally weights underestimates and overestimates. Smaller sMAPE values (closer to zero) reflect greater accuracy. The formula for sMAPE is in Equation 17.38:\n\\[\n\\small\n\\begin{aligned}\n  \\text{symmetric mean absolute percentage error (sMAPE)} = \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{|\\text{predicted}_i - \\text{observed}_i|}{|\\text{predicted}_i| + |\\text{observed}_i|}\n\\end{aligned}\n\\tag{17.38}\\]\nNote: sMAPE is undefined when one or more of the individuals has a prediction–observed combination such that the sum of the absolute value of the predicted value and the absolute value of the observed value equals zero (\\(|\\text{predicted}_i| + |\\text{observed}_i|\\)), due to division by zero. The accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.4 Mean Absolute Scaled Error (MASE)\nMean absolute scaled error (MASE) is described by (Hyndman & Athanasopoulos, 2021). Values closer to zero reflect greater accuracy.\nThe adapted formula for MASE with non-time series data is described by Hyndman (2014) at the following link: https://stats.stackexchange.com/a/108963/20338 (archived at https://perma.cc/G469-8NAJ). Scaled errors are calculated using Equation 17.39:\n\\[\n\\begin{aligned}\n  \\text{scaled error}(q_i) &= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{scaling factor}} \\\\\n  &= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|}\n\\end{aligned}\n\\tag{17.39}\\]\nThen, we calculate the mean of the absolute value of the scaled errors to get MASE, as in Equation 17.40:\n\\[\n\\begin{aligned}\n  \\text{mean absolute scaled error (MASE)} &= \\frac{1}{n} \\sum\\limits_{i = 1}^n |q_i| \\\\\n  &= \\text{mean(|scaled error|)} \\\\\n  &= \\text{mean}(|q_i|)\n\\end{aligned}\n\\tag{17.40}\\]\nNote: MASE is undefined when the scaling factor is zero, due to division by zero. With non-time series data, the scaling factor is the average of the absolute value of individuals’ observed scores minus the average observed score (\\(\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|\\)).\n\n17.7.1.2.5 Root Mean Squared Log Error (RMSLE)\nThe squared log of the accuracy ratio is described by Tofallis (2015). The accuracy ratio is in Equation 17.41:\n\\[\n\\begin{aligned}\n  \\text{accuracy ratio} &= \\frac{\\text{predicted}_i}{\\text{observed}_i}\n\\end{aligned}\n\\tag{17.41}\\]\nHowever, the accuracy ratio is undefined with observed or predicted values of zero, so it is common to modify it by adding 1 to the predictor and denominator, as in Equation 17.42:\n\\[\n\\begin{aligned}\n  \\text{accuracy ratio} &= \\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\n\\end{aligned}\n\\tag{17.42}\\]\nSquaring the log values keeps the values positive, such that smaller values (values closer to zero) reflect greater accuracy. Then we take the mean of the squared log values, which keeps the values positive, and calculate the square root of the mean squared log values to put them back on the (pre-squared) log metric. This is known as the root mean squared log error (RMSLE). Division inside the log is equal to subtraction outside the log. So, the formula can be reformulated with the subtraction of two logs, as in Equation 17.43:\n\\[\n\\scriptsize\n\\begin{aligned}\n  \\text{root mean squared log error (RMSLE)} &= \\sqrt{\\sum\\limits_{i = 1}^n log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2} \\\\\n  &= \\sqrt{\\text{mean}\\Bigg[log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2\\Bigg]} \\\\\n  &= \\sqrt{\\text{mean}\\big[log(\\text{accuracy ratio})^2\\big]} = \\sqrt{\\text{mean}\\Big\\{\\big[log(\\text{predicted}_i + 1) - log(\\text{actual}_i + 1)\\big]^2\\Big\\}}\n\\end{aligned}\n\\tag{17.43}\\]\nRMSLE can be preferable when the scores have a wide range of values and are skewed. RMSLE can help to reduce the impact of outliers. RMSLE gives more weight to smaller errors in the prediction of small observed values, while also penalizing larger errors in the prediction of larger observed values. It overweights underestimates and underweights overestimates.\nThere are other variations of prediction accuracy metrics that use the log of the accuracy ratio. One variation makes it similar to median symmetric percentage error (Morley et al., 2018).\nNote: Root mean squared log error is undefined when one or more predicted values or actual values equals −1. When predicted or actual values are -1, this leads to \\(log(0)\\), which is undefined. The accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.6 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions: \\(R^2 = \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\\). Larger values indicate greater accuracy.\n\\(R^2\\) is commonly estimated in multiple regression, in which multiple predictors are allowed to predict one outcome.\n\n17.7.1.2.6.1 Adjusted \\(R^2\\) (\\(R^2_{adj}\\))\nAdjusted \\(R^2\\) is similar to the coefficient of determination, but it accounts for the number of predictors included in the regression model to penalize overfitting. Adjusted \\(R^2\\) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictors in the model. Larger values indicate greater accuracy. The formula for adjusted \\(R^2\\) is in Equation 11.4. Adjusted \\(R^2\\) is described further in Section 11.5.\n\n17.7.1.2.6.2 Predictive \\(R^2\\)\n\nPredictive \\(R^2\\) is described by Hopper (2014) here: https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/ (archived at https://perma.cc/BK8J-HFUK). Predictive \\(R^2\\) penalizes overfitting, unlike traditional \\(R^2\\). Larger values indicate greater accuracy.\n\n17.7.2 Discrimination\nWhen dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Threshold-dependent aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity) are described in Section 17.6.\n\n17.7.2.1 Area under the ROC curve (AUC)\nThe area under the ROC curve (AUC) is a general index of discrimination accuracy for a categorical outcome. It is also called the concordance (\\(c\\)) statistic. Larger values reflect greater discrimination accuracy. AUC was estimated using the pROC package (Robin et al., 2011, 2023).\n\n17.7.2.2 Effect Size (\\(\\beta\\)) of Regression\nThe effect size of a predictor, i.e., the standardized regression coefficient is called a beta (\\(\\beta\\)) coefficient, is a general index of discrimination accuracy for a continuous outcome. Larger values reflect greater accuracy. We can obtain standardized regression coefficients by standardizing the predictors and outcome using the scale() function in R.\n\n17.7.3 Calibration\nWhen dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by Lindhiem et al. (2020). Calibration can be examined in several ways, including Spiegelhalter’s \\(z\\) (see Section 17.7.3.2), and the mean difference between predicted and observed values at different binned thresholds as depicted graphically with a calibration plot (see Figure 17.7).\n\n17.7.3.1 Calibration Plot\nCalibration plots can be helpful for identifying miscalibration. A calibration plot depicts the predicted probability of an event on the x-axis, and the actual (observed) probability of the event on the y-axis. The predictions are binned into a certain number of groups (commonly 10). The diagonal line reflects predictions that are perfectly calibrated. To the extent that predictions deviate from the diagonal line, the predictions are miscalibrated.\nWell-calibrated predictions are depicted in Figure 17.6:\n\nCode# Specify data\nexamplePredictionsWellCalibrated &lt;- seq(from = 0, to = 1, by = .1)\nexampleOutcomesWellCalibrated &lt;- seq(from = 0, to = 1, by = .1)\n\n# Plot\nplot(\n  examplePredictionsWellCalibrated,\n  exampleOutcomesWellCalibrated,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  type = \"n\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\npoints(\n  examplePredictionsWellCalibrated,\n  exampleOutcomesWellCalibrated,\n  cex = 1.5,\n  col = \"#e41a1c\",\n  lwd = 2,\n  type = \"p\")\n\n\n\n\n\n\nFigure 17.6: Predictions that are Well-Calibrated. That is, the predicted values are close to the observed values.\n\n\n\n\nThe various types of general miscalibration are depicted in Figure 17.7:\n\nCode# Specify data\nexamplePredictions &lt;- seq(from = 0, to = 1, by = .1)\nexampleOutcomes &lt;- c(0, .15, .3, .4, .45, .5, .55, .6, .7, .85, 1)\n\noverPrediction &lt;- c(0, .02, .05, .1, .15, .2, .3, .4, .5, .7, 1)\nunderPrediction &lt;- c(0, .3, .5, .6, .7, .8, .85, .9, .95, .98, 1)\noverExtremity &lt;- c(0, .3, .38, .42, .47, .5, .53, .58, .62, .7, 1)\nunderExtremity &lt;- c(0, .05, .08, .11, .2, .5, .8, .89, .92, .95, 1)\n\n# Plot\npar(\n  mfrow = c(2,2),\n  mar = c(5,4,1,1) + 0.1) #margins: bottom, left, top, right\n\nplot(\n  examplePredictions,\n  overExtremity,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Overextremity\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  underExtremity,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Underextremity\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  overPrediction,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Overprediction\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  underPrediction,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Underprediction\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\n\n\n\n\n\nFigure 17.7: Types of Miscalibration. From Petersen (2024) and Petersen (2025b).\n\n\n\n\nHowever, predictions could also be miscalibrated in more specific ways. For instance, predictions could be well-calibrated at all predicted probabilities except for a given predicted probability (e.g., 20%). Or, the predictions could be miscalibrated but not systematically over- or underpredicted. Thus, it is important to evaluate a calibration plot to evaluate the extent to which the predictions are miscalibrated and the pattern of that miscalibration.\n\n17.7.3.2 Spiegelhalter’s z\n\nSpiegelhalter’s z was calculated using the rms package (Harrell, Jr., 2024). Smaller z values (and larger associated p-values) reflect greater calibration accuracy. A statistically significant Spiegelhalter’s z (p &lt; .05) indicates a significant degree of miscalibration.\n\n17.7.3.3 Calibration for predicting a continuous outcome\nWhen predicting a continuous outcome, calibration of the predicted values in relation to the outcome values can be examined in multiple ways including:\n\nin a calibration plot, the extent to which the intercept is near zero and the slope is near one\nin a calibration plot, the extent to which the 95% confidence interval of the observed value, across all values of the predicted values, includes the diagonal reference line with an intercept of zero and a slope of one\nmean error\nmean absolute error\nmean squared error\nroot mean squared error\n\nWith a plot of the predictions on the x-axis, and the outcomes on the y-axis (i.e., a calibration plot), calibration can be examined graphically as the extent to which the best-fit regression line has an intercept (alpha) close to zero and a slope (beta) close to one (Stevens & Poppe, 2020; Steyerberg & Vergouwe, 2014). The intercept is also called “calibration-in-the-large”, whereas “calibration-in-the-small” refers to the extent to which the predicted values match the observed values at a specific predicted value (e.g., when the weather forecaster says that there is a 10% chance of rain, does it actually rain 10% of the time?). For predictions to be well calibrated, the intercept should be close to zero and the slope should be close to one. If the slope is close to one but the intercept is not close to zero (or the intercept is close to zero but the slope is not close to one), the predictions would not be considered well calibrated. The 95% confidence interval of the observed value, across all values of the predicted values, should include the diagonal reference line whose intercept is zero and whose slope is one.\nFor instance, based on the intercept and slope of the calibration plot in Figure INSERT, the predictions are not well calibrated, despite having a slope near one, because the 95% confidence interval of the intercept does not include zero. The best-fit line is the yellow line. The intercept from the best-fit line is positive, as shown in the regression equation. This is a case of underprediction, where the predicted values are consistently less than the observed values. The confidence interval of the observed value (i.e., the purple band) is the interval within which we have 95% confidence that the true observed value would lie for a given predicted value, based on the model The 95% prediction interval of the observed value (i.e., the dashed red lines) is the interval within which we would expect that 95% of future observations would lie for a given predicted value. The black diagonal line indicates the reference line with an intercept of zero and a slope of one. The predictions would be significantly miscalibrated at a given level of the predicted values if the 95% confidence interval of the observed value does not include the reference line at that level of the predicted value. In this case, the 95% confidence interval of the observed value does not include the reference line (i.e., the actual observed value) at lower levels of the predicted values, so the predictions are miscalibrated lower levels of the predicted values.\nGold-standard recommendations include examining the predicted values in relation to the observed values using locally estimated scatterplot smoothing (LOESS) (Austin & Steyerberg, 2014), such as in Figure INSERT. We can examine whether the LOESS-based 95% confidence interval of the observed value at every level of the predicted values includes the diagonal reference line (i.e., the actual observed value). In this case, the 95% confidence interval of the observed value does not include the reference line at lower levels of the predicted values, so the predictions are miscalibrated at lower levels of the predicted values.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-integratingAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-integratingAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.8 Integrating the Accuracy Indices",
    "text": "17.8 Integrating the Accuracy Indices\nAfter computing the accuracy indices of discrimination and (2) calibration, it is then the task to integrate the indices to determine (a) which are the most accurate predictions for the given goals, and (b) whether additional improvements and refinements to the predictions need to be made. Each of the accuracy indices is computed differently and thus reward (and penalize) predictive (in)accuracy differently. Sometimes, the the accuracy indices will paint a consistent picture regarding which predictions are the most accurate. Other times, the accuracy indices may disagree about which predictions are most accurate.\nIn fantasy football, when evaluating the accuracy of seasonal projections, we care most about accurately distinguishing between higher levels of points (e.g., 200 vs 150) as opposed to lower levels of points (e.g., 0 vs 10). Thus, it can be helpful to punish larger errors more heavily than smaller errors, as RMSE (unlike MAE).\nThus, we would emphasize the following metrics:\n\n\ndiscrimination:\n\nadjusted \\(R^2\\)\n\n\n\ncalibration:\n\ncalibration plot\n\n\n\ngeneral accuracy:\n\nMAE\nRMSE\n\n\n\nIf you focus on only one accuracy index, MAE or RMSE would be a good choice. However, I would also examine a calibration plot to evaluate whether predictions are poorly calibrated at higher levels of points. I would also examine ME—not to compare the accuracy of various predictions per se—but to determine whether predictions are systematically under- or overestimating actual points. If so, predictions may be able to be refined by adding or subtracting a constant to the predictions (or to a subset of the predictions); however, this could worsen other accuracy indices, so it is important to conduct an iterative process of modifying then evaluating, then further modifying and evaluating, etc. It may also be valuable to evaluate the accuracy of various subsets of the predictions. For instance, you might examine the predictive accuracy of players whose projected points are greater than 100, to evaluate the accuracy of predictions specifically to distinguish between players at higher levels of points, which is one of the key goals when selecting which players to draft.\nIf we are making predictions about a categorical variable, we would emphasize the following metrics:\n\n\ndiscrimination:\n\narea under the receiver operating curve\nand, secondarily—depending on the particulary cutoff and the relative costs of false positives versus false negatives:\n\nsensitivity\nspecificicity\npositive predictive value\nnegative predictive value\n\n\n\n\n\ncalibration:\n\ncalibration plot\nSpiegelhalter’s z\nMean difference between observed and predicted values",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-theoryVsEmpiricism",
    "href": "evaluating-prediction-accuracy.html#sec-theoryVsEmpiricism",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.9 Theory Versus Empiricism",
    "text": "17.9 Theory Versus Empiricism\nOne question that inevitably arises when making predictions is the extent to which one should leverage theory versus empiricism. Theory involves conceptual claims of understanding how the causal system works (i.e., what influences what). For example, use of theory in prediction might involve specification of the causal system that influences player performance, measurement of those factors, and the integration of that information to make a prediction. Empiricism involves “letting the data speak for themselves” and is an atheoretical approach. For example, empiricism might involve examining how thousands of variables are associated with the criterion of interest (e.g., fantasy points) and developing the best-fitting model based on those thousands of predictor variables.\nAlthough the atheoretical approach can perform reasonably well, it can be improved by making better use of theory. An empirical result (e.g., a correlation) might not necessarily have a lot of meaning associated with it. As the maxim goes, correlation does not imply causation. Moreover, empiricism can lead to overfitting. So, empiricism is often not enough.\nAs Silver (2012) notes, “The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.” (p. 9). If we understand the variables in the system and how they influence each other, we can predict things more accurately than predicting for the sake of predicting. For instance, we have made great strides in the last decades when it comes to more accurate weather forecasts [Rosalsky (2023); archived at https://perma.cc/PF8P-BT3D], including extreme weather events like hurricanes. These great strides have more to do with a better causal understanding of the weather system and the ability to conduct simulations of the atmosphere than merely because of big data (Silver, 2012). By contrast, other events are still incredibly difficult to predict, including earthquakes, in large part because we do not have a strong understanding of the system (and because we do not have ways of precisely measuring those causes because they occur at a depth below which we are realistically able to drill) (Silver, 2012).\nAt the same time, in the social and behavioral sciences, our theories of the causal processes that influence outcomes are not yet very strong. Indeed, I have misgivings calling them theories because they do not meet the traditional scientific standard for a theory. A scientific theory is an explanation of the natural world that is testable and falsifiable, and that has withstood rigorous scientific testing and scrutiny. In psychology (and other areas of social and behavioral sciences), our “theories” are more like conceptual frameworks. And these conceptual frameworks are often vague, do not make specific predictions of effects and noneffects, and do not hold up consistently when rigorously tested. As described by Meehl (1978):\n\nI consider it unnecessary to persuade you that most so-called “theories” in the soft areas of psychology (clinical, counseling, social, personality, community, and school psychology) are scientifically unimpressive and technologically worthless … Perhaps the easiest way to convince yourself is by scanning the literature of soft psychology over the last 30 years and noticing what happens to theories. Most of them suffer the fate that General MacArthur ascribed to old generals—They never die, they just slowly fade away. In the developed sciences, theories tend either to become widely accepted and built into the larger edifice of well-tested human knowledge or else they suffer destruction in the face of recalcitrant facts and are abandoned, perhaps regretfully as a “nice try.” But in fields like personology and social psychology, this seems not to happen. There is a period of enthusiasm about a new theory, a period of attempted application to several fact domains, a period of disillusionment as the negative data come in, a growing bafflement about inconsistent and unreplicable empirical results, multiple resort to ad hoc excuses, and then finally people just sort of lose interest in the thing and pursue other endeavors.\nMeehl (1978, pp. 806–807)\n\nEven if we had strong theoretical understanding of the causal system that influences behavior, we would likely still have difficulty making accurate predictions because the field has largely relied on relatively crude instruments. According to one philosophical perspective known as LaPlace’s demon, if we were able to know the exact conditions of everything in the universe, we would be able to know how the conditions would be in the future. This is an example of scientific determinism, where if you know the initial conditions, you also know the future. Other perspectives, such as quantum mechanics and chaos theory, would say that, even if we knew the initial conditions with 100% certainty, there would still be uncertainty in our understanding of the future. But assume, for a moment, that LaPlace’s demon is true. A challenge in the social and behavioral sciences is that we have a relatively poor understanding of the initial conditions of the universe. Thus, our predictions would necessarily be probabilistic, similar to weather forecasts. Despite having a strong understanding of how weather systems behave, we have imperfect understanding of the initial conditions (e.g., the position and movement of all molecules) (Silver, 2012).\nTheories tend to make grand conceptual claims that one observed variable influences another observed variable through a complex chain of intervening processes that are unobservable. Empiricism provides rich lower-level information, but lacks the broader picture. So, it seems, that we need both theory and empiricism. Theory and empiricism can—and should—inform each other.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-testBias",
    "href": "evaluating-prediction-accuracy.html#sec-testBias",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.10 Test Bias",
    "text": "17.10 Test Bias\nTest bias refers to systematic error (in measurement, prediction, etc.) as a function of group membership that leads the same score to have different meaning for different groups. For instance, if the Wonderlic Contemporary Cognitive Ability Test is a strong predictor of performance for Quarterbacks but not for Running Backs, the test is biased. Test bias, including how to identify and address it, is described in Petersen (2025b).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-waysToImprovePredictionAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-waysToImprovePredictionAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.11 Ways to Improve Prediction Accuracy",
    "text": "17.11 Ways to Improve Prediction Accuracy\nOn the whole, experts’ predictions are inaccurate. Experts’ predictions from many different domains tend to be inaccurate, including political scientists (Tetlock, 2017), physicians (Koehler et al., 2002), clinical psychologists (Oskamp, 1965), stock market traders and corporate financial officers (Skala, 2008), seismologists’ predictions of earthquakes (Hough, 2016), economists’ predictions about the economy (Makridakis et al., 2009), lawyers (Koehler et al., 2002), and business managers (Russo & Schoemaker, 1992). Thus, I would not put much confidence in the predictions by fantasy football fundits. The most common pattern of experts’ predictions is that they show overextremity, that is, their predictions have probability judgments that tend to be too extreme, as described in Section Section 17.3.2. Overextremity of experts’ predictions reflects the overprecision type of overconfidence bias. The degree of confidence of a person’s predictions is often not a good indicator of the accuracy of their predictions [and confidence and prediction accuracy are sometimes inversely associated; Silver (2012)]. Heuristics such as the anchoring and adjustment heuristic, cognitive biases such as confirmation bias (Hoch, 1985; Koriat et al., 1980), fallacies such as the base rate fallacy (Eddy, 1982; Koehler et al., 2002) could contribute to overconfidence of predictions. Poorly calibrated predictions are especially likely when the base rate is very low (e.g., suicide) or when the base rate is very high (Koehler et al., 2002).\nNevertheless, there are some domains that have shown greater predictive accuracy, from which we may learn what practices may lead to greater accuracy. For instance, experts have shown stronger predictive accuracy in weather forecasting (Murphy & Winkler, 1984), horse race betting (Johnson & Bruce, 2001), and playing the card game of bridge (Keren, 1987), but see Koehler et al. (2002) for exceptions.\nHere are some potential ways to improve the accuracy (and honesty) of predictions and judgments:\n\nProvide appropriate anchoring of your predictions to the base rate of the phenomenon you are predicting. To the extent that the base rate of the event you are predicting is low, more extreme evidence should be necessary to consistently and accurately predict that the event will occur. Applying actuarial formulas and Bayes’ theorem can help you appropriately weigh the base rate and evidence.\nInclude multiple predictors, ideally from different measures and measurement methods. Include the predictors with the strongest validity based on theory of the causal process and based on criterion-related validity.\nWhen possible, aggregate multiple perspectives of predictions, especially predictions made independently (from different people/methods/etc.). The “wisdom of the crowd” is often more accurate than individuals’ predictions, including predictions by so-called “experts” (Silver, 2012).\nA goal of prediction is to capture as much signal as possible and as little noise (error) as possible (Silver, 2012). Parsimony (i.e., not having too many predictors) can help reduce the amount of error variance captured by the prediction model. However, to accurately model complex systems like human behavior, complex models may be necessary. However, strong theory of the causal processes and dynamics may be necessary to develop accurate complex models.\nAlthough incorporating theory can be helpful, provide more weight to empiricism than to theory, until our theories and measures are stronger. Ideally, we would use theory to design a model that mirrors the causal system, with accurate measures of each process in the system, so we could make accurate predictions. However, as described in Section 17.9, our psychological theories of the causal processes that influence behavior are not yet very strong. Until we have stronger theories that specify the causal process for a given outcome, and until we have accurate measures of those causal processes, actuarial approaches are likely to be most accurate, as discussed in Chapter 15. At the same time, keep in mind that measures involving human behavior, and their resulting data, are often noisy. As a result, theoretically (conceptually) informed empirical approaches may lead to more accuracy than empiricism alone.\nUse an empirically validated and cross-validated statistical algorithm to combine information from the predictors in a formalized way. Give each predictor appropriate weight in the statistical algorithm, according to its strength of association with the outcome. Use measures with strong reliability and validity for assessing these processes to be used in the algorithm. Cross-validation will help reduce the likelihood that your model is fitting to noise and will maximize the likelihood that the model predicts accurately when applied to new data (i.e., the model’s predictions accurately generalize), as described in Section 15.8.\nWhen presenting your predictions, acknowledge what you do not know.\nExpress your predictions in terms of probabilistic estimates and present the uncertainty in your predictions with confidence intervals [even though bolder, more extreme predictions tend to receive stronger television ratings; Silver (2012)].\nQualify your predictions by identifying and noting counter-examples that would not be well fit by your prediction model, such as extreme cases, edge cases, and “broken leg” (Meehl, 1957) cases.\nProvide clear, consistent, and timely feedback on the outcomes of the predictions to the people making the predictions (Bolger & Önkal-Atay, 2004).\nBe self-critical about your predictions. Update your judgments based on their accuracy, rather than trying to confirm your beliefs (Atanasov et al., 2020).\nIn addition to considering the accuracy of the prediction, consider the quality of the prediction process, especially when random chance is involved to a degree, such as in poker and fantasy football (Silver, 2012).\nWork to identify and mitigate potential blindspots; be aware of cognitive biases and fallacies, such as confirmation bias and the base rate fallacy.\nEvaluate for the possibility of test bias. Correct for any test bias.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyConclusion",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyConclusion",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.12 Conclusion",
    "text": "17.12 Conclusion\nWhen the base rate of a behavior is very low or very high, you can be highly accurate in predicting the behavior by predicting from the base rate. Thus, you cannot judge how accurate your prediction is until you know how accurate your predictions would be by random chance. Moreover, maximizing percent accuracy may not be the ultimate goal because different errors have different costs. Though there are many indices of accuracy, there are two general types of accuracy: discrimination and calibration. Discrimination accuracy is frequently evaluated with the area under the receiver operating characteristic curve, or with sensitivity and specificity, or with standardized regression coefficients or the coefficient of determination. Calibration accuracy is frequently evaluated graphically and with various indices. Sensitivity and specificity depend on the cutoff. It is important to evaluate both discrimination and calibration when evaluating prediction accuracy.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracySessionInfo",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracySessionInfo",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.13 Session Info",
    "text": "17.13 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] viridis_0.6.5     viridisLite_0.4.2 magrittr_2.0.3    pROC_1.18.5      \n [5] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [9] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[13] ggplot2_3.5.1     tidyverse_2.0.0   petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       xfun_0.51          htmlwidgets_1.6.4  psych_2.5.3       \n [5] lattice_0.22-6     tzdb_0.5.0         quadprog_1.5-8     vctrs_0.6.5       \n [9] tools_4.4.3        generics_0.1.3     stats4_4.4.3       parallel_4.4.3    \n[13] cluster_2.1.8      pkgconfig_2.0.3    data.table_1.17.0  checkmate_2.3.2   \n[17] RColorBrewer_1.1-3 lifecycle_1.0.4    farver_2.1.2       compiler_4.4.3    \n[21] munsell_0.5.1      mnormt_2.1.1       mitools_2.4        htmltools_0.5.8.1 \n[25] yaml_2.3.10        htmlTable_2.4.3    Formula_1.2-5      pillar_1.10.1     \n[29] Hmisc_5.2-3        rpart_4.1.24       nlme_3.1-167       lavaan_0.6-19     \n[33] tidyselect_1.2.1   digest_0.6.37      mvtnorm_1.3-3      stringi_1.8.4     \n[37] reshape2_1.4.4     labeling_0.4.3     fastmap_1.2.0      grid_4.4.3        \n[41] colorspace_2.1-1   cli_3.6.4          base64enc_0.1-3    pbivnorm_0.6.0    \n[45] foreign_0.8-88     withr_3.0.2        scales_1.3.0       backports_1.5.0   \n[49] timechange_0.3.0   rmarkdown_2.29     nnet_7.3-20        gridExtra_2.3     \n[53] hms_1.1.3          evaluate_1.0.3     knitr_1.50         mix_1.0-13        \n[57] rlang_1.1.5        Rcpp_1.0.14        xtable_1.8-4       glue_1.8.0        \n[61] DBI_1.2.3          rstudioapi_0.17.1  jsonlite_1.9.1     R6_2.6.1          \n[65] plyr_1.8.9        \n\n\n\n\n\n\nAtanasov, P., Witkowski, J., Ungar, L., Mellers, B., & Tetlock, P. (2020). Small steps to accuracy: Incremental belief updaters are better forecasters. Organizational Behavior and Human Decision Processes, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001\n\n\nAustin, P. C., & Steyerberg, E. W. (2014). Graphical assessment of internal and external calibration of logistic regression models by using loess smoothers. Statistics in Medicine, 33(3), 517–535. https://doi.org/10.1002/sim.5941\n\n\nBolger, F., & Önkal-Atay, D. (2004). The effects of feedback on judgmental interval predictions. International Journal of Forecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nEddy, D. M. (1982). Probabilistic reasoning in clinical medicine: Problems and opportunities. In D. Kahneman, P. Slovic, & A. Tversky (Eds.), Judgment under uncertainty: Heuristics and biases (pp. 249–267). Cambridge University Press. https://doi.org/10.1017/CBO9780511809477.019\n\n\nFarrington, D. P., & Loeber, R. (1989). Relative improvement over chance (RIOC) and phi as measures of predictive efficiency and strength of association in 2×2 tables. Journal of Quantitative Criminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737\n\n\nGoodman, Z. T., Casline, E., Jensen-Doss, A., Ehrenreich-May, J., & Bainter, S. A. (2022). shinyDLRs: A dashboard to facilitate derivation of diagnostic likelihood ratios. Psychological Assessment, 34(6), 558–569. https://doi.org/10.1037/pas0001114\n\n\nHarrell, Jr., F. E. (2024). rms: Regression modeling strategies. https://hbiostat.org/R/rms/\n\n\nHoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting personal events. Journal of Experimental Psychology: Learning, Memory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719\n\n\nHopper, T. (2014). Can we do better than r-squared? https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared\n\n\nHough, S. E. (2016). Predicting the unpredictable: The tumultuous science of earthquake prediction. Princeton University Press.\n\n\nHyndman, R. J. (2014). Alternative to MAPE when the data is not a time series. https://stats.stackexchange.com/a/108963/20338\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nJohnson, J. E. V., & Bruce, A. C. (2001). Calibration of subjective probability judgments in a naturalistic setting. Organizational Behavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nKeren, G. (1987). Facing uncertainty in the game of bridge: A calibration study. Organizational Behavior and Human Decision Processes, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1\n\n\nKessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., & Zubizarreta, J. R. (2020). Suicide prediction models: A critical review of recent research with recommendations for the way forward. Molecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0\n\n\nKoehler, D. J., Brenner, L., & Griffin, D. (2002). The calibration of expert judgment: Heuristics and biases beyond the laboratory. In T. Gilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics and biases: The psychology of intuitive judgment. Cambridge University Press. https://doi.org/10.1017/CBO9780511808098.041\n\n\nKoriat, A., Lichtenstein, S., & Fischhoff, B. (1980). Reasons for confidence. Journal of Experimental Psychology: Human Learning and Memory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107\n\n\nLindhiem, O., Petersen, I. T., Mentch, L. K., & Youngstrom, E. A. (2020). The importance of calibration in clinical psychology. Assessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055\n\n\nMakridakis, S., Hogarth, R. M., & Gaba, A. (2009). Forecasting and uncertainty in the economic and business world. International Journal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. Journal of Consulting and Clinical Psychology, 46(4), 806–834. https://doi.org/10.1037/0022-006x.46.4.806\n\n\nMeehl, P. E., & Rosen, A. (1955). Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores. Psychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070\n\n\nMorley, S. K., Brito, T. V., & Welling, D. T. (2018). Measures of model performance based on the log accuracy ratio. Space Weather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669\n\n\nMurphy, A. H., & Winkler, R. L. (1984). Probability forecasting in meterology. Journal of the American Statistical Association, 79(387), 489–500. https://doi.org/10.2307/2288395\n\n\nOskamp, S. (1965). Overconfidence in case-study judgments. Journal of Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & Müller, M. (2011). pROC: An open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics, 12, 77. https://doi.org/10.1186/1471-2105-12-77\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & Müller, M. (2023). pROC: Display and analyze ROC curves. https://xrobin.github.io/pROC/\n\n\nRosalsky, G. (2023). Should we invest more in weather forecasting? It may save your life. https://www.npr.org/sections/money/2023/07/11/1186458991/should-we-invest-more-in-weather-forecasting-it-may-save-your-life\n\n\nRusso, J. E., & Schoemaker, P. J. (1992). Managing overconfidence. Sloan Management Review, 33(2), 7.\n\n\nSchwartz, A. (2006). Diagnostic test calculator. http://araw.mede.uic.edu/cgi-bin/testcalc.pl\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.\n\n\nSkala, D. (2008). Overconfidence in psychology and finance–an interdisciplinary literature review. Bank i Kredyt, 4, 33–50.\n\n\nStevens, R. J., & Poppe, K. K. (2020). Validation of clinical prediction models: What does the “calibration slope” really measure? Journal of Clinical Epidemiology, 118, 93–99. https://doi.org/10.1016/j.jclinepi.2019.09.016\n\n\nSteyerberg, E. W., & Vergouwe, Y. (2014). Towards better clinical prediction models: Seven steps for development and an ABCD for validation. European Heart Journal, 35(29), 1925–1931. https://doi.org/10.1093/eurheartj/ehu207\n\n\nTetlock, P. E. (2017). Expert political judgment: How good is it? How can we know? - New edition. Princeton University Press.\n\n\nTofallis, C. (2015). A better measure of relative prediction accuracy for model selection and model estimation. Journal of the Operational Research Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103\n\n\nTreat, T. A., & Viken, R. J. (2023). Measuring test performance with signal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M. McMullen, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA handbook of research methods in psychology: Foundations, planning, measures, and psychometrics (2nd ed., Vol. 1, pp. 837–858). American Psychological Association. https://doi.org/10.1037/0000318-038\n\n\nYahoo! Sports. (2024). How cognitive bias affects your fantasy draft strategy with neuroscience professor Dr. Renee Miller. https://www.youtube.com/watch?v=gmpLFWs5ae0",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "mythbusters.html",
    "href": "mythbusters.html",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "",
    "text": "18.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersGettingStarted",
    "href": "mythbusters.html#sec-mythbustersGettingStarted",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "",
    "text": "18.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"nflreadr\")\nlibrary(\"lme4\")\nlibrary(\"lmerTest\")\nlibrary(\"MuMIn\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")\n\n\n\n18.1.2 Specify Package Options\n\nCodeemm_options(lmerTest.limit = 100000)\nemm_options(pbkrtest.limit = 100000)\n\n\n\n18.1.3 Load Data\n\nCodeload(file = \"./data/nfl_playerContracts.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/nfl_espnQBR_seasonal.RData\")\nload(file = \"./data/nfl_espnQBR_weekly.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-contractYear",
    "href": "mythbusters.html#sec-contractYear",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n18.2 Do Players Perform Better in their Contract Year?",
    "text": "18.2 Do Players Perform Better in their Contract Year?\nConsiderable speculation exists regarding whether players perform better in their last year of their contract (i.e., their “contract year”). Fantasy football talking heads and commentators frequently discuss the benefit of selecting players who are in their contract year, because it supposedly means that player has more motivation to perform well so they get a new contract and get paid more. To our knowledge, no peer-reviewed studies have examined this question for football players. One study found that National Basketball Association (NBA) players improved in field goal percentage, points, and player efficiency rating (but not other statistics: rebounds, assists, steals, or blocks) from their pre-contract year to their contract year, and that Major League Baseball (MLB) players improved in runs batted in (RBIs; but not other statistics: batting average, slugging percentage, on base percentage, home runs, fielding percentage) from their pre-contract year to their contract year (White & Sheldon, 2014). Other casual analyses have been examined contract-year performance of National Football League (NFL) players, including articles in 2012 [Bales (2012); archived here] and 2022 [Niles (2022); archived here].\nLet’s examine the question empirically. In order to do that, we have to make some assumptions/constraints. In this example, we will make the following constraints:\n\nWe will determine a player’s contract year programmatically based on the year the contract was signed. For instance, if a player signed a 3-year contract in 2015, their contract would expire in 2018, and thus their contract year would be 2017. Note: this is a coarse way of determining a player’s contract year because it could depend on when during the year the player’s contract is signed. If we were submitting this analysis as a paper to a scientific journal, it would be important to verify each player’s contract year.\nWe will examine performance in all seasons since 2011, beginning when most data for player contracts are available.\nFor maximum statistical power to detect an effect if a contract year effect exists, we will examine all seasons for a player (since 2011), not just their contract year and their pre-contract year.\nTo ensure a more fair, apples-to-apples comparison of the games in which players played, we will examine per-game performance (except for yards per carry, which is based on \\(\\frac{\\text{rushing yards}}{\\text{carries}}\\) from the entire season).\nWe will examine regular season games only (no postseason).\nTo ensure we do not make generalization about a player’s performance in a season from a small sample, the player has to play at least 5 games in a given season for that player–season combination to be included in analysis.\n\nFor analysis, the same player contributes multiple observations of performance (i.e., multiple seasons) due to the longitudinal nature of the data. Inclusion of multiple data points from the same player would violate the assumption of multiple regression that all observations are independent. Thus, we use mixed-effects models that allow nonindependent observations. In our mixed-effects models, we include a random intercept for each player, to allow our model to account for players’ differing level of performance. We examine two mixed-effects models for each outcome variable: one model that accounts for the effects of age and experience, and one model that does not.\nThe model that does not account for the effects of age and experience includes:\n\nrandom intercepts to allow the model to estimate a different starting point for each player\na fixed effect for whether the player is in a contract year\n\nThe model that accounts for the effects of age and experience includes:\n\nrandom intercepts to allow the model to estimate a different starting point for each player\nrandom linear slopes (i.e., random effect of linear age) to allow the model to estimate a different form of change for each player\na fixed quadratic effect of age to allow for curvilinear effects\na fixed effect of experience\na fixed effect for whether the player is in a contract year\n\n\nCode# Subset to remove players without a year signed\nnfl_playerContracts_subset &lt;- nfl_playerContracts %&gt;% \n  dplyr::filter(!is.na(year_signed) & year_signed != 0)\n\n# Determine the contract year for a given contract\nnfl_playerContracts_subset$contractYear &lt;- nfl_playerContracts_subset$year_signed + nfl_playerContracts_subset$years - 1\n\n# Arrange contracts by player and year_signed\nnfl_playerContracts_subset &lt;- nfl_playerContracts_subset %&gt;%\n  dplyr::group_by(player, position) %&gt;% \n  dplyr::arrange(player, position, -year_signed) %&gt;% \n  dplyr::ungroup()\n\n# Determine if the player played in the original contract year\nnfl_playerContracts_subset &lt;- nfl_playerContracts_subset %&gt;%\n  dplyr::group_by(player, position) %&gt;%\n  dplyr::mutate(\n    next_contract_start = lag(year_signed)) %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::mutate(\n    played_in_contract_year = ifelse(\n      is.na(next_contract_start) | contractYear &lt; next_contract_start,\n      TRUE,\n      FALSE))\n\n# Check individual players\n#nfl_playerContracts_subset %&gt;% \n#  dplyr::filter(player == \"Aaron Rodgers\") %&gt;% \n#  dplyr::select(player:years, contractYear, next_contract_start, played_in_contract_year)\n#\n#nfl_playerContracts_subset %&gt;% \n#  dplyr::filter(player %in% c(\"Jared Allen\", \"Aaron Rodgers\")) %&gt;% \n#  dplyr::select(player:years, contractYear, next_contract_start, played_in_contract_year)\n\n# Subset data\nnfl_playerContractYears &lt;- nfl_playerContracts_subset %&gt;% \n  dplyr::filter(played_in_contract_year == TRUE) %&gt;% \n  dplyr::filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  dplyr::select(player, position, team, contractYear) %&gt;% \n  dplyr::mutate(merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)) %&gt;% \n  dplyr::rename(season = contractYear) %&gt;% \n  dplyr::mutate(contractYear = 1)\n\n# Merge with weekly and seasonal stats data\nplayer_stats_weekly_offense &lt;- player_stats_weekly %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  dplyr::mutate(merge_name = nflreadr::clean_player_names(player_display_name, lowercase = TRUE))\n#nfl_actualStats_offense_seasonal &lt;- nfl_actualStats_offense_seasonal %&gt;% \n#  mutate(merge_name = nflreadr::clean_player_names(player_display_name, lowercase = TRUE))\n\nplayer_statsContracts_offense_weekly &lt;- dplyr::full_join(\n  player_stats_weekly_offense,\n  nfl_playerContractYears,\n  by = c(\"merge_name\", \"position_group\" = \"position\", \"season\")\n) %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\n#player_statsContracts_offense_seasonal &lt;- full_join(\n#  player_stats_seasonal_offense,\n#  nfl_playerContractYears,\n#  by = c(\"merge_name\", \"position_group\" = \"position\", \"season\")\n#) %&gt;% \n#  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\nplayer_statsContracts_offense_weekly$contractYear[which(is.na(player_statsContracts_offense_weekly$contractYear))] &lt;- 0\n#player_statsContracts_offense_seasonal$contractYear[which(is.na(player_statsContracts_offense_seasonal$contractYear))] &lt;- 0\n\n#player_statsContracts_offense_weekly$contractYear &lt;- factor(\n#  player_statsContracts_offense_weekly$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\n#player_statsContracts_offense_seasonal$contractYear &lt;- factor(\n#  player_statsContracts_offense_seasonal$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\nplayer_statsContracts_offense_weekly &lt;- player_statsContracts_offense_weekly %&gt;% \n  dplyr::arrange(merge_name, season, season_type, week)\n\n#player_statsContracts_offense_seasonal &lt;- player_statsContracts_offense_seasonal %&gt;% \n#  arrange(merge_name, season)\n\nplayer_statsContractsSubset_offense_weekly &lt;- player_statsContracts_offense_weekly %&gt;% \n  dplyr::filter(season_type == \"REG\")\n\n#table(nfl_playerContracts$year_signed) # most contract data is available beginning in 2011\n\n# Calculate Per Game Totals\nplayer_statsContracts_seasonal &lt;- player_statsContractsSubset_offense_weekly %&gt;% \n  dplyr::group_by(player_id, season) %&gt;% \n  dplyr::summarise(\n    player_display_name = petersenlab::Mode(player_display_name),\n    position_group = petersenlab::Mode(position_group),\n    age = min(age, na.rm = TRUE),\n    years_of_experience = min(years_of_experience, na.rm = TRUE),\n    rushing_yards = sum(rushing_yards, na.rm = TRUE), # season total\n    carries = sum(carries, na.rm = TRUE), # season total\n    rushing_epa = mean(rushing_epa, na.rm = TRUE),\n    receiving_yards = mean(receiving_yards, na.rm = TRUE),\n    receiving_epa = mean(receiving_epa, na.rm = TRUE),\n    fantasyPoints = sum(fantasyPoints, na.rm = TRUE), # season total\n    contractYear = mean(contractYear, na.rm = TRUE),\n    games = n(),\n    .groups = \"drop_last\"\n  ) %&gt;% \n  dplyr::mutate(\n    player_id = as.factor(player_id),\n    ypc = rushing_yards / carries,\n    contractYear = factor(\n      contractYear,\n      levels = c(0, 1),\n      labels = c(\"no\", \"yes\")\n    ))\n\nplayer_statsContracts_seasonal[sapply(player_statsContracts_seasonal, is.infinite)] &lt;- NA\n\nplayer_statsContracts_seasonal$ageCentered20 &lt;- player_statsContracts_seasonal$age - 20\nplayer_statsContracts_seasonal$ageCentered20Quadratic &lt;- player_statsContracts_seasonal$ageCentered20 ^ 2\n\n# Merge with seasonal fantasy points data\n\n\n\n18.2.1 QB\nFirst, we prepare the data by merging and performing additional processing:\n\nCode# Merge with QBR data\nnfl_espnQBR_weekly$merge_name &lt;- paste(nfl_espnQBR_weekly$name_first, nfl_espnQBR_weekly$name_last, sep = \" \") %&gt;% \n  nflreadr::clean_player_names(., lowercase = TRUE)\n\nnfl_contractYearQBR_weekly &lt;- nfl_playerContractYears %&gt;% \n  dplyr::filter(position == \"QB\") %&gt;% \n  dplyr::full_join(\n    .,\n    nfl_espnQBR_weekly,\n    by = c(\"merge_name\",\"team\",\"season\")\n  )\n\nnfl_contractYearQBR_weekly$contractYear[which(is.na(nfl_contractYearQBR_weekly$contractYear))] &lt;- 0\n#nfl_contractYearQBR_weekly$contractYear &lt;- factor(\n#  nfl_contractYearQBR_weekly$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\nnfl_contractYearQBR_weekly &lt;- nfl_contractYearQBR_weekly %&gt;% \n  dplyr::arrange(merge_name, season, season_type, game_week)\n\nnfl_contractYearQBRsubset_weekly &lt;- nfl_contractYearQBR_weekly %&gt;% \n  dplyr::filter(season_type == \"Regular\") %&gt;% \n  dplyr::arrange(merge_name, season, season_type, game_week) %&gt;% \n  mutate(\n    player = coalesce(player, name_display),\n    position = \"QB\") %&gt;% \n  group_by(merge_name, player_id) %&gt;% \n  fill(player, .direction = \"downup\")\n\n# Merge with age and experience\nnfl_contractYearQBRsubset_weekly &lt;- player_statsContractsSubset_offense_weekly %&gt;% \n  dplyr::filter(position == \"QB\") %&gt;% \n  dplyr::select(merge_name, season, week, age, years_of_experience, fantasyPoints) %&gt;% \n  full_join(\n    nfl_contractYearQBRsubset_weekly,\n    by = c(\"merge_name\",\"season\", c(\"week\" = \"game_week\"))\n  ) %&gt;% select(player_id, season, week, player, everything()) %&gt;% \n  arrange(player_id, season, week)\n\n#hist(nfl_contractYearQBRsubset_weekly$qb_plays) # players have at least 20 dropbacks per game\n\n# Calculate Per Game Totals\nnfl_contractYearQBR_seasonal &lt;- nfl_contractYearQBRsubset_weekly %&gt;% \n  dplyr::group_by(merge_name, season) %&gt;% \n  dplyr::summarise(\n    age = min(age, na.rm = TRUE),\n    years_of_experience = min(years_of_experience, na.rm = TRUE),\n    qbr = mean(qbr_total, na.rm = TRUE),\n    pts_added = mean(pts_added, na.rm = TRUE),\n    epa_pass = mean(pass, na.rm = TRUE),\n    qb_plays = sum(qb_plays, na.rm = TRUE), # season total\n    fantasyPoints = sum(fantasyPoints, na.rm = TRUE), # season total\n    contractYear = mean(contractYear, na.rm = TRUE),\n    games = n(),\n    .groups = \"drop_last\"\n  ) %&gt;% \n  dplyr::mutate(\n    contractYear = factor(\n      contractYear,\n      levels = c(0, 1),\n      labels = c(\"no\", \"yes\")\n    ))\n\nnfl_contractYearQBR_seasonal[sapply(nfl_contractYearQBR_seasonal, is.infinite)] &lt;- NA\n\nnfl_contractYearQBR_seasonal$ageCentered20 &lt;- nfl_contractYearQBR_seasonal$age - 20\nnfl_contractYearQBR_seasonal$ageCentered20Quadratic &lt;- nfl_contractYearQBR_seasonal$ageCentered20 ^ 2\n\nnfl_contractYearQBR_seasonal &lt;- nfl_contractYearQBR_seasonal %&gt;% \n  group_by(merge_name) %&gt;%\n  mutate(player_id = as.factor(as.character(cur_group_id())))\n\nnfl_contractYearQBRsubset_seasonal &lt;- nfl_contractYearQBR_seasonal %&gt;% \n  dplyr::filter(\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\nThen, we analyze the data. Below is a mixed model that examines whether a player has a higher QBR per game when they are in a contract year compared to when they are not in a contract year.\n\nCodemixedModel_qbr &lt;- lmerTest::lmer(\n  qbr ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_qbr)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: qbr ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 8905.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2658 -0.5446  0.0914  0.5732  3.2578 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 111.6    10.56   \n Residual              198.6    14.09   \nNumber of obs: 1063, groups:  player_id, 253\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)      44.2363     0.8738 231.4295  50.624   &lt;2e-16 ***\ncontractYearyes   0.2432     1.2010 950.6905   0.202     0.84    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.241\n\nCodeMuMIn::r.squaredGLMM(mixedModel_qbr)\n\n              R2m      R2c\n[1,] 2.915379e-05 0.359728\n\nCodeemmeans::emmeans(mixedModel_qbr, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             44.2 0.874 262     42.5       46\n yes            44.5 1.300 752     41.9       47\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_qbr &lt;- lmerTest::lmer(\n  qbr ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_qbr)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: qbr ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 8833\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3838 -0.5220  0.0928  0.5506  3.2853 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   135.206  11.628        \n           ageCentered20   0.433   0.658   -0.41\n Residual                191.543  13.840        \nNumber of obs: 1055, groups:  player_id, 249\n\nFixed effects:\n                        Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)             39.47167    2.29478 174.07664  17.201  &lt; 2e-16 ***\ncontractYearyes          0.31817    1.23969 946.72121   0.257  0.79750    \nageCentered20            0.85795    0.64998 265.02249   1.320  0.18799    \nageCentered20Quadratic  -0.07527    0.02354  98.16828  -3.197  0.00187 ** \nyears_of_experience      0.61694    0.54694 289.48076   1.128  0.26026    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.055                     \nageCentrd20 -0.734 -0.059              \nagCntrd20Qd  0.764  0.055 -0.628       \nyrs_f_xprnc  0.100 -0.043 -0.662 -0.120\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_qbr)\n\n            R2m       R2c\n[1,] 0.01316462 0.3966867\n\nCodeemmeans::emmeans(mixedModelAge_qbr, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             44.1 0.916 240     42.3     45.9\n yes            44.4 1.330 706     41.8     47.0\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_ptsAdded &lt;- lmerTest::lmer(\n  pts_added ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_ptsAdded)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pts_added ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4855.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.7107 -0.4882  0.0804  0.5366  4.4282 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.570    1.603   \n Residual              4.332    2.081   \nNumber of obs: 1063, groups:  player_id, 253\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)      -0.85472    0.13129 219.85741  -6.510 5.02e-10 ***\ncontractYearyes  -0.06893    0.17768 939.65392  -0.388    0.698    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.237\n\nCodeMuMIn::r.squaredGLMM(mixedModel_ptsAdded)\n\n              R2m       R2c\n[1,] 0.0001052534 0.3723735\n\nCodeemmeans::emmeans(mixedModel_ptsAdded, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no           -0.855 0.131 262    -1.11   -0.596\n yes          -0.924 0.194 745    -1.31   -0.542\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_ptsAdded &lt;- lmerTest::lmer(\n  pts_added ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_ptsAdded)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pts_added ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4825.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8624 -0.4856  0.0839  0.5194  4.4052 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   3.54809  1.8836        \n           ageCentered20 0.01153  0.1074   -0.55\n Residual                4.18409  2.0455        \nNumber of obs: 1055, groups:  player_id, 249\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             -1.597976   0.347877 168.940692  -4.594 8.49e-06 ***\ncontractYearyes         -0.076689   0.183346 936.388386  -0.418  0.67584    \nageCentered20            0.100485   0.097484 270.759302   1.031  0.30356    \nageCentered20Quadratic  -0.011106   0.003522 100.614226  -3.153  0.00213 ** \nyears_of_experience      0.133743   0.081042 281.434965   1.650  0.10000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.055                     \nageCentrd20 -0.737 -0.060              \nagCntrd20Qd  0.761  0.059 -0.638       \nyrs_f_xprnc  0.108 -0.044 -0.663 -0.107\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_ptsAdded)\n\n           R2m       R2c\n[1,] 0.0139585 0.4014263\n\nCodeemmeans::emmeans(mixedModelAge_ptsAdded, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no           -0.846 0.136 242    -1.11   -0.579\n yes          -0.923 0.197 708    -1.31   -0.536\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_epaPass &lt;- lmerTest::lmer(\n  epa_pass ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: epa_pass ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4533.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0315 -0.5088  0.0398  0.5664  4.3662 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.454    1.566   \n Residual              3.049    1.746   \nNumber of obs: 1063, groups:  player_id, 253\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       1.0733     0.1218 239.6969   8.810 2.58e-16 ***\ncontractYearyes   0.4241     0.1504 928.0954   2.821   0.0049 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.214\n\nCodeMuMIn::r.squaredGLMM(mixedModel_epaPass)\n\n             R2m       R2c\n[1,] 0.004973305 0.4486446\n\nCodeemmeans::emmeans(mixedModel_epaPass, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             1.07 0.122 263    0.833     1.31\n yes            1.50 0.172 699    1.159     1.84\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaPass &lt;- lmerTest::lmer(\n  epa_pass ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 | player_id), # removed random slopes to address convergence issue\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: epa_pass ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4496.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1285 -0.5002  0.0415  0.5381  4.2903 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.426    1.557   \n Residual              2.993    1.730   \nNumber of obs: 1055, groups:  player_id, 249\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)   \n(Intercept)             3.315e-01  2.817e-01  9.490e+02   1.177  0.23962   \ncontractYearyes         2.468e-01  1.552e-01  9.498e+02   1.590  0.11219   \nageCentered20           2.496e-03  8.169e-02  6.844e+02   0.031  0.97563   \nageCentered20Quadratic -5.555e-03  2.704e-03  1.010e+03  -2.054  0.04022 * \nyears_of_experience     1.935e-01  7.206e-02  3.935e+02   2.685  0.00756 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.063                     \nageCentrd20 -0.711 -0.055              \nagCntrd20Qd  0.740  0.057 -0.574       \nyrs_f_xprnc  0.141 -0.048 -0.709 -0.133\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_epaPass)\n\n            R2m       R2c\n[1,] 0.03051935 0.4645375\n\nCodeemmeans::emmeans(mixedModelAge_epaPass, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             1.19 0.124 260    0.942     1.43\n yes            1.43 0.173 687    1.094     1.77\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_fantasyPtsPass &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_fantasyPtsPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 12460.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8564 -0.5600 -0.0851  0.6047  2.8720 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 5791     76.1    \n Residual              5041     71.0    \nNumber of obs: 1063, groups:  player_id, 253\n\nFixed effects:\n                Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)      103.696      5.628 293.908  18.424  &lt; 2e-16 ***\ncontractYearyes  -25.186      6.171 935.684  -4.081 4.86e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.189\n\nCodeMuMIn::r.squaredGLMM(mixedModel_fantasyPtsPass)\n\n             R2m       R2c\n[1,] 0.008874502 0.5387338\n\nCodeemmeans::emmeans(mixedModel_fantasyPtsPass, \"contractYear\")\n\n contractYear emmean   SE  df lower.CL upper.CL\n no            103.7 5.63 262     92.6    114.8\n yes            78.5 7.53 635     63.7     93.3\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_fantasyPtsPass &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 | player_id), # removed random slopes to address convergence issue\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_fantasyPtsPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 12337.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8942 -0.5618 -0.0743  0.6125  2.7478 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 5764     75.92   \n Residual              4923     70.17   \nNumber of obs: 1055, groups:  player_id, 249\n\nFixed effects:\n                       Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)            126.8448    12.0072 955.8120  10.564  &lt; 2e-16 ***\ncontractYearyes        -20.8005     6.3731 948.2146  -3.264 0.001139 ** \nageCentered20          -11.7659     3.5535 760.2272  -3.311 0.000973 ***\nageCentered20Quadratic  -0.1167     0.1117 994.4649  -1.044 0.296722    \nyears_of_experience     12.8752     3.2246 500.9729   3.993 7.51e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.065                     \nageCentrd20 -0.693 -0.052              \nagCntrd20Qd  0.713  0.054 -0.536       \nyrs_f_xprnc  0.172 -0.045 -0.741 -0.134\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_fantasyPtsPass)\n\n            R2m       R2c\n[1,] 0.05246165 0.5634855\n\nCodeemmeans::emmeans(mixedModelAge_fantasyPtsPass, \"contractYear\")\n\n contractYear emmean   SE  df lower.CL upper.CL\n no            104.3 5.74 262     93.0    115.6\n yes            83.5 7.56 621     68.7     98.4\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.2 RB\n\nCodeplayer_statsContractsRB_seasonal &lt;- player_statsContracts_seasonal %&gt;% \n  dplyr::filter(\n    position_group == \"RB\",\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\n\nCodemixedModel_ypc &lt;- lmerTest::lmer(\n  ypc ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_ypc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ypc ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 5973.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.9077 -0.3904  0.0127  0.3862 14.7586 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.4971   0.7051  \n Residual              1.9117   1.3826  \nNumber of obs: 1630, groups:  player_id, 509\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)     3.895e+00  5.279e-02 4.969e+02  73.779   &lt;2e-16 ***\ncontractYearyes 2.525e-02  8.555e-02 1.568e+03   0.295    0.768    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.372\n\nCodeMuMIn::r.squaredGLMM(mixedModel_ypc)\n\n              R2m       R2c\n[1,] 5.050989e-05 0.2064248\n\nCodeemmeans::emmeans(mixedModel_ypc, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no             3.89 0.0528  602     3.79     4.00\n yes            3.92 0.0822 1202     3.76     4.08\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_ypc &lt;- lmerTest::lmer(\n  ypc ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_ypc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ypc ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 5956.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.6944 -0.3728  0.0039  0.3912 14.1281 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.32825  0.5729        \n           ageCentered20 0.01158  0.1076   -0.30\n Residual                1.84189  1.3572        \nNumber of obs: 1628, groups:  player_id, 507\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             4.222e+00  1.776e-01  6.717e+02  23.778   &lt;2e-16 ***\ncontractYearyes         1.265e-01  9.244e-02  1.505e+03   1.368    0.171    \nageCentered20          -7.950e-02  6.140e-02  7.141e+02  -1.295    0.196    \nageCentered20Quadratic -3.626e-03  4.461e-03  3.764e+02  -0.813    0.417    \nyears_of_experience     5.539e-02  3.738e-02  4.306e+02   1.482    0.139    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.157                     \nageCentrd20 -0.872 -0.173              \nagCntrd20Qd  0.820  0.123 -0.824       \nyrs_f_xprnc -0.070 -0.074 -0.292 -0.215\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_ypc)\n\n            R2m       R2c\n[1,] 0.02173972 0.2602485\n\nCodeemmeans::emmeans(mixedModelAge_ypc, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no             3.85 0.0561  519     3.74     3.96\n yes            3.97 0.0860 1163     3.80     4.14\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_epaRush &lt;- lmerTest::lmer(\n  rushing_epa ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: rushing_epa ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4697.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.5842 -0.5074  0.0844  0.5913  3.4362 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.1091   0.3303  \n Residual              0.9480   0.9737  \nNumber of obs: 1630, groups:  player_id, 509\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       -0.65276    0.03262  612.67321 -20.011   &lt;2e-16 ***\ncontractYearyes    0.04100    0.05822 1626.69621   0.704    0.481    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.429\n\nCodeMuMIn::r.squaredGLMM(mixedModel_epaRush)\n\n              R2m       R2c\n[1,] 0.0003033165 0.1034567\n\nCodeemmeans::emmeans(mixedModel_epaRush, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no           -0.653 0.0326  617   -0.717   -0.589\n yes          -0.612 0.0532 1124   -0.716   -0.507\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaRush &lt;- lmerTest::lmer(\n  rushing_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: rushing_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4704.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6207 -0.5022  0.0780  0.5854  3.4459 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.228839 0.47837       \n           ageCentered20 0.002858 0.05346  -0.73\n Residual                0.927374 0.96300       \nNumber of obs: 1628, groups:  player_id, 507\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)            -6.448e-01  1.230e-01  3.952e+02  -5.243 2.58e-07 ***\ncontractYearyes         7.553e-02  6.173e-02  1.440e+03   1.224   0.2213    \nageCentered20           4.237e-02  4.081e-02  3.764e+02   1.038   0.2999    \nageCentered20Quadratic -1.401e-03  2.871e-03  1.958e+02  -0.488   0.6261    \nyears_of_experience    -4.881e-02  2.240e-02  4.306e+02  -2.179   0.0298 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.154                     \nageCentrd20 -0.886 -0.197              \nagCntrd20Qd  0.833  0.153 -0.856       \nyrs_f_xprnc -0.060 -0.059 -0.268 -0.193\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_epaRush)\n\n             R2m       R2c\n[1,] 0.006806377 0.1273214\n\nCodeemmeans::emmeans(mixedModelAge_epaRush, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no           -0.664 0.0336  552   -0.730   -0.598\n yes          -0.589 0.0553 1111   -0.697   -0.480\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_fantasyPtsRush &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_fantasyPtsRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 18806.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0576 -0.5022 -0.1804  0.3944  3.8668 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2298     47.93   \n Residual              2109     45.92   \nNumber of obs: 1724, groups:  player_id, 525\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       64.887      2.543  630.937  25.513  &lt; 2e-16 ***\ncontractYearyes  -11.289      2.998 1490.473  -3.765 0.000173 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.238\n\nCodeMuMIn::r.squaredGLMM(mixedModel_fantasyPtsRush)\n\n             R2m       R2c\n[1,] 0.005379707 0.5239906\n\nCodeemmeans::emmeans(mixedModel_fantasyPtsRush, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             64.9 2.54  572     59.9     69.9\n yes            53.6 3.44 1223     46.8     60.4\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_fantasyPtsRush &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_fantasyPtsRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 18667.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2611 -0.4880 -0.1577  0.3980  3.6091 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   4600.19  67.825        \n           ageCentered20   39.61   6.294   -0.76\n Residual                1816.43  42.620        \nNumber of obs: 1722, groups:  player_id, 523\n\nFixed effects:\n                        Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)              57.4155     7.3822  669.8502   7.778 2.80e-14 ***\ncontractYearyes          -6.4763     3.1078 1459.9084  -2.084   0.0373 *  \nageCentered20            -0.6904     2.5240  872.0844  -0.274   0.7845    \nageCentered20Quadratic   -0.8427     0.1574  459.6570  -5.353 1.37e-07 ***\nyears_of_experience      10.7026     1.6790  619.9360   6.374 3.59e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.176                     \nageCentrd20 -0.848 -0.171              \nagCntrd20Qd  0.748  0.156 -0.770       \nyrs_f_xprnc  0.155 -0.075 -0.505 -0.090\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_fantasyPtsRush)\n\n            R2m      R2c\n[1,] 0.07533867 0.604956\n\nCodeemmeans::emmeans(mixedModelAge_fantasyPtsRush, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             63.9 2.56  573     58.8     68.9\n yes            57.4 3.42 1200     50.7     64.1\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.3 WR/TE\n\nCodeplayer_statsContractsWRTE_seasonal &lt;- player_statsContracts_seasonal %&gt;% \n  dplyr::filter(\n    position_group %in% c(\"WR\",\"TE\"),\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\n\nCodemixedModel_receivingYards &lt;- lmerTest::lmer(\n  receiving_yards ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_receivingYards)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: receiving_yards ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 30379.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8618 -0.5297 -0.1068  0.5113  4.5629 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 280.0    16.73   \n Residual              184.1    13.57   \nNumber of obs: 3560, groups:  player_id, 1034\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)       25.1047     0.6073 1237.9557  41.336  &lt; 2e-16 ***\ncontractYearyes   -3.4995     0.5868 3007.2060  -5.964 2.75e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.233\n\nCodeMuMIn::r.squaredGLMM(mixedModel_receivingYards)\n\n             R2m       R2c\n[1,] 0.005322244 0.6053999\n\nCodeemmeans::emmeans(mixedModel_receivingYards, \"contractYear\")\n\n contractYear emmean    SE   df lower.CL upper.CL\n no             25.1 0.607 1129     23.9     26.3\n yes            21.6 0.740 2021     20.2     23.1\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_receivingYards &lt;- lmerTest::lmer(\n  receiving_yards ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_receivingYards)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nreceiving_yards ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 29952.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9222 -0.5213 -0.0916  0.4772  3.8044 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   532.259  23.071        \n           ageCentered20   6.139   2.478   -0.70\n Residual                137.910  11.744        \nNumber of obs: 3558, groups:  player_id, 1033\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              13.42269    1.57295 1376.84753   8.533  &lt; 2e-16 ***\ncontractYearyes          -2.83756    0.57200 2796.87598  -4.961 7.44e-07 ***\nageCentered20             2.11751    0.53966 2028.63105   3.924 9.01e-05 ***\nageCentered20Quadratic   -0.43924    0.02844 1383.02319 -15.445  &lt; 2e-16 ***\nyears_of_experience       3.96901    0.41384 1221.21592   9.591  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.121                     \nageCentrd20 -0.804 -0.149              \nagCntrd20Qd  0.696  0.079 -0.663       \nyrs_f_xprnc  0.211  0.021 -0.633 -0.082\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_receivingYards)\n\n           R2m       R2c\n[1,] 0.1265075 0.7433555\n\nCodeemmeans::emmeans(mixedModelAge_receivingYards, \"contractYear\")\n\n contractYear emmean    SE   df lower.CL upper.CL\n no             24.2 0.633 1134     23.0     25.5\n yes            21.4 0.734 1855     19.9     22.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_epaReceiving &lt;- lmerTest::lmer(\n  receiving_epa ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: receiving_epa ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 11672.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.5671 -0.5715 -0.0364  0.5264  3.8987 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.5536   0.7441  \n Residual              1.3060   1.1428  \nNumber of obs: 3490, groups:  player_id, 1018\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)        0.65163    0.03448 1371.93211  18.901  &lt; 2e-16 ***\ncontractYearyes   -0.14793    0.04753 3317.24675  -3.112  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.356\n\nCodeMuMIn::r.squaredGLMM(mixedModel_epaReceiving)\n\n            R2m      R2c\n[1,] 0.00238466 0.299384\n\nCodeemmeans::emmeans(mixedModel_epaReceiving, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no            0.652 0.0345 1199    0.584    0.719\n yes           0.504 0.0478 2355    0.410    0.597\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaReceiving &lt;- lmerTest::lmer(\n  receiving_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nreceiving_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 11650.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.4465 -0.5584 -0.0289  0.5261  3.8701 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.873149 0.9344        \n           ageCentered20 0.005505 0.0742   -0.62\n Residual                1.256079 1.1207        \nNumber of obs: 3489, groups:  player_id, 1017\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             3.152e-01  1.055e-01  1.022e+03   2.988  0.00287 ** \ncontractYearyes        -1.565e-01  4.996e-02  3.238e+03  -3.133  0.00174 ** \nageCentered20           5.594e-02  3.443e-02  1.064e+03   1.625  0.10451    \nageCentered20Quadratic -9.972e-03  1.967e-03  4.232e+02  -5.070 5.95e-07 ***\nyears_of_experience     9.926e-02  2.348e-02  1.052e+03   4.228 2.57e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.134                     \nageCentrd20 -0.836 -0.197              \nagCntrd20Qd  0.785  0.120 -0.754       \nyrs_f_xprnc  0.074  0.029 -0.486 -0.142\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_epaReceiving)\n\n            R2m      R2c\n[1,] 0.01823977 0.334487\n\nCodeemmeans::emmeans(mixedModelAge_epaReceiving, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no            0.656 0.0354 1133    0.587    0.726\n yes           0.500 0.0486 2385    0.404    0.595\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_fantasyPtsReceiving &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_fantasyPtsReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 36611.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2907 -0.5245 -0.1494  0.4380  4.6460 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 1247     35.31   \n Residual              1140     33.77   \nNumber of obs: 3560, groups:  player_id, 1034\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       49.380      1.342 1291.947   36.81  &lt; 2e-16 ***\ncontractYearyes   -8.623      1.444 3112.753   -5.97 2.64e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.263\n\nCodeMuMIn::r.squaredGLMM(mixedModel_fantasyPtsReceiving)\n\n             R2m       R2c\n[1,] 0.006276973 0.5253879\n\nCodeemmeans::emmeans(mixedModel_fantasyPtsReceiving, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             49.4 1.34 1151     46.7     52.0\n yes            40.8 1.69 2167     37.4     44.1\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_fantasyPtsReceiving &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_fantasyPtsReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 36247\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1178 -0.4991 -0.1295  0.4295  4.9895 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   2501.70  50.017        \n           ageCentered20   26.88   5.185   -0.72\n Residual                 904.91  30.082        \nNumber of obs: 3558, groups:  player_id, 1033\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              27.43781    3.69149 1339.44865   7.433 1.89e-13 ***\ncontractYearyes          -6.37326    1.43120 2951.87067  -4.453 8.78e-06 ***\nageCentered20             3.29076    1.24364 1919.16753   2.646  0.00821 ** \nageCentered20Quadratic   -0.88027    0.06781 1135.74348 -12.981  &lt; 2e-16 ***\nyears_of_experience       8.87280    0.91446 1172.95041   9.703  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.127                     \nageCentrd20 -0.817 -0.163              \nagCntrd20Qd  0.722  0.091 -0.697       \nyrs_f_xprnc  0.178  0.024 -0.592 -0.091\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_fantasyPtsReceiving)\n\n           R2m       R2c\n[1,] 0.1150593 0.6622461\n\nCodeemmeans::emmeans(mixedModelAge_fantasyPtsReceiving, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             47.5 1.39 1137     44.7     50.2\n yes            41.1 1.68 2040     37.8     44.4\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.4 QB/RB/WR/TE\n\nCode# Placeholder for model predicting fantasy points\n# Include player position as a covariate",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersConclusion",
    "href": "mythbusters.html#sec-mythbustersConclusion",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n18.3 Conclusion",
    "text": "18.3 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersSessionInfo",
    "href": "mythbusters.html#sec-mythbustersSessionInfo",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n18.4 Session Info",
    "text": "18.4 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   emmeans_1.11.0    MuMIn_1.48.4     \n[13] lmerTest_3.1-3    lme4_1.1-36       Matrix_1.7-2      nflreadr_1.4.1   \n[17] petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    psych_2.5.3         viridisLite_0.4.2  \n [4] fastmap_1.2.0       digest_0.6.37       rpart_4.1.24       \n [7] timechange_0.3.0    estimability_1.5.1  lifecycle_1.0.4    \n[10] cluster_2.1.8       magrittr_2.0.3      compiler_4.4.3     \n[13] rlang_1.1.5         Hmisc_5.2-3         tools_4.4.3        \n[16] yaml_2.3.10         data.table_1.17.0   knitr_1.50         \n[19] htmlwidgets_1.6.4   mnormt_2.1.1        plyr_1.8.9         \n[22] RColorBrewer_1.1-3  withr_3.0.2         foreign_0.8-88     \n[25] numDeriv_2016.8-1.1 nnet_7.3-20         grid_4.4.3         \n[28] stats4_4.4.3        lavaan_0.6-19       xtable_1.8-4       \n[31] colorspace_2.1-1    scales_1.3.0        MASS_7.3-64        \n[34] cli_3.6.4           mvtnorm_1.3-3       rmarkdown_2.29     \n[37] reformulas_0.4.0    generics_0.1.3      rstudioapi_0.17.1  \n[40] tzdb_0.5.0          reshape2_1.4.4      minqa_1.2.8        \n[43] DBI_1.2.3           cachem_1.1.0        splines_4.4.3      \n[46] parallel_4.4.3      base64enc_0.1-3     mitools_2.4        \n[49] vctrs_0.6.5         boot_1.3-31         jsonlite_1.9.1     \n[52] hms_1.1.3           pbkrtest_0.5.3      Formula_1.2-5      \n[55] htmlTable_2.4.3     glue_1.8.0          nloptr_2.2.1       \n[58] stringi_1.8.4       gtable_0.3.6        quadprog_1.5-8     \n[61] munsell_0.5.1       pillar_1.10.1       htmltools_0.5.8.1  \n[64] R6_2.6.1            Rdpack_2.6.3        mix_1.0-13         \n[67] evaluate_1.0.3      pbivnorm_0.6.0      lattice_0.22-6     \n[70] rbibutils_2.3       backports_1.5.0     broom_1.0.7        \n[73] memoise_2.0.1       Rcpp_1.0.14         coda_0.19-4.1      \n[76] gridExtra_2.3       nlme_3.1-167        checkmate_2.3.2    \n[79] xfun_0.51           pkgconfig_2.0.3    \n\n\n\n\n\n\nBales, J. (2012). 2012 contract year players and the myth of increased production. https://www.4for4.com/2012/preseason/2012-contract-year-players-and-myth-increased-production\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nNiles, B. (2022). Do players perform better in fantasy football in a contract year? https://www.4for4.com/2022/preseason/do-players-perform-better-fantasy-football-contract-year\n\n\nWhite, M. H., & Sheldon, K. M. (2014). The contract year syndrome in the NBA and MLB: A classic undermining pattern. Motivation and Emotion, 38(2), 196–205. https://doi.org/10.1007/s11031-013-9389-7",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html",
    "href": "modern-portfolio-theory.html",
    "title": "19  Modern Portfolio Theory",
    "section": "",
    "text": "19.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheoryGettingStarted",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheoryGettingStarted",
    "title": "19  Modern Portfolio Theory",
    "section": "",
    "text": "19.1.1 Load Packages\n\nCodelibrary(\"quantmod\")\nlibrary(\"fPortfolio\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheoryOverview",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheoryOverview",
    "title": "19  Modern Portfolio Theory",
    "section": "\n19.2 Overview",
    "text": "19.2 Overview",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-likeStockPicking",
    "href": "modern-portfolio-theory.html#sec-likeStockPicking",
    "title": "19  Modern Portfolio Theory",
    "section": "\n19.3 Fantasy Football is Like Stock Picking",
    "text": "19.3 Fantasy Football is Like Stock Picking\nSelecting players for your fantasy team is like picking stocks. In both fantasy football and the stock market, your goal is to pick assets (i.e., players/stocks) that will perform best and that others undervalue. But what is the best way to do that? Below, we discuss approaches to picking players/stocks.\n\n19.3.1 The Wisdom of the Crowd (or Market)\nIn picking players, there are various approaches one could take. You could do lots of research to pick players/stocks with strong fundamentals that you think will do particularly well next year. By picking these players/stocks, you are predicting that they will outperform their expectations. However, all of your information is likely already reflected in the current valuation of the player/stock, so your prediction is basically a gamble. This is evidenced by the fact that people do not reliably beat the crowd/market.\nEven so-called experts do not beat the market reliably. There is little consistency in the performance of mutual fund managers over time. In the book, “The Drunkard’s Walk: How Randomness Rules Our Lives”, Mlodinow (2008) reported essentially no correlation between performance of the top mutual funds in a five-year period with their performance over the subsequent five years. That is, the best funds in a one period were not necessarily the best funds in another period. This suggests that mutual fund managers differ in great part because of luck or chance rather than reliable skill. In any given year, some mutual funds will do better than other mutual funds. But this overperformance in a given year likely reflects more randomness than skill. That is likely why a cat beat professional investors in a stock market challenge [Goldstein (2013); archived at https://perma.cc/R3XU-K6J8]. That is, “few stock pickers, if any, have the skill needed to beat the market consistently, year after year” (Kahneman, 2011, p. 214). Although our sample size is much smaller with fantasy football projections, there also appears to be little consistency in fantasy football sites’ rank in accuracy over time [INSERT], suggesting that the projection sources are not reliably better than each other (or the crowd) over time.\nThe market reflects all of the knowledge of the crowd. One common misconception is that if you go with the market, you will receive “average” returns (by “average”, I mean that you will be in the 50th percentile among investors). This is not true—it has been shown that most mutual funds (about 80%) underperform the average returns of the stock market. So, by going with the market average, you will likely perform better than the “average” fund/investor. Consistent with this, crowd-averaged fantasy football projections tend to be more accurate than any individual’s projection: INSERT This evidence is consistent with the notion of the wisdom of the crowd, described in Section 24.3. Moreover, even if the stock market is relatively accurate (“efficient”) in terms of valuing stocks based on all (publicly) available information (i.e., the efficient market hypothesis), your fantasy football league is likely not. Thus, it may be effective to use crowd-based projections to identify players who are undervalued by your league.\n\n19.3.2 Diversification\nModern portfolio theory (mean-variance theory) is a framework for determining the optimal composition of an investment portfolio to maximize expected returns for a given level of risk. Here, risk refers to the variability (e.g., standard deviation or variance) of returns across time. Given two portfolios with the same expected returns over time, people will prefer the “safer” portfolio—that is, the portfolio with less variability/volatility across time. One of the powerful notions of modern portfolio theory is that, through diversification, one can achieve lower risk with the same expected returns. In investing, diversification involves owning multiple asset classes (e.g., domestic and international stocks and bonds), with the goal of having asset classes that are either uncorrelated or negatively correlated. That is, owning different types of assets is safer than owning only one type. If you have too much money in one asset and that asset tanks, you will lose your money. In other words, you do not want to put all of your eggs in one basket. By owning different asset classes, you can limit your downside risk without sacrificing much in terms of expected return.\nThis lesson can also apply to fantasy football. When assembling a team, you are essentially putting together a portfolio of assets (i.e., team of players). As with stocks, each player has an expected return (i.e., projection) and a degree of risk. In fantasy football, a player’s risk might be quantified in terms of the variability of projected scores for a player across projection sources (e.g., Projection Source A, Source B, Souce C, etc.), or as historical game-to-game variability. Variability of projected scores for a player across projection sources could reflect the uncertainty of projections for a player. Variability of historical (actual) fantasy points across games could reflect many factors, including risks due to injuries, situational changes (e.g., being traded to a new team or changes in team composition such as due to the acquisition of new players on the team), game scripts, and the tendency for the player to be “boom-or-bust” (e.g., if they are highly dependent on scoring touchdowns or long receptions for fantasy points). All things equal, we want to minimize our risk for a given level of expected returns. That way, we have the best chance of winning any given week. For the same level of expected returns, higher risk teams might have a few amazing games, but their teams might fall flat in other weeks. That is, for a given (high) rate of return, you are best off in the long run (i.e., over the course of a season) with a lower risk team compared to a higher risk team [Hitchings (2012); archived at https://perma.cc/NE35-G6LR].\nIn terms of diversification, it can be helpful to diversify in multiple ways. First, it can be helpful not to rely on just one or two “stud” players. If they are on bye or have a down week, your team is more likely to suffer. Also, there are risks in picking multiple offensive players from the same team. If you draft your starting Quarterback and Wide Receiver from the same team (e.g., the Cowboys), you are exposing your fantasy team to considerable risk. For instance, if you have the Quarterback and Wide Receiver from the same team, and the team has a poor offensive outing, that will have a greater impact. You can limit your downside risk by diversifying—drafting players from different teams. That way if the Cowboys’ offense does poorly in a given week, your fantasy team will not be as affected. Having multiple players on a juggernaut offense can be a boon, but it can be challenging to predict which offense will lead the league.\nHowever, sometimes having two players on the same team might be beneficial because some positions may be uncorrelated or even negatively correlated, which can also reduce risk. For instance, the performance of the Tight End and Running Back on the same team tends to be slightly negatively correlated, so it might not be a bad idea to start the Tight End and Running Back from the same team. For a correlation matrix of all positions on the team, see: https://assets-global.website-files.com/5f1af76ed86d6771ad48324b/607a4434a565aa7763bd1312_AndyAsh-Sharpstack-RPpaper.pdf [Sherman & Goldner (2021); archived at https://perma.cc/JQ6G-KSRT].\nAnother important idea from modern portfolio theory is that, if you want to achieve higher returns, you may be able to by accepting additional—and the right combination of—risk. In general, risk is positively correlated with return. That is, receiving higher returns generally requires taking on additional risk—at least as long as we stay along the efficient frontier, described next.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-efficientFrontier",
    "href": "modern-portfolio-theory.html#sec-efficientFrontier",
    "title": "19  Modern Portfolio Theory",
    "section": "\n19.4 The Efficient Frontier of a Stock Portfolio",
    "text": "19.4 The Efficient Frontier of a Stock Portfolio\nThe ultimate goal in fantasy football is to draft players for your starting lineup that provide the most projected points (i.e., the highest returns) and the smallest downside risk. That is, your goal is to achieve the optimal portfolio at a given level of risk, depending on how much risk you are willing to tolerate. One of the key tools in modern portfolio theory for identifying the optimal portfolio (for a given risk level) is the efficient frontier. The efficient frontier is a visual depiction of the maximum expected returns for a given level of risk (where risk is the variability in returns over time). The efficient frontier is helpful for identifying the optimal portfolio—the optimal combination and weighting of assets—for a given risk level. Anything below the efficient frontier is considered inefficient (i.e., lower-than-maximum returns for a given level of risk).\nIn the example below, we use historical returns (since YEAR) as the expected future returns. However, using historical returns as the expected future returns is risky because, as described in the common disclaimer, “Past performance does not guarantee future results.” If you select a relatively short period of historical returns, you may be selecting a period when the stock performed particularly well. When evaluating historical returns it is preferable to evaluate long time horizons and to evaluate how the stock performed during period of both boom (i.e., “bull markets”) and bust (i.e., “bear markets”, such as in a recession).\n\n19.4.1 Download Historical Stock Prices\nWe download historical stock prices using the quantmod package (Ryan & Ulrich, 2024):\n\nCodesymbols &lt;- c(\n  \"AAPL\",  # Apple\n  \"MSFT\",  # Microsoft\n  \"GOOGL\", # Google\n  \"AMZN\",  # Amazon\n  \"META\",  # Meta/Facebook\n  \"V\",     # Visa\n  \"DIS\",   # Disney\n  \"NKE\",   # Nike\n  \"TSLA\")  # Tesla\n\nquantmod::getSymbols(symbols)\n\n[1] \"AAPL\"  \"MSFT\"  \"GOOGL\" \"AMZN\"  \"META\"  \"V\"     \"DIS\"   \"NKE\"   \"TSLA\" \n\n\n\n19.4.2 Calculate Stock Returns\n\nCodeprices &lt;- do.call(\n  merge,\n  lapply(\n    symbols,\n    function(sym) quantmod::Cl(get(sym))))\n\nreturns &lt;- na.omit(\n  TTR::ROC(\n    prices,\n    type = \"discrete\"))\n\nreturns_ts &lt;- timeSeries::as.timeSeries(returns)\n\n\n\n19.4.3 Create Portfolio\nWe use the fPortfolio package (Wuertz et al., 2023) to determine the optimal portfolio.\n\nCodeportfolioSpec &lt;- fPortfolio::portfolioSpec()\n\nfPortfolio::setNFrontierPoints(portfolioSpec) &lt;- 1000\n\n\n\n19.4.4 Determine the Efficient Frontier\n\nCodeefficientFrontier &lt;- fPortfolio::portfolioFrontier(\n  returns_ts,\n  spec = portfolioSpec)\n\nefficientFrontier\n\n\nTitle:\n MV Portfolio Frontier \n Estimator:         covEstimator \n Solver:            solveRquadprog \n Optimize:          minRisk \n Constraints:       LongOnly \n Portfolio Points:  5 of 1000 \n\nPortfolio Weights:\n     AAPL.Close MSFT.Close GOOGL.Close AMZN.Close META.Close V.Close DIS.Close\n1        0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    1.0000\n250      0.1315     0.1261      0.0907     0.0609     0.0075  0.3340    0.1364\n500      0.0524     0.1055      0.0000     0.1490     0.0962  0.3414    0.0000\n750      0.0000     0.0000      0.0000     0.1818     0.1808  0.0546    0.0000\n1000     0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    0.0000\n     NKE.Close TSLA.Close\n1       0.0000     0.0000\n250     0.0900     0.0230\n500     0.0000     0.2556\n750     0.0000     0.5829\n1000    0.0000     1.0000\n\nCovariance Risk Budgets:\n     AAPL.Close MSFT.Close GOOGL.Close AMZN.Close META.Close V.Close DIS.Close\n1        0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    1.0000\n250      0.1343     0.1297      0.0920     0.0653     0.0081  0.3381    0.1204\n500      0.0353     0.0734      0.0000     0.1257     0.0864  0.2191    0.0000\n750      0.0000     0.0000      0.0000     0.0828     0.0920  0.0141    0.0000\n1000     0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    0.0000\n     NKE.Close TSLA.Close\n1       0.0000     0.0000\n250     0.0810     0.0311\n500     0.0000     0.4602\n750     0.0000     0.8112\n1000    0.0000     1.0000\n\nTarget Returns and Risks:\n       mean    Cov   CVaR    VaR\n1    0.0004 0.0164 0.0374 0.0234\n250  0.0008 0.0127 0.0302 0.0199\n500  0.0013 0.0165 0.0376 0.0256\n750  0.0017 0.0249 0.0553 0.0372\n1000 0.0022 0.0361 0.0782 0.0516\n\nDescription:\n Sun Mar 23 20:21:39 2025 by user:  \n\n\n\nCode# Extract the coordinates of individual assets\nasset_means &lt;- colMeans(returns)\nasset_sd &lt;- apply(returns, 2, sd)\n\n# Add some padding to plot limits (so ticker symbols don't get cut off)\nxlim &lt;- range(asset_sd) * c(0.9, 1.1)\nylim &lt;- range(asset_means) * c(0.9, 1.1)\n\nxlim[1] &lt;- 0\nylim[1] &lt;- 0\n\n# Set scientific notation penalty\noptions(scipen = 999)\n\nplot(\n  efficientFrontier,\n  which = c(\n    1,  # efficient frontier\n    3,  # tangency portfolio\n    4), # risk/return of individual assets\n  control = list(\n    xlim = xlim,\n    ylim = ylim\n  ))\n\n# Add text labels for individual assets\npoints(\n  asset_sd,\n  asset_means,\n  col = \"red\",\n  pch = 19)\n\ntext(\n  asset_sd,\n  asset_means,\n  labels = symbols,\n  pos = 4,\n  cex = 0.8,\n  col = \"black\")\n\n\n\n\n\n\nFigure 19.1: Efficient Frontier for a Stock Portfolio.\n\n\n\n\n\n19.4.5 Identify the Optimal Weights\n\n19.4.5.1 Tangency Portfolio\nThe tangency portfolio is the portfolio with the highest Sharpe ratio (i.e., the highest ratio of return to risk). In other words, it is the portfolio with the greatest risk-adjusted returns.\n\nCode# Find the tangency portfolio (portfolio with the highest Sharpe ratio)\ntangencyPortfolio &lt;- fPortfolio::tangencyPortfolio(\n  data = returns_ts,\n  spec = portfolioSpec)\n\n# Extract optimal weights\ntangencyPortfolio_optimalWeights &lt;- fPortfolio::getWeights(tangencyPortfolio)\ntangencyPortfolio_optimalWeights\n\n AAPL.Close  MSFT.Close GOOGL.Close  AMZN.Close  META.Close     V.Close \n 0.08020758  0.12459271  0.00000000  0.13803914  0.08067738  0.37125566 \n  DIS.Close   NKE.Close  TSLA.Close \n 0.00000000  0.00000000  0.20522753 \n\nCode# Output the results\nsummary(tangencyPortfolio)\n\n\nTitle:\n MV Tangency Portfolio \n Estimator:         covEstimator \n Solver:            solveRquadprog \n Optimize:          minRisk \n Constraints:       LongOnly \n\nPortfolio Weights:\n AAPL.Close  MSFT.Close GOOGL.Close  AMZN.Close  META.Close     V.Close \n     0.0802      0.1246      0.0000      0.1380      0.0807      0.3713 \n  DIS.Close   NKE.Close  TSLA.Close \n     0.0000      0.0000      0.2052 \n\nCovariance Risk Budgets:\n AAPL.Close  MSFT.Close GOOGL.Close  AMZN.Close  META.Close     V.Close \n     0.0607      0.0969      0.0000      0.1261      0.0778      0.2701 \n  DIS.Close   NKE.Close  TSLA.Close \n     0.0000      0.0000      0.3684 \n\nTarget Returns and Risks:\n  mean    Cov   CVaR    VaR \n0.0012 0.0155 0.0356 0.0243 \n\nDescription:\n Sun Mar 23 20:21:39 2025 by user:  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n19.4.5.2 Portfolio with Max Return at a Given Risk Level\n\nCode# Define target risk levels\ntargetRisks &lt;- seq(0, 0.3, by = 0.01)\n\n# Initialize storage for optimal portfolios\noptimalPortfolios &lt;- list()\noptimalWeights_list &lt;- list()\n\n# Find optimal weightings for each target risk level\nfor (risk in targetRisks) {\n  # Create a portfolio optimization specification with the target risk\n  portfolioSpec &lt;- fPortfolio::portfolioSpec()\n  fPortfolio::setTargetRisk(portfolioSpec) &lt;- risk\n  \n  # Solve for the maximum return at this target risk\n  optimal_portfolio &lt;- fPortfolio::maxreturnPortfolio(\n    returns_ts,\n    spec = portfolioSpec)\n  \n  # Store the optimal portfolio\n  optimalPortfolios[[as.character(risk)]] &lt;- optimal_portfolio\n  \n  # Store the optimal portfolio weights with risk level\n  optimal_weights &lt;- fPortfolio::getWeights(optimal_portfolio)\n  optimalWeights_list[[as.character(risk)]] &lt;- c(RiskLevel = risk, optimal_weights)\n}\n\noptimalWeightsByRisk &lt;- dplyr::bind_rows(optimalWeights_list)\noptimalWeightsByRisk",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-efficientFrontierFantasy",
    "href": "modern-portfolio-theory.html#sec-efficientFrontierFantasy",
    "title": "19  Modern Portfolio Theory",
    "section": "\n19.5 The Efficient Frontier of a Fantasy Team",
    "text": "19.5 The Efficient Frontier of a Fantasy Team\nIn fantasy football, the efficient frontier can be helpful for identifying the optimal players to draft for a given risk level (and potentially within the salary cap). It can also be helpful for identifying potential trades. In this way, modern portfolio theory and the efficient frontier can be helpful for arbitrage—buying and selling the same asset (in this case, player) to take advantage of different prices for the same asset. That is, you could buy low and, for players who outperform expectations, sell high—in the form of a trade.\n\n19.5.1 Based on Variability Across Projection Sources\n\n19.5.2 Based on Historical Game-to-Game Variability\nhttps://eng.wealthfront.com/2012/01/17/moneyball-using-modern-portfolio-theory-to-win-your-fantasy-sports-league [Hitchings (2012); archived at https://perma.cc/JQ6G-KSRT]",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheoryConclusion",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheoryConclusion",
    "title": "19  Modern Portfolio Theory",
    "section": "\n19.6 Conclusion",
    "text": "19.6 Conclusion\nIn summary, fantasy football is similar to stock picking. You are most likely to pick the best players if you go with the wisdom of the crowd (e.g., average projections across projection sources) and diversify. Most projections are public information, so you might wonder whether using crowd projections gains you anything because everybody else has access to public information. However, this is also the case with stocks, and people still consistently perform best over time when they go with the market. Nevertheless, crowd projections are not highly accurate. And fantasy football is a game, so feel free to have fun and deviate from the crowd! However, you may be just as (if not more) likely to be wrong by deviating from the crowd.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheorySessionInfo",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheorySessionInfo",
    "title": "19  Modern Portfolio Theory",
    "section": "\n19.7 Session Info",
    "text": "19.7 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n [4] dplyr_1.1.4         purrr_1.0.4         readr_2.1.5        \n [7] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[10] tidyverse_2.0.0     fPortfolio_4023.84  fAssets_4023.85    \n[13] fBasics_4041.97     timeSeries_4041.111 timeDate_4041.110  \n[16] quantmod_0.4.26     TTR_0.24.4          xts_0.14.1         \n[19] zoo_1.8-13         \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6        xfun_0.51           htmlwidgets_1.6.4  \n [4] lattice_0.22-6      tzdb_0.5.0          numDeriv_2016.8-1.1\n [7] generics_0.1.3      quadprog_1.5-8      vctrs_0.6.5        \n[10] tools_4.4.3         bitops_1.0-9        stats4_4.4.3       \n[13] curl_6.2.1          parallel_4.4.3      DEoptimR_1.1-3-1   \n[16] pkgconfig_2.0.3     lifecycle_1.0.4     truncnorm_1.0-9    \n[19] compiler_4.4.3      fMultivar_4031.84   munsell_0.5.1      \n[22] mnormt_2.1.1        htmltools_0.5.8.1   RCurl_1.98-1.17    \n[25] Rsolnp_1.16         yaml_2.3.10         pillar_1.10.1      \n[28] MASS_7.3-64         boot_1.3-31         robustbase_0.99-4-1\n[31] rneos_0.4-0         tidyselect_1.2.1    digest_0.6.37      \n[34] stringi_1.8.4       mvtnorm_1.3-3       slam_0.1-55        \n[37] kernlab_0.9-33      mvnormtest_0.1-9-3  gsl_2.1-8          \n[40] fastmap_1.2.0       grid_4.4.3          colorspace_2.1-1   \n[43] cli_3.6.4           magrittr_2.0.3      XML_3.99-0.18      \n[46] withr_3.0.2         sn_2.1.1            scales_1.3.0       \n[49] timechange_0.3.0    energy_1.7-12       rmarkdown_2.29     \n[52] igraph_2.1.4        hms_1.1.3           ecodist_2.1.3      \n[55] evaluate_1.0.3      knitr_1.50          Rglpk_0.6-5.1      \n[58] rlang_1.1.5         spatial_7.3-18      Rcpp_1.0.14        \n[61] glue_1.8.0          jsonlite_1.9.1      fCopulae_4022.85   \n[64] R6_2.6.1           \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGoldstein, J. (2013). Cat beats investors in stock market challenge. https://www.npr.org/sections/money/2013/01/14/169326326/housecat-beats-investors-in-stock-market-challenge\n\n\nHitchings, J. (2012). Moneyball: Using modern portfolio theory to win your fantasy sports league. https://eng.wealthfront.com/2012/01/17/moneyball-using-modern-portfolio-theory-to-win-your-fantasy-sports-league\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nMlodinow, L. (2008). The drunkard’s walk: How randomness rules our lives. Pantheon Books.\n\n\nRyan, J. A., & Ulrich, J. M. (2024). quantmod: Quantitative financial modelling framework. https://www.quantmod.com/\n\n\nSherman, A., & Goldner, K. (2021). Sharpstack: Cholesky correlations for building better lineups. https://assets-global.website-files.com/5f1af76ed86d6771ad48324b/607a4434a565aa7763bd1312_AndyAsh-Sharpstack-RPpaper.pdf\n\n\nWuertz, D., Setz, T., Chalabi, Y., & Theussl, S. (2023). fPortfolio: Rmetrics - portfolio selection and optimization. https://r-forge.r-project.org/projects/rmetrics/",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html",
    "href": "cluster-analysis.html",
    "title": "20  Cluster Analysis",
    "section": "",
    "text": "20.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#sec-clusterAnalysisGettingStarted",
    "href": "cluster-analysis.html#sec-clusterAnalysisGettingStarted",
    "title": "20  Cluster Analysis",
    "section": "",
    "text": "20.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"nflreadr\")\nlibrary(\"mclust\")\nlibrary(\"plotly\")\nlibrary(\"tidyverse\")\n\n\n\n20.1.2 Load Data\n\nCodeload(file = \"./data/nfl_players.RData\")\nload(file = \"./data/nfl_combine.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/nfl_advancedStatsPFR_seasonal.RData\")\nload(file = \"./data/nfl_actualStats_career.RData\")\n\n\n\n20.1.3 Overview\nWhereas factor analysis evaluates how variables do or do not hang together—in terms of their associations and non-associations, cluster analysis evaluates how people are or or not similar—in terms of their scores on one or more variables. The goal of cluster analysis is to identify distinguishable subgroups of people. The people within a subgroup are expected to be more similar to each other than they are to people in other subgroups. For instance, we might expect that there are distinguishable subtypes of Wide Receivers: possession, deep threats, and slot-type Wide Receivers. Possession Wide Receivers tend to be taller and heavier, with good hands who catch the ball at a high rate. Deep threat Wide Receivers tend to be fast. Slot-type Wide Receivers tend to be small, quick, and agile. In order to identify these clusters of Wide Receivers, we might conduct a cluster analysis with variables relating to the players’ height, weight, percent of (catchable) targets caught, air yards received, and various metrics from the National Football League (NFL) Combine, including their times in the 40-yard dash, 20-yard shuttle run, and three cone drill.\nThere are many approaches to cluster analysis, including model-based clustering, density-based clustering, centroid-based clustering, hierarchical clustering (aka connectivity-based clustering), etc. An overview of approaches to cluster analysis in R is provided by Kassambara (2017). In this chapter, we focus on examples using model-based clustering with the R package mclust (Fraley et al., 2024; Scrucca et al., 2023), which uses Gaussian finite mixture modeling. The various types of mclust models are provided here: https://mclust-org.github.io/mclust/reference/mclustModelNames.html.\n\n20.1.4 Tiers of Prior Season Fantasy Points\n\n20.1.4.1 Prepare Data\n\nCoderecentSeason &lt;- max(player_stats_seasonal$season, na.rm = TRUE) # also works: nflreadr::most_recent_season()\nrecentSeason\n\n[1] 2023\n\nCodeplayer_stats_seasonal_offense_recent &lt;- player_stats_seasonal %&gt;% \n  filter(season == recentSeason) %&gt;% \n  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\nplayer_stats_seasonal_offense_recentQB &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"QB\")\n\nplayer_stats_seasonal_offense_recentRB &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"RB\")\n\nplayer_stats_seasonal_offense_recentWR &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"WR\")\n\nplayer_stats_seasonal_offense_recentTE &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"TE\")\n\n\n\n20.1.4.2 Identify the Optimal Number of Tiers by Position\n\n20.1.4.2.1 Quarterbacks\n\nCodetiersQB_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = 1:9\n)\n\ntiersQB_bic\n\nBayesian Information Criterion (BIC): \n           E          V\n1 -1003.1739 -1003.1739\n2  -969.9747  -948.2264\n3  -978.7935  -945.1457\n4  -981.3619  -938.0253\n5  -990.1507  -949.1264\n6  -985.2384  -961.0139\n7  -988.2083  -963.9640\n8  -996.9990  -974.5630\n9  -991.9542  -985.9481\n\nTop 3 models based on the BIC criterion: \n      V,4       V,3       V,2 \n-938.0253 -945.1457 -948.2264 \n\nCodesummary(tiersQB_bic)\n\nBest BIC values:\n               V,4         V,3       V,2\nBIC      -938.0253 -945.145680 -948.2264\nBIC diff    0.0000   -7.120384  -10.2011\n\nCodeplot(tiersQB_bic)\n\n\n\n\n\n\nCodetiersQB_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = 1:9\n)\n\ntiersQB_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n           E          V\n1 -1003.1739 -1003.1739\n2  -973.2894  -954.7750\n3 -1052.9192  -957.0398\n4 -1053.7941  -947.0387\n5 -1076.4310  -961.5372\n6 -1080.9352  -989.5764\n7 -1077.4745  -985.7520\n8 -1092.8528  -991.9345\n9 -1087.3713  -999.5436\n\nTop 3 models based on the ICL criterion: \n      V,4       V,2       V,3 \n-947.0387 -954.7750 -957.0398 \n\nCodesummary(tiersQB_icl)\n\nBest ICL values:\n               V,4         V,2        V,3\nICL      -947.0387 -954.775034 -957.03983\nICL diff    0.0000   -7.736319  -10.00112\n\nCodeplot(tiersQB_icl)\n\n\n\n\n\n\nCodetiersQB_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\nnumTiersQB &lt;- as.numeric(summary(tiersQB_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersQB_boostrap\n\n------------------------------------------------------------- \nBootstrap sequential LRT for the number of mixture components \n------------------------------------------------------------- \nModel        = V \nReplications = 999 \n             LRTS bootstrap p-value\n1 vs 2   68.13089             0.001\n2 vs 3   16.26406             0.007\n3 vs 4   20.30373             0.001\n4 vs 5    2.08224             0.559\n\nCodeplot(\n  tiersQB_boostrap,\n  G = numTiersQB - 1)\n\n\n\n\n\n\n\n\n20.1.4.2.2 Running Backs\n\nCodetiersRB_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  G = 1:9\n)\n\ntiersRB_bic\n\nBayesian Information Criterion (BIC): \n          E         V\n1 -1782.627 -1782.627\n2 -1711.492 -1644.547\n3 -1721.605 -1616.735\n4 -1716.604 -1603.824\n5 -1726.701 -1605.815\n6 -1736.797 -1621.023\n7 -1724.416 -1632.489\n8 -1734.512 -1642.516\n9 -1713.319 -1657.425\n\nTop 3 models based on the BIC criterion: \n      V,4       V,5       V,3 \n-1603.824 -1605.815 -1616.735 \n\nCodesummary(tiersRB_bic)\n\nBest BIC values:\n               V,4          V,5         V,3\nBIC      -1603.824 -1605.815293 -1616.73521\nBIC diff     0.000    -1.990827   -12.91075\n\nCodeplot(tiersRB_bic)\n\n\n\n\n\n\nCodetiersRB_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  G = 1:9\n)\n\ntiersRB_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n          E         V\n1 -1782.627 -1782.627\n2 -1719.075 -1667.789\n3 -1878.928 -1654.432\n4 -1878.834 -1642.374\n5 -1982.539 -1636.166\n6 -2021.769 -1665.660\n7 -1988.509 -1692.219\n8 -2020.196 -1681.704\n9 -2012.945 -1726.370\n\nTop 3 models based on the ICL criterion: \n      V,5       V,4       V,3 \n-1636.166 -1642.374 -1654.432 \n\nCodesummary(tiersRB_icl)\n\nBest ICL values:\n               V,5          V,4         V,3\nICL      -1636.166 -1642.373643 -1654.43164\nICL diff     0.000    -6.207432   -18.26543\n\nCodeplot(tiersRB_icl)\n\n\n\n\n\n\nCodenumTiersRB &lt;- 3\n\n\nThe model-based bootstrap clustering of Running Backs’ fantasy points is unable to run due to an error:\n\nCodetiersRB_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\n\nThus, we cannot use the following code, which would otherwise summarize the model results, specify the number of tiers, and plot model comparisons:\n\nCodenumTiersRB &lt;- as.numeric(summary(tiersRB_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersRB_boostrap\nplot(\n  tiersRB_boostrap,\n  G = numTiersRB - 1)\n\n\n\n20.1.4.2.3 Wide Receivers\n\nCodetiersWR_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  G = 1:9\n)\n\ntiersWR_bic\n\nBayesian Information Criterion (BIC): \n          E         V\n1 -2471.492 -2471.492\n2 -2392.220 -2345.420\n3 -2403.093 -2311.206\n4 -2393.793 -2311.058\n5 -2404.635        NA\n6 -2415.466        NA\n7 -2399.516        NA\n8 -2410.356        NA\n9 -2401.250        NA\n\nTop 3 models based on the BIC criterion: \n      V,4       V,3       V,2 \n-2311.058 -2311.206 -2345.420 \n\nCodesummary(tiersWR_bic)\n\nBest BIC values:\n               V,4           V,3         V,2\nBIC      -2311.058 -2311.2064360 -2345.42033\nBIC diff     0.000    -0.1479841   -34.36188\n\nCodeplot(tiersWR_bic)\n\n\n\n\n\n\nCodetiersWR_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  G = 1:9\n)\n\ntiersWR_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n          E         V\n1 -2471.492 -2471.492\n2 -2403.467 -2391.202\n3 -2631.957 -2373.020\n4 -2622.979 -2402.523\n5 -2769.787        NA\n6 -2859.127        NA\n7 -2782.991        NA\n8 -2830.362        NA\n9 -2825.306        NA\n\nTop 3 models based on the ICL criterion: \n      V,3       V,2       V,4 \n-2373.020 -2391.202 -2402.523 \n\nCodesummary(tiersWR_icl)\n\nBest ICL values:\n              V,3         V,2         V,4\nICL      -2373.02 -2391.20158 -2402.52344\nICL diff     0.00   -18.18181   -29.50367\n\nCodeplot(tiersWR_icl)\n\n\n\n\n\n\nCodetiersWR_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\nnumTiersWR &lt;- as.numeric(summary(tiersWR_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersWR_boostrap\n\n------------------------------------------------------------- \nBootstrap sequential LRT for the number of mixture components \n------------------------------------------------------------- \nModel        = V \nReplications = 999 \n              LRTS bootstrap p-value\n1 vs 2   142.32029             0.001\n2 vs 3    50.46219             0.001\n3 vs 4    16.39629             0.003\n\nCodeplot(\n  tiersWR_boostrap,\n  G = numTiersWR - 1)\n\n\n\n\n\n\n\n\n20.1.4.2.4 Tight Ends\n\nCodetiersTE_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  G = 1:9\n)\n\ntiersTE_bic\n\nBayesian Information Criterion (BIC): \n          E         V\n1 -1260.169 -1260.169\n2 -1215.361 -1190.813\n3 -1225.026 -1170.432\n4 -1234.698 -1175.238\n5 -1220.548 -1186.239\n6 -1219.179 -1196.909\n7 -1228.848 -1202.941\n8 -1238.592 -1214.896\n9 -1248.213 -1222.066\n\nTop 3 models based on the BIC criterion: \n      V,3       V,4       V,5 \n-1170.432 -1175.238 -1186.239 \n\nCodesummary(tiersTE_bic)\n\nBest BIC values:\n               V,3          V,4         V,5\nBIC      -1170.432 -1175.237575 -1186.23910\nBIC diff     0.000    -4.805739   -15.80726\n\nCodeplot(tiersTE_bic)\n\n\n\n\n\n\nCodetiersTE_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  G = 1:9\n)\n\ntiersTE_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n          E         V\n1 -1260.169 -1260.169\n2 -1221.676 -1216.683\n3 -1361.049 -1205.327\n4 -1438.081 -1223.003\n5 -1423.891 -1236.280\n6 -1391.645 -1248.686\n7 -1428.558 -1241.746\n8 -1459.351 -1249.834\n9 -1500.949 -1254.242\n\nTop 3 models based on the ICL criterion: \n      V,3       V,2       E,2 \n-1205.327 -1216.683 -1221.676 \n\nCodesummary(tiersTE_icl)\n\nBest ICL values:\n               V,3         V,2         E,2\nICL      -1205.327 -1216.68330 -1221.67588\nICL diff     0.000   -11.35599   -16.34858\n\nCodeplot(tiersTE_icl)\n\n\n\n\n\n\nCodetiersTE_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\nnumTiersTE &lt;- as.numeric(summary(tiersTE_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersTE_boostrap\n\n------------------------------------------------------------- \nBootstrap sequential LRT for the number of mixture components \n------------------------------------------------------------- \nModel        = V \nReplications = 999 \n              LRTS bootstrap p-value\n1 vs 2   83.841186             0.001\n2 vs 3   34.865788             0.001\n3 vs 4    9.679202             0.032\n4 vs 5    3.483417             0.360\n\nCodeplot(\n  tiersTE_boostrap,\n  G = numTiersTE - 1)\n\n\n\n\n\n\n\n\n20.1.4.3 Fit the Cluster Model to the Optimal Number of Tiers\n\n20.1.4.3.1 Quarterbacks\nIn our data, all of the following models are equivalent—i.e., they result in the same unequal variance model with a 4-cluster solution—but they arrive there in different ways.\n\nCodemclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = numTiersQB,\n)\n\nmclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = 4,\n)\n\nmclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n)\n\nmclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  x = tiersQB_bic\n)\n\n\nLet’s fit one of these:\n\nCodeclusterModelQBs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = numTiersQB,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelQBs$classification)\n\n\n 1  2  3  4 \n22 12 24 23 \n\n\n\n20.1.4.3.2 Running Backs\n\nCodeclusterModelRBs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  G = numTiersRB,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelRBs$classification)\n\n\n 1  2  3 \n62 38 56 \n\n\n\n20.1.4.3.3 Wide Receivers\n\nCodeclusterModelWRs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  G = numTiersWR,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelWRs$classification)\n\n\n 1  2  3 \n53 92 80 \n\n\n\n20.1.4.3.4 Tight Ends\n\nCodeclusterModelTEs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  G = numTiersTE,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelTEs$classification)\n\n\n 1  2  3  4 \n31 33 33 28 \n\n\n\n20.1.4.4 Plot the Tiers\nWe can merge the player’s classification into the dataset and plot each player’s classification.\n\n20.1.4.4.1 Quarterbacks\n\nCodeplayer_stats_seasonal_offense_recentQB$tier &lt;- clusterModelQBs$classification\n\nplayer_stats_seasonal_offense_recentQB &lt;- player_stats_seasonal_offense_recentQB %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentQB$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentQB$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_qbTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentQB,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Quarterback Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nggplotly(plot_qbTiers)\n\n\n\n\n\n\nFigure 20.1: Quarterback Fantasy Points by Tier.\n\n\n\n\n20.1.4.4.2 Running Backs\n\nCodeplayer_stats_seasonal_offense_recentRB$tier &lt;- clusterModelRBs$classification\n\nplayer_stats_seasonal_offense_recentRB &lt;- player_stats_seasonal_offense_recentRB %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentRB$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentRB$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_rbTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentRB,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Running Back Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nggplotly(plot_rbTiers)\n\n\n\n\n\n\nFigure 20.2: Running Back Fantasy Points by Tier.\n\n\n\n\n20.1.4.4.3 Wide Receivers\n\nCodeplayer_stats_seasonal_offense_recentWR$tier &lt;- clusterModelWRs$classification\n\nplayer_stats_seasonal_offense_recentWR &lt;- player_stats_seasonal_offense_recentWR %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentWR$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentWR$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_wrTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentWR,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Wide Receiver Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nggplotly(plot_wrTiers)\n\n\n\n\n\n\nFigure 20.3: Quarterback Fantasy Points by Tier.\n\n\n\n\n20.1.4.4.4 Tight Ends\n\nCodeplayer_stats_seasonal_offense_recentTE$tier &lt;- clusterModelTEs$classification\n\nplayer_stats_seasonal_offense_recentTE &lt;- player_stats_seasonal_offense_recentTE %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentTE$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentTE$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_teTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentTE,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Tight End Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nggplotly(plot_teTiers)\n\n\n\n\n\n\nFigure 20.4: Tight End Fantasy Points by Tier.\n\n\n\n\n20.1.5 Types of Wide Receivers\n\nCode# Compute Advanced PFR Stats by Career\npfrVars &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  select(pocket_time.pass:cmp_percent.def, g, gs) %&gt;% \n  names()\n\nweightedAverageVars &lt;- c(\n  \"pocket_time.pass\",\n  \"ybc_att.rush\",\"yac_att.rush\",\n  \"ybc_r.rec\",\"yac_r.rec\",\"adot.rec\",\"rat.rec\",\n  \"yds_cmp.def\",\"yds_tgt.def\",\"dadot.def\",\"m_tkl_percent.def\",\"rat.def\"\n)\n\nrecomputeVars &lt;- c(\n  \"drop_pct.pass\", # drops.pass / pass_attempts.pass\n  \"bad_throw_pct.pass\", # bad_throws.pass / pass_attempts.pass\n  \"on_tgt_pct.pass\", # on_tgt_throws.pass / pass_attempts.pass\n  \"pressure_pct.pass\", # times_pressured.pass / pass_attempts.pass\n  \"drop_percent.rec\", # drop.rec / tgt.rec\n  \"rec_br.rec\", # rec.rec / brk_tkl.rec\n  \"cmp_percent.def\" # cmp.def / tgt.def\n)\n\nsumVars &lt;- pfrVars[pfrVars %ni% c(\n  weightedAverageVars, recomputeVars,\n  \"merge_name\", \"loaded.pass\", \"loaded.rush\", \"loaded.rec\", \"loaded.def\")]\n\nnfl_advancedStatsPFR_career &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  group_by(pfr_id, merge_name) %&gt;% \n  summarise(\n    across(all_of(weightedAverageVars), ~ weighted.mean(.x, w = g, na.rm = TRUE)),\n    across(all_of(sumVars), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\") %&gt;% \n  mutate(\n    drop_pct.pass = drops.pass / pass_attempts.pass,\n    bad_throw_pct.pass = bad_throws.pass / pass_attempts.pass,\n    on_tgt_pct.pass = on_tgt_throws.pass / pass_attempts.pass,\n    pressure_pct.pass = times_pressured.pass / pass_attempts.pass,\n    drop_percent.rec = drop.rec / tgt.rec,\n    rec_br.rec = drop.rec / tgt.rec,\n    cmp_percent.def = cmp.def / tgt.def\n  )\n\nuniqueCases &lt;- nfl_advancedStatsPFR_seasonal %&gt;% select(pfr_id, merge_name, gsis_id) %&gt;% unique()\n\nuniqueCases %&gt;%\n  group_by(pfr_id) %&gt;% \n  filter(n() &gt; 1)\n\n\n  \n\n\nCodenfl_advancedStatsPFR_seasonal &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  filter(pfr_id != \"WillMa06\" | merge_name != \"MARCUSWILLIAMS\" | !is.na(gsis_id))\n\n\nnfl_advancedStatsPFR_career &lt;- left_join(\n  nfl_advancedStatsPFR_career,\n  nfl_advancedStatsPFR_seasonal %&gt;% select(pfr_id, merge_name, gsis_id) %&gt;% unique(),\n  by = c(\"pfr_id\", \"merge_name\")\n)\n\n# Compute Player Stats Per Season\nplayer_stats_seasonal_careerWRs &lt;- player_stats_seasonal %&gt;% \n  filter(position == \"WR\") %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(\n    across(all_of(c(\"targets\", \"receptions\", \"receiving_air_yards\")), ~ weighted.mean(.x, w = games, na.rm = TRUE)),\n    .groups = \"drop\")\n\n# Drop players with no receiving air yards\nplayer_stats_seasonal_careerWRs &lt;- player_stats_seasonal_careerWRs %&gt;% \n  filter(receiving_air_yards != 0) %&gt;% \n  rename(\n    targets_per_season = targets,\n    receptions_per_season = receptions,\n    receiving_air_yards_per_season = receiving_air_yards\n  )\n\n# Merge\nplayerListToMerge &lt;- list(\n  nfl_players %&gt;% select(gsis_id, display_name, position, height, weight),\n  nfl_combine %&gt;% select(gsis_id, vertical, forty, ht, wt),\n  player_stats_seasonal_careerWRs %&gt;% select(player_id, targets_per_season, receptions_per_season, receiving_air_yards_per_season) %&gt;% \n    rename(gsis_id = player_id),\n  nfl_actualStats_offense_career %&gt;% select(player_id, receptions, targets, receiving_air_yards, air_yards_share, target_share) %&gt;% \n    rename(gsis_id = player_id),\n  nfl_advancedStatsPFR_career %&gt;% select(gsis_id, adot.rec, rec.rec, brk_tkl.rec, drop.rec, drop_percent.rec)\n)\n\nmerged_data &lt;- playerListToMerge %&gt;% \n  reduce(\n    full_join,\n    by = c(\"gsis_id\"),\n    na_matches = \"never\")\n\n\nAdditional processing:\n\nCodemerged_data &lt;- merged_data %&gt;% \n  mutate(\n    height_coalesced = coalesce(height, ht),\n    weight_coalesced = coalesce(weight, wt),\n    receptions_coalesced = pmax(receptions, rec.rec, na.rm = TRUE),\n    receiving_air_yards_per_rec = receiving_air_yards / receptions\n  )\n\nmerged_data$receiving_air_yards_per_rec[which(merged_data$receptions == 0)] &lt;- 0\n\nmerged_dataWRs &lt;- merged_data %&gt;% \n  filter(position == \"WR\")\n\nmerged_dataWRs_cluster &lt;- merged_dataWRs %&gt;% \n  filter(receptions_coalesced &gt;= 100) %&gt;% # keep WRs with at least 100 receptions\n  select(gsis_id, display_name, vertical, forty, height_coalesced, weight_coalesced, adot.rec, drop_percent.rec, receiving_air_yards_per_rec, brk_tkl.rec, receptions_per_season) %&gt;% #targets_per_season, receiving_air_yards_per_season, air_yards_share, target_share\n  na.omit()\n\n\n\n20.1.5.1 Identify the Number of WR Types\n\nCodewrTypes_bic &lt;- mclust::mclustBIC(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  G = 1:9\n)\n\nwrTypes_bic\n\nBayesian Information Criterion (BIC): \n        EII       VII       EEI       VEI       EVI       VVI       EEE\n1 -7482.735 -7482.735 -4561.469 -4561.469 -4561.469 -4561.469 -4424.918\n2 -7069.465 -7060.309 -4545.235 -4547.365 -4423.586 -4418.226 -4450.498\n3 -6984.351 -6950.264 -4503.058 -4506.280 -4406.444 -4430.201 -4447.640\n4 -6907.932 -6912.275 -4474.819 -4463.312 -4421.581 -4418.622 -4426.244\n5 -6888.135 -6835.162 -4502.738 -4471.429 -4465.251 -4436.841 -4442.256\n6 -6786.346 -6804.813 -4500.716 -4487.499        NA -4507.223 -4415.348\n7 -6774.977 -6795.726 -4525.136 -4508.300 -4551.689 -4531.304 -4466.937\n8 -6801.211        NA -4507.213        NA        NA        NA -4389.612\n9 -6795.108        NA -4501.406        NA        NA        NA -4419.689\n        VEE       EVE       VVE       EEV       VEV       EVV       VVV\n1 -4424.918 -4424.918 -4424.918 -4424.918 -4424.918 -4424.918 -4424.918\n2 -4426.622 -4202.118 -4221.758 -4497.511 -4524.723 -4267.603 -4522.266\n3 -4439.796 -4184.023 -4188.497 -4586.325 -4634.248 -4378.666 -4618.591\n4 -4365.534 -4228.019 -4245.737 -4461.915 -4517.861 -4600.034 -4602.499\n5 -4378.848 -4284.730 -4283.048 -4649.401 -4694.084 -4752.524 -4783.750\n6        NA        NA        NA -4790.647 -4792.252        NA        NA\n7        NA        NA        NA -4863.861 -4938.251        NA        NA\n8        NA        NA        NA -5136.433        NA        NA        NA\n9        NA        NA        NA -5285.508        NA        NA        NA\n\nTop 3 models based on the BIC criterion: \n    EVE,3     VVE,3     EVE,2 \n-4184.023 -4188.497 -4202.118 \n\nCodesummary(wrTypes_bic)\n\nBest BIC values:\n             EVE,3        VVE,3       EVE,2\nBIC      -4184.023 -4188.496769 -4202.11831\nBIC diff     0.000    -4.474093   -18.09563\n\nCodeplot(wrTypes_bic)\n\n\n\n\n\n\nCodewrTypes_icl &lt;- mclust::mclustICL(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  G = 1:9\n)\n\nwrTypes_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n        EII       VII       EEI       VEI       EVI       VVI       EEE\n1 -7482.735 -7482.735 -4561.469 -4561.469 -4561.469 -4561.469 -4424.918\n2 -7074.102 -7064.913 -4565.234 -4560.073 -4436.147 -4429.725 -4456.924\n3 -6996.209 -6955.518 -4524.137 -4521.743 -4424.851 -4445.911 -4456.676\n4 -6916.756 -6924.213 -4493.582 -4479.353 -4436.919 -4435.650 -4434.176\n5 -6900.974 -6845.971 -4522.845 -4483.312 -4486.557 -4452.154 -4451.323\n6 -6795.561 -6821.553 -4520.176 -4504.718        NA -4524.248 -4422.690\n7 -6784.729 -6803.342 -4547.771 -4524.232 -4568.733 -4544.326 -4480.555\n8 -6809.503        NA -4531.893        NA        NA        NA -4406.718\n9 -6806.244        NA -4525.086        NA        NA        NA -4433.442\n        VEE       EVE       VVE       EEV       VEV       EVV       VVV\n1 -4424.918 -4424.918 -4424.918 -4424.918 -4424.918 -4424.918 -4424.918\n2 -4436.890 -4204.376 -4228.829 -4498.894 -4528.079 -4269.452 -4526.382\n3 -4450.955 -4198.344 -4195.063 -4588.533 -4637.602 -4380.088 -4619.783\n4 -4371.217 -4238.253 -4255.601 -4467.263 -4520.560 -4604.718 -4605.003\n5 -4385.455 -4294.120 -4288.610 -4652.299 -4694.999 -4756.338 -4785.120\n6        NA        NA        NA -4791.628 -4793.263        NA        NA\n7        NA        NA        NA -4864.065 -4939.017        NA        NA\n8        NA        NA        NA -5136.887        NA        NA        NA\n9        NA        NA        NA -5286.423        NA        NA        NA\n\nTop 3 models based on the ICL criterion: \n    VVE,3     EVE,3     EVE,2 \n-4195.063 -4198.344 -4204.376 \n\nCodesummary(wrTypes_icl)\n\nBest ICL values:\n             VVE,3        EVE,3        EVE,2\nICL      -4195.063 -4198.343937 -4204.375594\nICL diff     0.000    -3.280838    -9.312495\n\nCodeplot(wrTypes_icl)\n\n\n\n\n\n\n\nBased on the cluster analyses, it appears that three clusters are the best fit to the data.\n\nCodenumTypesWR &lt;- 3\n\n\n\nCodewrTypes_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  modelName = \"EVE\") # ellipsoidal with equal volume, variable shape, and equal orientation (for multivariate data)\n\nwrTypes_boostrap\nplot(\n  wrTypes_boostrap,\n  G = numTypesWR - 1)\n\n\n\n20.1.5.2 Fit the Cluster Model to the Optimal Number of WR Types\n\nCodeclusterModelWRtypes &lt;- mclust::Mclust(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  G = numTypesWR,\n)\n\nsummary(clusterModelWRtypes)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust EVE (ellipsoidal, equal volume and orientation) model with 3 components: \n\n log-likelihood   n df       BIC       ICL\n      -1879.679 112 90 -4184.023 -4198.344\n\nClustering table:\n 1  2  3 \n33 15 64 \n\n\n\n20.1.5.3 Plots of the Cluster Model\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"BIC\")\n\n\n\n\n\n\n\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"classification\")\n\n\n\n\n\n\n\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"uncertainty\")\n\n\n\n\n\n\n\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"density\")\n\n\n\n\n\n\n\n\n20.1.5.4 Interpreting the Clusters\n\nCodetable(clusterModelWRtypes$classification)\n\n\n 1  2  3 \n33 15 64 \n\nCodemerged_dataWRs_cluster$type &lt;- clusterModelWRtypes$classification\n\nmerged_dataWRs_cluster %&gt;% \n  group_by(type) %&gt;% \n  summarise(across(\n    where(is.numeric),\n    ~ mean(., na.rm = TRUE)\n    )) %&gt;% \n  t() %&gt;% \n  round(., 2)\n\n                              [,1]   [,2]   [,3]\ntype                          1.00   2.00   3.00\nvertical                     36.29  36.57  36.12\nforty                         4.48   4.46   4.46\nheight_coalesced             73.24  73.07  72.58\nweight_coalesced            207.39 206.53 198.12\nadot.rec                     10.20  12.27  10.68\ndrop_percent.rec              0.04   0.07   0.05\nreceiving_air_yards_per_rec  16.04  22.85  17.92\nbrk_tkl.rec                  23.42   0.40   7.53\nreceptions_per_season        77.39  41.29  43.97\n\n\nBased on this analysis (and the variables included), there appear to be three types of Wide Receivers. Type 1 Wide Receivers includes the Elite WR1s who are strong possession receivers (note: not all players in a given cluster map on perfectly to the typology—i.e., not all Type 1 Wide Receivers are elite WR1s). They tend to have the lowest drop percentage, the shortest average depth of target, and the fewest receiving air yards per reception. They tend to have the most receptions per season and break the most tackles.\nType 2 Wide Receivers includes the consistent contributor, WR2 types. They had fewer receptions and fewer broken tackles than Type 1 Wide Receivers. Their average depth of target was longer than Type 1, and they had more receiving air yards per reception than Type 1.\nType 3 Wide Receivers includes the deep threats. They have the greatest average depth of target and the most receiving yards per reception. However, they also have the fewest receptions, the highest drop percentage, and the fewest broken tackles. Thus, they may be considered the boom-or-bust Wide Receivers.\nThe tiers were not particularly distinguishable based on their height, weight, vertical jump, or forty-yard dash time.\nType 1 (“Elite/WR1”) WRs:\n\nCodemerged_dataWRs_cluster %&gt;% \n  filter(type == 1) %&gt;% \n  select(display_name)\n\n\n  \n\n\n\nType 2 (“Consistent Contributor/WR2”) WRs:\n\nCodemerged_dataWRs_cluster %&gt;% \n  filter(type == 2) %&gt;% \n  select(display_name)\n\n\n  \n\n\n\nType 3 (“Deep Threat/Boom-or-Bust”) WRs:\n\nCodemerged_dataWRs_cluster %&gt;% \n  filter(type == 3) %&gt;% \n  select(display_name)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#sec-clusterAnalysisConclusion",
    "href": "cluster-analysis.html#sec-clusterAnalysisConclusion",
    "title": "20  Cluster Analysis",
    "section": "\n20.2 Conclusion",
    "text": "20.2 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#sec-clusterAnalysisSessionInfo",
    "href": "cluster-analysis.html#sec-clusterAnalysisSessionInfo",
    "title": "20  Cluster Analysis",
    "section": "\n20.3 Session Info",
    "text": "20.3 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] tidyverse_2.0.0   plotly_4.10.4     ggplot2_3.5.1     mclust_6.1.1     \n[13] nflreadr_1.4.1    petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       xfun_0.51          htmlwidgets_1.6.4  psych_2.5.3       \n [5] lattice_0.22-6     tzdb_0.5.0         crosstalk_1.2.1    quadprog_1.5-8    \n [9] vctrs_0.6.5        tools_4.4.3        generics_0.1.3     stats4_4.4.3      \n[13] parallel_4.4.3     cluster_2.1.8      pkgconfig_2.0.3    data.table_1.17.0 \n[17] checkmate_2.3.2    RColorBrewer_1.1-3 lifecycle_1.0.4    farver_2.1.2      \n[21] compiler_4.4.3     munsell_0.5.1      mnormt_2.1.1       mitools_2.4       \n[25] htmltools_0.5.8.1  lazyeval_0.2.2     yaml_2.3.10        htmlTable_2.4.3   \n[29] Formula_1.2-5      pillar_1.10.1      cachem_1.1.0       Hmisc_5.2-3       \n[33] rpart_4.1.24       nlme_3.1-167       lavaan_0.6-19      tidyselect_1.2.1  \n[37] digest_0.6.37      mvtnorm_1.3-3      stringi_1.8.4      reshape2_1.4.4    \n[41] labeling_0.4.3     fastmap_1.2.0      grid_4.4.3         colorspace_2.1-1  \n[45] cli_3.6.4          magrittr_2.0.3     base64enc_0.1-3    pbivnorm_0.6.0    \n[49] foreign_0.8-88     withr_3.0.2        scales_1.3.0       backports_1.5.0   \n[53] timechange_0.3.0   httr_1.4.7         rmarkdown_2.29     nnet_7.3-20       \n[57] gridExtra_2.3      hms_1.1.3          memoise_2.0.1      evaluate_1.0.3    \n[61] knitr_1.50         mix_1.0-13         viridisLite_0.4.2  rlang_1.1.5       \n[65] Rcpp_1.0.14        xtable_1.8-4       glue_1.8.0         DBI_1.2.3         \n[69] rstudioapi_0.17.1  jsonlite_1.9.1     R6_2.6.1           plyr_1.8.9        \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFraley, C., Raftery, A. E., & Scrucca, L. (2024). mclust: Gaussian mixture modelling for model-based clustering, classification, and density estimation. https://mclust-org.github.io/mclust/\n\n\nKassambara, A. (2017). Practical guide to cluster analysis in R: Unsupervised machine learning (Vol. 1). Sthda.\n\n\nScrucca, L., Fraley, C., Murphy, T. B., & Raftery, A. E. (2023). Model-based clustering, classification, and density estimation using mclust in R. Chapman; Hall/CRC. https://doi.org/10.1201/9781003277965",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html",
    "href": "factor-analysis.html",
    "title": "21  Factor Analysis",
    "section": "",
    "text": "21.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisGettingStarted",
    "href": "factor-analysis.html#sec-factorAnalysisGettingStarted",
    "title": "21  Factor Analysis",
    "section": "",
    "text": "21.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisOverview",
    "href": "factor-analysis.html#sec-factorAnalysisOverview",
    "title": "21  Factor Analysis",
    "section": "\n21.2 Overview of Factor Analysis",
    "text": "21.2 Overview of Factor Analysis\nFactor analysis involves the estimation of latent variables. Latent variables are ways of studying and operationalizing theoretical constructs that cannot be directly observed or quantified. Factor analysis is a class of latent variable models that is designated to identify the structure of a measure or set of measures, and ideally, a construct or set of constructs. It aims to identify the optimal latent structure for a group of variables. The goal of factor analysis is to identify simple, parsimonious factors that underlie the “junk” (i.e., scores filled with measurement error) that we observe.\nFactor analysis encompasses two general types: confirmatory factor analysis and exploratory factor analysis. Exploratory factor analysis (EFA) is a latent variable modeling approach that is used when the researcher has no a priori hypotheses about how a set of variables is structured. EFA seeks to identify the empirically optimal-fitting model in ways that balance accuracy (i.e., variance accounted for) and parsimony (i.e., simplicity). Confirmatory factor analysis (CFA) is a latent variable modeling approach that is used when a researcher wants to evaluate how well a hypothesized model fits, and the model can be examined in comparison to alternative models. Using a CFA approach, the researcher can pit models representing two theoretical frameworks against each other to see which better accounts for the observed data.\nFactor analysis involves observed (manifest) variables and unobserved (latent) factors. Factor analysis assumes that the latent factor influences the manifest variables, and the latent factor therefore reflects the common variance among the variables. A factor model potentially includes factor loadings, residuals (errors or disturbances), intercepts/means, covariances, and regression paths. When depicting a factor analysis model, rectangles represent variables we observe (i.e., manifest variables), and circles represent latent (i.e., unobserved) variables. A regression path indicates a hypothesis that one variable (or factor) influences another, and it is depicted using a single-headed arrow. The standardized regression coefficient (i.e., beta or \\(\\beta\\)) represents the strength of association between the variables or factors. A factor loading is a regression path from a latent factor to an observed (manifest) variable. The standardized factor loading represents the strength of association between the variable and the latent factor. A residual is variance in a variable (or factor) that is unexplained by other variables or factors. A variable’s intercept is the expected value of the variable when the factor(s) (onto which it loads) is equal to zero. A covariance is the unstandardized index of the strength of association between between variables (or factors), and it is depicted with a double-headed arrow. Because a covariance is unstandardized, its scale depends on the scale of the variables. A covariance path between two variables represents omitted shared cause(s) of the variables. For instance, if you depict a covariance path between two variables, it means that there is a shared cause of the two variables that is omitted from the model (for instance, if the common cause is not known or was not assessed).\nIn factor analysis, the relation between an indicator (\\(\\text{X}\\)) and its underlying latent factor(s) (\\(\\text{F}\\)) can be represented with a regression formula as in Equation 21.1:\n\\[\n\\bar{X} = \\lambda \\cdot \\text{F} + \\text{Item Intercept} + \\text{Error Term}\n\\tag{21.1}\\]\nwhere:\n\n\n\\(\\text{X}\\) is the observed value of the indicator\n\n\\(\\lambda\\) is the factor loading, indicating the strength of the association between the indicator and the latent factor(s)\n\n\\(\\text{F}\\) is the person’s value on the latent factor(s)\n\n\\(\\text{Item Intercept}\\) represents the constant term that accounts for the expected value of the indicator when the latent factor(s) are zero\n\n\\(\\text{Error Term}\\) is the residual, indicating the extent of variance in the indicator that is not explained by the latent factor(s)\n\nWhen the latent factors are uncorrelated, the (standardized) error term for an indicator is calculated as 1 minus the sum of squared standardized factor loadings for a given item (including cross-loadings). A cross-loadings is when a variable loads onto more than one latent factor.\nFactor analysis is a powerful technique to help identify the factor structure that underlies a measure or construct. However, given the extensive method variance that influences scores on measure, factor analysis (and principal component analysis) tends to extract method factors. Method factors are factors that are related to the methods being assessed rather than the construct of interest. To better estimate construct factors, it is sometimes necessary to estimate both construct and method factors.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-efa",
    "href": "factor-analysis.html#sec-efa",
    "title": "21  Factor Analysis",
    "section": "\n21.3 Exploratory Factor Analysis",
    "text": "21.3 Exploratory Factor Analysis\nExploratory factor analysis (EFA) is used if you have no a priori hypotheses about the factor structure of the model, but you would like to understand the latent variables represented by your items.\nEFA is partly induced from the data. You feed in the data and let the program build the factor model. You can set some parameters going in, including how to extract or rotate the factors. The factors are extracted from the data without specifying the number and pattern of loadings between the items and the latent factors (Bollen, 2002). All cross-loadings are freely estimated.\n\n21.3.1 Factor Rotation\nWhen using EFA or principal component analysis, an important step is, possibly, to rotate the factors to make them more interpretable and simple, which is the whole goal. To interpret the results of a factor analysis, we examine the factor matrix. The columns refer to the different factors; the rows refer to the different observed variables. The cells in the table are the factor loadings—they are basically the correlation between the variable and the factor. Our goal is to achieve a model with simple structure because it is easily interpretable. Simple structure means that every variable loads perfectly on one and only one factor, as operationalized by a matrix of factor loadings with values of one and zero and nothing else. An example of a factor matrix that follows simple structure is depicted in Figure 21.1.\n\n\n\n\n\nFigure 21.1: Example of a Factor Matrix That Follows Simple Structure.\n\n\nAn example of a factor analysis model that follows simple structure is depicted in Figure 21.2. Each variable loads onto one and only one factor, which makes it easy to interpret the meaning of each factor, because a given factor represents the common variance among the items that load onto it.\n\n\n\n\n\nFigure 21.2: Example of a Factor Analysis Model That Follows Simple Structure. ‘INT’ = internalizing problems; ‘EXT’ = externalizing problems; ‘TD’ = thought-disordered problems.\n\n\nHowever, pure simple structure only occurs in simulations, not in real-life data. In reality, our unrotated factor analysis model might look like the model in Figure 21.3. In this example, the factor analysis model does not show simple structure because the items have cross-loadings—that is, the items load onto more than one factor. The cross-loadings make it difficult to interpret the factors, because all of the items load onto all of the factors, so the factors are not very distinct from each other, which makes it difficult to interpret what the factors mean.\n\n\n\n\n\nFigure 21.3: Example of a Factor Analysis Model That Does Not Follow Simple Structure. ‘INT’ = internalizing problems; ‘EXT’ = externalizing problems; ‘TD’ = thought-disordered problems.\n\n\nAs a result of the challenges of interpretability caused by cross-loadings, factor rotations are often performed. An example of an unrotated factor matrix is in Figure 21.4.\n\n\n\n\n\nFigure 21.4: Example of a Factor Matrix.\n\n\nIn the example factor matrix in Figure 21.5, the factor analysis is not very helpful—it tells us very little because it did not distinguish between the two factors. The variables have similar loadings on Factor 1 and Factor 2. An example of a unrotated factor solution is in Figure 21.5. In the figure, all of the variables are in the midst of the quadrants—they are not on the factors’ axes. Thus, the factors are not very informative.\n\n\n\n\n\nFigure 21.5: Example of an Unrotated Factor Solution.\n\n\nAs a result, to improve the interpretability of the factor analysis, we can do what is called rotation. Rotation leverages the idea that there are infinite solutions to the factor analysis model that fit equally well. Rotation involves changing the orientation of the factors by changing the axes so that variables end up with very high (close to one or negative one) or very low (close to zero) loadings, so that it is clear which factors include which variables. That is, rotation rescales the factors and tries to identify the ideal solution (factor) for each variable. It searches for simple structure and keeps searching until it finds a minimum. After rotation, if the rotation was successful for imposing simple structure, each factor will have loadings close to one (or negative one) for some variables and close to zero for other variables. The goal of factor rotation is to achieve simple structure, to help make it easier to interpret the meaning of the factors.\nTo perform factor rotation, orthogonal rotations are often used. Orthogonal rotations make the rotated factors uncorrelated. An example of a commonly used orthogonal rotation is varimax rotation. Varimax rotation maximizes the sum of the variance of the squared loadings (i.e., so that items have either a very high or very low loading on a factor) and yields axes with a 90-degree angle.\nAn example of a factor matrix following an orthogonal rotation is depicted in Figure 21.6. An example of a factor solution following an orthogonal rotation is depicted in Figure 21.7.\n\n\n\n\n\nFigure 21.6: Example of a Rotated Factor Matrix.\n\n\n\n\n\n\n\nFigure 21.7: Example of a Rotated Factor Solution.\n\n\nAn example of a factor matrix from SPSS following an orthogonal rotation is depicted in Figure 21.8.\n\n\n\n\n\nFigure 21.8: Example of a Rotated Factor Matrix From SPSS.\n\n\nAn example of a factor structure from an orthogonal rotation is in Figure 21.9.\n\n\n\n\n\nFigure 21.9: Example of a Factor Structure From an Orthogonal Rotation.\n\n\nSometimes, however, the two factors and their constituent variables may be correlated. Examples of two correlated factors may be depression and anxiety. When the two factors are correlated in reality, if we make them uncorrelated, this would result in an inaccurate model. Oblique rotation allows for factors to be correlated and yields axes with less an angle of less than 90 degrees. However, if the factors have low correlation (e.g., .2 or less), you can likely continue with orthogonal rotation. Nevertheless, just because an oblique rotation allows for correlated factors does not mean that the factors will be correlated, so oblique rotation provides greater flexibility than orthogonal rotation. An example of a factor structure from an oblique rotation is in Figure 21.10. Results from an oblique rotation are more complicated than orthogonal rotation—they provide lots of output and are more complicated to interpret. In addition, oblique rotation might not yield a smooth answer if you have a relatively small sample size.\n\n\n\n\n\nFigure 21.10: Example of a Factor Structure From an Oblique Rotation.\n\n\nAs an example of rotation based on interpretability, consider the Five-Factor Model of Personality (the Big Five), which goes by the acronym, OCEAN: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Although the five factors of personality are somewhat correlated, we can use rotation to ensure they are maximally independent. Upon rotation, extraversion and neuroticism are essentially uncorrelated, as depicted in Figure 21.11. The other pole of extraversion is intraversion and the other pole of neuroticism might be emotional stability or calmness.\n\n\n\n\n\nFigure 21.11: Example of a Factor Rotation of Neuroticism and Extraversion.\n\n\nSimple structure is achieved when each variable loads highly onto as few factors as possible (i.e., each item has only one significant or primary loading). Oftentimes this is not the case, so we choose our rotation method in order to decide if the factors can be correlated (an oblique rotation) or if the factors will be uncorrelated (an orthogonal rotation). If the factors are not correlated with each other, use an orthogonal rotation. The correlation between an item and a factor is a factor loading, which is simply a way to ask how much a variable is correlated with the underlying factor. However, its interpretation is more complicated if there are correlated factors!\nAn orthogonal rotation (e.g., varimax) can help with simplicity of interpretation because it seeks to yield simple structure without cross-loadings. Cross-loadings are instances where a variable loads onto multiple factors. My recommendation would always be to use an orthogonal rotation if you have reason to believe that finding simple structure in your data is possible; otherwise, the factors are extremely difficult to interpret—what exactly does a cross-loading even mean? However, you should always try an oblique rotation, too, to see how strongly the factors are correlated. Examples of oblique rotations include oblimin and promax.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-cfa",
    "href": "factor-analysis.html#sec-cfa",
    "title": "21  Factor Analysis",
    "section": "\n21.4 Confirmatory Factor Analysis",
    "text": "21.4 Confirmatory Factor Analysis\nConfirmatory factor analysis (CFA) is used to (dis)confirm a priori hypotheses about the factor structure of the model. CFA is a test of the hypothesis. In CFA, you specify the model and ask how well this model represents the data. The researcher specifies the number, meaning, associations, and pattern of free parameters in the factor loading matrix (Bollen, 2002). A key advantage of CFA is the ability to directly compare alternative models (i.e., factor structures), which is valuable for theory testing (Strauss & Smith, 2009). For instance, you could use CFA to test whether the variance in several measures’ scores is best explained with one factor or two factors. In CFA, cross-loadings are not estimated unless the researcher specifies them.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisNumFactors",
    "href": "factor-analysis.html#sec-factorAnalysisNumFactors",
    "title": "21  Factor Analysis",
    "section": "\n21.5 Determining the Number of Factors to Retain",
    "text": "21.5 Determining the Number of Factors to Retain\nA goal of factor analysis and principal component analysis is simplification or parsimony, while still explaining as much variance as possible. The hope is that you can have fewer factors that explain the associations between the variables than the number of observed variables. But how do you decide on the number of factors?\nThere are a number of criteria that one can use to help determine how many factors/components to keep:\n\nKaiser-Guttman criterion: factors with eigenvalues greater than zero\n\nor, for principal component analysis, components with eigenvalues greater than 1\n\n\nCattell’s scree test: the “elbow” in a scree plot minus one; sometimes operationalized with optimal coordinates (OC) or the acceleration factor (AF)\nParallel analysis: factors that explain more variance than randomly simulated data\nVery simple structure (VSS) criterion: larger is better\nVelicer’s minimum average partial (MAP) test: smaller is better\nAkaike information criterion (AIC): smaller is better\nBayesian information criterion (BIC): smaller is better\nSample size-adjusted BIC (SABIC): smaller is better\nRoot mean square error of approximation (RMSEA): smaller is better\nChi-square difference test: smaller is better; a significant test indicates that the more complex model is significantly better fitting than the less complex model\nStandardized root mean square residual (SRMR): smaller is better\nComparative Fit Index (CFI): larger is better\nTucker Lewis Index (TLI): larger is better\n\nThere is not necessarily a “correct” criterion to use in determining how many factors to keep, so it is generally recommended that researchers use multiple criteria in combination with theory and interpretability.\nA scree plot provides lots of information. A scree plot has the factor number on the x-axis and the eigenvalue on the y-axis. The eigenvalue is the variance accounted for by a factor; when using a varimax (orthogonal) rotation, an eigenvalue (or factor variance) is calculated as the sum of squared standardized factor (or component) loadings on that factor. An example of a scree plot is in Figure 21.12.\n\n\n\n\n\nFigure 21.12: Example of a Scree Plot.\n\n\nThe total variance is equal to the number of variables you have, so one eigenvalue is approximately one variable’s worth of variance. The first factor accounts for the most variance, the second factor accounts for the second-most variance, and so on. The more factors you add, the less variance is explained by the additional factor.\nOne criterion for how many factors to keep is the Kaiser-Guttman criterion. According to the Kaiser-Guttman criterion, you should keep any factors whose eigenvalue is greater than 1. That is, for the sake of simplicity, parsimony, and data reduction, you should take any factors that explain more than a single variable would explain. According to the Kaiser-Guttman criterion, we would keep three factors from Figure 21.12 that have eigenvalues greater than 1. The default in SPSS is to retain factors with eigenvalues greater than 1. However, keeping factors whose eigenvalue is greater than 1 is not the most correct rule. If you let SPSS do this, you may get many factors with eigenvalues around 1 (e.g., factors with an eigenvalue ~ 1.0001) that are not adding so much that it is worth the added complexity. The Kaiser-Guttman criterion usually results in keeping too many factors. Factors with small eigenvalues around 1 could reflect error shared across variables. For instance, factors with small eigenvalues could reflect method variance (i.e., method factor), such as a self-report factor that turns up as a factor in factor analysis, but that may be useless to you as a conceptual factor of a construct of interest.\nAnother criterion is Cattell’s scree test, which involves selecting the number of factors from looking at the scree plot. “Scree” refers to the rubble of stones at the bottom of a mountain. According to Cattell’s scree test, you should keep the factors before the last steep drop in eigenvalues—i.e., the factors before the rubble, where the slope approaches zero. The beginning of the scree (or rubble), where the slope approaches zero, is called the “elbow” of a scree plot. Using Cattell’s scree test, you retain the number of factors that explain the most variance prior to the explained variance drop-off, because, ultimately, you want to include only as many factors in which you gain substantially more by the inclusion of these factors. That is, you would keep the number of factors at the elbow of the scree plot minus one. If the last steep drop occurs from Factor 4 to Factor 5 and the elbow is at Factor 5, we would keep four factors. In Figure 21.12, the last steep drop in eigenvalues occurs from Factor 3 to Factor 4; the elbow of the scree plot occurs at Factor 4. We would keep the number of factors at the elbow minus one. Thus, using Cattell’s scree test, we would keep three factors based on Figure 21.12.\nThere are more sophisticated ways of using a scree plot, but they usually end up at a similar decision. Examples of more sophisticated tests include parallel analysis and very simple structure (VSS) plots. In a parallel analysis, you examine where the eigenvalues from observed data and random data converge, so you do not retain a factor that explains less variance than would be expected by random chance.\nIn general, my recommendation is to use Cattell’s scree test, and then test the factor solutions with plus or minus one factor. You should never accept factors with eigenvalues less than zero (or components from principal component analysis with eigenvalues less than one), because they are likely to be largely composed of error. If you are using maximum likelihood factor analysis, you can compare the fit of various models with model fit criteria to see which model fits best for its parsimony. A model will always fit better when you add additional parameters or factors, so you examine if there is significant improvement in model fit when adding the additional factor—that is, we keep adding complexity until additional complexity does not buy us much. Always try a factor solution that is one less and one more than suggested by Cattell’s scree test to buffer your final solution because the purpose of factor analysis is to explain things and to have interpretability. Even if all rules or indicators suggest to keep X number of factors, maybe \\(\\pm\\) one factor helps clarify things. Even though factor analysis is empirical, theory and interpretatability should also inform decisions.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-interpretingAndUsingLatentFactors",
    "href": "factor-analysis.html#sec-interpretingAndUsingLatentFactors",
    "title": "21  Factor Analysis",
    "section": "\n21.6 Interpreting and Using Latent Factors",
    "text": "21.6 Interpreting and Using Latent Factors\nThe next step is interpreting the model and latent factors. One data matrix can lead to many different (correct) models—you must choose one based on the factor structure and theory. Use theory to interpret the model and label the factors.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisConclusion",
    "href": "factor-analysis.html#sec-factorAnalysisConclusion",
    "title": "21  Factor Analysis",
    "section": "\n21.7 Conclusion",
    "text": "21.7 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisSessionInfo",
    "href": "factor-analysis.html#sec-factorAnalysisSessionInfo",
    "title": "21  Factor Analysis",
    "section": "\n21.8 Session Info",
    "text": "21.8 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.3    fastmap_1.2.0     cli_3.6.4        \n [5] tools_4.4.3       htmltools_0.5.8.1 rmarkdown_2.29    knitr_1.50       \n [9] jsonlite_1.9.1    xfun_0.51         digest_0.6.37     rlang_1.1.5      \n[13] evaluate_1.0.3   \n\n\n\n\n\n\nBollen, K. A. (2002). Latent variables in psychology and the social sciences. Annual Review of Psychology, 53(1), 605–634. https://doi.org/10.1146/annurev.psych.53.100901.135239\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nStrauss, M. E., & Smith, G. T. (2009). Construct validity: Advances in theory and methodology. Annual Review of Clinical Psychology, 5(1), 1–25. https://doi.org/10.1146/annurev.clinpsy.032408.153639",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "",
    "text": "22.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaGettingStarted",
    "href": "pca.html#sec-pcaGettingStarted",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "",
    "text": "22.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaOverview",
    "href": "pca.html#sec-pcaOverview",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.2 Overview of Principal Component Analysis",
    "text": "22.2 Overview of Principal Component Analysis\nPrincipal component analysis (PCA) is used if you want to reduce your data matrix. PCA composites represent the variances of an observed measure in as economical a fashion as possible, with no latent underlying variables. The goal of PCA is to identify a smaller number of components that explain as much variance in a set of variables as possible. It is an atheoretical way to decompose a matrix. PCA involves decomposition of a data matrix into a set of eigenvectors, which are transformations of the old variables.\nThe eigenvectors attempt to simplify the data in the matrix. PCA takes the data matrix and identifies the weighted sum of all variables that does the best job at explaining variance: these are the principal components, also called eigenvectors. Principal components reflect optimally weighted sums.\nPCA decomposes the data matrix into any number of components—as many as the number of variables, which will always account for all variance. After the PCA is performed, you can look at the results and discard the components which likely reflect error variance. Judgments about which components to retain are based on empirical criteria in conjunction with theory to select a parsimonious number of components that account for the majority of variance.\nThe eigenvalue reflects the amount of variance explained by the component (eigenvector). When using a varimax (orthogonal) rotation, an eigenvalue for a component is calculated as the sum of squared standardized component loadings on that component. When using oblique rotation, however, the items explain more variance than is attributable to their factor loadings because the factors are correlated.\nPCA pulls the first principal component out (i.e., the eigenvector that explains the most variance) and makes a new data matrix: i.e., new correlation matrix. Then the PCA pulls out the component that explains the next most variance—i.e., the eigenvector with the next largest eigenvalue, and it does this for all components, equal to the same number of variables. For instance, if there are six variables, it will iteratively extract an additional component up to six components. You can extract as many eigenvectors as there are variables. If you extract all six components, the data matrix left over will be the same as the correlation matrix in Figure 22.1. That is, the remaining variables (as part of the leftover data matrix) will be entirely uncorrelated with each other, because six components explain 100% of the variance from six variables. In other words, you can explain (6) variables with (6) new things!\n\n\n\n\n\nFigure 22.1: Example Correlation Matrix 2.\n\n\nHowever, it does no good if you have to use all (6) components because there is no data reduction from the original number of variables. When the goal is data reduction (as in PCA), the hope is that the first few components will explain most of the variance, so we can explain the variability in the data with fewer components than there are variables.\nThe sum of all eigenvalues is equal to the number of variables in the analysis. PCA does not have the same assumptions as factor analysis, which assumes that measures are partly from common variance and error. But if you estimate (6) eigenvectors and only keep (2), the model is a two-component model and whatever left becomes error. Therefore, PCA does not have the same assumptions as factor analysis, but it often ends up in the same place.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaStandardize",
    "href": "pca.html#sec-pcaStandardize",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.3 Ensuring the Variables are on the Same Scale",
    "text": "22.3 Ensuring the Variables are on the Same Scale\nBefore performing a PCA, it is important to ensure that the variables included in the PCA are on the same scale. PCA seeks to identify components that explain variance in the data, so if the variables are not on the same scale, some variables may contribute considerably more variance than others. A common way of ensuring that variables are on the same scale is to standardize them using, for example, z-scores.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaRotation",
    "href": "pca.html#sec-pcaRotation",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.4 Component Rotation",
    "text": "22.4 Component Rotation\nSimilar considerations as in factor analysis can be used to determine whether and how to rotate components in PCA. The considerations for determining whether and how to rotate factors in factor analysis are described in Section 21.3.1.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaNumComponents",
    "href": "pca.html#sec-pcaNumComponents",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.5 Determining the Number of Components to Retain",
    "text": "22.5 Determining the Number of Components to Retain\nSimilar criteria as in factor analysis can be used to determine the number of components to retain in PCA. The criteria for determining the number of factors to retain in factor analysis are described in Section 21.5.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-interpretingAndUsingComponentsPCA",
    "href": "pca.html#sec-interpretingAndUsingComponentsPCA",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.6 Interpreting and Using PCA Components",
    "text": "22.6 Interpreting and Using PCA Components\nThe next step is interpreting the PCA components. Use theory to interpret and label the components.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaVsFactorAnalysis",
    "href": "pca.html#sec-pcaVsFactorAnalysis",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.7 PCA Versus Factor Analysis",
    "text": "22.7 PCA Versus Factor Analysis\nBoth factor analysis and PCA can be used for data reduction. The key distinction between factor analysis and PCA is depicted in Figure 22.2.\n\n\n\n\n\nFigure 22.2: Distinction Between Factor Analysis and Principal Component Analysis.\n\n\nThere are several differences between factor analysis and PCA. Factor analysis has greater sophistication than PCA, but greater sophistication often results in greater assumptions. Factor analysis does not always work; the data may not always fit to a factor analysis model. However, PCA can decompose any data matrix; it always works. PCA is okay if you are not interested in the factor structure. PCA uses all variance of variables and assumes variables have no error, so it does not account for measurement error. PCA is good if you just want to form a linear composite and perform data reduction. However, if you are interested in the factor structure, use factor analysis, which estimates a latent variable that accounts for the common variance and discards error variance. Factor analysis better handles error than PCA—factor analysis assumes that what is in the variable is the combination of common construct variance and error. By contrast, PCA assumes that the measures have no measurement error. Factor analysis is useful for the identification of latent constructs—i.e., underlying dimensions or factors that explain (cause) observed scores.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaConclusion",
    "href": "pca.html#sec-pcaConclusion",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.8 Conclusion",
    "text": "22.8 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaSessionInfo",
    "href": "pca.html#sec-pcaSessionInfo",
    "title": "22  Data Reduction: Principal Component Analysis",
    "section": "\n22.9 Session Info",
    "text": "22.9 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.3    fastmap_1.2.0     cli_3.6.4        \n [5] tools_4.4.3       htmltools_0.5.8.1 rmarkdown_2.29    knitr_1.50       \n [9] jsonlite_1.9.1    xfun_0.51         digest_0.6.37     rlang_1.1.5      \n[13] evaluate_1.0.3   \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html",
    "href": "time-series-analysis.html",
    "title": "23  Time Series Analysis",
    "section": "",
    "text": "23.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesGettingStarted",
    "href": "time-series-analysis.html#sec-timeSeriesGettingStarted",
    "title": "23  Time Series Analysis",
    "section": "",
    "text": "23.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"xts\")\nlibrary(\"zoo\")\nlibrary(\"forecast\")\nlibrary(\"brms\")\nlibrary(\"rstan\")\nlibrary(\"plotly\")\nlibrary(\"tidyverse\")\n\n\n\n23.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nThe following code loads the Bayesian model object that was fit in Section 12.3.5.\n\nCodeload(url(\"https://osf.io/download/q6rjf/\")) # Bayesian model object",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesOverview",
    "href": "time-series-analysis.html#sec-timeSeriesOverview",
    "title": "23  Time Series Analysis",
    "section": "\n23.2 Overview of Time Series Analysis",
    "text": "23.2 Overview of Time Series Analysis\nTime series analysis is useful when trying to generate forecasts from longitudinal data. That is, time series analysis seeks to evaluate change over time to predict future values.\nThere many different types of time series analyses. For simplicity, in this chapter, we use autoregressive integrated moving average (ARIMA) models to demonstrate one approach to time series analysis. We also leverage Bayesian mixed models to generate forecasts of future performance and plots of individuals model-implied performance by age and position.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesARIMA",
    "href": "time-series-analysis.html#sec-timeSeriesARIMA",
    "title": "23  Time Series Analysis",
    "section": "\n23.3 Autoregressive Integrated Moving Average (ARIMA) Models",
    "text": "23.3 Autoregressive Integrated Moving Average (ARIMA) Models\nHyndman & Athanasopoulos (2021) provide a nice overview of ARIMA models. As noted by Hyndman & Athanasopoulos (2021), ARIMA models aim to describe how a variable is correlated with itself over time (autocorrelation)—i.e., how earlier levels of a variable are correlated with later levels of the same variable. ARIMA models perform best when there is a clear pattern where later values are influenced by earlier values. ARIMA models incorporate autoregression effects, moving average effects, and differencing.\nARIMA models can have various numbers of terms and model complexity. They are specified in the following form: \\(\\text{ARIMA}(p,d,q)\\), where:\n\n\n\\(p =\\) the number of autoregressive terms\n\n\\(d =\\) the number of differences between consecutive scores (to make the time series stationary by reducing trends and seasonality)\n\n\\(q =\\) the number of moving average terms\n\nARIMA models assume that the data are stationary (i.e., there are no long-term trends), are non-seasonal (i.e., there is no consistency of the timing of the peaks or troughs in the line), and that earlier values influence later values. This may not strongly be the case in fantasy football, so ARIMA models may not be particularly useful in forecasting fantasy football performance. Other approaches, such as exponential smoothing, may be useful for data that show longer-term trends and seasonality (Hyndman & Athanasopoulos, 2021). Nevertheless, ARIMA models are widely used in forecasting financial markets and economic indicators. Thus, it is a useful technique to learn.\nAdapted from: https://rc2e.com/timeseriesanalysis [Long & Teetor (2019); archived at https://perma.cc/U5P6-2VWC].\n\n23.3.1 Create the Time Series Objects\n\nCodeweeklyFantasyPoints_tomBrady &lt;- player_stats_weekly %&gt;% \n  filter(\n    player_id == \"00-0019596\" | player_display_name == \"Tom Brady\")\n\nweeklyFantasyPoints_peytonManning &lt;- player_stats_weekly %&gt;% \n  filter(\n    player_id == \"00-0010346\" | player_display_name == \"Peyton Manning\")\n\nts_tomBrady &lt;- xts::xts(\n  x = weeklyFantasyPoints_tomBrady[\"fantasyPoints\"],\n  order.by = weeklyFantasyPoints_tomBrady$gameday)\n\nts_peytonManning &lt;- xts::xts(\n  x = weeklyFantasyPoints_peytonManning[\"fantasyPoints\"],\n  order.by = weeklyFantasyPoints_peytonManning$gameday)\n\nts_tomBrady\n\n           fantasyPoints\n2000-11-23          0.24\n2001-09-23          2.74\n2001-09-30          6.92\n2001-10-07          4.34\n2001-10-14         22.56\n2001-10-21         19.88\n2001-10-28          6.02\n2001-11-04         22.00\n2001-11-11          7.18\n2001-11-18          7.00\n       ...              \n2022-10-27         17.10\n2022-11-06         15.20\n2022-11-13         16.02\n2022-11-27         18.04\n2022-12-05         16.14\n2022-12-11          8.12\n2022-12-18         18.58\n2022-12-25          9.34\n2023-01-01         37.68\n2023-01-08          7.36\n\nCodets_peytonManning\n\n           fantasyPoints\n1999-09-12         13.06\n1999-09-19         15.22\n1999-09-26         28.56\n1999-10-10         19.66\n1999-10-17          9.10\n1999-10-24         16.86\n1999-10-31         18.52\n1999-11-07         19.60\n1999-11-14         14.18\n1999-11-21         22.80\n       ...              \n2015-09-13          3.90\n2015-09-17         19.24\n2015-09-27         17.86\n2015-10-04          6.32\n2015-10-11          4.64\n2015-10-18          6.60\n2015-11-01         10.60\n2015-11-08         13.24\n2015-11-15        -10.60\n2016-01-03          2.56\n\nCodets_combined &lt;- merge(\n  ts_tomBrady,\n  ts_peytonManning\n)\n\nnames(ts_combined) &lt;- c(\"Tom Brady\",\"Peyton Manning\")\n\n\n\n23.3.2 Plot the Time Series\n\nCodeplot(\n  ts_tomBrady,\n  main = \"Tom Brady's Fantasy Points by Game\")\n\n\n\n\n\n\nFigure 23.1: Tom Brady’s Historical Fantasy Points by Game.\n\n\n\n\n\nCodeplot(\n  ts_combined,\n  legend,\n  legend.loc = \"topright\",\n  main = \"Fantasy Points by Game\")\n\n\n\n\n\n\nFigure 23.2: Historical Fantasy Points by Game for Tom Brady and Peyton Manning.\n\n\n\n\n\n23.3.3 Rolling Mean/Median\n\nCodezoo::rollmean(\n  x = ts_tomBrady,\n  k = 5)\n\n           fantasyPoints\n2001-09-30         7.360\n2001-10-07        11.288\n2001-10-14        11.944\n2001-10-21        14.960\n2001-10-28        15.528\n2001-11-04        12.416\n2001-11-11        13.984\n2001-11-18        14.484\n2001-11-25        10.568\n2001-12-02        10.888\n       ...              \n2022-10-16        18.332\n2022-10-23        15.892\n2022-10-27        15.148\n2022-11-06        16.012\n2022-11-13        16.500\n2022-11-27        14.704\n2022-12-05        15.380\n2022-12-11        14.044\n2022-12-18        17.972\n2022-12-25        16.216\n\nCodezoo::rollmedian(\n  x = ts_tomBrady,\n  k = 5)\n\n           fantasyPoints\n2001-09-30          4.34\n2001-10-07          6.92\n2001-10-14          6.92\n2001-10-21         19.88\n2001-10-28         19.88\n2001-11-04          7.18\n2001-11-11          7.18\n2001-11-18          8.52\n2001-11-25          7.18\n2001-12-02          8.52\n       ...              \n2022-10-16         17.10\n2022-10-23         15.20\n2022-10-27         15.20\n2022-11-06         16.02\n2022-11-13         16.14\n2022-11-27         16.02\n2022-12-05         16.14\n2022-12-11         16.14\n2022-12-18         16.14\n2022-12-25          9.34\n\n\n\n23.3.4 Autocorrelation\nThe autocorrelation function (ACF) plot depicts the autocorrelation of scores as a function of the length of the lag. Significant autocorrelation is detected when the autocorrelation exceeds the dashed blue lines, as is depicted in Figure 23.3.\n\nCodeacf(ts_tomBrady)\n\n\n\n\n\n\nFigure 23.3: Autocorrelation Function (ACF) Plot of Tom Brady’s Historical Fantasy Points by Game.\n\n\n\n\n\nCodeBox.test(ts_tomBrady)\n\n\n    Box-Pierce test\n\ndata:  ts_tomBrady\nX-squared = 3.8957, df = 1, p-value = 0.04841\n\n\n\n23.3.5 Fit an Autoregressive Integrated Moving Average Model\nUsing the forecast package (Hyndman et al., 2024; Hyndman & Khandakar, 2008):\n\nCodeforecast::auto.arima(ts_tomBrady)\n\nSeries: ts_tomBrady \nARIMA(1,1,2) \n\nCoefficients:\n         ar1      ma1     ma2\n      0.8007  -1.7041  0.7111\ns.e.  0.1381   0.1527  0.1479\n\nsigma^2 = 67.3:  log likelihood = -1176.57\nAIC=2361.13   AICc=2361.25   BIC=2376.38\n\nCodeforecast::auto.arima(ts_peytonManning)\n\nSeries: ts_peytonManning \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ma1     mean\n      0.8795  -0.7465  17.2154\ns.e.  0.0861   0.1152   1.0481\n\nsigma^2 = 62.99:  log likelihood = -871.18\nAIC=1750.36   AICc=1750.53   BIC=1764.45\n\n\n\nCodearima_tomBrady &lt;- arima(\n  ts_tomBrady,\n  order = c(5, 1, 4))\n\nsummary(arima_tomBrady)\n\n\nCall:\narima(x = ts_tomBrady, order = c(5, 1, 4))\n\nCoefficients:\n         ar1      ar2      ar3      ar4     ar5      ma1     ma2     ma3\n      0.6376  -0.0319  -0.4470  -0.1250  0.2078  -1.5598  0.6323  0.5117\ns.e.  0.2066   0.2223   0.1676   0.0706  0.0667   0.2078  0.3149  0.3352\n          ma4\n      -0.5574\ns.e.   0.1461\n\nsigma^2 estimated as 63.92:  log likelihood = -1169.59,  aic = 2359.17\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.6798849 7.983157 6.455929 -9.382029 62.84247 0.7287981\n                    ACF1\nTraining set 0.001251287\n\nCodeconfint(arima_tomBrady)\n\n         2.5 %      97.5 %\nar1  0.2326528  1.04261876\nar2 -0.4676647  0.40383415\nar3 -0.7755181 -0.11841000\nar4 -0.2633125  0.01336914\nar5  0.0771301  0.33839489\nma1 -1.9670637 -1.15248737\nma2  0.0150989  1.24940961\nma3 -0.1454023  1.16874068\nma4 -0.8438263 -0.27099009\n\nCodeforecast::checkresiduals(arima_tomBrady)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(5,1,4)\nQ* = 3.1415, df = 3, p-value = 0.3703\n\nModel df: 9.   Total lags used: 12\n\n\n\n\n\n\n\nFigure 23.4: Model Summary of Autoregressive Integrated Moving Average Model fit to Tom Brady’s Historical Performance by Game.\n\n\n\n\n\nCodearima_tomBrady_removeNonSigTerms &lt;- arima(\n  ts_tomBrady,\n  order = c(5, 1, 4),\n  fixed = c(NA, NA, 0, NA, NA, NA, NA, NA, NA))\n\nsummary(arima_tomBrady_removeNonSigTerms)\n\n\nCall:\narima(x = ts_tomBrady, order = c(5, 1, 4), fixed = c(NA, NA, 0, NA, NA, NA, \n    NA, NA, NA))\n\nCoefficients:\n          ar1      ar2  ar3     ar4     ar5      ma1      ma2      ma3      ma4\n      -0.8409  -0.7345    0  0.0804  0.1507  -0.0637  -0.0167  -0.6063  -0.2275\ns.e.   0.3120   0.2164    0  0.1024  0.0840   0.3197   0.2207   0.1873   0.1068\n\nsigma^2 estimated as 63.88:  log likelihood = -1169.47,  aic = 2356.93\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.6574145 7.980413 6.385955 -9.143247 62.73514 0.7208988\n                     ACF1\nTraining set -0.007136381\n\nCodeconfint(arima_tomBrady_removeNonSigTerms)\n\n          2.5 %      97.5 %\nar1 -1.45240974 -0.22948037\nar2 -1.15869850 -0.31030272\nar3          NA          NA\nar4 -0.12027200  0.28102794\nar5 -0.01390496  0.31531240\nma1 -0.69025865  0.56278078\nma2 -0.44934933  0.41594092\nma3 -0.97335654 -0.23917503\nma4 -0.43672063 -0.01826288\n\nCodeforecast::checkresiduals(arima_tomBrady_removeNonSigTerms)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(5,1,4)\nQ* = 3.1606, df = 3, p-value = 0.3675\n\nModel df: 9.   Total lags used: 12\n\n\n\n\n\n\n\nFigure 23.5: Model Summary of modified Autoregressive Integrated Moving Average Model fit to Tom Brady’s Historical Performance by Game.\n\n\n\n\n\nCodearima_peytonManning &lt;- arima(\n  ts_peytonManning,\n  order = c(1, 0, 1))\n\nsummary(arima_peytonManning)\n\n\nCall:\narima(x = ts_peytonManning, order = c(1, 0, 1))\n\nCoefficients:\n         ar1      ma1  intercept\n      0.8795  -0.7465    17.2154\ns.e.  0.0861   0.1152     1.0481\n\nsigma^2 estimated as 62.24:  log likelihood = -871.18,  aic = 1750.36\n\nTraining set error measures:\n                      ME     RMSE     MAE       MPE     MAPE      MASE\nTraining set 0.004669414 7.888962 6.28244 -89.05348 136.3438 0.7615486\n                   ACF1\nTraining set 0.02492534\n\nCodeconfint(arima_peytonManning)\n\n               2.5 %     97.5 %\nar1        0.7106760  1.0482992\nma1       -0.9722609 -0.5206973\nintercept 15.1612132 19.2695648\n\nCodeforecast::checkresiduals(arima_peytonManning)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1) with non-zero mean\nQ* = 8.4306, df = 8, p-value = 0.3926\n\nModel df: 2.   Total lags used: 10\n\n\n\n\n\n\n\nFigure 23.6: Model Summary of Autoregressive Integrated Moving Average Model fit to Peyton Manning’s Historical Performance by Game.\n\n\n\n\n\n23.3.6 Generate the Model Forecasts\n\nCodeforecast_tomBrady &lt;- forecast::forecast(\n  arima_tomBrady,\n  level = c(80, 95)) # 80% and 95% confidence intervals\n\nforecast_peytonManning &lt;- forecast::forecast(\n  arima_peytonManning,\n  level = c(80, 95)) # 80% and 95% confidence intervals\n\nforecast_tomBrady\n\n    Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95\n336       16.79157  6.545441 27.03770  1.121467 32.46168\n337       21.31309 11.035944 31.59023  5.595554 37.03062\n338       13.69041  3.371785 24.00903 -2.090564 29.47138\n339       21.48471 10.979805 31.98961  5.418846 37.55057\n340       17.19892  6.693126 27.70471  1.131696 33.26614\n341       19.01892  8.370031 29.66780  2.732851 35.30498\n342       18.72444  8.060966 29.38792  2.416061 35.03283\n343       17.83641  7.157114 28.51571  1.503835 34.16899\n344       18.62106  7.938214 29.30391  2.283057 34.95907\n345       18.16347  7.480427 28.84651  1.825168 34.50177\n\nCodeforecast_peytonManning\n\n    Point Forecast     Lo 80    Hi 80      Lo 95    Hi 95\n251       10.55320 0.4430843 20.66331 -4.9088854 26.01528\n252       11.35607 1.1569232 21.55522 -4.2421805 26.95433\n253       12.06219 1.7947024 22.32968 -3.6405790 27.76497\n254       12.68322 2.3631750 23.00326 -3.0999254 28.46636\n255       13.22940 2.8688924 23.58991 -2.6156293 29.07443\n256       13.70976 3.3180615 24.10146 -2.1829722 29.60250\n257       14.13223 3.7164703 24.54800 -1.7973016 30.06177\n258       14.50379 4.0694543 24.93813 -1.4541504 30.46173\n259       14.83057 4.3818907 25.27926 -1.1493076 30.81045\n260       15.11797 4.6582086 25.57774 -0.8788563 31.11480\n\n\n\n23.3.7 Plot the Model Forecasts\n\nCodeforecast::autoplot(forecast_tomBrady) + \n  labs(\n    x = \"Game Number\",\n    y = \"Fantasy Points\",\n    title = \"Tom Brady's Historical and Projected Fantasy Points by Game\",\n    subtitle = \"(if he were to have continued playing additional seasons)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 23.7: Tom Brady’s Historical and Projected Fantasy Points by Game.\n\n\n\n\n\nCodeforecast::autoplot(forecast_peytonManning) + \n  labs(\n    x = \"Game Number\",\n    y = \"Fantasy Points\",\n    title = \"Peyton Manning's Historical and Projected Fantasy Points by Game\",\n    subtitle = \"(if he were to have continued playing additional seasons)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 23.8: Peyton Manning’s Historical and Projected Fantasy Points by Game.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesBayesian",
    "href": "time-series-analysis.html#sec-timeSeriesBayesian",
    "title": "23  Time Series Analysis",
    "section": "\n23.4 Bayesian Mixed Models",
    "text": "23.4 Bayesian Mixed Models\nThe Bayesian longitudinal mixed models were estimated in Section 12.3.5.\n\n23.4.1 Prepare New Data Object\n\nCodeplayer_stats_seasonal_offense_subset &lt;- player_stats_seasonal %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\") | position %in% c(\"K\"))\n\nplayer_stats_seasonal_offense_subset$position[which(player_stats_seasonal_offense_subset$position == \"HB\")] &lt;- \"RB\"\n\nplayer_stats_seasonal_offense_subset$player_idFactor &lt;- factor(player_stats_seasonal_offense_subset$player_id)\nplayer_stats_seasonal_offense_subset$positionFactor &lt;- factor(player_stats_seasonal_offense_subset$position)\n\n\n\nCodeplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subset %&gt;%\n  filter(\n    !is.na(player_idFactor),\n    !is.na(fantasyPoints),\n    !is.na(positionFactor),\n    !is.na(ageCentered20),\n    !is.na(ageCentered20Quadratic),\n    !is.na(years_of_experience))\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;% \n  filter(player_id %in% bayesianMixedModelFit$data$player_idFactor) %&gt;% \n  mutate(positionFactor = droplevels(positionFactor))\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;%\n  group_by(player_id) %&gt;% \n  group_modify(~ add_row(.x, season = max(player_stats_seasonal_offense_subsetCC$season) + 1)) %&gt;% \n  fill(player_display_name, player_idFactor, position, position_group, positionFactor, team, .direction = \"downup\") %&gt;% \n  ungroup\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;% \n  left_join(\n    player_stats_seasonal_offense_subsetCC %&gt;% \n      filter(season == max(player_stats_seasonal_offense_subsetCC$season) - 1) %&gt;% \n      select(player_id, age_lastYear = age, years_of_experience_lastYear = years_of_experience),\n    by = \"player_id\") %&gt;%\n  mutate(\n    age = if_else(season == max(player_stats_seasonal_offense_subsetCC$season), age_lastYear + 1, age), # increment age by 1\n    ageCentered20 = age - 20,\n    years_of_experience = if_else(season == max(player_stats_seasonal_offense_subsetCC$season), years_of_experience_lastYear + 1, years_of_experience)) # increment experience by 1\n\nactivePlayers &lt;- unique(player_stats_seasonal_offense_subsetCC[c(\"player_id\",\"season\")]) %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season) - 1) %&gt;% \n  select(player_id) %&gt;% \n  pull()\n\ninactivePlayers &lt;- player_stats_seasonal_offense_subsetCC$player_id[which(player_stats_seasonal_offense_subsetCC$player_id %ni% activePlayers)]\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;% \n  filter(player_id %in% activePlayers | (player_id %in% inactivePlayers & season &lt; max(player_stats_seasonal_offense_subsetCC$season) - 1)) %&gt;% \n  mutate(\n    player_idFactor = droplevels(player_idFactor) \n  )\n\n\n\n23.4.2 Generate Predictions\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_bayesian &lt;- predict(\n  bayesianMixedModelFit,\n  newdata = player_stats_seasonal_offense_subsetCC\n)[,\"Estimate\"]\n\n\n\n23.4.3 Table of Next Season Predictions\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"QB\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"RB\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"WR\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"TE\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\n\n\n23.4.4 Plot of Individuals’ Model-Implied Predictions\n\n23.4.4.1 Quarterbacks\n\nCodeplot_individualFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"QB\"),\n  mapping = aes(\n    x = round(age, 2),\n    y = round(fantasyPoints_bayesian, 2),\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 23.9: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Quarterbacks.\n\n\n\n\n23.4.4.2 Running Backs\n\nCodeplot_individualFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"RB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_bayesian,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 23.10: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Running Backs.\n\n\n\n\n23.4.4.3 Wide Receivers\n\nCodeplot_individualFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"WR\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_bayesian,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 23.11: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Wide Receivers.\n\n\n\n\n23.4.4.4 Tight Ends\n\nCodeplot_individualFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"TE\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_bayesian,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic()\n\nggplotly(\n  plot_individualFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 23.12: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Tight Ends.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesConclusion",
    "href": "time-series-analysis.html#sec-timeSeriesConclusion",
    "title": "23  Time Series Analysis",
    "section": "\n23.5 Conclusion",
    "text": "23.5 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesSessionInfo",
    "href": "time-series-analysis.html#sec-timeSeriesSessionInfo",
    "title": "23  Time Series Analysis",
    "section": "\n23.6 Session Info",
    "text": "23.6 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n [4] dplyr_1.1.4         purrr_1.0.4         readr_2.1.5        \n [7] tidyr_1.3.1         tibble_3.2.1        tidyverse_2.0.0    \n[10] plotly_4.10.4       ggplot2_3.5.1       rstan_2.32.7       \n[13] StanHeaders_2.32.10 brms_2.22.0         Rcpp_1.0.14        \n[16] forecast_8.23.0     xts_0.14.1          zoo_1.8-13         \n[19] petersenlab_1.1.1  \n\nloaded via a namespace (and not attached):\n  [1] DBI_1.2.3            mnormt_2.1.1         gridExtra_2.3       \n  [4] inline_0.3.21        rlang_1.1.5          magrittr_2.0.3      \n  [7] tseries_0.10-58      matrixStats_1.5.0    compiler_4.4.3      \n [10] mgcv_1.9-1           loo_2.8.0            vctrs_0.6.5         \n [13] reshape2_1.4.4       quadprog_1.5-8       pkgconfig_2.0.3     \n [16] fastmap_1.2.0        backports_1.5.0      labeling_0.4.3      \n [19] pbivnorm_0.6.0       cmdstanr_0.8.1.9000  rmarkdown_2.29      \n [22] tzdb_0.5.0           ps_1.9.0             xfun_0.51           \n [25] jsonlite_1.9.1       psych_2.5.3          parallel_4.4.3      \n [28] lavaan_0.6-19        cluster_2.1.8        R6_2.6.1            \n [31] stringi_1.8.4        RColorBrewer_1.1-3   rpart_4.1.24        \n [34] lmtest_0.9-40        estimability_1.5.1   knitr_1.50          \n [37] base64enc_0.1-3      bayesplot_1.11.1     splines_4.4.3       \n [40] timechange_0.3.0     Matrix_1.7-2         nnet_7.3-20         \n [43] tidyselect_1.2.1     rstudioapi_0.17.1    abind_1.4-8         \n [46] yaml_2.3.10          timeDate_4041.110    codetools_0.2-20    \n [49] processx_3.8.6       curl_6.2.1           pkgbuild_1.4.6      \n [52] lattice_0.22-6       plyr_1.8.9           quantmod_0.4.26     \n [55] withr_3.0.2          bridgesampling_1.1-2 urca_1.3-4          \n [58] posterior_1.6.1      coda_0.19-4.1        evaluate_1.0.3      \n [61] foreign_0.8-88       RcppParallel_5.1.10  pillar_1.10.1       \n [64] tensorA_0.36.2.1     checkmate_2.3.2      stats4_4.4.3        \n [67] distributional_0.5.0 generics_0.1.3       TTR_0.24.4          \n [70] hms_1.1.3            mix_1.0-13           rstantools_2.4.0    \n [73] munsell_0.5.1        scales_1.3.0         xtable_1.8-4        \n [76] glue_1.8.0           emmeans_1.11.0       Hmisc_5.2-3         \n [79] lazyeval_0.2.2       tools_4.4.3          data.table_1.17.0   \n [82] mvtnorm_1.3-3        grid_4.4.3           mitools_2.4         \n [85] crosstalk_1.2.1      QuickJSR_1.6.0       colorspace_2.1-1    \n [88] nlme_3.1-167         fracdiff_1.5-3       htmlTable_2.4.3     \n [91] Formula_1.2-5        cli_3.6.4            viridisLite_0.4.2   \n [94] Brobdingnag_1.2-9    V8_6.0.2             gtable_0.3.6        \n [97] digest_0.6.37        farver_2.1.2         htmlwidgets_1.6.4   \n[100] htmltools_0.5.8.1    lifecycle_1.0.4      httr_1.4.7          \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nHyndman, R. J., Athanasopoulos, G., Bergmeir, C., Caceres, G., Chhay, L., Kuroptev, K., O’Hara-Wild, M., Petropoulos, F., Razbash, S., Wang, E., & Yasmeen, F. (2024). forecast: Forecasting functions for time series and linear models. https://pkg.robjhyndman.com/forecast/\n\n\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(3), 1–22. https://doi.org/10.18637/jss.v027.i03\n\n\nLong, J. D., & Teetor, P. (2019). R cookbook: Proven recipes for data analysis, statistics, and graphics (2nd ed.). O’Reilly Media. https://rc2e.com",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "decision-making.html",
    "href": "decision-making.html",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "",
    "text": "24.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingGettingStarted",
    "href": "decision-making.html#sec-decisionMakingGettingStarted",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "",
    "text": "24.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingOverview",
    "href": "decision-making.html#sec-decisionMakingOverview",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.2 Overview of Decision Making in Uncertainty",
    "text": "24.2 Overview of Decision Making in Uncertainty",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-wisdomOfCrowd",
    "href": "decision-making.html#sec-wisdomOfCrowd",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.3 Wisdom of the Crowd",
    "text": "24.3 Wisdom of the Crowd\nIn many domains, the average of forecasters’ predictions is more accurate than the accuracy of the constituent individuals. In some domains, the average of non-expert forecasts is more accurate than the forecasts by individual experts. This phenomenon is called “collective intelligence”, the “wisdom of the crowd”, or the “wisdom of crowds” (Larrick et al., 2024; Rader et al., 2017; Simoiu et al., 2019; Surowiecki, 2005; Wagner & Vinaimont, 2010).\nAggregation of predictions from multiple people leverages several important features, including cognitive diversity and error cancellation.\nCognitive diversity refers to the representation of individuals with different perspectives because of their “differences in knowledge, training, experience, or thinking styles” (Rader et al., 2017, p. 8). Cognitive diversity is important because judgments from a cognitively homogeneous group will tend to err systematically. That is, they tend to err in the same direction—either consistently above or below the truth; thus, their errors are correlated. By contrast, a cognitively diverse group will not tend to err systematically. Individuals of a cognitively diverse group will bring different areas of expertise (e.g., determination of player skill, player opportunity, matchup strength, etc.) to bear in making their predictions and will thus make different mistakes (Larrick et al., 2024). Consequently, for the people comprising a cognitively diverse group, their judgments will tend to err randomly—where some people’s predictions fall above the truth and some people’s predictions fall below the truth—i.e., individual judgments “bracket” the truth (Mannes et al., 2014). Thus, judgments from a cognitively diverse group tend to have uncorrelated errors.\nError cancellation deals with the idea that, when individuals’ judgments bracket the truth and show random rather than systematic error, the average of the predictions will “cancel out” some of the errors so that the predictions average out to more closely approximate the truth. However, when individuals’ judgments do not bracket the truth, the average of the predictions will not cancel out the errors.\nAveraging projections from individuals tends to yield predictions that are more accurate than the accuracy of most forecasters in the group (Mannes et al., 2014). Indeed, when at least some of of the projections bracket the truth, averaged predictions must be more accurate than the average individual forecaster—in terms of mean absolute error (MAE)—and averaged predictions are often much more accurate (Larrick et al., 2024). When referring to the accuracy of the “average individual forecaster”, we are referring to accuracy in terms of mean absolute error (MAE)—not to the accuracy of the forecaster at the 50th percentile. If none of the projections bracket the truth—e.g., all projections overestimate the truth—averaged predictions will be as accurate than the average individual forecaster in terms of mean absolute error (Larrick et al., 2024). In sum, “Averaging the answers of a crowd, therefore, ensures a level of accuracy no worse than the average member of the crowd and, in some cases, a level better than nearly all members” (Larrick et al., 2024, p. 126). Moreover, averaged projections tend to be more accurate than consensus-based judgments from groups of people that interact and discuss, due to cognitive biases associated with the social interaction among groups, such as herding in which people align their behavior with others (Mannes et al., 2014; Simoiu et al., 2019), though discussion can be helpful in some contexts (Larrick et al., 2024).\nThere are well-known prediction markets, in which people bet money to make predictions for various events, which allows determining the crowd-averaged prediction for events:\n\nhttps://www.predictit.org\nhttps://polymarket.com\n\nThere are also betting markets for football:\n\nhttps://www.rotowire.com/betting/nfl/player-futures.php\nhttps://vegasprojections.com\nhttps://www.actionnetwork.com/nfl/props\n\nCrowd-averaged projections tend to be most accurate when:\n\nthe crowd consists of individuals who hold expertise in the domain such that they will make predictions that fall close to the truth\nthere is relatively low variability in the expertise of the individual forecasters in terms of their ability to make accurate forecasts\nthere is cognitive diversity among the forecasters\nthe projections are made independently—i.e., the forecasters are not aware of others’ forecasts and do not discuss or interact with the other forecasters\nthe bracketing rate—i.e., the frequency with which any two forecasters’ predictions fall on opposite sides of the truth—is high\nthere are at least 5–10 sources of projections\n\nHowever, the crowd is not more accurate than the expert or best forecaster in all situations or domains. For instance, the crowd tends to be less accurate than the (prospectively identified) best forecaster when there is great variability in forecasters’ expertise (in terms of the forecasters’ ability to forecast accurately) and when the bracketing rate is low (Mannes et al., 2014). Some forecasters may provide terrible projections; thus, including them in an average may make the average projections substantially less accurate. Thus, it may be necessary to examine the average of a “select crowd”, by aggregating the projections of the most consistently accurate forecasters (Mannes et al., 2014). Incorporating at least 5–10 forecasters leverages most of the benefits of the crowd; adding additional forecasters tends to result in diminishing returns (Larrick et al., 2024). However, to the extent that those who are most accurate in a given period reflects luck, you are better off averaging the predictions of all forecasters than selecting the forecasters who were most accurate in the most recent period (Larrick et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-accuracyOfCrowd",
    "href": "decision-making.html#sec-accuracyOfCrowd",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.4 Accuracy of Fantasy Football Crowd Projections",
    "text": "24.4 Accuracy of Fantasy Football Crowd Projections\nEven though the crowd tends to be more accurate than individual forecasters, crowd-averaged projections (at least among experts) are not necessarily highly accurate. In fantasy football, [DESCRIBE THEIR ACCURACY–BOTH FOR ALL PLAYERS AND WHEN SUBSETTING TO THE TOP X PLAYERS]. Nevertheless, individual sources tend to be even less accurate. [DESCRIBE THEIR ACCURACY].\nThe petersenlab package (Petersen, 2025) has the wisdomOfCrowd() function that computes the overall accuracy of the crowd-averaged projections, including the bracketing rate of the individual projections.\n\nCode# insert example using historical projections: wisdomOfCrowd()\n\n\n\nCodepredictedValues &lt;- c(10,20,30,40,60,70,80,90)\nactualValue &lt;- 50\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -40 -30 -20 -10  10  20  30  40\n\nCodemedian(errors)\n\n[1] 0\n\nCodemean(predictedValues)\n\n[1] 50\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodepredictedValues &lt;- c(10,29,29,40,60,70,80,90)\nactualValue &lt;- 71\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -61 -42 -42 -31 -11  -1   9  19\n\nCodemedian(errors)\n\n[1] -21\n\nCodemean(predictedValues)\n\n[1] 51\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodepredictedValues &lt;- c(10,20,30,40,60,70,80,90)\nactualValue &lt;- 100\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -90 -80 -70 -60 -40 -30 -20 -10\n\nCodemedian(errors)\n\n[1] -50\n\nCodemean(predictedValues)\n\n[1] 50\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\nEven though some sources are more accurate than the average in a given year, they are not consistently more accurate than the average. Prediction involves a combination of luck and skill. In some years, a prediction will invariably do better than others, in part, based on luck. However, luck is unlikely to continue systematically into future years, so a source that got lucky in a given year is likely to regress to the mean in subsequent years. That is, determining the most accurate source in a given year, after the fact, is not necessarily the same as identifying the most skilled forecaster. It is easy to identify the most accurate source after the fact, but it is challenging to predict, in advance, who the best forecaster will be (Larrick et al., 2024). It requires a large sample of predictions to determine whether a given forecaster is reliably (i.e., consistently) more accurate than other forecasters and to identify the most accurate forecaster (Larrick et al., 2024). Thus, it can be challenging to know, in advance, who the most accurate forecasters will be. Because average projections are as or more accurate than the average forecaster’s prediction, averaging projections across all forecasters is superior to choosing individual forecasters when the forecasters are roughly similar in forecasting ability or when it is hard to distinguish their ability in advance (Larrick et al., 2024).\nThe relatively modest accuracy of the projections by so-called fantasy “experts’” and of their average of their projections could occur for a number of reasons. One possibility is that the level of expertise of the “expert” forecasters in terms of being able to provide accurate forecasts is not strong. That is, because football performance and injuries are so challenging to predict, individual forecasters’ projections may not be particularly close to the truth.\nA second possibility is that the bracketing rate of the predictions is not particularly high (Mannes et al., 2014). Even if the individual forecasters’ projections are not close to the truth, if ~50% of them overestimate the truth and the other 50% of the underestimate the truth, the average will more closely approximate the truth. However, if all forecasters overestimate the truth for a given player, averaging the projections will not necessarily lead to more accurate projections.\nA third possibility is that the forecasts of the different experts are not independent.\nEach of these possibilities is likely true to some degree. First, individuals’ predictions are unlikely to be highly accurate consistently. Second, there are many players who are systematically overpredicted (e.g., due to their injury) or underpredicted (e.g., due to their becoming the starter after a teammate becomes injured, is traded, etc.)—an example of overextremity miscalibration. In general, it is likely for players who are projected to score more points to be overpredicted and for players who are projected to score fewer points to be underpredicted [PRESENT CALIBRATION STATS]. Third, the experts may interact and discuss with one another. Interaction and discussion among experts may lead them to follow the herd and conform their projections to what each other predict. This has been terms informational influence and may reflect the anchoring and adjustment heuristic (Larrick et al., 2024). In any case, they are able to see each other’s projections and make change their projections, accordingly.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-incoporateAdvice",
    "href": "decision-making.html#sec-incoporateAdvice",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.5 How Well Do People Incorporate Advice From Others?",
    "text": "24.5 How Well Do People Incorporate Advice From Others?\nAn important question is how well people incorporate advice from others. In the context of fantasy football, advice might be sources of projections. In general, evidence from social psychology suggests that people tend to underweight how much weight they give to advice from others relative to their own opinions, a phenomenon called egocentric discounting (Larrick et al., 2024; Rader et al., 2017). People tend to weight others’ advice around 30% in terms of the proportion of a shift a person makes toward another person’s perspective, though this depends on the perceived accuracy of the advisor. Moreover, people frequently ignores others’ advice entirely. In general, people make use of crowds too little—they put too much weight on their own prediction and not enough weight on others’ predictions whose diversity can be leveraged for error cancellation (Larrick et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-managinRiskUncertainty",
    "href": "decision-making.html#sec-managinRiskUncertainty",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.6 Strategies for Managing Risk and Uncertainty",
    "text": "24.6 Strategies for Managing Risk and Uncertainty",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-riskManagementCognitivePsych",
    "href": "decision-making.html#sec-riskManagementCognitivePsych",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.7 Risk Management Principles from Cognitive Psychology",
    "text": "24.7 Risk Management Principles from Cognitive Psychology",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-sportsBetting",
    "href": "decision-making.html#sec-sportsBetting",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.8 Sports Betting/Gambling",
    "text": "24.8 Sports Betting/Gambling\n[overconfidence; confirmation bias; variable ratio reinforcement; addiction/delay discounting; wisdom of the crowd, and the challenge of beating “the market”]\nThere is substantial overconfidence (in particular overestimation) in sports betting/gambling. A study (archived at https://perma.cc/X2AW-SUBZ) of frequent sports bettors found that they tended to predict that they would gain 0.3 cents for every dollar wagered, but in fact lost 7.5 cents for every dollar wagered (Brown et al., 2025). Overconfidence was greatest among those who frequently wagered multi-leg bets (parlays), losing ~25 cents for every dollar wagered (Brown et al., 2025).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingSuggestions",
    "href": "decision-making.html#sec-decisionMakingSuggestions",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.9 Suggestions",
    "text": "24.9 Suggestions\nBased on the above discussion, here are some suggestions for decision making in the context of uncertainty:\n\nseek advice from diverse perspectives and incorporate it into your decision making\ntry to get opinions from others before you state your perspective and before the various sources of advice discuss with each other, to ensure independence of advice\n\nAs noted by Kahneman (2011), “before an issue is discussed, all members of the committee should be asked to write a very brief summary of their position… The standard practice of open discussion gives too much weight to the opinions of those who speak early and assertively, causing others to line up behind them.” (p. 85).\n\n\nif some sources of advice (or projections) are clearly more skilled and accurate than others, you can average this “select” crowd of projections or give them greater weight in a weighted average\nif it is unclear whether or which sources are reliably more accurate than others, using a simple average across all sources (i.e., crowd-averaged projections) can be a useful approach that is as accurate as—if not more accurate than—the average individual forecaster\nincorporate at least 5–10 sources of projections",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingConclusion",
    "href": "decision-making.html#sec-decisionMakingConclusion",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.10 Conclusion",
    "text": "24.10 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingSessionInfo",
    "href": "decision-making.html#sec-decisionMakingSessionInfo",
    "title": "24  Decision Making in the Context of Uncertainty",
    "section": "\n24.11 Session Info",
    "text": "24.11 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] petersenlab_1.1.1\n\nloaded via a namespace (and not attached):\n [1] generics_0.1.3     lattice_0.22-6     stringi_1.8.4      digest_0.6.37     \n [5] magrittr_2.0.3     evaluate_1.0.3     grid_4.4.3         RColorBrewer_1.1-3\n [9] mvtnorm_1.3-3      fastmap_1.2.0      plyr_1.8.9         jsonlite_1.9.1    \n[13] nnet_7.3-20        backports_1.5.0    DBI_1.2.3          Formula_1.2-5     \n[17] gridExtra_2.3      purrr_1.0.4        viridisLite_0.4.2  scales_1.3.0      \n[21] pbivnorm_0.6.0     mnormt_2.1.1       cli_3.6.4          mitools_2.4       \n[25] rlang_1.1.5        munsell_0.5.1      Hmisc_5.2-3        base64enc_0.1-3   \n[29] mix_1.0-13         parallel_4.4.3     tools_4.4.3        reshape2_1.4.4    \n[33] checkmate_2.3.2    htmlTable_2.4.3    dplyr_1.1.4        colorspace_2.1-1  \n[37] ggplot2_3.5.1      vctrs_0.6.5        R6_2.6.1           rpart_4.1.24      \n[41] stats4_4.4.3       lifecycle_1.0.4    stringr_1.5.1      htmlwidgets_1.6.4 \n[45] psych_2.5.3        foreign_0.8-88     cluster_2.1.8      pkgconfig_2.0.3   \n[49] pillar_1.10.1      gtable_0.3.6       Rcpp_1.0.14        glue_1.8.0        \n[53] data.table_1.17.0  xfun_0.51          tibble_3.2.1       tidyselect_1.2.1  \n[57] rstudioapi_0.17.1  knitr_1.50         xtable_1.8-4       nlme_3.1-167      \n[61] htmltools_0.5.8.1  rmarkdown_2.29     lavaan_0.6-19      compiler_4.4.3    \n[65] quadprog_1.5-8    \n\n\n\n\n\n\nBrown, M., Grasley, N., & Guido, M. (2025). Do sports bettors need consumer protection? Evidence from a field experiment. https://mattbrownecon.github.io/assets/papers/jmp/sportsbetting.pdf\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nLarrick, R. P., Mannes, A. E., & Soll, J. B. (2024). The social psychology of the wisdom of crowds (with a new section on recent advances). In F. M. Federspiel, G. Montibeller, & M. Seifert (Eds.), Behavioral decision analysis (pp. 121–143). Springer. https://doi.org/10.1007/978-3-031-44424-1_7\n\n\nMannes, A. E., Soll, J. B., & Larrick, R. P. (2014). The wisdom of select crowds. Journal of Personality and Social Psychology, 107(2), 276–299. https://doi.org/10.1037/a0036677\n\n\nPetersen, I. T. (2025). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nRader, C. A., Larrick, R. P., & Soll, J. B. (2017). Advice as a form of social influence: Informational motives and the consequences for accuracy. Social and Personality Psychology Compass, 11(8), e12329. https://doi.org/10.1111/spc3.12329\n\n\nSimoiu, C., Sumanth, C., Mysore, A., & Goel, S. (2019). Studying the \"wisdom of crowds\" at scale. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 7(1), 171–179. https://doi.org/10.1609/hcomp.v7i1.5271\n\n\nSurowiecki, J. (2005). The wisdom of crowds. Anchor Books.\n\n\nWagner, C., & Vinaimont, T. (2010). Evaluating the wisdom of crowds. Issues in Information Systems, 11(1), 724–732. http://iacis.org/iis/2010/724-732_LV2010_1546.pdf",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html",
    "href": "sports-cognitive-psychology.html",
    "title": "25  Sports and Cognitive Psychology",
    "section": "",
    "text": "25.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyGettingStarted",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyGettingStarted",
    "title": "25  Sports and Cognitive Psychology",
    "section": "",
    "text": "25.1.1 Load Packages\n\nCodelibrary(\"nflreadr\")\nlibrary(\"tidyverse\")\nlibrary(\"ggtext\")\n\n\n\n25.1.2 Download Football Data\n\nCodeload(file = \"./data/nfl_pbp.RData\")\nload(file = \"./data/nfl_4thdown.RData\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyOverview",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyOverview",
    "title": "25  Sports and Cognitive Psychology",
    "section": "\n25.2 Overview",
    "text": "25.2 Overview\nThere are a number of ways in which analytics have changed sports [Underwood (2019); archived at https://perma.cc/PQ5R-TWFA]. Here is a summary of some of the key ways that analytics has led to changes.\nMuch of the history of analytics in sports traces back to Bill James. James was a member of the Society for American Baseball Research (SABR). He published a series of Baseball Abstracts, which included players’ statistics. James published the first Baseball Abstract in 1977 (https://www.pbs.org/thinktank/transcript1197.html; archived at https://perma.cc/Y7J7-GB9V). He coined the term Sabermetrics (which was originally named SABRmetrics, based on the name of the society) to refer to advanced metrics and statistical/empirical analysis of baseball. James believed that traditional statistics like batting average, runs batted in (RBIs), and pitchers’ wins did not reflect a player’s true value—i.e., their contribution to the team’s success. Instead, he advocated for the use of more advanced metrics that could provide a deeper understanding of a player’s performance and value such as on-base percentage (OBP), slugging percentage (SLG), wins above replacement (WAR), and fielding independent pitching (FIP). He also developed advanced metrics such as runs created (RC) and defense efficiency rating (DER).\nJames’ ideas were slow to catch on among those in baseball. Nevertheless, some people eventually caught on to his ideas—and to good success. Billy Beane, a general manager, used Sabermetrics to help the Oakland Athletics, a small market team with a limited budget, better compete with teams with larger budgets. He used statistics such as on-base percentage to identify player value more accurately, especially for identifying undervalued players. The story was described in Michael Lewis’ book, Moneyball, which was turned into a movie. Following publication of Moneyball, Theo Epstein, who was president of the Boston Red Sox and then of the Chicago Cubs, used sabermetrics to help each win the World Series.\nIn addition to teams using Sabermetrics to evaluate player talent, teams also began to frequently use statistical analysis to inform decision making during games, which led to key changes in the style of play. For instance, defensive shifts—where defensive players moved to locations on the field where particular hitters were most likely to hit the ball—became more common, attempts to steal bases became less common, there were fewer bunts, batters took more pitches (i.e., watched more pitches without swinging), there were more frequent pitching changes (for particular pitcher–batter matchups; such as to have a right-handed pitcher face a left-handed batter or vice versa), and a greater focus on velocity and spin rate among pitchers. Some of these analytics-driven changes in play style eventually led Major League Baseball (MLB) to make rule changes in an attempt to make the game more exciting to watch, including banning defensive shifts, reducing the number of pitching changes allowed, and making the bases larger and easier to steal.\nAlthough baseball was one of the first major sports to embrace analytics, other sports have been transformed by analytics, as well. For instance, in basketball, there has been a greater focus on three points and buckets close to the rim (e.g., dunks and layups), with way fewer midrange shots (Partnow, 2021). Moreover, star players rest more games. Here is an article on how Shane Battier, who was not the quickest, fastest, or most athletic player, used analytics successfully to guard players who were quicker or more athletic than him: https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html [Lewis (2009); archived at https://web.archive.org/web/20250219225627/https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html].\nFootball has also seen greater use of analytics, although its uptake has been somewhat slower than in many other sports. When asked about how he makes the decision about whether to go for a two-point conversion after a touchdown, Steelers Head Coach Mike Tomlin stated:\n\nWe work a menu of plays in that area over the course of the week. We rank them at the latter part of the week, and then we get into the stadium and we play it by ear. A lot of it has to do with the feel or the flow of the game—maybe what personnel group we think they’re going to match our personnel group with. As we start to play and work the ball down the field on the drives that produce the touchdowns before the point after, we have a little inclination of what their personality might be at least in terms of matching our personnel. All of those things weigh into the decision. It legitimately is a feel thing. I think that’s why you play the game. You can take analytics to baseball and things like that, but football is always going to be football. I got a lot of respect for analytics and numbers, but I’m not going to make judgments based on those numbers. The game is the game. It’s an emotional one played by emotional and driven men. That’s an element of the game you can’t measure. Oftentimes, decisions such as that weigh heavily into the equation.\n— Steelers Head Coach Mike Tomlin, 2015 (archived at https://perma.cc/7CHJ-BTWX)\n\nThat is, Mike Tomlin, suggested that he does not take into account analytics when making decisions. Perhaps that is why the Pittsburgh Steelers were voted one of the least analytically advanced teams in the NFL in 2020 [Walder (2020); archived at https://perma.cc/R7FA-HGGB]. Nevertheless, the Steelers did eventually hire a person focused on analytics [Fowler (2015); archived at https://perma.cc/W7VE-JZQX].\nA player talent evaluator for the NFL noted that many people around the league believe that the use of analytics is better suited for baseball than football because baseball involves more games, players, and one-on-one matchups [Reed (2016); archived at https://web.archive.org/web/20200803205803/https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html]. By contrast, player performance in football may be more dependent on teammates and play calls. The player talent evaluator noted, “In football they don’t know how to use all the numbers yet…They have the data, they have all these different stats but the people I have talked to aren’t completely sure how to use it.” [Reed (2016); archived at https://web.archive.org/web/20200803205803/https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html]. Some dismiss analytics entirely.\nFor instance, here is a 2016 video clip of a football commentator, Mike Patrick, making fun of people who compile statistics for informing decision-making in football, calling the people who compile the numbers “guys who wear socks with flip-flops”, suggesting that people who compile analytics to make recommendations in football are out of touch with the realities of football and do not have the same on-the-field understanding as the coaches and players.\nFootball Commentator Making Fun of Analytics in Football. From: https://streamable.com/1hw8 (2016).\nNevertheless, more and more people around the league are using analytics—likely because it works; for examples of articles, see here [Clark (2018); archived at https://perma.cc/7BKP-A4GJ] and here [Awbrey (2020); archived at https://perma.cc/WXE3-53E6]. Head Coach Doug Peterson heavily relied on analytics with the Philadelphia Eagles to help them win the 2017 Super Bowl; for more information, see here [Fox (2021); archived at https://perma.cc/49KQ-R785] and here [Rosenthal (2018); archived at https://perma.cc/2GRF-8Z5K].\nExamples of changes in football due to the use of analytics include more often going for it on fourth down, a greater emphasis on the passing game, drafting Running Backs later in the draft (and, more generally, valuing Running Backs less), and trading down in the draft to obtain more low picks (rather than having fewer high picks) because top picks are frequently overvalued (Massey & Thaler, 2013).",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-coachingRiskAversion",
    "href": "sports-cognitive-psychology.html#sec-coachingRiskAversion",
    "title": "25  Sports and Cognitive Psychology",
    "section": "\n25.3 Coaching and Loss/Risk Aversion",
    "text": "25.3 Coaching and Loss/Risk Aversion\n\n25.3.1 Going for It on Fourth Down\nIt had been known for a long time that going for it on fourth down would frequently increase a team’s chances of winning. Despite that, historically, teams rarely went for it on fourth down and elected to punt or kick a field goal instead. It is curious that there was such a discrepancy between the decisions that would maximize teams’ winning percentage and the decisions coaches actually made. In many fourth down situations, by punting the ball, coaches actively and systematically made decisions that reduced their team’s chances of winning. One potential explanation for the discrepancy is because of coaches’ risk aversion. As noted in Section 14.4.10, when it is possible to experience either a gain or a loss from a decision, loss aversion bias tends to lead people to make risk-averse decisions (Kahneman, 2011). According to this idea, in the case of failing to successfully convert on fourth down, coaches do not want to have to defend their decision to go for it to the media or the owner or general manager. That is, they may often play not to lose, rather than to win, in order to keep their job. However, in many cases, this is the wrong choice (Moskowitz & Wertheim, 2011).\nRates of going for it on fourth down in the NFL were low until 2017. After the 2017 season, rates of going for it on fourth down increased dramatically, as depicted in Figure 25.1.\n\nCodenfl_pbp4thDown &lt;- nfl_pbp %&gt;% \n  filter(down == 4) %&gt;% \n  filter(!(play_type %in% c(\"no_play\",\"qb_kneel\")))\n\nnfl_pbp4thDown$goForIt &lt;- NA\nnfl_pbp4thDown$goForIt[which(nfl_pbp4thDown$play_type %in% c(\"field_goal\",\"punt\"))] &lt;- 0\nnfl_pbp4thDown$goForIt[which(nfl_pbp4thDown$play_type %in% c(\"pass\",\"run\"))] &lt;- 1\n\nnfl_pbp4thDownPlotData &lt;- nfl_pbp4thDown %&gt;% \n  filter(!is.na(goForIt)) %&gt;% \n  group_by(season) %&gt;% \n  summarise(\n    goForItPct = mean(goForIt, na.rm = TRUE),\n    n = n(),\n    sd = sd(goForIt),\n    se = sd / n\n  )\n\nggplot2::ggplot(\n  data = nfl_pbp4thDownPlotData,\n  ggplot2::aes(\n    x = season,\n    y = goForItPct)) +\n  geom_point() +\n  geom_line() +\n  geom_ribbon(\n    aes(\n      y = goForItPct,\n      ymin = goForItPct - qnorm(0.975)*se,\n      ymax = goForItPct + qnorm(0.975)*se),\n    alpha = 0.2) +\n  scale_y_continuous(\n    limits = c(0, NA)\n  ) +\n  scale_x_continuous(\n    minor_breaks = seq(0, 3000, 1),\n    breaks = seq(0, 3000, 5)\n  ) +\n  labs(\n    x = \"Season\",\n    y = \"Proportion of 4th Down Plays that\\nare Attempts (to Get the First Down)\",\n    title = \"4th Down Attempts (Proportion) by Season\",\n  ) +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) + # horizontal y-axis title\n  annotate(\n    \"segment\",\n    x = 2015,\n    xend = 2017,\n    y = 0.08,\n    yend = 0.123,\n    color = \"blue\",\n    linewidth = 1.5,\n    alpha = 0.6,\n    arrow = arrow()) +\n  annotate(\n    \"text\",\n    x = 2015,\n    y = 0.07,\n    label = \"Rapid increases in 4th down\\nattempts after the 2017 season\",\n    hjust = 0.5) # center\n\n\n\n\n\n\nFigure 25.1: The Proportion of Fourth Downs That are Attempts to Go for it (Rather Than Punts or Field Goals).\n\n\n\n\nAs depicted in the Figure 25.2 [which was adapted from a figure by Ben Baldwin; Baldwin (2023); archived at https://perma.cc/S5D8-3NCU], teams went for it on fourth down nearly twice as often in 2021 compared to 2017, when when their win probability was at least 20% (given the current situation at the start of the given play) and they would increase their win probably by going for it. We use the ggtext package (Wilke & Wiernik, 2022) to annotate text:\n\nCode# labels on the plot\ntext_df &lt;- tibble(\n  label = c(\n    \"NFL coaches&lt;br&gt;in &lt;span style='color:#00BFC4'&gt;**2021**&lt;/span&gt;\",\n    \"NFL coaches&lt;br&gt;in &lt;span style='color:#F8766D'&gt;**2017**&lt;/span&gt;\"),\n  x = c(5, 7),\n  y = c(87.5, 22.5),\n  angle = c(10, 10),\n  color = c(\"black\", \"black\")\n)\n\nnfl_4thdown %&gt;%\n  filter(\n    vegas_wp &gt; .2,\n    between(go_boost, -10, 10),\n    season %in% c(2017, 2021)) %&gt;%\n  ggplot(\n    aes(\n      x = go_boost,\n      y = go,\n      color = as.factor(season))) + \n  geom_vline(xintercept = 0) +\n  stat_smooth(\n    method = \"gam\",\n    method.args = list(gamma = 1),\n    formula = y ~ s(x, bs = \"cs\", k = 10),\n    show.legend = FALSE,\n    se = FALSE,\n    linewidth = 4) +\n  # this is just to get the plot to draw the full 0 to 100 range\n  geom_hline(\n    yintercept = 100,\n    alpha = 0) +\n  geom_hline(\n    yintercept = 0,\n    alpha = 0) +\n  ggtext::geom_richtext(\n    data = text_df,\n    aes(\n      x,\n      y,\n      label = label,\n      angle = angle),\n    color = \"black\",\n    fill = NA,\n    label.color = NA,\n    size = 5) + \n  theme_classic() +\n  labs(\n    x = \"Gain in win probability by going for it\",\n    y = \"Go-for-it percentage\",\n    subtitle = \"4th down decisions in 2021 versus 2017, win prob. &gt; 20%\",\n    title = glue::glue(\"How &lt;span style='color:red'&gt;math&lt;/span&gt; is changing football\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_markdown(\n      size = 22,\n      hjust = 0.5),\n    plot.subtitle = element_markdown(\n      size = 13.5,\n      hjust = 0.5),\n    axis.title.x = element_text(\n      size = 14,\n      face = \"bold\"),\n    axis.title.y = element_text(\n      size = 14,\n      face = \"bold\",\n      angle = 0, # horizontal y-axis title\n      vjust = 0.5)\n  ) +\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(n = 4),\n    limits = c(0, 100),\n    expand = c(0, 0)) +\n  scale_x_continuous(\n    breaks = scales::pretty_breaks(n = 10),\n    limits = c(-10, 10),\n    expand = c(0, 0)) +\n  annotate(\n    \"text\",\n    x = -1.5,\n    y = 70,\n    label = \"Should\\nkick\",\n    color = \"black\",\n    size = 5) +\n  annotate(\n    \"text\",\n    x = 1.75,\n    y = 70,\n    label = \"Should\\ngo for it\",\n    color = \"black\",\n    size = 5) +\n  annotate(\n    \"segment\",\n    x = -.1,\n    y = 80,\n    xend = -2,\n    yend = 80,\n    arrow = arrow(length = unit(0.05, \"npc\")),\n    color = \"black\",\n    linewidth = 2) +\n  annotate(\n    \"segment\",\n    x = .1,\n    y = 80,\n    xend = 2,\n    yend = 80,\n    arrow = arrow(length = unit(0.05, \"npc\")),\n    color = \"black\",\n    linewidth = 2)\n\n\n\n\n\n\nFigure 25.2: Go-For-It Percentage on Fourth Downs in 2021 Versus 2017 as a Function of the Change in Win Probability for Going for it. To remove desperation fourth down attempts, the figure focuses on just plays in which the estimated win probability for the offensive team (given the current situation at the start of the given play and incorporating the pre-game Vegas line) was greater than 20%.\n\n\n\n\nNFL teams were later to implement analytics into their decision making than even teams at lower levels of the sport. Some high school teams, such as the Pulaski Academy Bruins (Little Rock, AR), coached by Kevin Kelley, consistently went for it on fourth down well before 2017 (Moskowitz & Wertheim, 2011).\n\n[Bruins Head Football Coach Kevin] Kelley believes that the “quant jocks” don’t go far enough to validate the no-punting worldview and, more generally, the virtues of risk-taking. “The math guys, the astrophysicist guys, they just do the raw numbers and they don’t figure emotion into it—and that’s the biggest thing of all,” he says. “The built-in emotion involved in football is unbelievable, and that’s where the benefits really pay off.” What he means is this: A defense that stops an opponent on third down is usually ecstatic. They’ve done their job. The punting unit comes on, and the offense takes over. When that defense instead gives up a fourth-down conversion, it has a hugely deflating effect. At Pulaski’s games, you can see the shoulders of the opposing defensive players slump and their eyes look down when they fail to stop the Bruins on fourth down.\n(Moskowitz & Wertheim, 2011, p. 37)\n\nBased on Romer’s (2006) analysis of third down plays (because few teams went for it on fourth down), focusing on plays in the first quarter (to remove desperation plays), he identified several general conclusions about fourth-down situations:\n\nInside the opponent’s 45-yard line, a team is better off going for it than punting with 6 (or less) yards to go (for a first down).\nInside the opponent’s 33-yard line, a team is better off going for it with 10 (or less) yards to go (unless little time remains and a field goal would decide the game).\nOnce reaching the opponent’s 5-yard line, a team is better off going for it.\nRegardless of field position, a team is always better off going for it with 3 (or less) yards to go.\n\nNevertheless, out of the fourth down plays Romer (2006) identified between 1998–2000 in which it would have been advantageous to go for it, the team made the suboptimal decision (punting or kicking) 90% of the time. “Inasmuch as an academic paper can become a cult hit, Romer’s made the rounds in NFL executive offices, but most NFL coaches seemed to dismiss his findings as the handiwork of an egghead, polluting art with science.” (Moskowitz & Wertheim, 2011, p. 39).\nHere is an analysis by the New York Times on when to go for it on fourth down: https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html [NYT 4th Down Bot (2014); archived at https://perma.cc/KA9Y-BRUD]. Here is a fourth down calculator: https://rbsdm.com/stats/fourth_calculator/.\n\n\n\n\n\nFigure 25.3: Analysis of When to Go for it on Fourth Down Versus What Coaches Actually Do on 4th Down (From 2014). Retrieved from https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html (archived at https://perma.cc/KA9Y-BRUD).",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-beliefsInSports",
    "href": "sports-cognitive-psychology.html#sec-beliefsInSports",
    "title": "25  Sports and Cognitive Psychology",
    "section": "\n25.4 Other Beliefs in Sports",
    "text": "25.4 Other Beliefs in Sports\n\n25.4.1 Icing the Kicker\nIn a 2008 game between the Jets and Raiders, Jets Kicker Jay Feely missed a game-tying field goal to send the game to overtime. However, moments before the snap, the Raiders called a timeout in an attempt to “ice the kicker.” The idea behind icing the kicker is making the kicker think about the pressure of upcoming kick so he gets “cold feet” (metaphorically speaking) and misses the kick. After the brief timeout, Feely got a second chance and made the field goal to send the game into overtime.\nAfter the game, Feely noted, “I heard the whistle before I started, which is an advantage to the kicker…If you’re going to do that, do that before he kicks. I can kick it down the middle, see what the wind does and adjust. It helps the kicker tremendously.” [Press (2008); archived at https://perma.cc/U7SV-UE2E].\nHowever, does icing the kicker work? There is mixed evidence on icing the kicker. Some studies find some evidence for lower percentage of iced kicks made compared to non-iced kicks (Goldschmied et al., in press), whereas other studies do not (e.g., Berry & Wood, 2004; Gonzalez Sanchez et al., 2024; Moskowitz & Wertheim, 2011). This suggests that, if there is such an effect, the effect size is likely very small, and it may better suit the team to use your timeouts in other situations.\n\n25.4.2 Hot Hand\nThe belief in the “hot hand” is described in Section 14.5.3.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyConclusion",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyConclusion",
    "title": "25  Sports and Cognitive Psychology",
    "section": "\n25.5 Conclusion",
    "text": "25.5 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologySessionInfo",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologySessionInfo",
    "title": "25  Sports and Cognitive Psychology",
    "section": "\n25.6 Session Info",
    "text": "25.6 Session Info\n\nCodesessionInfo()\n\nR version 4.4.3 (2025-02-28)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggtext_0.1.2    lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1  \n [5] dplyr_1.1.4     purrr_1.0.4     readr_2.1.5     tidyr_1.3.1    \n [9] tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0 nflreadr_1.4.1 \n\nloaded via a namespace (and not attached):\n [1] generics_0.1.3    xml2_1.3.8        stringi_1.8.4     lattice_0.22-6   \n [5] hms_1.1.3         digest_0.6.37     magrittr_2.0.3    evaluate_1.0.3   \n [9] grid_4.4.3        timechange_0.3.0  fastmap_1.2.0     Matrix_1.7-2     \n[13] jsonlite_1.9.1    mgcv_1.9-1        scales_1.3.0      cli_3.6.4        \n[17] rlang_1.1.5       commonmark_1.9.5  munsell_0.5.1     splines_4.4.3    \n[21] withr_3.0.2       cachem_1.1.0      yaml_2.3.10       tools_4.4.3      \n[25] tzdb_0.5.0        memoise_2.0.1     colorspace_2.1-1  vctrs_0.6.5      \n[29] R6_2.6.1          lifecycle_1.0.4   htmlwidgets_1.6.4 pkgconfig_2.0.3  \n[33] pillar_1.10.1     gtable_0.3.6      data.table_1.17.0 glue_1.8.0       \n[37] Rcpp_1.0.14       xfun_0.51         tidyselect_1.2.1  knitr_1.50       \n[41] farver_2.1.2      htmltools_0.5.8.1 nlme_3.1-167      rmarkdown_2.29   \n[45] labeling_0.4.3    compiler_4.4.3    markdown_1.13     gridtext_0.1.5   \n\n\n\n\n\n\nAwbrey, J. (2020). The future of NFL data analytics. https://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics\n\n\nBaldwin, B. (2023). nfl4th: Functions to calculate optimal fourth down decisions in the National Football League. https://www.nfl4th.com/\n\n\nBerry, S., & Wood, C. (2004). The cold-foot effect. CHANCE, 17(4), 47–51. https://doi.org/10.1080/09332480.2004.10554926\n\n\nClark, K. (2018). The NFL’s analytics revolution has arrived. https://www.theringer.com/2018/12/19/nfl/nfl-analytics-revolution\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFowler, J. (2015). Why the Steelers hired a Carnegie Mellon professor for advanced analytics. https://www.espn.com/blog/pittsburgh-steelers/post/_/id/14521/why-the-steelers-hired-a-carnegie-mellon-professor-for-advanced-analytics\n\n\nFox, L. (2021). How the NFL uses analytics, according to the lead analyst of a Super Bowl champion. https://www.forbes.com/sites/liamfox/2021/08/12/how-the-nfl-uses-analytics-according-to-the-lead-analyst-of-a-super-bowl-champion\n\n\nGoldschmied, N., Ratkovich, T., & Raphaeli, M. (in press). Brief report: Exploring the icing the kicker strategy in the NFL. Journal of Applied Sport Psychology. https://doi.org/10.1080/10413200.2024.2437166\n\n\nGonzalez Sanchez, A., Martinez, S., Yurko, R., Elmore, R., & Macdonald, B. (2024). Beyond the box score: Does icing the field goal kicker work in the NFL? CHANCE, 37(3), 41–48. https://doi.org/10.1080/09332480.2024.2415841\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nLewis, M. (2009). The no-stats all-star. https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html\n\n\nMassey, C., & Thaler, R. H. (2013). The loser’s curse: Decision making and market efficiency in the National Football League draft. Management Science, 59(7), 1479–1495. https://doi.org/10.1287/mnsc.1120.1657\n\n\nMoskowitz, T. J., & Wertheim, L. J. (2011). Scorecasting: The hidden influences behind how sports are played and games are won. Three Rivers Press.\n\n\nNYT 4th Down Bot. (2014). 4th down: When to go for it and why. https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html\n\n\nPartnow, S. (2021). The midrange theory: Basketball’s evolution in the age of analytics. Triumph Books.\n\n\nPress, T. A. (2008). Janikowski gives Raiders win over Jets in overtime. https://www.nfl.com/news/janikowski-gives-raiders-win-over-jets-in-overtime-09000d5d80bc3910\n\n\nReed, T. (2016). In an NFL divided over analytics, Cleveland Browns look to make numbers add up in their favor. https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html\n\n\nRomer, D. (2006). Do firms maximize? Evidence from professional football. Journal of Political Economy, 114(2), 340–365. https://doi.org/10.1086/501171\n\n\nRosenthal, G. (2018). Super Bowl LII: How the 2017 Philadelphia Eagles were built. https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753\n\n\nUnderwood, A. (2019). 15 ways analytics has changed sports. https://stacker.com/stories/sports/15-ways-analytics-has-changed-sports\n\n\nWalder, S. (2020). 2020 NFL analytics survey: Which teams are most, least analytically inclined? https://www.espn.com/nfl/story/_/id/29939438/2020-nfl-analytics-survey-which-teams-most-least-analytically-inclined\n\n\nWilke, C. O., & Wiernik, B. M. (2022). ggtext: Improved text rendering support for ggplot2. https://wilkelab.org/ggtext/",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "I need your help!\n    \n    I want your feedback to make the book better for you and other readers. If you find typos, errors, or places where the text may be improved, please let me know.\n    The best ways to provide feedback are by GitHub or hypothes.is annotations.\n    \n\n    \n      \n      Opening an issue or submitting a pull request on GitHub: https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook\n    \n    \n      \n      Adding an annotation using hypothes.is.\n      To add an annotation, select some text and then click the\n      \n      \n    \n    \n      symbol on the pop-up menu.\n      To see the annotations of others, click the\n      \n\n      symbol in the upper right-hand corner of the page.\n    \n  \n\n\n\n\n\n\n\nAden-Buie, G., Schloerke, B., Allaire, J., & Rossell Hayes, A.\n(2023). learnr: Interactive tutorials\nfor R. https://rstudio.github.io/learnr/\n\n\nAdobe Express. (2020). 8 basic design principles to help you make\nawesome graphics. https://www.adobe.com/express/learn/blog/8-basic-design-principles-to-help-you-create-better-graphics\n\n\nÆgisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S.,\nAnderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K.,\nWalker, B. S., Cohen, G., & Rush, J. D. (2006). The meta-analysis of\nclinical judgment project: Fifty-six years of accumulated research on\nclinical versus statistical prediction. The Counseling\nPsychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875\n\n\nAIDSVu. (2022). Understanding the current HIV epidemic\nin the United States. https://map.aidsvu.org/profiles/nation/usa/overview\n\n\nAkinshin, A. (2023). Weighted quantile estimators. arXiv. https://doi.org/10.48550/arXiv.2304.07265\n\n\nAltarejos, J., & Hayward, R. (2025). Likelihood ratio\nnomogram. Centre for Health Evidence. https://jamaevidence.mhmedical.com/data/calculators/LR_nomogram.html\n\n\nAndersen, D., Petersen, I. T., & Tungate, A. (2025). ffanalytics: Scrape data for fantasy\nfootball. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nAtanasov, P., Witkowski, J., Ungar, L., Mellers, B., & Tetlock, P.\n(2020). Small steps to accuracy: Incremental belief updaters are better\nforecasters. Organizational Behavior and Human Decision\nProcesses, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001\n\n\nAtaneka, A., Kelcey, B., Dong, N., Bulus, M., & Bai, F. (2023).\nPowerUp R Shiny app (v. 0.9)\nmanual. https://www.causalevaluation.org/uploads/7/3/3/6/73366257/r_shinnyapp_manual_0.9.pdf\n\n\nAttali, D., & Baker, C. (2023). ggExtra: Add marginal histograms to ggplot2, and more ggplot2 enhancements. https://github.com/daattali/ggExtra\n\n\nAustin, P. C., & Steyerberg, E. W. (2014). Graphical assessment of\ninternal and external calibration of logistic regression models by using\nloess smoothers. Statistics in Medicine, 33(3),\n517–535. https://doi.org/10.1002/sim.5941\n\n\nAvugos, S., Köppen, J., Czienskowski, U., Raab, M., & Bar-Eli, M.\n(2013). The “hot hand” reconsidered: A meta-analytic\napproach. Psychology of Sport and Exercise, 14(1),\n21–27. https://doi.org/10.1016/j.psychsport.2012.07.005\n\n\nAwbrey, J. (2020). The future of NFL data\nanalytics. https://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics\n\n\nBaird, C., & Wagner, D. (2000). The relative validity of actuarial-\nand consensus-based risk assessment systems. Children and Youth\nServices Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5\n\n\nBaldwin, B. (2023). nfl4th: Functions to\ncalculate optimal fourth down decisions in the National Football\nLeague. https://www.nfl4th.com/\n\n\nBales, J. (2012). 2012 contract year players and the myth of\nincreased production. https://www.4for4.com/2012/preseason/2012-contract-year-players-and-myth-increased-production\n\n\nBar-Eli, M., Avugos, S., & Raab, M. (2006). Twenty years of\n“hot hand” research: Review and critique. Psychology of\nSport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001\n\n\nBarrett, M. (2024). ggdag: Analyze and\ncreate elegant directed acyclic graphs. https://github.com/r-causal/ggdag\n\n\nBartoń, K. (2024). MuMIn: Multi-model inference.\nhttps://CRAN.R-project.org/package=MuMIn\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting\nlinear mixed-effects models using lme4.\nJournal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2025). lme4: Linear mixed-effects models using\nEigen and S4. https://github.com/lme4/lme4/\n\n\nBengtsson, H. (2024). progressr: An\ninclusive, unifying API for progress updates. https://progressr.futureverse.org\n\n\nBengtsson, H. (2025). parallelly:\nEnhancing the parallel package. https://parallelly.futureverse.org\n\n\nBerry, S., & Wood, C. (2004). The cold-foot effect. CHANCE,\n17(4), 47–51. https://doi.org/10.1080/09332480.2004.10554926\n\n\nBocskocsky, A., Ezekowitz, J., & Stein, C. (2014). The hot hand: A\nnew approach to an old “fallacy.” MIT Sloan Sports\nAnalytics Conference. https://www.sloansportsconference.com/research-papers/the-hot-hand-a-new-approach-to-an-old-fallacy\n\n\nBolger, F., & Önkal-Atay, D. (2004). The effects of feedback on\njudgmental interval predictions. International Journal of\nForecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8\n\n\nBolker, B., & R Development Core Team. (2023). bbmle: Tools for general maximum likelihood\nestimation. https://github.com/bbolker/bbmle\n\n\nBollen, K. A. (2002). Latent variables in psychology and the social\nsciences. Annual Review of Psychology, 53(1), 605–634.\nhttps://doi.org/10.1146/annurev.psych.53.100901.135239\n\n\nBrown, M., Grasley, N., & Guido, M. (2025). Do sports bettors\nneed consumer protection? Evidence from a field experiment. https://mattbrownecon.github.io/assets/papers/jmp/sportsbetting.pdf\n\n\nBryan, J., Hester, J., Robinson, D., Wickham, H., Dervieux, C., &\nPosit. (2025). Reprex do’s and don’ts. https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\n\n\nBulus, M. (2023). pwrss: Statistical\npower and sample size calculation tools. https://CRAN.R-project.org/package=pwrss\n\n\nBulus, M., Dong, N., Kelcey, B., & Spybrook, J. (2021).\nPowerUpR: Power analysis tools for multilevel\nrandomized experiments. https://doi.org/10.32614/CRAN.package.PowerUpR\n\n\nBürkner, P.-C. (2017). brms: An\nR package for Bayesian multilevel models using\nStan. Journal of Statistical Software,\n80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel\nmodeling with the R package brms. The R Journal, 10(1),\n395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2024). brms:\nBayesian regression models using Stan. https://github.com/paul-buerkner/brms\n\n\nCarl, S. (2024). nflplotR:\nNFL logo plots in ggplot2 and\ngt. https://nflplotr.nflverse.com\n\n\nCarl, S., & Baldwin, B. (2024). nflfastR: Functions to efficiently access\nNFL play by play data. https://www.nflfastr.com/\n\n\nChakravarthy, P. (2012). Optimizing draft strategies in fantasy\nfootball. https://harvardsportsanalysis.wordpress.com/wp-content/uploads/2012/04/fantasyfootballdraftanalysis1.pdf\n\n\nChampely, S. (2020). pwr: Basic\nfunctions for power analysis. https://github.com/heliosdrm/pwr\n\n\nChang, W. (2018). R graphics cookbook: Practical recipes for\nvisualizing data (2nd ed.). O’Reilly Media. https://r-graphics.org\n\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of\nthe American Statistical Association, 116(536), 2009–2022.\nhttps://doi.org/10.1080/01621459.2020.1758115\n\n\nChatterjee, S., & Holmes, S. (2023). XICOR: Robust\nand generalized correlation coefficients. https://CRAN.R-project.org/package=XICOR\n\n\nChekroud, A. (2017). nomogrammer: Fagan’s nomograms with ggplot2. https://github.com/achekroud/nomogrammer\n\n\nClark, K. (2018). The NFL’s analytics\nrevolution has arrived. https://www.theringer.com/2018/12/19/nfl/nfl-analytics-revolution\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral\nsciences (2nd ed.). Lawrence Erlbaum Associates, Publishers. https://doi.org/10.4324/9780203771587\n\n\nCongelio, B. J. (2023). Introduction to NFL analytics\nwith R. CRC Press. https://bradcongelio.com/nfl-analytics-with-r-book\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for\nWindows. Wiley-Blackwell.\n\n\nCritcher, C. R., & Rosenzweig, E. L. (2014). The performance\nheuristic: A misguided reliance on past success when predicting\nprospects for improvement. Journal of Experimental Psychology:\nGeneral, 143(2), 480–485. https://doi.org/10.1037/a0034129\n\n\nCsárdi, G., Hester, J., Wickham, H., Chang, W., Morgan, M., &\nTenenbaum, D. (2024). remotes:\nR package installation from remote repositories, including\nGitHub. https://remotes.r-lib.org\n\n\nD’Onofrio, B. M., Sjölander, A., Lahey, B. B., Lichtenstein, P., &\nÖberg, A. S. (2020). Accounting for confounding in observational\nstudies. Annual Review of Clinical Psychology, 16(1),\n25–48. https://doi.org/10.1146/annurev-clinpsy-032816-045030\n\n\nDana, J., & Thomas, R. (2006). In defense of clinical judgment … and\nmechanical prediction. Journal of Behavioral Decision Making,\n19(5), 413–428. https://doi.org/10.1002/bdm.537\n\n\nDawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus\nactuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573\n\n\nDelignette-Muller, M. L., & Dutang, C. (2015). fitdistrplus: An R package for\nfitting distributions. Journal of Statistical Software,\n64(4), 1–34. https://doi.org/10.18637/jss.v064.i04\n\n\nDelignette-Muller, M.-L., Dutang, C., & Siberchicot, A. (2025).\nfitdistrplus: Help to fit of a\nparametric distribution to non-censored or censored data. https://lbbe-software.github.io/fitdistrplus/\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., &\nMeijer, R. R. (2018). Selection procedures in sports:\nImproving predictions of athletes’ future performance.\nEuropean Journal of Sport Science, 18(9), 1191–1198.\nhttps://doi.org/10.1080/17461391.2018.1480662\n\n\nDigitale, J. C., Martin, J. N., & Glymour, M. M. (2022). Tutorial on\ndirected acyclic graphs. Journal of Clinical Epidemiology,\n142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001\n\n\nEastwell, P. (2014). Understanding hypotheses, predictions, laws, and\ntheories. Science Education Review, 13(1), 16–21. https://eric.ed.gov/?id=EJ1057150\n\n\nEddy, D. M. (1982). Probabilistic reasoning in clinical medicine:\nProblems and opportunities. In D. Kahneman, P. Slovic, & A. Tversky\n(Eds.), Judgment under uncertainty: Heuristics and biases (pp.\n249–267). Cambridge University Press. https://doi.org/10.1017/CBO9780511809477.019\n\n\nEnke, B. (2020). What you see is all there is. The Quarterly Journal\nof Economics, 135(3), 1363–1398. https://doi.org/10.1093/qje/qjaa012\n\n\nFantasy Sports & Gaming Association. (2023). Industry\ndemographics. https://thefsga.org/industry-demographics/\n\n\nFarrington, D. P., & Loeber, R. (1989). Relative improvement over\nchance (RIOC) and phi as measures of predictive efficiency\nand strength of association in 2×2 tables. Journal of Quantitative\nCriminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737\n\n\nFowler, J. (2015). Why the Steelers hired a\nCarnegie Mellon professor for advanced analytics. https://www.espn.com/blog/pittsburgh-steelers/post/_/id/14521/why-the-steelers-hired-a-carnegie-mellon-professor-for-advanced-analytics\n\n\nFox, L. (2021). How the NFL uses analytics, according\nto the lead analyst of a Super Bowl champion. https://www.forbes.com/sites/liamfox/2021/08/12/how-the-nfl-uses-analytics-according-to-the-lead-analyst-of-a-super-bowl-champion\n\n\nFraley, C., Raftery, A. E., & Scrucca, L. (2024). mclust: Gaussian mixture modelling for model-based\nclustering, classification, and density estimation. https://mclust-org.github.io/mclust/\n\n\nFree, H., Groenewold, M. R., & Luckhaupt, S. E. (2020). Lifetime\nprevalence of self-reported work-related health problems among\nUS workers—United States, 2018. MMWR.\nMorbidity and Mortality Weekly Report, 69(13), 361–365. https://doi.org/10.15585/mmwr.mm6913a1\n\n\nGabry, J., Češnovar, R., & Johnson, A. (2024). cmdstanr: R interface to\nCmdStan. https://mc-stan.org/cmdstanr/\n\n\nGandrud, C. (2020). Reproducible research with R and\nR studio (3rd ed.). CRC Press. https://www.routledge.com/Reproducible-Research-with-R-and-RStudio/Gandrud/p/book/9780367143985\n\n\nGarb, H. N., & Wood, J. M. (2019). Methodological advances in\nstatistical prediction. Psychological Assessment,\n31(12), 1456–1466. https://doi.org/10.1037/pas0000673\n\n\nGarnier, S. (2024). viridis:\nColorblind-friendly color maps for R. https://sjmgarnier.github.io/viridis/\n\n\nGarnier, S., Ross, N., Rudis, B., Sciaini, M., Camargo, A. P., &\nScherer, C. (2024). viridis(Lite) -\ncolorblind-friendly color maps for R. https://doi.org/10.5281/zenodo.4679423\n\n\nGet Up ESPN. (2021). @nfldraftscout on\nthe Cowboys’ interest in drafting Kyle\nPitts. https://x.com/GetUpESPN/status/1380165126108672001\n\n\nGetty, D., Li, H., Yano, M., Gao, C., & Hosoi, A. E. (2018). Luck\nand the law: Quantifying chance in fantasy sports and other contests.\nSIAM Review, 60(4), 869–887. https://doi.org/10.1137/16m1102094\n\n\nGilovich, T., Vallone, R., & Tversky, A. (1985). The hot hand in\nbasketball: On the misperception of random sequences. Cognitive\nPsychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6\n\n\nGoldschmied, N., Ratkovich, T., & Raphaeli, M. (in press). Brief\nreport: Exploring the icing the kicker strategy in the NFL. Journal\nof Applied Sport Psychology. https://doi.org/10.1080/10413200.2024.2437166\n\n\nGoldstein, J. (2013). Cat beats investors in stock market\nchallenge. https://www.npr.org/sections/money/2013/01/14/169326326/housecat-beats-investors-in-stock-market-challenge\n\n\nGonzalez Sanchez, A., Martinez, S., Yurko, R., Elmore, R., &\nMacdonald, B. (2024). Beyond the box score: Does icing the field goal\nkicker work in the NFL? CHANCE, 37(3), 41–48. https://doi.org/10.1080/09332480.2024.2415841\n\n\nGoodman, S. (2008). A dirty dozen: Twelve p-value\nmisconceptions. Seminars in Hematology, 45(3),\n135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nGoodman, Z. T., Casline, E., Jensen-Doss, A., Ehrenreich-May, J., &\nBainter, S. A. (2022). shinyDLRs: A\ndashboard to facilitate derivation of diagnostic likelihood ratios.\nPsychological Assessment, 34(6), 558–569. https://doi.org/10.1037/pas0001114\n\n\nGrove, W. M., & Meehl, P. E. (1996). Comparative efficiency of\ninformal (subjective, impressionistic) and formal (mechanical,\nalgorithmic) prediction procedures: The clinical–statistical\ncontroversy. Psychology, Public Policy, and Law, 2(2),\n293–323. https://doi.org/10.1037/1076-8971.2.2.293\n\n\nGrove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C.\n(2000). Clinical versus mechanical prediction: A meta-analysis.\nPsychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19\n\n\nGuo, J., Gabry, J., Goodrich, B., Johnson, A., Weber, S., & Badr, H.\nS. (2025). rstan: R\ninterface to Stan. https://mc-stan.org/rstan/\n\n\nHarrell, Jr., F. E. (2024). rms:\nRegression modeling strategies. https://hbiostat.org/R/rms/\n\n\nHarris, C. (2012). How to make VBD work for you.\nhttps://www.espn.com/fantasy/football/ffl/story?page=nfldk2k12_vbdwork\n\n\nHitchings, J. (2012). Moneyball: Using modern portfolio theory to\nwin your fantasy sports league. https://eng.wealthfront.com/2012/01/17/moneyball-using-modern-portfolio-theory-to-win-your-fantasy-sports-league\n\n\nHo, T., & Carl, S. (2024). nflreadr:\nDownload nflverse data. https://nflreadr.nflverse.com\n\n\nHo, T., & Carl, S. (2025a). Articles. https://nflreadr.nflverse.com/articles/index.html\n\n\nHo, T., & Carl, S. (2025b). Data dictionary - combine. https://nflreadr.nflverse.com/articles/dictionary_combine.html\n\n\nHo, T., & Carl, S. (2025c). Data dictionary - contracts. https://nflreadr.nflverse.com/articles/dictionary_contracts.html\n\n\nHo, T., & Carl, S. (2025d). Data dictionary - depth charts.\nhttps://nflreadr.nflverse.com/articles/dictionary_depth_charts.html\n\n\nHo, T., & Carl, S. (2025e). Data dictionary - draft picks.\nhttps://nflreadr.nflverse.com/articles/dictionary_draft_picks.html\n\n\nHo, T., & Carl, S. (2025f). Data dictionary - ESPN\nQBR. https://nflreadr.nflverse.com/articles/dictionary_espn_qbr.html\n\n\nHo, T., & Carl, S. (2025g). Data dictionary - FF\nopportunity. https://nflreadr.nflverse.com/articles/dictionary_ff_opportunity.html\n\n\nHo, T., & Carl, S. (2025h). Data dictionary - FF\nplayer IDs. https://nflreadr.nflverse.com/articles/dictionary_ff_playerids.html\n\n\nHo, T., & Carl, S. (2025i). Data dictionary - FF\nrankings. https://nflreadr.nflverse.com/articles/dictionary_ff_rankings.html\n\n\nHo, T., & Carl, S. (2025j). Data dictionary - FTN\ncharting. https://nflreadr.nflverse.com/articles/dictionary_ftn_charting.html\n\n\nHo, T., & Carl, S. (2025k). Data dictionary - injuries. https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n\n\nHo, T., & Carl, S. (2025l). Data dictionary - next gen\nstats. https://nflreadr.nflverse.com/articles/dictionary_nextgen_stats.html\n\n\nHo, T., & Carl, S. (2025m). Data dictionary -\nparticipation. https://nflreadr.nflverse.com/articles/dictionary_participation.html\n\n\nHo, T., & Carl, S. (2025n). Data dictionary -\nPBP. https://nflreadr.nflverse.com/articles/dictionary_pbp.html\n\n\nHo, T., & Carl, S. (2025o). Data dictionary - PFR passing.\nhttps://nflreadr.nflverse.com/articles/dictionary_pfr_passing.html\n\n\nHo, T., & Carl, S. (2025p). Data dictionary - player stats.\nhttps://nflreadr.nflverse.com/articles/dictionary_player_stats.html\n\n\nHo, T., & Carl, S. (2025q). Data dictionary - player stats\ndefense. https://nflreadr.nflverse.com/articles/dictionary_player_stats_def.html\n\n\nHo, T., & Carl, S. (2025r). Data dictionary - rosters. https://nflreadr.nflverse.com/articles/dictionary_rosters.html\n\n\nHo, T., & Carl, S. (2025s). Data dictionary - schedules. https://nflreadr.nflverse.com/articles/dictionary_schedules.html\n\n\nHo, T., & Carl, S. (2025t). Data dictionary - snap counts.\nhttps://nflreadr.nflverse.com/articles/dictionary_snap_counts.html\n\n\nHoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting\npersonal events. Journal of Experimental Psychology: Learning,\nMemory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719\n\n\nHolmes, S., & Chatterjee, S. (2023). XICOR:\nAssociation measurement through cross rank increments. https://CRAN.R-project.org/package=XICOR\n\n\nHopper, T. (2014). Can we do better than r-squared? https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared\n\n\nHough, S. E. (2016). Predicting the unpredictable: The tumultuous\nscience of earthquake prediction. Princeton University Press.\n\n\nHyndman, R. J. (2014). Alternative to MAPE when the\ndata is not a time series. https://stats.stackexchange.com/a/108963/20338\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting:\nPrinciples and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nHyndman, R. J., Athanasopoulos, G., Bergmeir, C., Caceres, G., Chhay,\nL., Kuroptev, K., O’Hara-Wild, M., Petropoulos, F., Razbash, S., Wang,\nE., & Yasmeen, F. (2024). forecast:\nForecasting functions for time series and linear models. https://pkg.robjhyndman.com/forecast/\n\n\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series\nforecasting: The forecast package for\nR. Journal of Statistical Software,\n27(3), 1–22. https://doi.org/10.18637/jss.v027.i03\n\n\nIacobucci, D., Schneider, M. J., Popovich, D. L., & Bakamitsos, G.\nA. (2016). Mean centering helps alleviate “micro” but not\n“macro” multicollinearity. Behavior Research\nMethods, 48(4), 1308–1317. https://doi.org/10.3758/s13428-015-0624-x\n\n\nInternational Society of Genetic Genealogy. (2022). Autosomal DNA\nstatistics. https://isogg.org/wiki/Autosomal_DNA_statistics\n\n\nJackson-Wood, M. (2017). statistical test\nflowchart. https://www.statsflowchart.co.uk\n\n\nJak, S., Jorgensen, T. D., Verdam, M. G. E., Oort, F. J., & Elffers,\nL. (2020). Analytical power calculations for structural equation\nmodeling: A tutorial and shiny app. Behavior Research Methods.\nhttps://doi.org/10.3758/s13428-020-01479-0\n\n\nJohnson, J. E. V., & Bruce, A. C. (2001). Calibration of subjective\nprobability judgments in a naturalistic setting. Organizational\nBehavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949\n\n\nJones, J. M. (2024). Football retains dominant position as favorite\nU.S. sport. https://news.gallup.com/poll/610046/football-retains-dominant-position-favorite-sport.aspx\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar,\nStraus, and Giroux.\n\n\nKassambara, A. (2017). Practical guide to cluster analysis in\nR: Unsupervised machine learning (Vol.\n1). Sthda.\n\n\nKeren, G. (1987). Facing uncertainty in the game of bridge: A\ncalibration study. Organizational Behavior and Human Decision\nProcesses, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1\n\n\nKessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., &\nZubizarreta, J. R. (2020). Suicide prediction models: A critical review\nof recent research with recommendations for the way forward.\nMolecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0\n\n\nKievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013).\nSimpson’s paradox in psychological science: A practical guide.\nFrontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513\n\n\nKilin, I. (2022). The best charts for color blind viewers. https://www.datylon.com/blog/data-visualization-for-colorblind-readers\n\n\nKoehler, D. J., Brenner, L., & Griffin, D. (2002). The calibration\nof expert judgment: Heuristics and biases beyond the laboratory. In T.\nGilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics and\nbiases: The psychology of intuitive judgment. Cambridge University\nPress. https://doi.org/10.1017/CBO9780511808098.041\n\n\nKoriat, A., Lichtenstein, S., & Fischhoff, B. (1980). Reasons for\nconfidence. Journal of Experimental Psychology: Human Learning and\nMemory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107\n\n\nKotrba, V. (2020). Heuristics in fantasy sports: Is it profitable to\nstrategize based on favourite of the match? Mind & Society,\n19(1), 195–206. https://doi.org/10.1007/s11299-020-00231-7\n\n\nKuznetsova, A., Bruun Brockhoff, P., & Haubo Bojesen Christensen, R.\n(2020). lmerTest: Tests in linear mixed\neffects models. https://github.com/runehaubo/lmerTestR\n\n\nLarrick, R. P., Mannes, A. E., & Soll, J. B. (2024). The social\npsychology of the wisdom of crowds (with a new section on recent\nadvances). In F. M. Federspiel, G. Montibeller, & M. Seifert (Eds.),\nBehavioral decision analysis (pp. 121–143). Springer. https://doi.org/10.1007/978-3-031-44424-1_7\n\n\nLederer, D. J., Bell, S. C., Branson, R. D., Chalmers, J. D., Marshall,\nR., Maslove, D. M., Ost, D. E., Punjabi, N. M., Schatz, M., Smyth, A.\nR., Stewart, P. W., Suissa, S., Adjei, A. A., Akdis, C. A., Azoulay, É.,\nBakker, J., Ballas, Z. K., Bardin, P. G., Barreiro, E., … Vincent, J.-L.\n(2019). Control of confounding and reporting of results in causal\ninference studies. Guidance for authors from editors of respiratory,\nsleep, and critical care journals. Annals of the American Thoracic\nSociety, 16(1), 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS\n\n\nLee, M. D., & Liu, S. (2022). Drafting strategies in fantasy\nfootball: A study of competitive sequential human decision making.\nJudgment and Decision Making, 17(4), 691–719. https://doi.org/10.1017/S1930297500008901\n\n\nLenth, R. V. (2025). emmeans: Estimated\nmarginal means, aka least-squares\nmeans. https://rvlenth.github.io/emmeans/\n\n\nLewis, M. (2009). The no-stats all-star. https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html\n\n\nLilienfeld, S. O. (2007). Psychological treatments that cause harm.\nPerspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x\n\n\nLindhiem, O., Petersen, I. T., Mentch, L. K., & Youngstrom, E. A.\n(2020). The importance of calibration in clinical psychology.\nAssessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055\n\n\nLong, J. D., & Teetor, P. (2019). R cookbook: Proven recipes for\ndata analysis, statistics, and graphics (2nd ed.). O’Reilly Media.\nhttps://rc2e.com\n\n\nLy, N. (2015). The rules of American football -\nEXPLAINED! (NFL). https://www.youtube.com/watch?v=Ddwp1HyEFRE\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J.\n(2011). On the predictive efficiency of past performance and physical\nability: The case of the National Football League.\nHuman Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218\n\n\nMagnusson, K. (2013). Creating a typical textbook illustration of\nstatistical power using either ggplot or\nbase graphics. https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics\n\n\nMagnusson, K. (2014). Understanding statistical power and\nsignificance testing. https://rpsychologist.com/d3/nhst/\n\n\nMagnusson, K. (2015). Distribution of p-values\nwhen comparing two groups. https://rpsychologist.com/d3/pdist\n\n\nMagnusson, K. (2020). Interpreting correlations. https://rpsychologist.com/correlation\n\n\nMagnusson, K. (2021). Understanding p-values\nthrough simulations. https://rpsychologist.com/pvalue\n\n\nMakridakis, S., Hogarth, R. M., & Gaba, A. (2009). Forecasting and\nuncertainty in the economic and business world. International\nJournal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012\n\n\nMannes, A. E., Soll, J. B., & Larrick, R. P. (2014). The wisdom of\nselect crowds. Journal of Personality and Social Psychology,\n107(2), 276–299. https://doi.org/10.1037/a0036677\n\n\nMassey, C., & Thaler, R. H. (2013). The loser’s curse: Decision\nmaking and market efficiency in the National Football\nLeague draft. Management Science, 59(7),\n1479–1495. https://doi.org/10.1287/mnsc.1120.1657\n\n\nMathieu, J. E., Aguinis, H., Culpepper, S. A., & Chen, G. (2012).\nUnderstanding and estimating the power to detect cross-level interaction\neffects in multilevel modeling. Journal of Applied Psychology,\n97(5), 951–966. https://doi.org/10.1037/a0028380\n\n\nMazerolle, M. J. (2025). AICcmodavg: Model selection\nand multimodel inference based on (Q)AIC(c). https://CRAN.R-project.org/package=AICcmodavg\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree:\nThe case of r and d. Psychological Methods,\n11(4), 386–401. https://doi.org/10.1037/1082-989X.11.4.386\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula?\nJournal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks:\nSir Karl, Sir\nRonald, and the slow progress of soft psychology.\nJournal of Consulting and Clinical Psychology, 46(4),\n806–834. https://doi.org/10.1037/0022-006x.46.4.806\n\n\nMeehl, P. E. (1986). Causes and effects of my disturbing little book.\nJournal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6\n\n\nMeehl, P. E., & Rosen, A. (1955). Antecedent probability and the\nefficiency of psychometric signs, patterns, or cutting scores.\nPsychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070\n\n\nMiller, J. B., & Sanjurjo, A. (2014). A cold shower for the hot hand\nfallacy. Innocenzo Gasparini Institute for Economic Research.\nhttps://repec.unibocconi.it/igier/igi/wp/2014/518.pdf\n\n\nMiller, J. B., & Sanjurjo, A. (2024). A cold shower for the hot hand\nfallacy: Robust evidence from controlled settings. The Review of\nEconomics and Statistics, 106(6), 1607–1619. https://doi.org/10.1162/rest_a_01280\n\n\nMiller, R. M. (2013). Cognitive bias in fantasy sports: Is your\nbrain sabotaging your team? Xlibris Press.\n\n\nMlodinow, L. (2008). The drunkard’s walk: How randomness rules our\nlives. Pantheon Books.\n\n\nMoore, D. A., & Healy, P. J. (2008). The trouble with\noverconfidence. Psychological Review, 115(2), 502–517.\nhttps://doi.org/10.1037/0033-295X.115.2.502\n\n\nMorley, S. K., Brito, T. V., & Welling, D. T. (2018). Measures of\nmodel performance based on the log accuracy ratio. Space\nWeather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669\n\n\nMoshagen, M., & Bader, M. (2024). semPower: General power analysis\nfor structural equation models. Behavior Research Methods,\n56(4), 2901–2922. https://doi.org/10.3758/s13428-023-02254-7\n\n\nMoskowitz, T. J., & Wertheim, L. J. (2011). Scorecasting: The\nhidden influences behind how sports are played and games are won.\nThree Rivers Press.\n\n\nMotz, B. (2013). Fantasy football: A touchdown for undergraduate\nstatistics education. Proceedings of the Games, Learning, and\nSociety Conference, 9.0, 222–228. https://doi.org/10.1184/R1/6686804.v1\n\n\nMurayama, K., Usami, S., & Sakaki, M. (2022).\nSummary-statistics-based power analysis: A new and practical method to\ndetermine sample size for mixed-effects modeling. Psychological\nMethods, 27(6), 1014–1038. https://doi.org/10.1037/met0000330\n\n\nMurphy, A. H., & Winkler, R. L. (1984). Probability forecasting in\nmeterology. Journal of the American Statistical Association,\n79(387), 489–500. https://doi.org/10.2307/2288395\n\n\nNFL Fantasy Football. (2020). How to play fantasy football for\nBEGINNERS. https://www.youtube.com/watch?v=XhrBapdhLEc\n\n\nNFL Films Presents. (2014). Playing fantasy football for college\ncredit?? Welcome to C105 - Prediction, Probability,\n& Pigskin. https://www.facebook.com/watch/?v=10155572257183615\n\n\nNiles, B. (2022). Do players perform better in fantasy football in a\ncontract year? https://www.4for4.com/2022/preseason/do-players-perform-better-fantasy-football-contract-year\n\n\nNivison, A. (2021). Florida TE Kyle\nPitts draws comparison to Lebron James. https://247sports.com/article/kyle-pitts-lebron-james-2021-nfl-draft-florida-gators-football-163882176\n\n\nNuñez, J. R., Anderton, C. R., & Renslow, R. S. (2018). Optimizing\ncolormaps with consideration for color vision deficiency to enable\naccurate interpretation of scientific data. PLOS ONE,\n13(7), e0199239. https://doi.org/10.1371/journal.pone.0199239\n\n\nNYT 4th Down Bot. (2014). 4th down: When to go for it and why.\nhttps://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html\n\n\nOskamp, S. (1965). Overconfidence in case-study judgments. Journal\nof Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125\n\n\nPartnow, S. (2021). The midrange theory: Basketball’s evolution in\nthe age of analytics. Triumph Books.\n\n\nPelechrinis, K., & Winston, W. (2022). The hot hand in the wild.\nPLOS ONE, 17(1), e0261890. https://doi.org/10.1371/journal.pone.0261890\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With\napplied examples in R. Chapman and\nHall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A\ncollection of R functions by the Petersen\nLab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment:\nWith applied examples in R. University of Iowa\nLibraries. https://doi.org/10.25820/work.007199\n\n\nPress, T. A. (2008). Janikowski gives Raiders win over\nJets in overtime. https://www.nfl.com/news/janikowski-gives-raiders-win-over-jets-in-overtime-09000d5d80bc3910\n\n\nPro Football Reference. (2024). 2024 NFL advanced stats. https://www.pro-football-reference.com/years/2024/advanced.htm\n\n\nPro Football Reference. (2025). About our advanced stats. https://www.pro-football-reference.com/about/advanced_stats.htm\n\n\nR Core Team. (2024). R: A language and environment for\nstatistical computing. R Foundation for Statistical\nComputing. https://www.R-project.org\n\n\nRader, C. A., Larrick, R. P., & Soll, J. B. (2017). Advice as a form\nof social influence: Informational motives and the consequences for\naccuracy. Social and Personality Psychology Compass,\n11(8), e12329. https://doi.org/10.1111/spc3.12329\n\n\nReed, T. (2016). In an NFL divided over analytics,\nCleveland Browns look to make numbers add up in their\nfavor. https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html\n\n\nRice, M. E., Harris, G. T., & Lang, C. (2013). Validation of and\nrevision to the VRAG and SORAG: The\nViolence Risk Appraisal Guide—Revised\n(VRAG-R). Psychological Assessment,\n25(3), 951–965. https://doi.org/10.1037/a0032878\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez,\nJ.-C., & Müller, M. (2011). pROC: An\nopen-source package for R and S+ to analyze\nand compare ROC curves. BMC Bioinformatics,\n12, 77. https://doi.org/10.1186/1471-2105-12-77\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez,\nJ.-C., & Müller, M. (2023). pROC:\nDisplay and analyze ROC curves. https://xrobin.github.io/pROC/\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation:\nGraphical causal models for observational data.\nAdvances in Methods and Practices in Psychological Science,\n1(1), 27–42. https://doi.org/10.1177/2515245917745629\n\n\nRomer, D. (2006). Do firms maximize? Evidence from professional\nfootball. Journal of Political Economy, 114(2),\n340–365. https://doi.org/10.1086/501171\n\n\nRosalsky, G. (2023). Should we invest more in weather forecasting?\nIt may save your life. https://www.npr.org/sections/money/2023/07/11/1186458991/should-we-invest-more-in-weather-forecasting-it-may-save-your-life\n\n\nRosenthal, G. (2018). Super Bowl LII: How the 2017\nPhiladelphia Eagles were built. https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753\n\n\nRusso, J. E., & Schoemaker, P. J. (1992). Managing overconfidence.\nSloan Management Review, 33(2), 7.\n\n\nRyan, J. (2013). Beating the NBA draft: Does any team\noutperform expectations? https://harvardsportsanalysis.org/2013/11/beating-the-nba-draft-does-any-team-outperform-expectations\n\n\nRyan, J. A., & Ulrich, J. M. (2024). quantmod: Quantitative financial modelling\nframework. https://www.quantmod.com/\n\n\nSalmon, M. (2018). Where to get help with your R\nquestion? https://masalmon.eu/2018/07/22/wheretogethelp/\n\n\nSchalter, T. (2022). The NFL preseason is not\npredictive — but it can often seem that way. https://fivethirtyeight.com/features/the-nfl-preseason-is-not-predictive-but-it-can-often-seem-that-way\n\n\nScherer, C. (2021). Beyond bar and box plots. https://z3tt.github.io/beyond-bar-and-box-plots\n\n\nSchoemann, A. M., Boulton, A. J., & Short, S. D. (2017). Determining\npower and sample size for simple and complex mediation models.\nSocial Psychological and Personality Science, 8(4),\n379–386. https://doi.org/10.1177/1948550617715068\n\n\nSchwabish, J. (2021). Better data visualizations: A guide for\nscholars, researchers, and wonks. Columbia University Press. https://doi.org/10.7312/schw19310\n\n\nSchwartz, A. (2006). Diagnostic test calculator. http://araw.mede.uic.edu/cgi-bin/testcalc.pl\n\n\nScrucca, L., Fraley, C., Murphy, T. B., & Raftery, A. E. (2023).\nModel-based clustering, classification, and density estimation using\nmclust in R. Chapman;\nHall/CRC. https://doi.org/10.1201/9781003277965\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002).\nExperimental and quasi-experimental designs for generalized causal\ninference. Houghton Mifflin.\n\n\nSharpe, L. (2020a). NFL data sets. https://github.com/nflverse/nfldata/blob/master/DATASETS.md\n\n\nSharpe, L. (2020b). NFL data sets - draft values.\nhttps://github.com/nflverse/nfldata/blob/master/DATASETS.md#draft_values\n\n\nSharpe, L. (2020c). NFL data sets - rosters. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#rosters\n\n\nSharpe, L. (2020d). NFL data sets - standings. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#standings\n\n\nSharpe, L. (2020e). NFL data sets - trades. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#trades\n\n\nSherman, A., & Goldner, K. (2021). Sharpstack: Cholesky\ncorrelations for building better lineups. https://assets-global.website-files.com/5f1af76ed86d6771ad48324b/607a4434a565aa7763bd1312_AndyAsh-Sharpstack-RPpaper.pdf\n\n\nSievert, C. (2020). Interactive web-based data visualization with\nR, plotly, and shiny. Chapman; Hall/CRC. https://plotly-r.com\n\n\nSievert, C., Parmer, C., Hocking, T., Chamberlain, S., Ram, K.,\nCorvellec, M., & Despouy, P. (2024). plotly: Create interactive web graphics via plotly.js. https://plotly-r.com\n\n\nSignorell, A. (2025). DescTools: Tools for descriptive\nstatistics. https://andrisignorell.github.io/DescTools/\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions\nfail–but some don’t. Penguin.\n\n\nSimoiu, C., Sumanth, C., Mysore, A., & Goel, S. (2019). Studying the\n\"wisdom of crowds\" at scale. Proceedings of the AAAI Conference on\nHuman Computation and Crowdsourcing, 7(1), 171–179. https://doi.org/10.1609/hcomp.v7i1.5271\n\n\nSkala, D. (2008). Overconfidence in psychology and finance–an\ninterdisciplinary literature review. Bank i Kredyt, 4,\n33–50.\n\n\nSmith, B., Sharma, P., & Hooper, P. (2006). Decision making in\nonline fantasy sports communities. Interactive Technology and Smart\nEducation, 3(4), 347–360. https://doi.org/10.1108/17415650680000072\n\n\nSmith, G. (2016). The Sports Illustrated cover\njinx. https://www.psychologytoday.com/us/blog/what-the-luck/201610/the-sports-illustrated-cover-jinx\n\n\nSpector, P. E., & Brannick, M. T. (2010). Methodological urban\nlegends: The misuse of statistical control variables.\nOrganizational Research Methods, 14(2), 287–305. https://doi.org/10.1177/1094428110369842\n\n\nSpinu, V., Grolemund, G., & Wickham, H. (2024). lubridate: Make dealing with dates a little\neasier. https://lubridate.tidyverse.org\n\n\nStack Overflow. (2018). How to make a great R\nreproducible example. https://stackoverflow.com/a/5963610\n\n\nStack Overflow. (2025). How to create a minimal, reproducible\nexample. https://stackoverflow.com/help/minimal-reproducible-example\n\n\nstatistica. (2023a). Fantasy sports in the U.S.-\nstatistics & facts. https://www.statista.com/topics/10895/fantasy-sports-in-the-us/\n\n\nstatistica. (2023b). Most watched sports leagues in the United\nStates in 2023, by minutes watched. https://www.statista.com/statistics/1430289/most-watched-sports-leagues-usa/\n\n\nStevens, R. J., & Poppe, K. K. (2020). Validation of clinical\nprediction models: What does the “calibration slope” really\nmeasure? Journal of Clinical Epidemiology, 118, 93–99.\nhttps://doi.org/10.1016/j.jclinepi.2019.09.016\n\n\nSteyerberg, E. W., & Vergouwe, Y. (2014). Towards better clinical\nprediction models: Seven steps for development and an ABCD for\nvalidation. European Heart Journal, 35(29), 1925–1931.\nhttps://doi.org/10.1093/eurheartj/ehu207\n\n\nStrauss, M. E., & Smith, G. T. (2009). Construct validity: Advances\nin theory and methodology. Annual Review of Clinical\nPsychology, 5(1), 1–25. https://doi.org/10.1146/annurev.clinpsy.032408.153639\n\n\nSurowiecki, J. (2005). The wisdom of crowds. Anchor Books.\n\n\nTetlock, P. E. (2017). Expert political judgment: How good is it?\nHow can we know? - New edition. Princeton University\nPress.\n\n\nTextor, J., van der Zander, B., Gilthorpe, M. S., Liśkiewicz, M., &\nEllison, G. T. (2016). Robust causal inference using directed acyclic\ngraphs: The r package ’dagitty’.\nInternational Journal of Epidemiology, 45(6),\n1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nTofallis, C. (2015). A better measure of relative prediction accuracy\nfor model selection and model estimation. Journal of the Operational\nResearch Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103\n\n\nTotalProSports.com. (2017). 10 most ridiculous things ever said by\nStephen A. Smith or Skip Bayless. https://www.youtube.com/watch?v=lTjBuEPcLlc\n\n\nTreat, T. A., & Viken, R. J. (2023). Measuring test performance with\nsignal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M.\nMcMullen, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA\nhandbook of research methods in psychology: Foundations, planning,\nmeasures, and psychometrics (2nd ed., Vol. 1, pp. 837–858).\nAmerican Psychological Association. https://doi.org/10.1037/0000318-038\n\n\nTufte, E. R. (2001). The visual display of quantitative\ninformation. Graphics Press.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty:\nHeuristics and biases. Science, 185(4157), 1124–1131.\nhttps://doi.org/10.1126/science.185.4157.1124\n\n\nUnderwood, A. (2019). 15 ways analytics has changed sports. https://stacker.com/stories/sports/15-ways-analytics-has-changed-sports\n\n\nUrsenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D.,\nKosteniuk, J., & Spiteri, R. J. (2019). Scoring algorithms for a\ncomputer-based cognitive screening tool: An illustrative example of\noverfitting machine learning approaches and the impact on estimates of\nclassification accuracy. Psychological Assessment,\n31(11), 1377–1382. https://doi.org/10.1037/pas0000764\n\n\nWagner, C., & Vinaimont, T. (2010). Evaluating the wisdom of crowds.\nIssues in Information Systems, 11(1), 724–732. http://iacis.org/iis/2010/724-732_LV2010_1546.pdf\n\n\nWalder, S. (2020). 2020 NFL analytics survey: Which\nteams are most, least analytically inclined? https://www.espn.com/nfl/story/_/id/29939438/2020-nfl-analytics-survey-which-teams-most-least-analytically-inclined\n\n\nWang, Y. A., & Rhemtulla, M. (2021). Power analysis for parameter\nestimation in structural equation modeling: A discussion and tutorial.\nAdvances in Methods and Practices in Psychological Science,\n4(1), 1–17. https://doi.org/10.1177/2515245920918253\n\n\nWetzels, R., Tutschkow, D., Dolan, C., Sluis, S. van der, Dutilh, G.,\n& Wagenmakers, E.-J. (2016). A Bayesian test for the\nhot hand phenomenon. Journal of Mathematical Psychology,\n72, 200–209. https://doi.org/10.1016/j.jmp.2015.12.003\n\n\nWhite, M. H., & Sheldon, K. M. (2014). The contract year syndrome in\nthe NBA and MLB: A classic\nundermining pattern. Motivation and Emotion, 38(2),\n196–205. https://doi.org/10.1007/s11031-013-9389-7\n\n\nWickham, H. (2023). tidyverse: Easily\ninstall and load the Tidyverse. https://tidyverse.tidyverse.org\n\n\nWickham, H. (2024). ggplot2: Elegant graphics for data analysis\n(3rd ed.). Springer. https://ggplot2-book.org\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D.,\nFrançois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M.,\nPedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J.,\nRobinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to\nthe tidyverse. Journal of Open Source\nSoftware, 4(43), 1686. https://doi.org/10.21105/joss.01686\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K.,\nWilke, C., Woo, K., Yutani, H., Dunnington, D., & van den Brand, T.\n(2024). ggplot2: Create elegant data\nvisualisations using the grammar of graphics. https://ggplot2.tidyverse.org\n\n\nWilke, C. O. (2024). ggridges: Ridgeline\nplots in ggplot2. https://wilkelab.org/ggridges/\n\n\nWilke, C. O., & Wiernik, B. M. (2022). ggtext: Improved text rendering support for ggplot2. https://wilkelab.org/ggtext/\n\n\nWilliams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., &\nSakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific\nreview of evidential value. Clinical Psychology: Science and\nPractice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331\n\n\nWood, S. N. (2017). Generalized additive models: An introduction\nwith R (2nd ed.). CRC press. https://doi.org/10.1201/9781315370279\n\n\nWood, S. N. (2023). mgcv: Mixed\nGAM computation vehicle with automatic smoothness\nestimation. https://doi.org/10.32614/CRAN.package.mgcv\n\n\nWoodland, L. M., & Woodland, B. M. (2015). The National\nFootball League season wins total betting market: The impact of\nheuristics on behavior. Southern Economic Journal,\n82(1), 38–54. https://doi.org/10.4284/0038-4038-2013.145\n\n\nWuertz, D., Setz, T., Chalabi, Y., & Theussl, S. (2023). fPortfolio: Rmetrics - portfolio\nselection and optimization. https://r-forge.r-project.org/projects/rmetrics/\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical\ncontrol requires causal justification. Advances in Methods and\nPractices in Psychological Science, 5(2),\n25152459221095823. https://doi.org/10.1177/25152459221095823\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R\nMarkdown cookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2024). R markdown\ncookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook\n\n\nYahoo! Sports. (2024). How cognitive bias affects your fantasy draft\nstrategy with neuroscience professor Dr. Renee Miller.\nhttps://www.youtube.com/watch?v=gmpLFWs5ae0\n\n\nYutani, H. (2023). gghighlight:\nHighlight lines and points in ggplot2.\nhttps://yutannihilation.github.io/gghighlight/\n\n\nZhang, Z., & Yuan, K.-H. (2018). Practical statistical power\nanalysis using WebPower and R. ISDSA\nPress. https://doi.org/10.35566/power",
    "crumbs": [
      "References"
    ]
  }
]