[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "",
    "text": "Preface\nThis is a book in progress—it is incomplete. I will continue to add to and update it as I am able.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-contribute",
    "href": "index.html#sec-contribute",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "How to Contribute",
    "text": "How to Contribute\nThis is an open-access textbook. My goal is to share data analysis strategies for free! Anyone is welcome to contribute to the project. If you would like to contribute, please consider one of the following:\n\n\nopen an issue or create a pull request on the book’s GitHub repository.\n\nbuy me a coffee—Support me in developing this (free!) resource for fantasy football analytics… Even a cup of coffee helps me stay awake!\n\n(or use PayPal)\n\n\n\nYou can also sponsor my open source work using GitHub Sponsors.\n\n\nThe GitHub repository for the book is located here: https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook. If you have data or analysis examples that you are willing to share and include in the book, feel free to contact me.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-openAccess",
    "href": "index.html#sec-openAccess",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Open Access",
    "text": "Open Access\nThis is an open-access book. This means that it is freely available for anyone to access.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-license",
    "href": "index.html#sec-license",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "License",
    "text": "License\n\nThe online version of this book is licensed under the Creative Commons Attribution License. In short, you can use my work as long as you cite it.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-citation",
    "href": "index.html#sec-citation",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Citation",
    "text": "Citation\nThe APA-style citation for the book is:\nPetersen, I. T. (2025). Fantasy football analytics: Statistics, prediction, and empiricism using R. Version 0.0.1. University of Iowa Libraries. https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook. [INSERT DOI LINK]\n\nThe BibTeX citation for the book is:\n\nCode@book{petersenFantasyFootballAnalytics,\n  title = {Fantasy football analytics: Statistics, prediction, and empiricism using {R}},\n  author = {Petersen, Isaac T.},\n  year = {2025},\n  publisher = {{University of Iowa Libraries}},\n  note = {Version 0.0.1},\n  doi = {INSERT},\n  isbn = {INSERT},\n  url = {https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-author",
    "href": "index.html#sec-author",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "About the Author",
    "text": "About the Author\nI am an Associate Professor in the Department of Psychological and Brain Sciences at the University of Iowa. I am a licensed psychologist with expertise in child clinical psychology. Why am I writing about fantasy football and data analysis? Because fantasy football involves the intersection of two things I love: sports and statistics.\nThrough my training, I have learned the value of statistics for answering important questions that I find interesting. In graduate training, I came to the realization that statistics is relevant not only for psychology and science, but also for domains that I enjoy as hobbies, including sports and fantasy sports. I have played in a longstanding fantasy football league for over 20 years (since my junior year of high school) with old friends from high school. I wanted to apply what I was learning about statistics to help others improve their performance in fantasy football and to help people—including those who might not otherwise be interested—to learn statistics. So I began blogging online about the value of applying statistics to improve decision making in fantasy football. Apparently, many people were interested in learning statistics when they could apply them to a domain that they find interesting, like fantasy football. My blog eventually became FantasyFootballAnalytics.net, a website that uses advanced statistics to help people win their fantasy football leagues.\nIn terms of my R and statistics background, I have published many peer-reviewed publications that employ advanced statistical methods, have published a book on psychological assessment (Petersen, 2024, 2025b) that includes applied examples in R, and have published the petersenlab R package (Petersen, 2025a) on the Comprehensive R Archive Network (CRAN). Several sections in this book come from Petersen (2025b). I am also a co-author of the ffanalytics R package (Tungate et al., 2025) that provides free utilities for downloading fantasy football projections and additional fantasy-relevant data, and for calculating projected points given your league settings.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-accessibility",
    "href": "index.html#sec-accessibility",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Accessibility",
    "text": "Accessibility\nI strive to follow principles of accessibility [Xie et al. (2024); Xie et al. (2020); archived at https://perma.cc/8XJ9-Q6QJ] to make the book content accessible to people with visual impairments and physical disabilities. If there are additional ways I can make the content more accessible, please let me know.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-exampleSyllabus",
    "href": "index.html#sec-exampleSyllabus",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Example Syllabus",
    "text": "Example Syllabus\nIf you are interested in teaching a course using this textbook, here is an example syllabus based on my class: https://isaactpetersen.github.io/psy-3170-2025-fall/syllabus.html",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-acknowledgments",
    "href": "index.html#sec-acknowledgments",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book was supported by an OpenHawks Open Educational Resources grant program from the University of Iowa Libraries.\nThis work would not be possible without the important contributions of many people. This book was made possible by the hard work of the package developers of the nflverse ecosystem of R packages, who made football data available, including Sebastian Carl, Ben Baldwin, Lee Sharpe, Tan Ho, and John Edwards. I thank Dr. Benjamin Motz, who provided consultation and many helpful resources based on his fantasy football statistics class. I also thank key members of FantasyFootballAnalytics.net, including Val Pinskiy, Andrew Tungate, Dennis Andersen, Adam Peterson, Jesse Kartes, and Chris Russo, who helped develop and provide fantasy football-related resources and who helped sharpen my thinking about the topic. In particular, Andrew Tungate and Dennis Andersen helped develop the ffanalytics R package (Tungate et al., 2025), which scrapes fantasy football projections from the web and calculates projections averaged across sources. Andrew Tungate also made important additions to the ffanalytics R package (Tungate et al., 2025) to allow computing fantasy points given projected (or actual) player statistics, which was necessary for conducting a bootstrap simulation of players’ projected fantasy points. I am fortunate to work with such a talented, passionate, and creative team. I also thank Professor Patrick Carroll, who taught me the value of statistics for answering important questions. In addition, a special thanks to those who contributed to the book via comments, suggestions, and pull requests, including George Ellis (@MargareeMan). Finally, I thank my wife who, in addition to designing the book cover, is a constant source of love and support.\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nTungate, A., Andersen, D., & Petersen, I. T. (2025). ffanalytics: Scrape data for fantasy football. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R Markdown cookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2024). R markdown cookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 About this Book\nHow can we use information to make predictions about uncertain events? This book is about empiricism (basing theories on observed data) and judgment, prediction, and decision making in the context of uncertainty. The book provides an introduction to modern analytical techniques used to make informed predictions, test theories, and draw conclusions from a given dataset. The book leverages the software R for providing applied data analysis examples.\nThis book was originally written for an undergraduate-level course entitled, “Fantasy Football: Predictive Analytics and Empiricism”. The chapters provide an overview of topics—each of which could have its own class and textbook, such as causal inference, factor analysis, cluster analysis, principal component analysis, machine learning, cognitive biases, modern portfolio theory, data visualization, simulation, etc. The book gives readers an overview of the breadth of the approaches to prediction and empiricism. As a consequence, the book does not cover any one technique or approach in great depth.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whatIsFantasyFootball",
    "href": "intro.html#sec-whatIsFantasyFootball",
    "title": "1  Introduction",
    "section": "\n1.2 What is Fantasy Football?",
    "text": "1.2 What is Fantasy Football?\nFantasy football is an online game where participants assemble (i.e., “draft”) imaginary teams composed of real-life National Football League (NFL) players. In this game, participants compete against their opponents (e.g., friends/coworkers/classmates), accumulating points based on players’ actual statistical performances in games. The goal is to outscore one’s opponent each week to win matches and ultimately claim victory in the league.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whyFantasyFootball",
    "href": "intro.html#sec-whyFantasyFootball",
    "title": "1  Introduction",
    "section": "\n1.3 Why Focus on Fantasy Football?",
    "text": "1.3 Why Focus on Fantasy Football?\nI was fortunate to have an excellent instructor who taught me the value of learning statistics to answer interesting and important questions. That is, I do not find statistics intrinsically interesting; rather, I find them interesting because of what they allow me to do. Many students find statistics intimidating in part because of how it is typically taught—with examples like dice rolls and coin flips that are (seemingly irrelevant and) boring to students. My contention is that applied examples are a more effective lens to teach many concepts in psychology and data analysis. It can be more engaging and relatable to learn statistics in the applied context of sports, a domain that is more intuitive to many. Many people play fantasy sports. This book involves applying statistics to a particular domain (football). People actually want to learn statistical principles and methods when they can apply them to interesting questions (e.g., sports). In my opinion [and supported by evidence; Motz (2013)], this is a much more effective way of engaging people and teaching statistics than in the context of abstract coin flips and dice rolls. Fantasy football relies heavily on prediction—trying to predict which players will perform best and selecting them accordingly. In this way, fantasy football provides a plethora of decision making opportunities in the face of uncertainty, and a wealth of data for analyzing these decisions. However, unlike many other applied domains in psychology, fantasy football (1) allows a person to see the accuracy of their predictions on a timely basis and (2) provides a safe environment for friendly competition. Thus, it provides a unique domain to evaluate—and improve—the accuracy of various prediction models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whyR",
    "href": "intro.html#sec-whyR",
    "title": "1  Introduction",
    "section": "\n1.4 Why R?",
    "text": "1.4 Why R?\nThe book provides data analysis examples using the statistical analysis software, R (R Core Team, 2025). Why R?\n\n\nR is free! Anyone can use it.\n\nR is open source—it is not a black box. You can see what is going on “under the hood” and can examine the code for any function or computation you perform. You can even modify and improve these functions by changing the code, and you can create your own functions.\n\nR is open platform—you can use it on multiple platforms, including Windows, MacOS, and Linux.\n\nR has advanced statistics capabilities. It was designed for statistical analysis and has strong capabilities for data wrangling.\n\nR has capabilities for state-of-the-art graphics. It has advanced capabilities for creating statistical graphics.\n\nR is widely used—there is a large community of people who use R for data analysis that you can draw upon for help from others.\n\nR analyses are based on code (rather than a graphical user interface), which allows reproducibility—with the same data, code, and setup (platform, R version, package versions, etc.), you should get the same answer every time. There are strong resources available for ensuring your analyses in R are reproducible by others (Gandrud, 2020).\nAnyone (including you) can contribute R packages to the community to improve its functionality. Statistical experts from all over the world have contributed open source packages to R for specialized tasks. In the chance there is not an R package that does what you need to do, you can write a function to perform the task and can contribute it as a package to the community for others to use and improve. The number of R packages contributed to the community is growing at a rapid rate. As of this writing, over 20,000 packages have been contributed to the Comprehensive R Archive Network (CRAN). And many more are stored on publicly available version control repositories like GitHub and GitLab. Chances are, if there is an analysis you need to do, an R package exists to do it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-educationalValue",
    "href": "intro.html#sec-educationalValue",
    "title": "1  Introduction",
    "section": "\n1.5 Educational Value",
    "text": "1.5 Educational Value\nSkills in statistics, statistical programming, and data analysis are highly valuable. This book includes practical and conceptual tools that build a foundation for critical thinking. The book aims to help readers evaluate theory in the light of evidence (and vice versa) and to refine decision making in the context of uncertainty. Readers will learn about the ways that psychological science (and related disciplines) poses questions, formulates hypotheses, designs studies to test those questions, and interprets the findings, collectively with the aim of answering questions, improving decision making, and solving problems.\nOf course, this is not a traditional psychology textbook. However, the book incorporates important psychological concepts, such as cognitive biases in judgment and prediction, etc. In the modern world of big data, research and society need people who know how to make sense of the information around us. Psychology is in a prime position to teach applied statistics to a wide variety of students, most of whom will not have careers as psychologists. Psychology can teach the importance of statistics given humans’ cognitive biases. It can also teach about how these biases can influence how people interpret statistics. This book will teach readers the applications of statistics (prediction) and research methods (empiricism) to answer questions they find interesting, while applying scientific and psychological rigor.\nFigure 1.1 is a video of Professor Benjamin Motz that describes the value of teaching statistics through the lens of fantasy football (NFL Films Presents, 2014):\n\n\n\n\n\nFigure 1.1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#learningGoals",
    "href": "intro.html#learningGoals",
    "title": "1  Introduction",
    "section": "\n1.6 Goals",
    "text": "1.6 Goals\nThe book has three overarching goals; to help students:\n\nAppreciate the value of statistics for answering questions you find interesting\nThink critically and be a critical consumer of information in society\nGet excited about learning stats!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#learningOutcomes",
    "href": "intro.html#learningOutcomes",
    "title": "1  Introduction",
    "section": "\n1.7 Learning Outcomes",
    "text": "1.7 Learning Outcomes\nThrough reading the book, readers will be able to:\n\nThink critically and empirically about human behavior and performance\nMake sense of big data\nUse, evaluate, and interpret statistical analyses\nCommunicate data-driven insights\n\n\n1.7.1 How Readers Will Achieve the Learning Outcomes\nReaders will achieve the learning outcomes through the following specific outcomes:\n\n1.7.1.1 1. Think critically and empirically about human behavior and performance\n\nApply empirical inference and articulate its advantages over speculative supposition.\nFormulate research questions, hypotheses, and predictions.\nProvide well-reasoned guidance for decision-making under conditions of uncertainty.\nDescribe common flaws in human judgment and decision making, including heuristics and cognitive biases, and explain how they can be mitigated analytically.\nCritically evaluate causal claims by generating plausible alternative explanations.\nApply core concepts in causal inference, including confounding, causal pathways, and counterfactuals.\nCompare the strengths and limitations of human and machine approaches to prediction.\n\n1.7.1.2 2. Make sense of big data\n\nApply foundational skills in statistical programming using R to combine, manipulate, clean, and summarize large datasets.\n\n1.7.1.3 3. Use, evaluate, and interpret statistical analyses\n\nConduct data analyses using R, including appropriate application of statistical models.\nCritically evaluate the strengths and limitations of statistical models and methods used for predicting uncertain events.\nApply analytical techniques to predict outcomes and uncover latent causes in observed data.\nInterpret results from statistical analyses and evaluate the accuracy of predictions.\nEngage in iterative problem-solving, refining analytical strategies based on results and feedback.\nUse practical analytical skills that can be applied in future research and job settings.\n\n1.7.1.4 4. Communicate data-driven insights\n\nCommunicate statistical findings in writing.\nIdentify characteristics of effective data visualizations.\nCreate data visualizations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-disclosures",
    "href": "intro.html#sec-disclosures",
    "title": "1  Introduction",
    "section": "\n1.8 Disclosures",
    "text": "1.8 Disclosures\nI am the Owner of Fantasy Football Analytics, LLC, which operates https://fantasyfootballanalytics.net.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-disclaimer",
    "href": "intro.html#sec-disclaimer",
    "title": "1  Introduction",
    "section": "\n1.9 Disclaimer",
    "text": "1.9 Disclaimer\n\nThis material probably won’t win you fantasy football championships. You could take what we learn and apply it to fantasy football and you might become 5 percent more likely to win. Or… Consider the broader relevance of this. You could learn data analysis and figure out ways to apply it to other systems. And you could be making a six-figure salary within the next five years.\n— Benjamin Motz, Ph.D. (NFL Films Presents, 2014)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-introConclusion",
    "href": "intro.html#sec-introConclusion",
    "title": "1  Introduction",
    "section": "\n1.10 Conclusion",
    "text": "1.10 Conclusion\nIn conclusion, fantasy football provides an exciting lens through which to learn statistics, and R is a useful (and free!) software for conducting statistical analysis and creating statistical graphics.\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGandrud, C. (2020). Reproducible research with R and R studio (3rd ed.). CRC Press. https://www.routledge.com/Reproducible-Research-with-R-and-RStudio/Gandrud/p/book/9780367143985\n\n\nMotz, B. (2013). Fantasy football: A touchdown for undergraduate statistics education. Proceedings of the Games, Learning, and Society Conference, 9.0, 222–228. https://doi.org/10.1184/R1/6686804.v1\n\n\nNFL Films Presents. (2014). Playing fantasy football for college credit?? Welcome to C105 - Prediction, Probability, & Pigskin. https://www.facebook.com/watch/?v=10155572257183615\n\n\nR Core Team. (2025). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html",
    "href": "fantasy-football.html",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "2.1 Football\nThis chapter provides a brief primer on (American) football and fantasy football. If you are already familiar with fantasy football, feel free to skip this chapter.\nFootball is the most widely watched sport in the United States [Jones (2024); archived at https://perma.cc/X2UG-RAAK; statistica (2023b); archived at https://perma.cc/JNU6-S96A].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#sec-football",
    "href": "fantasy-football.html#sec-football",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "2.1.1 The Objective\nThe goal in football is for a team to score more points than their opponent. A game lasts 60 minutes, and it is separated into four 15-minute quarters. The team with the most points when the time runs out wins.\nFigure 2.1 is a video that provides a brief introduction to American football (Ly, 2015):\n\n\n\n\n\nFigure 2.1\n\n\n\n2.1.2 The Roster\n\n2.1.2.1 Overview\nEach team has 11 players on the field at a time. The particular players who are on the field will depend on the situation, but usually includes one of the three subsets of players:\n\nOffense\nDefense\nSpecial Teams\n\nAn example formation is depicted in Figure 2.2.\n\n\n\n\n\nFigure 2.2: An Example Football Formation for the Offense and Defense. The solid line indicates the line of scrimmage. The arrow indicates the direction the offense tries to advance the ball.\n\n\n\n2.1.2.2 Offense\nThe offense is on the field when the team has the ball.\nPlayers on offense include:\n\nQuarterback (QB)\nRunning Back (RB)\n\nHalfback (HB) or Tailback (TB)\nFullback (FB)\n\n\nWide Receiver (WR)\nTight End (TE)\nOffensive Linemen (OL), part of the “Offensive Line”\n\nCenter (C)\nOffensive Guard (OG)\nOffensive Tackle (OT)\n\n\n\nThe quarterback is the most important player on the offense. They help lead the team down the field. The quarterback receives the ball from the Center at the beginning of the play, and they can either hand the ball off (typically to a Running Back or Fullback), pass the ball (typically to a Wide Receiver or Tight End), or run the ball. Quarterbacks tend to have a strong arm for throwing the ball far and accurately. Some quarterbacks are fast and are considered “dual threats” to pass or run.\nRunning Backs take a hand-off from the Quarterback to execute a running play (i.e., a rush). They may also catch short passes from the Quarterback or help protect (i.e., block for) the Quarterback from the defensive players who are trying to tackle the Quarterback. Halfbacks and Tailbacks tend to be quick and agile. Fullbacks tend to be strong and powerful.\nWide Receivers catch passes from the Quarterback to execute a passing play. On running plays, they provide protection for the player running the ball (e.g., the Running Back) so the ball carrier can get as far as possible without being tackled. Wide receivers tend to be tall, fast, have good hands (can catch the ball well), and can jump high.\nTight Ends block for running and passing plays, and they catch passes from the Quarterback. Tight ends tend to be strong and have good hands.\nOffensive Linemen block for running and passing plays. On passing plays, they provide protection for the Quarterback so the Quarterback has time to pass the ball without being tackled. On running plays, they provide protection for the player running the ball (e.g., the Running Back) so the ball carrier can get as far as possible without being tackled. Offensive Linemen tend to be large so they can provide adequate protection for the Quarterback and Running Back.\n\n2.1.2.3 Defense\nThe defense is on the field when the team does not have the ball (i.e., when the opposing team has the ball).\nPlayers on defense include:\n\nDefensive Linemen (DL), part of the “Defensive Line”\n\nDefensive End (DE)\nDefensive Tackle (DT)\n\n\nLinebacker (LB)\n\nMiddle (or Inside) Linebacker (MLB)\nOutside Linebacker (OLB)\n\n\nDefensive Back (DB), part of the “Secondary”\n\nCornerback (CB)\nSafety (S)\n\nFree Safety (FS)\nStrong Safety (SS)\n\n\n\n\n\nThe players on the defense attempt to tackle the offensive players for as short of gains as possible and attempt to prevent completed passes.\nOn passing plays, Defensive Linemen try to apply pressure to the Quarterback and try to tackle the Quarterback behind the line of scrimmage before the Quarterback can throw the ball (i.e., a sack). On rushing plays, Defensive Linemen try to tackle the ball carrier to prevent the ball carrier from advancing the ball (i.e., gaining yards). Defensive Linemen tend to be large yet quick so they can apply pressure to the Quarterback.\nLinebackers are versatile in that, on a given play, they may attempt to a) “blitz” to sack the Quarterback, b) stop the Running Back, or c) prevent a completed pass. Linebackers tend to be strong yet agile.\nDefensive Backs are specialist pass defenders. The main role of Cornerbacks is to cover the Wide Receivers. Safeties serve as the last line of defense for longer passes. Defensive Backs tend to be quick and agile.\n\n2.1.2.4 Special Teams\nThe special teams involves specialist players who are on the field during all kicking plays including kickoffs, field goals, and punts.\nPlayers on special teams include:\n\nKicker (K)\nPunter (P)\nHolder\nLong Snapper\nPunt Returner\nKick Returner\nand other players intended to block for or to tackle the ball carrier\n\nOn a field goal attempt, the Long Snapper snaps the ball to the Holder, who holds the ball for the Kicker. The Kicker attempts field goals and, during kickoffs, kicks the ball to the opposing team. During kickoffs, the Kick Returner catches the kicked ball and returns it for as many yards as possible. During a punt play, the Long Snapper snaps the ball to the Punter who kicks (i.e., punts) the ball to the opposing team. The Punt Returner catches the punted ball and returns it for as many yards as possible.\n\n2.1.3 The Field\nThe football field is rectangular and is 120 yards long and 53 1/3 yards wide (109.73 m x 48.77 m).1 At each end of the 120-yard field is a team’s end zone. Each end zone is 10 yards long (9.14 m). Thus, the distance from one end zone to the other end zone is 100 yards (91.44 m). Behind each end zone is a field goal post. A diagram of a football field is depicted in Figure 2.3.\n\n\n\n\n\nFigure 2.3: A Diagram of a Football Field. The yard markers depict the distance from the nearest end zone. The orange shaded area is called the “red zone”, where chances of scoring points are highest. The original figure was modified to depict field goal posts. (Figure retrieved from https://commons.wikimedia.org/wiki/File:American_football_field.svg)\n\n\n\n2.1.4 The Gameplay\nAt the beginning of the game, there is a coin flip to determine which teams receives the ball first and which team takes which side of the field. During the kickoff, the kicking team kicks the ball to the receiving team, who has the option to return the kick. The offense starts their possession at the 25 yard line—if there is no return (i.e., a touchback)—or wherever the kick returner is tackled or goes out of bounds.\nThe team with the ball (i.e., the offense) has four opportunities (“downs”) to advance the ball (i.e., gain) 10 yards. A team can advance the ball either by running it or by throwing (i.e., passing) and catching it. At the end of a rushing play, the ball advances to wherever the ball carrier is tackled or goes out of bounds (i.e., wherever the player is “down”). At the end of a passing play, if the thrown ball is caught (i.e., a completed pass), the ball advances to wherever the ball carrier is tackled or goes out of bounds. If the thrown ball is not caught in bounds before the ball hits the ground (i.e., an incomplete pass), the ball does not advance. Wherever the ball is advanced to dictates where the next play begins. The yard position on the field where the next play takes place from is known as the “line of scrimmage”. Neither team can cross the line the line of scrimmage until the next play begins. To begin the play, the ball is placed on the line of scrimmage and the Center gives (or “snaps”) the ball to the Quarterback.\nIf the team advances the ball 10 or more yards within four downs, the team receives a “first down” and is awarded a new set of downs—four more downs to advance the ball 10 more yards. If the team advances the ball all the way to the other team’s end zone, they score a touchdown. If the team fails to advance the ball 10 or more yards within four downs, the team loses the ball, and the other team takes possession at that spot on the field. There are risks of giving the other team the ball with a short distance to score. Thus, on fourth down, instead of trying to advance the ball for a first down, a team may choose to kick a field goal—to get points—or to punt.\nA field goal involves a kicker kicking the ball with an intent to kick the ball through the field goal posts (“uprights”). To score points by making a field goal, the kicked ball must go between the uprights (extended vertically) and over the cross bar.\nPunting involves a punter kicking the ball to the other team with an intent to give their opponent worse field position, thus making it harder for the other team to score. The punting team tries to pin the opponent as close as possible to the opponent’s end zone (i.e., as far as possible from the own team’s end zone), so they have a longer distance to go to score a touchdown.\nThere are multiple ways that ball possession can switch from the offense to the other team. After scoring a touchdown, field goal, or safety, there is a kickoff, in which the scoring team kicks the ball to the opponent. Another way that the ball switches possession to the other team is if the team commits a turnover. The defense can force a turnover by an interception, fumble recovery, or turnover on downs. A turnover due to an interception occurs when a defensive player catches the quarterback’s pass. A turnover due to a fumble recovery occurs when an offensive player, who had possession of the ball, loses the ball before being down or scoring a touchdown and the ball is recovered by the opponent. A turnover on downs occurs when the team attempts on fourth down to achieve the remainder of the needed 10 yards to go but fails.\nOther football-related situations include tackles for loss and sacks. A tackle for loss occurs when a ball carrier is tackled behind the line of scrimmage. A sack occurs when a Quarterback is tackled with the ball behind the line of scrimmage. A pass defended occurs when a defensive player knocks down the ball in the air so that the indended receiver cannot catch the ball.\n\n2.1.5 The Scoring\nThe goal of the team with the ball (i.e., the offense) is to score points. It can do this by either advancing the ball into the other team’s end zone (6 points) or by kicking a field goal (3 points). Advancing the ball in the other team’s end zone is called a touchdown. After a touchdown, the offense chooses to attempt either a point-after-touchdown (PAT) or a two-point conversion. A PAT is a short kick attempt from the 15-yard line (i.e., 15 yards away from the end zone) that, if it goes through the goal posts (“uprights”) and over the cross bar, is worth 1 point. A two-point conversion is a single-scoring opportunity from the 3-yard line (i.e., 3 yards away from the end zone). If the offense scores (i.e., advances the ball into the end zone) from the 3-yard line, the team is awarded 2 points.\nA team can kick a field goal from any distance as long as the kick goes through the goal posts. The current record for the longest field goal is 66 yards (by Justin Tucker in 2021).\nA safety occurs when the offense is tackled with the ball in their own end zone. When a safety occurs, the opposing team (i.e., defense) is awarded two points and the ball.\n\n2.1.6 Glossary of Terms\n\nrunning play (“run”) or rushing play (or “rush”)—the attempt by an offensive player, typically the Running Back or Quarterback, to advance the ball “on the ground” by running it—not by passing it forward\npassing play (or “pass”)—the attempt by an offensive player, typically the Quarterback, to advance the ball by throwing it forward to an offensive player\npassing attempt—the attempt to advance the ball by passing it (i.e., a thrown pass)\nrushing attempt—the attempt to advance the ball by running it\npassing completion—a thrown pass that is succesfully caught by an offensive player\npassing incompletion—a thrown pass that is not caught by an offensive player\npassing yards—the distance (in yards) the player advanced the ball by throwing it\nrushing yards—the distance (in yards) the player advanced the ball by running it\nreceving yards—the distance (in yards) the player advanced the ball by catching thrown passes and then running with it further upfield\nkick/punt return yards—the distance (in yards) the player advanced the ball by returning kicks or punts\nturnover return yards—the distance (in yards) the player advanced the ball by returning turnovers\nreception—a pass that is caught by the offensive player\ntouchdown—advancing the ball into the opponent’s end zone either by a) throwing a completed pass that ends up in the end zone, b) running it into the end zone, c) catching it in the end zone, or d) catching it and then running it into the end zone\npassing touchdown—advancing the ball into the opponent’s end zone either by throwing a completed pass that ends up in the end zone\nrushing touchdown—advancing the ball into the opponent’s end zone either by running it into the end zone\nreceiving touchdown—advancing the ball into the opponent’s end zone either by catching it in the end zone or by catching it and then running it into the end zone\nkick/punt return touchdown—advancing the ball into the opponent’s end zone when returning a kick or punt\nturnover return touchdown—advancing the ball into the opponent’s end zone when returning a turnover (i.e., interception or fumble)\ntwo-point conversion—a single-scoring opportunity from the 3-yard line (i.e., 3 yards away from the end zone) that is an option given to a team that scores a touchdown; if the offense scores (i.e., advances the ball into the end zone) from the 3-yard line, the team is awarded 2 points\nblock—when the defense/special teams blocks a kick or field goal by hitting the ball just after it is kicked to prevent the ball from going far\nkickoff—the kicking team kicks the ball to the receiving team, who has the option to return the kick\nfield goal—a kicker kicks the ball with an intent to kick the ball through the field goal posts (“uprights”). To score points by making a field goal, the kicked ball must go between the uprights (extended vertically) and over the cross bar. If the field goal attempt is successful, the team gains 3 points.\npoint after touchdown (PAT)—a short kick attempt from the 15-yard line (i.e., 15 yards away from the end zone) that, if it goes through the goal posts (“uprights”) and over the cross bar, is worth 1 point\nextra point returned—if the defense/special teams returns the ball into the opponent’s end zone during a point after touchdown (PAT) attempt, it is worth 2 points\npunt—a punter kicks the ball to the other team with an intent to give their opponent worse field position, thus making it harder for the other team to score\nfumble lost—when an offensive player, who had possession of the ball, loses the ball before being down or scoring a touchdown and the ball is recovered by the opponent\nfumble forced—when a defensive player knocks the ball out of the hands of an offensive player, who had possession of the ball\nfumble recovery—when a defensive player recovers a fumble by the opponent\ninterception—when a defensive player catches a pass from an offensive player\ntackle—when a player brings down the ball carrier\ntackle solo—when a player is the main tackler (i.e., the primary player to bring down the ball carrier)\ntackle assist—when a player is one of two or more players who, together, bring down the ball carrier\ntackle for loss—when an offensive player is tackled with the ball behind the line of scrimmage\nsack—when a Quarterback is tackled with the ball behind the line of scrimmage\npass defended—when a defensive player knocks down the ball in the air so that the indended receiver cannot catch the ball\nsafety—when the offense is tackled with the ball in their own end zone",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#sec-fantasyFootball",
    "href": "fantasy-football.html#sec-fantasyFootball",
    "title": "2  Intro to Football and Fantasy",
    "section": "\n2.2 Fantasy Football",
    "text": "2.2 Fantasy Football\n\n2.2.1 Overview of Fantasy Football\nFantasy football is one of the most widely played games in the history of games. It is estimated that around 62 million people play fantasy sports [Fantasy Sports & Gaming Association (2023); archived at https://perma.cc/9PB8-ZDJJ], of whom around 29 million play fantasy football [statistica (2023a); archived at https://perma.cc/8YSN-UUNT]. As noted in the Introduction, fantasy football is an online game where participants assemble (i.e., “draft”) imaginary teams composed of real-life National Football League (NFL) players.2 The participants are in charge of managing and making strategic decisions for their imaginary team to have the best possible team that will score the most points. Thus, the participants are called “managers”. Managers make decisions such as selecting which players to draft, selecting which players to play (i.e., “start”) on a weekly basis, identifying players to pick up from the remaining pool of available players (i.e., waiver wire), and making trades with other teams.\nThere are variety of types of fantasy football leagues. In standard re-draft leagues, managers re-draft players each season. In keeper leagues, managers are allowed to keep one or more players from one season to the next, possibly for some cost (e.g., toward a keeper cap, loss of future draft pick). In dynasty leagues, managers act like a general manager and keep most of their roster from year to year. They may involve player contracts, salary caps, and free agent drafts. In best ball leagues, the manager’s best possible lineup (in terms of the highest-scoring players for the necessary roster positions) are automatically selected for that week’s lineup.\nFantasy football relies heavily on prediction—trying to predict which players will perform best and selecting them accordingly.\nFigure 2.4 is a video that provides a brief introduction to fantasy football (NFL Fantasy Football, 2020):\n\n\n\n\n\nFigure 2.4\n\n\n\n2.2.2 The Fantasy League\nA fantasy football “league” is composed of various imaginary (i.e., “fantasy”) teams—and their associated manager. In the fantasy league, the managers’ fantasy teams play against each other. A fantasy league is commonly composed of 8, 10, or 12 fantasy teams, but leagues can have more or fewer teams.\n\n2.2.3 The Roster of a Fantasy Team\nOn a given roster, a manager has a “starting lineup” and a “bench”. Each week, the manager decides which players on their roster to put in the starting lineup, and which to keep on the bench. In many leagues, a starting lineup is composed of offensive players, a kicker, and defense/special teams:\nOffensive players:\n\n\nTable 2.1: Offensive Players in the Starting Lineup\n\n\n\n\n\n\n\nPosition\nTypical Number of Players in Starting Lineup\n\n\n\nQuarterback (QB)\n1\n\n\nRunning Back (RB)\n2\n\n\nWide Receiver (WR)\n2\n\n\nTight End (TE)\n1\n\n\nFlex Position\n1\n\n\n\n\n\n\nA “flex position” is a flexible position that can involve a player from various positions: e.g., a Running Back, Wide Receiver, or Tight End.\nKickers:\n\none Kicker (K)\n\nDefense/Special Teams:\n\none Team Defense (DST/D/DEF) or multiple Individual Defensive Players (IDP)\n\n2.2.4 Scoring\n\n2.2.4.1 Scoring Overview\nIn the game of fantasy football, managers accumulate points on a weekly basis based on players’ actual statistical performances in NFL games. Managers receive points for only those players who are on their starting lineup (not players on their bench). In a standard league, a manager’s goal is to outscore their opponent each week to win matches and ultimately claim victory in the league. At the end of the regular season, many leagues have a playoffs to determine the league champion. In total points leagues, the league champion is determined by how many points they score throughout the entire season, rather than based on weekly matchups and playoffs.\nScoring settings can differ from league to league. Below are common scoring settings for fantasy leagues.\n\n2.2.4.2 Offensive Players\n\n\nTable 2.2: Common Scoring Settings for Offensive Players\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\nRushing or receiving TD\n6\n\n\nReturning a kick or punt for a TD\n6\n\n\nReturning or recovering a fumble for a TD\n6\n\n\nPassing TD\n4\n\n\nPassing INT\n−2\n\n\nFumble lost\n−2\n\n\nRushing, passing, or receiving 2-point conversion\n2\n\n\nRushing or receiving yards\n1 point per 10 yards\n\n\nPassing yards\n1 point per 25 yards\n\n\n\n\n\n\nNote: “TD” = touchdown; “INT” = interception\nOther common (but not necessarily standard) statistical categories include:\n\nreceptions (called “point per reception” [PPR] leagues)\nreturn yards\npassing attempts\nrushing attempts\n\n2.2.4.3 Kickers\n\n\nTable 2.3: Common Scoring Settings for Kickers\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\nFG made: 50+ yards\n5\n\n\nFG made: 40–49 yards\n4\n\n\nFG made: 39 yards or less\n3\n\n\nRushing, passing, or receiving 2-point conversion\n2\n\n\nPoint after touchdown attempt made\n1\n\n\nPoint after touchdown attempt missed\n−1\n\n\nMissed FG: 0–39 yards\n−2\n\n\nMissed FG: 40–49 yards\n−1\n\n\n\n\n\n\nNote: “FG” = field goal\n\n2.2.4.4 Team Defense/Special Teams\n\n\nTable 2.4: Common Scoring Settings for Team Defense/Special Teams\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\nDefensive or special teams TD\n6\n\n\nInterception\n2\n\n\nFumble recovery\n2\n\n\nBlocked punt, PAT, or FG\n2\n\n\nSafety\n2\n\n\nSack\n1\n\n\n\n\n\n\nNote: “TD” = touchdown; “PAT” = point after touchdown; “FG” = field goal\n\n2.2.4.5 Individual Defensive Players\n\n\nTable 2.5: Common Scoring Settings for Individual Defensive Players\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\nTackle solo\n1\n\n\nTackle assist\n0.5\n\n\nTackle for loss\n1\n\n\nSack\n2\n\n\nInterception\n4\n\n\nFumble forced\n2\n\n\nFumble recovery\n2\n\n\nTD\n6\n\n\nSafety\n2\n\n\nPass defended\n1\n\n\nBlocked kick\n2\n\n\nExtra point returned\n2\n\n\n\n\n\n\nNote: “TD” = touchdown\nOther common (but not necessarily standard) statistical categories include:\n\nturnover return yards\n\n2.2.4.6 Common Scoring Abbreviations\n\n“TD” = touchdown\n“INT” = interception\n“yds” = yards\n“ATT” = attempts\n“2-pt conversion” = two-point conversion\n“FG” = field goal\n“PAT” = point after touchdown (i.e., extra point/point after attempt)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#footnotes",
    "href": "fantasy-football.html#footnotes",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "One yard is equal to three feet. A yard is just smaller than a meter (0.9144 meters).↩︎\nFantasy leagues are also available for baseball, basketball, and many other sports.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "",
    "text": "3.1 Learning Statistics\nThe book uses the software R (R Core Team, 2025) for statistical analyses (http://www.r-project.org). R is a free software environment; you can download it at no charge here: https://cran.r-project.org. This chapter provides an overview of how to install and learn the software R, how to troubleshoot code, and how to perform various data management operations.\nHere are resources for learning statistics:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-learningStats",
    "href": "getting-started.html#sec-learningStats",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "",
    "text": "OpenIntro Statistics: https://www.openintro.org/book/os/\n\nIntroductory Statistics: https://openstax.org/details/books/introductory-statistics-2e",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-learningR",
    "href": "getting-started.html#sec-learningR",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.2 Learning R\n",
    "text": "3.2 Learning R\n\nHere are a various resources for learning R:\n\nIntro to R: https://www.statmethods.net\n\nVideo training courses in R skills: https://www.pluralsight.com/search?q=R\n\n\nR for Data Science: https://r4ds.hadley.nz\n\nBrowse the Cookbook for R to find solutions to common tasks and problems: http://www.cookbook-r.com\n\nBrowse the R Graph Gallery to find examples of various graphs: https://r-graph-gallery.com\n\nFree Codeacademy course on R: https://www.codecademy.com/learn/learn-r\n\nFree Coursera courses on R: https://www.coursera.org/search?query=R\n\nWatch these videos from Coursera: https://blog.revolutionanalytics.com/2012/12/coursera-videos.html\n\n\nPosit/Rstudio Webinars: https://posit.co/resources/videos/\n\nUCLA Stats Website: https://stats.idre.ucla.edu/r/\n\nIntroduction to R course on Datacamp: https://www.datacamp.com/courses/free-introduction-to-r\n\nLearning Statistics with R: https://learningstatisticswithr.com\n\nTeaching R in a Kinder, Gentler, More Effective Manner: https://github.com/matloff/TidyverseSkeptic\n\nLearn R interactively with swirl: https://swirlstats.com\n\nUse the learnr package (Aden-Buie et al., 2023): https://rstudio.github.io/learnr/\n\nResources for learning tidyverse (Wickham et al., 2019; Wickham, 2023), which is a collection of R packages for data management: https://www.tidyverse.org/learn/\n\nYou will sometimes find relevant articles on R-bloggers: https://www.r-bloggers.com",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingHelpR",
    "href": "getting-started.html#sec-gettingHelpR",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.3 Getting Help with R\n",
    "text": "3.3 Getting Help with R\n\nIf you have R questions, you can ask them in a number of places:\n\nForums:\n\n\nPosit: https://forum.posit.co\n\n\nStackOverflow: https://stackoverflow.com/questions/tagged/r\n\n\nReddit: https://www.reddit.com/r/rstats/\n\n\n\nThe R mailing list: https://stat.ethz.ch/mailman/listinfo/r-help\n\n\nSalmon (2018) provides additional resources and good guidance for getting help with R: https://masalmon.eu/2018/07/22/wheretogethelp/ (archived at https://perma.cc/4RRE-KL33).\nWhen posting a question on forums or mailing lists, keep a few things in mind:\n\nRead the posting guidelines before posting!\nBe respectful of other people and their time. R is free software. People are offering their free time to help. They are under no obligation to help you. If you are disrespectful or act like they owe you anything, you will rub people the wrong way and will be less likely to get help.\nProvide a minimal, reproducible example. Providing a minimal, reproducible example can be crucial for getting a helpful response. By going to the trouble of creating a minimal, reproducible example and identifying the minimum conditions necessary to reproduce the issue, you will often figure out how to resolve it. Stack Overflow (2025) offers guidelines on providing a minimal, reproducible example: https://stackoverflow.com/help/minimal-reproducible-example (archived at https://perma.cc/6NUB-UTYF). Stack Overflow (2018) offers a good example and guidelines for providing a minimal, reproducible example in R: https://stackoverflow.com/a/5963610 (archived at https://perma.cc/PC9L-DQZG). My strong recommendation is to provide a reprex whenever possible: https://reprex.tidyverse.org. Bryan et al. (2025) provide reprex do’s and don’ts: https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-initialSetup",
    "href": "getting-started.html#sec-initialSetup",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.4 Initial Setup",
    "text": "3.4 Initial Setup\nTo get started, follow the following steps (for each step that involves installing or running software, make sure to right click and select “Run as Administrator”):\n\nDownload and install R: https://cran.r-project.org\nAfter installing R, download and install RStudio Desktop—this is a graphical user interface for R: https://posit.co/download/rstudio-desktop\n\nInstall tools to allow you to compile R packages so you can install packages from source, if necessary (i.e., if package binaries are not available):\n\nIf on Windows, download and install the version of RTools (https://cran.r-project.org/bin/windows/Rtools/) that corresponds to your version of R\n\nIf on Mac, download and install R Compiler Tools for Rcpp on MacOS: https://thecoatlessprofessor.com/programming/cpp/r-compiler-tools-for-rcpp-on-macos/\n\n\n\n\nAfter installing R and RStudio, open RStudio (by right clicking and selecting “Run as Administrator”) and run the following code in the console to install the R packages used in this book (note: this will take a while):\n\nCodeinstall.packages(c(\n  \"petersenlab\",\"remotes\",\"knitr\",\"rmarkdown\",\"tidyverse\",\"nflverse\",\n  \"tidymodels\",\"easystats\",\"broom\",\"broom.mixed\",\"psych\",\"downlit\",\"xml2\",\n  \"gsisdecoder\",\"progressr\",\"DescTools\",\"pwr\",\"pwrss\",\"WebPower\",\"XICOR\",\n  \"dagitty\",\"ggdag\",\"ggtext\",\"gghighlight\",\"ggExtra\",\"patchwork\",\"pROC\",\n  \"lme4\",\"lmerTest\",\"MuMIn\",\"emmeans\",\"pbkrtest\",\"sjstats\",\"AICcmodavg\",\n  \"rstan\",\"brms\",\"tidybayes\",\"bbmle\",\"fitdistrplus\",\"sn\",\"mclust\",\"magrittr\",\n  \"viridis\",\"viridisLite\",\"msir\",\"plotly\",\"webshot2\",\"quantmod\",\"fPortfolio\",\n  \"NMOF\",\"nFactors\",\"xts\",\"zoo\",\"forecast\",\"parallelly\",\"doParallel\",\n  \"missRanger\",\"ggridges\",\"powerjoin\",\"bestNormalize\",\"LongituRF\",\"gpboost\",\n  \"mgcv\",\"rms\",\"car\",\"lavaan\",\"lavaanPlot\",\"lavaangui\",\"mice\",\"miceadds\",\n  \"interactions\",\"robustbase\",\"ordinal\",\"MASS\",\"data.table\",\"future\",\n  \"future.apply\",\"SimDesign\",\"domir\",\"GGally\",\"Rglpk\",\"TTR\"))\n\n\n\n\nSome necessary packages, including the ffanalytics package (Tungate et al., 2025), are hosted in GitHub (and are not hosted on the Comprehensive R Archive Network [CRAN]) and thus need to be installed using the following code (after installing the remotes package (Csárdi et al., 2024) above)1:\n\nCoderemotes::install_github(\"DevPsyLab/petersenlab\")\nremotes::install_github(\"FantasyFootballAnalytics/ffanalytics\")\nremotes::install_github(\"stan-dev/cmdstanr\")\n\n\n\n\n\n\n\n\n\n\nNote 3.1: If you are in Professor Petersen’s class\n\n\n\nIf you are in Professor Petersen’s class, also perform the following steps, to set up version control and your blog:\n\nDownload and install git: https://git-scm.com/downloads\n\nSet up a free account on GitHub.com.\nDownload and install GitHub Desktop: https://desktop.github.com\n\nMake sure you are logged into your GitHub account on GitHub.com.\nGo to the following GitHub repository: https://github.com/isaactpetersen/QuartoBlogFantasyFootball and complete the following steps:\n\nClick “Use this Template” (in the top right of the screen) &gt; “Create a new repository”\nMake sure the checkbox is selected for the following option: “Include all branches”\nMake sure your Owner account is selected\nSpecify the repository name to whatever you want, such as FantasyFootballBlog\n\nType a brief description, such as Files for my fantasy football blog\n\nKeep the repository public (this is necessary for generating your blog)\nSelect “Create repository”\n\n\nAfter creating the new repository, make sure you are on the page of of your new repository and complete the following steps:\n\nClick “Settings” (in the top of the screen)\nClick “Actions” (in the left sidebar) &gt; “General”\nMake sure the following are selected:\n\n“Read and write permissions” (under “Workflow permissions”)\n“Allow GitHub Actions to create and approve pull requests”\nthen click “Save”\n\n\nClick “Pages” (in the left sidebar)\nMake sure the following are selected:\n\n“Deploy from a branch” (under “Source”)\n“gh-pages/(root)” (under “Branch”)\nthen click “Save”\n\n\n\n\nClone the repository to your local computer by clicking “Code” &gt; “Open with GitHub Desktop”, select the folder where you want the repository to be saved on your local computer, and click “Clone”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-generalWorkflowAnalysis",
    "href": "getting-started.html#sec-generalWorkflowAnalysis",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.5 General Workflow",
    "text": "3.5 General Workflow\nHere is a general workflow for statistical analysis:\n\nLoad the packages you need\nLoad the data files\nDo any data processing needed to prepare the data files to be merged.\n\nFor example, make sure each data file has unique values on the keys.\n\n\nMerge the data files\nDo any data processing needed to prepare the data files for analysis. For example:\n\ncalculate/recode variables\ntransform wide to long or long to wide\nfilter to the relevant rows\nexamine descriptive statistics to ensure data are plausible\n\n\nDo the analysis!\n\nevaluate analysis assumptions\nconduct sensitivity analyses to see extent to which findings differ using different subsets of the data, different modeling approaches/assumptions, etc.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-installingPackages",
    "href": "getting-started.html#sec-installingPackages",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.6 Install Packages",
    "text": "3.6 Install Packages\nYou can install R packages using the utils::install.packages() function with the following syntax:\n\nCodeinstall.packages(\"INSERT_PACKAGE_NAME_HERE\")\n\n\nFor instance, you can use the following code to install the tidyverse package (Wickham, 2023):\n\nCodeinstall.packages(\"tidyverse\")\n\n\nYou can also install multiple packages with one line of code (replacing each package with its respective name):\n\nCodeinstall.packages(c(\"PACKAGE1\",\"PACKAGE2\",\"PACKAGE3\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-loadPackages",
    "href": "getting-started.html#sec-loadPackages",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.7 Load Packages",
    "text": "3.7 Load Packages\nYou can load a given library using the base::library() function with the following syntax:\n\nCodelibrary(\"INSERT_PACKAGE_NAME_HERE\")\n\n\nFor instance, you can use the following code to load the petersenlab (Petersen, 2025) and tidyverse (Wickham, 2023) packages:\n\nCodelibrary(\"petersenlab\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-functionsArguments",
    "href": "getting-started.html#sec-functionsArguments",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.8 Using Functions and Arguments",
    "text": "3.8 Using Functions and Arguments\nA function often takes particular input(s) and produces some form of output. The name of the function is followed by parentheses; the inputs go in between the parentheses. The possible inputs that a function can accept are called “arguments”. You can learn about a particular function and its arguments by entering a question mark before the name of the function:\n\nCode?NAME_OF_FUNCTION()\n\n\nBelow, we provide examples for how to learn about and use functions and arguments, by using the base::seq() function as an example. The base::seq() function creates a sequence of numbers. To learn about the base::seq() function, which creates a sequence of numbers, you can execute the following command:\n\nCode?seq()\n\n\nThis is what the documentation shows for the base::seq() function in the Usage section:\n\nCodeseq(\n  from = 1,\n  to = 1,\n  by = ((to - from)/(length.out - 1)),\n  length.out = NULL,\n  along.with = NULL,\n  ...)\n\n\nBased on this information, we know that the base::seq() function takes the following arguments:\n\nfrom\nto\nby\nlength.out\nalong.with\n...\n\nThe arguments have default values that are used if the user does not specify values for the arguments. The default values are provided in the Usage section and are in Table 3.1:\n\n\nTable 3.1: Arguments and defaults for the seq() function. Arguments with a default of NULL are not used unless a value is provided by the user.\n\n\n\nArgument\nDefault Value for Argument\n\n\n\nfrom\n1\n\n\nto\n1\n\n\nby\n((to - from)/(length.out - 1))\n\n\nlength.out\nNULL\n\n\nalong.with\nNULL\n\n\n\n\n\n\nWhat each argument represents (i.e., the meaning of from, to, by, etc.) is provided in the Arguments section of the documentation. You can specify a function and its arguments either by providing values for each argument in the order indicated by the function, or by naming its arguments. Naming arguments explicitly (rather than merely relying on order) is considered best practice because it is safer—doing so prevents you from accidentally assigning an input to the wrong argument.\nHere is an example of providing values to the arguments in the order indicated by the function, to create a sequence of numbers from 1 to 9:\n\nCodeseq(1, 9)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nHere is an example of providing values to the arguments by naming its arguments:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 1)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nIf you provide values to arguments by naming the arguments, you can reorder the arguments and get the same answer:\n\nCodeseq(\n  by = 1,\n  to = 9,\n  from = 1)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nThere are various combinations of arguments that one could use to obtain the same result. For instance, here is code to generate a sequence from 1 to 9 by 2:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 2)\n\n[1] 1 3 5 7 9\n\n\nOr, alternatively, you could specify the length of the desired sequence (5 values):\n\nCodeseq(\n  from = 1,\n  to = 9,\n  length.out = 5)\n\n[1] 1 3 5 7 9\n\n\nIf you want to generate a series with decimal values, you could specify a long desired sequence of 81 values:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  length.out = 81)\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n[20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n[39] 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6\n[58] 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 8.5\n[77] 8.6 8.7 8.8 8.9 9.0\n\n\nThis is equivalent to specifying a sequence from 1 to 9 by 0.1:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 0.1)\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n[20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n[39] 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6\n[58] 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 8.5\n[77] 8.6 8.7 8.8 8.9 9.0\n\n\nHopefully, that provides an example for how to learn about a particular function, its arguments, and how to use them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createVector",
    "href": "getting-started.html#sec-createVector",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.9 Create a Vector",
    "text": "3.9 Create a Vector\nA vector is a series of elements that can be numeric or character. Character elements should be specified in quotes. A vector has one dimension (length). To create a vector, use the base::c() function to combine elements into a vector (“c” stands for combine). And, we use the assignment operator (&lt;-) to assign the vector to an object named exampleVector, so we can access it later. Anything on the right side of an assignment operator gets assigned to the object name on the left side of the assignment operator.\n\nCodeexampleVector &lt;- c(40, 30, 24, 20, 18, 23, 27, 32, 26, 23, NA, 37)\n\nexampleVector2 &lt;- c(\n  \"A\",\"B1\",\"B2\",\"This is a sentence.\",\"This is another sentence.\")\n\n\nWe can then access the contents of the object by calling its name:\n\nCodeexampleVector\n\n [1] 40 30 24 20 18 23 27 32 26 23 NA 37",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createDF",
    "href": "getting-started.html#sec-createDF",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.10 Create a Data Frame",
    "text": "3.10 Create a Data Frame\nA data frame has two dimensions: rows and columns. Here is an example of creating a data frame using the base::data.frame() function, while using the assignment operator (&lt;-) to assign the data frame to an object so we can access it later:\n\nCodeplayers &lt;- data.frame(\n  ID = 1:12,\n  name = c(\n    \"Ken Cussion\",\n    \"Ben Sacked\",\n    \"Chuck Downfield\",\n    \"Ron Ingback\",\n    \"Rhonda Ball\",\n    \"Hugo Long\",\n    \"Lionel Scrimmage\",\n    \"Drew Blood\",\n    \"Chase Emdown\",\n    \"Justin Time\",\n    \"Spike D'Ball\",\n    \"Isac Ulooz\"),\n  position = c(\"QB\",\"QB\",\"QB\",\"RB\",\"RB\",\"WR\",\"WR\",\"WR\",\"WR\",\"TE\",\"TE\",\"LB\"),\n  age = c(40, 30, 24, 20, 18, 23, 27, 32, 26, 23, NA, 37)\n  )\n\nfantasyPoints &lt;- data.frame(\n  ID = c(2, 7, 13, 14),\n  fantasyPoints = c(250, 170, 65, 15)\n)\n\n\nWe can also create a data frame from all combinations of variables, using the base::expand.grid() function:\n\nCodefantasyPoints_weekly &lt;- expand.grid(\n  ID = 1:12,\n  season = c(2022, 2023),\n  week = 1:17\n)\n\n\nWe can create a vector that is randomly sampled from values using the base::sample() function:\n\nCodeset.seed(52242)\nfantasyPoints_weekly$fantasyPoints &lt;- sample(\n  0:35,\n  size = nrow(fantasyPoints_weekly),\n  replace = TRUE\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createList",
    "href": "getting-started.html#sec-createList",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.11 Create a List",
    "text": "3.11 Create a List\nA list can store multiple data frames in one object using the base::list() function:\n\nCodeexampleList &lt;- list(players, fantasyPoints, fantasyPoints_weekly)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingStartedLoadData",
    "href": "getting-started.html#sec-gettingStartedLoadData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.12 Load a Data Frame",
    "text": "3.12 Load a Data Frame\nHere is how you load a .RData file using a relative path (i.e., a path relative to the working directory, where the working directory is represented by a period) using the base::load() function:\n\nCodeload(file = \"./data/nfl_players.RData\")\n\n\nThe direction of the slashes matters—you should use forward slashes (not backslashes)! To determine where you working directory is, you can use the base::getwd() function:\n\nCodegetwd()\n\n[1] \"/home/runner/work/Fantasy-Football-Analytics-Textbook/Fantasy-Football-Analytics-Textbook\"\n\n\nTo change your working directory, you can use the base::setwd() function:\n\nCodesetwd(\"C:/Users/myusername/\")\n\n\nThe following code loads a file from an absolute path:\n\nCodeload(\"C:/Users/myusername/nfl_players.RData\")\n\n\nHere is how you load a .csv file using the utils::read.csv() function:\n\nCodenfl_players &lt;- read.csv(\"./data/nfl_players.csv\") # relative path\nnfl_players &lt;- read.csv(\"C:/Users/myusername/nfl_players.csv\") # absolute path",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingStartedSaveData",
    "href": "getting-started.html#sec-gettingStartedSaveData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.13 Save a Data Frame",
    "text": "3.13 Save a Data Frame\nHere is how you save a .RData file using a relative path (i.e., relative to your working directory) using the base::save() function:\n\nCodesave(\n  nfl_players,\n  file = \"./data/nfl_players.RData\")\n\n\nThe direction of the slashes matters—you should use forward slashes (not backslashes)! The following code saves a file to an absolute path:\n\nCodesave(\n  nfl_players,\n  file = \"C:/Users/myusername/nfl_players.RData\")\n\n\nHere is how you save a .csv file using the utils::write.csv() function:\n\nCodewrite.csv(\n  nfl_players,\n  file = \"./data/nfl_players.csv\") # relative path\n\nwrite.csv(\n  nfl_players,\n  file = \"C:/Users/myusername/nfl_players.csv\") # absolute path",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-variableNames",
    "href": "getting-started.html#sec-variableNames",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.14 Variable Names",
    "text": "3.14 Variable Names\nTo see the names of variables in a data frame, use the base::names() function:\n\nCodenames(nfl_players)\n\n [1] \"gsis_id\"                  \"first_name\"              \n [3] \"last_name\"                \"position\"                \n [5] \"esb_id\"                   \"display_name\"            \n [7] \"rookie_year\"              \"college_conference\"      \n [9] \"current_team_id\"          \"draft_club\"              \n[11] \"draft_number\"             \"draftround\"              \n[13] \"entry_year\"               \"football_name\"           \n[15] \"gsis_it_id\"               \"headshot\"                \n[17] \"jersey_number\"            \"position_group\"          \n[19] \"short_name\"               \"smart_id\"                \n[21] \"status\"                   \"status_description_abbr\" \n[23] \"status_short_description\" \"team_abbr\"               \n[25] \"uniform_number\"           \"height\"                  \n[27] \"weight\"                   \"college_name\"            \n[29] \"years_of_experience\"      \"birth_date\"              \n[31] \"team_seq\"                 \"suffix\"                  \n\nCodenames(players)\n\n[1] \"ID\"       \"name\"     \"position\" \"age\"     \n\nCodenames(fantasyPoints)\n\n[1] \"ID\"            \"fantasyPoints\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-logicalOperators",
    "href": "getting-started.html#sec-logicalOperators",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.15 Logical Operators",
    "text": "3.15 Logical Operators\nLogical Operators evaluate one or more elements for a condition, and return TRUE, FALSE, or if the element was missing, NA.\n\n3.15.1 Is Equal To: ==\n\n\nCodeplayers$position == \"RB\"\n\n [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n3.15.2 Is Not Equal To: !=\n\n\nCodeplayers$position != \"RB\"\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\n3.15.3 Is Greater Than: &gt;\n\n\nCodeplayers$age &gt; 30\n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE    NA  TRUE\n\n\n\n3.15.4 Is Less Than: &lt;\n\n\nCodeplayers$age &lt; 30\n\n [1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA FALSE\n\n\n\n3.15.5 Is Greater Than or Equal To: &gt;=\n\n\nCodeplayers$age &gt;= 30\n\n [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE    NA  TRUE\n\n\n\n3.15.6 Is Less Than or Equal To: &lt;=\n\n\nCodeplayers$age &lt;= 30\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA FALSE\n\n\n\n3.15.7 Is In a Value of Another Vector: %in%\n\n\nCodeplayers$position %in% c(\"RB\",\"WR\")\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\n\n3.15.8 Is Not In a Value of Another Vector: !(%in%)\n\n\nCode!(players$position %in% c(\"RB\",\"WR\"))\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\nThe petersenlab package (Petersen, 2025) has a convenience function, petersenlab::\\%ni%``, for the logical operator of determining whether an element is not in a value of another vector:\n\nCodeplayers$position %ni% c(\"RB\",\"WR\")\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\n\n3.15.9 Is Missing: is.na()\n\nbase::is.na()\n\nCodeis.na(players$age)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n3.15.10 Is Not Missing: !is.na()\n\n\nCode!is.na(players$age)\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n\n\n\n3.15.11 And: &\n\nThe “and” operator (&) is used to string together multiple logical operators that all must evaluate as TRUE in order for the combined evaluation to evaluate as TRUE; otherwise, the combined evaluation evaluates as FALSE.\n\nCodeplayers$position == \"WR\" & players$age &gt; 26\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n3.15.12 Or: |\n\nThe “or” operator (|) is used to string together multiple logical operators, any of which must evaluate as TRUE in order for the combined evaluation to evaluate as TRUE; otherwise, the combined evaluation evaluates as FALSE.\n\nCodeplayers$position == \"WR\" | players$age &gt; 23\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE    NA  TRUE\n\n\n\n3.15.13 which()\nThe operator, base::which(), can be used to obtain the TRUE indices of an object, which is useful for subsetting.\n\nCodewhich(players$position == \"RB\")\n\n[1] 4 5",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-ifelse",
    "href": "getting-started.html#sec-ifelse",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.16 If…Else Conditions",
    "text": "3.16 If…Else Conditions\nWe can use the construction, if()...else if()...else() if we want to perform conditional operations. The typical construction of if()...else if()...else() operates such that it first checks if the first if() condition is true. If the first if() condition is true, it performs the operation specified and terminates the process. If the first if() condition is not true, it checks the else if() conditions in order until one of them is true. There can be multiple else if() conditions. For the first true else if() condition, it performs the operation specified and terminates the process. If none of the else if() conditions is true, it performs the operation specified under else() and then terminates the process. The construction, if()...else if()...else() can only be used on one value at a time.\n\nCodeplayer_rank &lt;- 15\n\nif(player_rank &lt;= 10){ # check this condition first\n  tier &lt;- 1\n  print(tier)\n} else if(player_rank &lt;= 20){ # if first condition was not met, check this condition next\n  tier &lt;- 2\n  print(tier)\n} else if(player_rank &lt;= 30){ # if first two conditions were not met, check this condition next\n  tier &lt;- 3\n  print(tier)\n} else{ # if all other conditions were not met, then do this\n  print(\"Don't draft this player!\")\n}\n\n[1] 2\n\n\nTo apply conditional operations to a vector, we can use the base::ifelse() function.\n\nCodeplayer_rank &lt;- c(1, 10, 20, 40, 100)\n\ntier &lt;- ifelse(\n  player_rank &lt;= 10, # check this condition\n  1, # assign this value if true\n  2) # assign this value if false\n\ntier\n\n[1] 1 1 2 2 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-piping",
    "href": "getting-started.html#sec-piping",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.17 Piping",
    "text": "3.17 Piping\nIn base R, if you want to perform multiple operations, it is common to either a) nest the operations, or b) save the object at each step.\nBelow is an example of nested operations:\n\nCodelength(names(nfl_players))\n\n[1] 32\n\n\nBelow is an example of saving the intermediate object at each step:\n\nCodevariableNames &lt;- names(nfl_players)\nvariableNames\n\n [1] \"gsis_id\"                  \"first_name\"              \n [3] \"last_name\"                \"position\"                \n [5] \"esb_id\"                   \"display_name\"            \n [7] \"rookie_year\"              \"college_conference\"      \n [9] \"current_team_id\"          \"draft_club\"              \n[11] \"draft_number\"             \"draftround\"              \n[13] \"entry_year\"               \"football_name\"           \n[15] \"gsis_it_id\"               \"headshot\"                \n[17] \"jersey_number\"            \"position_group\"          \n[19] \"short_name\"               \"smart_id\"                \n[21] \"status\"                   \"status_description_abbr\" \n[23] \"status_short_description\" \"team_abbr\"               \n[25] \"uniform_number\"           \"height\"                  \n[27] \"weight\"                   \"college_name\"            \n[29] \"years_of_experience\"      \"birth_date\"              \n[31] \"team_seq\"                 \"suffix\"                  \n\nCodelengthOfVariableNames &lt;- length(variableNames)\nlengthOfVariableNames\n\n[1] 32\n\n\nCode for performing nested operations can be challenging to read. Saving the intermediate object can be a waste of time to do if you are not interested in the intermediate object, and can take up unnecessary memory and computational resources. An alternative approach is to use piping. Piping allows taking the result from one computation and sending it to the next computation, thus allowing a chain of computations without saving the intermediate object at each step.\nIn base R, you can perform piping with the base::\\|&gt;`expression. Intidyverse, you can perform piping with themagrittr::`%&gt;%`` expression.\n\n3.17.0.1 Base R\n\n\nCodenfl_players |&gt;\n  names() |&gt;\n  length()\n\n[1] 32\n\n\n\n3.17.0.2 Tidyverse\n\nCodenfl_players %&gt;%\n  names() %&gt;%\n  length()\n\n[1] 32",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-subset",
    "href": "getting-started.html#sec-subset",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.18 Subset",
    "text": "3.18 Subset\nTo subset a vector, use brackets to specify the elements to keep:\n\nCodevector[elementsToKeep]\n\n\nTo subset a data frame, use brackets to specify the subset of rows and columns to keep, where the value/vector before the comma specifies the rows to keep, and the value/vector after the comma specifies the columns to keep:\n\nCodedataframe[rowsToKeep, columnsToKeep]\n\n\nYou can subset by using any of the following:\n\nnumeric indices of the elements/rows/columns to keep (or drop)\nnames of the rows/columns to keep (or drop)\nvalues of TRUE and FALSE corresponding to which elements/rows/columns to keep\n\n\n3.18.1 One Variable\nTo subset one variable, use the following syntax:\n\nCodeplayers$name\n\n [1] \"Ken Cussion\"      \"Ben Sacked\"       \"Chuck Downfield\"  \"Ron Ingback\"     \n [5] \"Rhonda Ball\"      \"Hugo Long\"        \"Lionel Scrimmage\" \"Drew Blood\"      \n [9] \"Chase Emdown\"     \"Justin Time\"      \"Spike D'Ball\"     \"Isac Ulooz\"      \n\n\nor:\n\nCodeplayers[,\"name\"]\n\n [1] \"Ken Cussion\"      \"Ben Sacked\"       \"Chuck Downfield\"  \"Ron Ingback\"     \n [5] \"Rhonda Ball\"      \"Hugo Long\"        \"Lionel Scrimmage\" \"Drew Blood\"      \n [9] \"Chase Emdown\"     \"Justin Time\"      \"Spike D'Ball\"     \"Isac Ulooz\"      \n\n\n\n3.18.2 Particular Rows of One Variable\nTo subset one variable, use the following syntax:\n\nCodeplayers$name[which(players$position == \"RB\")]\n\n[1] \"Ron Ingback\" \"Rhonda Ball\"\n\n\nor:\n\nCodeplayers[which(players$position == \"RB\"), \"name\"]\n\n[1] \"Ron Ingback\" \"Rhonda Ball\"\n\n\n\n3.18.3 Particular Columns (Variables)\nTo subset particular columns/variables, use the following syntax:\n\n3.18.3.1 Base R\n\n\nCodesubsetVars &lt;- c(\"name\",\"age\")\n\nplayers[,c(2,4)]\n\n\n  \n\n\nCodeplayers[,c(\"name\",\"age\")]\n\n\n  \n\n\nCodeplayers[,subsetVars]\n\n\n  \n\n\n\nOr, to drop columns:\n\nCodedropVars &lt;- c(\"name\",\"age\")\n\nplayers[,-c(2,4)]\n\n\n  \n\n\nCodeplayers[,!(names(players) %in% c(\"name\",\"age\"))]\n\n\n  \n\n\nCodeplayers[,!(names(players) %in% dropVars)]\n\n\n  \n\n\n\n\n3.18.3.2 Tidyverse\nIn tidyverse, you cans select particular columns using the dplyr::select() function.\n\nCodeplayers %&gt;%\n  dplyr::select(name, age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  dplyr::select(name:age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  dplyr::select(all_of(subsetVars))\n\n\n  \n\n\n\nOr, to drop columns:\n\nCodeplayers %&gt;%\n  dplyr::select(-name, -age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  dplyr::select(-c(name:age))\n\n\n  \n\n\nCodeplayers %&gt;%\n  dplyr::select(-all_of(dropVars))\n\n\n  \n\n\n\n\n3.18.4 Particular Rows\nTo subset particular rows, use the following syntax:\n\n3.18.4.1 Base R\n\n\nCodesubsetRows &lt;- c(4,5)\n\nplayers[c(4,5),]\n\n\n  \n\n\nCodeplayers[subsetRows,]\n\n\n  \n\n\nCodeplayers[which(players$position == \"RB\"),]\n\n\n  \n\n\n\n\n3.18.4.2 Tidyverse\nIn tidyverse, you cans select particular columns using the dplyr::filter() function.\n\nCodeplayers %&gt;%\n  filter(position == \"WR\")\n\n\n  \n\n\nCodeplayers %&gt;%\n  filter(position == \"WR\", age &lt;= 26)\n\n\n  \n\n\nCodeplayers %&gt;%\n  filter(position == \"WR\" | age &gt;= 26)\n\n\n  \n\n\n\n\n3.18.5 Particular Rows and Columns\nTo subset particular rows and columns, use the following syntax:\n\n3.18.5.1 Base R\n\n\nCodeplayers[c(4,5), c(2,4)]\n\n\n  \n\n\nCodeplayers[subsetRows, subsetVars]\n\n\n  \n\n\nCodeplayers[which(players$position == \"RB\"), subsetVars]\n\n\n  \n\n\n\n\n3.18.5.2 Tidyverse\nIn tidyverse, you cans select particular rows and columns using the dplyr::filter() and dplyr::select() functions, respectively.\n\nCodeplayers %&gt;%\n  dplyr::filter(position == \"RB\") %&gt;%\n  dplyr::select(all_of(subsetVars))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-viewData",
    "href": "getting-started.html#sec-viewData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.19 View Data",
    "text": "3.19 View Data\n\n3.19.1 All Data\nTo view data, use the utils::View() function:\n\nCodeView(players)\n\n\n\n3.19.2 First 6 Rows/Elements\nTo view only the first six rows (if a data frame) or elements (if a vector), use the utils::head() function:\n\nCodehead(nfl_players)\n\n\n  \n\n\nCodehead(nfl_players$display_name)\n\n[1] \"'Omar Ellison\"    \"A'Shawn Robinson\" \"A.J. Arcuri\"      \"A.J. Barner\"     \n[5] \"A.J. Bouye\"       \"A.J. Brown\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-sortData",
    "href": "getting-started.html#sec-sortData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.20 Sort Data",
    "text": "3.20 Sort Data\n\nCodeplayers %&gt;% \n  arrange(position, age) #sort by position (ascending) then by age (ascending)\n\n\n  \n\n\nCodeplayers %&gt;% \n  arrange(position, -age) #sort by position (ascending) then by age (descending)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-dataCharacteristics",
    "href": "getting-started.html#sec-dataCharacteristics",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.21 Data Characteristics",
    "text": "3.21 Data Characteristics\n\n3.21.1 Data Structure\nutils::str()\n\nCodestr(nfl_players)\n\nnflvrs_d [21,419 × 32] (S3: nflverse_data/tbl_df/tbl/data.table/data.frame)\n $ gsis_id                 : chr [1:21419] \"00-0004866\" \"00-0032889\" \"00-0037845\" \"00-0039793\" ...\n $ first_name              : chr [1:21419] \"'Omar\" \"A'Shawn\" \"A.J.\" \"A.J.\" ...\n $ last_name               : chr [1:21419] \"Ellison\" \"Robinson\" \"Arcuri\" \"Barner\" ...\n $ position                : chr [1:21419] \"WR\" \"DE\" \"T\" \"TE\" ...\n $ esb_id                  : chr [1:21419] \"ELL711319\" \"ROB367960\" \"ARC716900\" \"BAR235889\" ...\n $ display_name            : chr [1:21419] \"'Omar Ellison\" \"A'Shawn Robinson\" \"A.J. Arcuri\" \"A.J. Barner\" ...\n $ rookie_year             : int [1:21419] NA 2016 2022 2024 2013 2019 2015 2019 NA NA ...\n $ college_conference      : chr [1:21419] NA \"Southeastern Conference\" \"Big Ten Conference\" \"Big Ten Conference\" ...\n $ current_team_id         : chr [1:21419] \"4400\" \"0750\" \"2510\" \"4600\" ...\n $ draft_club              : chr [1:21419] NA \"DET\" \"LA\" \"SEA\" ...\n $ draft_number            : int [1:21419] NA 46 261 121 NA 51 67 NA NA NA ...\n $ draftround              : int [1:21419] NA 2 7 4 NA 2 3 NA NA NA ...\n $ entry_year              : int [1:21419] NA 2016 2022 2024 2013 2019 2015 2019 NA NA ...\n $ football_name           : chr [1:21419] NA \"A'Shawn\" \"A.J.\" \"A.J.\" ...\n $ gsis_it_id              : int [1:21419] NA 43335 54726 57242 40688 47834 42410 48335 NA NA ...\n $ headshot                : chr [1:21419] NA \"https://static.www.nfl.com/image/upload/f_auto,q_auto/league/erkfjohq7j2kqigny727\" \"https://static.www.nfl.com/image/upload/f_auto,q_auto/league/rqisaua66gwc8wxgajz1\" \"https://static.www.nfl.com/image/upload/f_auto,q_auto/league/msnzbeyjoemcas9dm8vt\" ...\n $ jersey_number           : int [1:21419] 84 94 61 88 24 11 60 6 81 63 ...\n $ position_group          : chr [1:21419] \"WR\" \"DL\" \"OL\" \"TE\" ...\n $ short_name              : chr [1:21419] NA \"A.Robinson\" \"A.Arcuri\" \"A.Barner\" ...\n $ smart_id                : chr [1:21419] \"3200454c-4c71-1319-728e-d49d3d236f8f\" \"3200524f-4236-7960-bf20-bc060ac0f49c\" \"32004152-4371-6900-5185-8cdd66b2ad11\" \"32004241-5223-5889-95d9-0ba3aeeb36ed\" ...\n $ status                  : chr [1:21419] \"RET\" \"ACT\" \"ACT\" \"ACT\" ...\n $ status_description_abbr : chr [1:21419] NA \"A01\" \"A01\" \"A01\" ...\n $ status_short_description: chr [1:21419] NA \"Active\" \"Active\" \"Active\" ...\n $ team_abbr               : chr [1:21419] \"LAC\" \"CAR\" \"LA\" \"SEA\" ...\n $ uniform_number          : chr [1:21419] NA \"94\" \"61\" \"88\" ...\n $ height                  : num [1:21419] 73 76 79 78 72 72 75 76 69 76 ...\n $ weight                  : int [1:21419] 200 330 320 251 191 226 325 220 190 280 ...\n $ college_name            : chr [1:21419] NA \"Alabama\" \"Michigan State\" \"Michigan\" ...\n $ years_of_experience     : chr [1:21419] \"2\" \"9\" \"2\" \"0\" ...\n $ birth_date              : chr [1:21419] \"1971-10-08\" \"1995-03-21\" \"1997-08-13\" \"2002-05-03\" ...\n $ team_seq                : int [1:21419] NA 1 NA NA 1 1 1 1 NA NA ...\n $ suffix                  : chr [1:21419] NA NA NA NA ...\n - attr(*, \"nflverse_type\")= chr \"players\"\n - attr(*, \"nflverse_timestamp\")= chr \"2025-05-31 22:15:12 EDT\"\n\n\n\n3.21.2 Data Dimensions\nNumber of rows and columns (base::dim()):\n\nCodedim(nfl_players)\n\n[1] 21419    32\n\n\nNumber of rows (base::nrow()):\n\nCodenrow(nfl_players)\n\n[1] 21419\n\n\nNumber of columns (base::ncol()):\n\nCodencol(nfl_players)\n\n[1] 32\n\n\n\n3.21.3 Number of Elements\nbase::length()\n\nCodelength(nfl_players$display_name)\n\n[1] 21419\n\n\n\n3.21.4 Number of Missing Elements\nMissing elements are stored in R as NA. You can determine how many missing elements there are in a data frame using the following syntax:\n\nCodelength(which(is.na(nfl_players)))\n\n[1] 239935\n\n\nYou can determine how many missing elements there are in a variable using the following syntax:\n\nCodelength(nfl_players$college_name[which(is.na(nfl_players$college_name))])\n\n[1] 12127\n\n\n\n3.21.5 Number of Non-Missing Elements\nYou can determine how many non-missing elements there are in a data frame using the following syntax:\n\nCodelength(which(!is.na(nfl_players)))\n\n[1] 445473\n\n\nYou can determine how many rows in the data frame have no missing data using the following syntax and the stats::na.omit() function:\n\nCodenrow(na.omit(nfl_players))\n\n[1] 176\n\n\nYou can determine how many non-missing elements there are in a vector using the following syntax:\n\nCodelength(nfl_players$college_name[which(!is.na(nfl_players$college_name))])\n\n[1] 9292\n\nCodelength(na.omit(nfl_players$college_name))\n\n[1] 9292",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createNewVars",
    "href": "getting-started.html#sec-createNewVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.22 Create New Variables",
    "text": "3.22 Create New Variables\nTo create a new variable, use the following syntax:\n\nCodeplayers$newVar &lt;- NA\n\n\nHere is an example of creating a new variable:\n\nCodeplayers$newVar &lt;- 1:nrow(players)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-recodeVars",
    "href": "getting-started.html#sec-recodeVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.23 Recode Variables",
    "text": "3.23 Recode Variables\nHere is an example of recoding a variable:\n\nCodeplayers$oldVar1 &lt;- NA\nplayers$oldVar1[which(players$position == \"QB\")] &lt;- \"quarterback\"\nplayers$oldVar1[which(players$position == \"RB\")] &lt;- \"running back\"\nplayers$oldVar1[which(players$position == \"WR\")] &lt;- \"wide receiver\"\nplayers$oldVar1[which(players$position == \"TE\")] &lt;- \"tight end\"\n\nplayers$oldVar2 &lt;- NA\nplayers$oldVar2[which(players$age &lt; 30)] &lt;- \"young\"\nplayers$oldVar2[which(players$age &gt;= 30)] &lt;- \"old\"\n\n\nRecode multiple variables using the dplyr::mutate() and dplyr::case_match() functions:\n\nCodeplayers %&gt;%\n  dplyr::mutate(across(c(\n    oldVar1:oldVar2),\n    ~ case_match(\n      .,\n      c(\"quarterback\",\"old\",\"running back\") ~ 0,\n      c(\"wide receiver\",\"tight end\",\"young\") ~ 1)))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#renameVars",
    "href": "getting-started.html#renameVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.24 Rename Variables",
    "text": "3.24 Rename Variables\ndplyr::rename()\n\nCodeplayers &lt;- players %&gt;% \n  dplyr::rename(\n    newVar1 = oldVar1,\n    newVar2 = oldVar2)\n\n\nUsing a vector of variable names with the dplyr::rename_with() function:\n\nCodevarNamesFrom &lt;- c(\"oldVar1\",\"oldVar2\")\nvarNamesTo &lt;- c(\"newVar1\",\"newVar2\")\n\nplayers &lt;- players %&gt;% \n  dplyr::rename_with(~ varNamesTo, all_of(varNamesFrom))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#convertVarTypes",
    "href": "getting-started.html#convertVarTypes",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.25 Convert the Types of Variables",
    "text": "3.25 Convert the Types of Variables\nOne variable (base::factor(); base::as.numeric(); base::as.integer(); base::as.character()):\n\nCodeplayers$factorVar &lt;- factor(players$ID)\nplayers$numericVar &lt;- as.numeric(players$age)\nplayers$integerVar &lt;- as.integer(players$newVar1)\nplayers$characterVar &lt;- as.character(players$newVar2)\n\n\nMultiple variables:\n\nCodeplayers %&gt;%\n  dplyr::mutate(across(c(\n    ID,\n    age),\n    as.numeric))\n\n\n  \n\n\nCodeplayers %&gt;%\n  dplyr::mutate(across(\n    age:newVar1,\n    as.character))\n\n\n  \n\n\nCodeplayers %&gt;%\n  dplyr::mutate(across(\n    where(is.factor),\n    as.character))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-merging",
    "href": "getting-started.html#sec-merging",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.26 Merging/Joins",
    "text": "3.26 Merging/Joins\n\n3.26.1 Overview\nMerging (also called joining) merges two data objects using a shared set of variables called “keys.” The keys are the variable(s) that are used to align the rows from the two objects. The data for the given key(s) in the first object get paired with (i.e., get placed in the same row as) the data for that same key in the second object. In general, each row should have a value on each of the keys; there should be no missingness in the keys. To merge two objects, the key(s) that will be used to match the records must be present in both objects. The keys are used to merge the variables in object 1 (x) with the variables in object 2 (y). Different merge types select different rows to merge.\nFor some data objects, you might want to combine information for the same player from multiple data objects. If each data object is in player form (i.e., player_id uniquely identifies each row), you might merge by the player’s identification number (e.g., player_id). In this case, the key uniquely identifies each row.\nHowever, some data objects have multiple keys. For instance, in long form data objects, each player may have multiple rows corresponding to multiple seasons. In this case, the keys may be player_id and season—that is, the data are in player-season form. If object 1 and object 2 are both in player-season form, we would use player_id and season as the keys to merge the two objects. In this case, the keys uniquely identify each row; that is, they account for the levels of nesting.\nHowever, if the data objects are of different form, we would select the keys as the variable(s) that represent the lowest common denominator of variables used to join the data objects that are present in both objects. For instance, assume that object 1 is in player-season form. For object 2, each player has multiple rows corresponding to seasons and games/weeks—in this case, object 2 is in player-season-week form. Object 1 does not have the week variable, so it cannot be used to join the objects. Thus, we would use player_id and season as the keys to merge the two objects, because both variables are present in both objects.\nIt is important not to have rows with duplicate values on the keys. For instance, if there is more than one row with the same player_id in each object (or multiple rows in object 2 with the same combination of player_id, season, and week), then each row with that player_id in object 1 gets paired with each row with that player_id in object 2. The many possible combinations can lead to the resulting object greatly expanding in terms of the number of rows. Thus, you want the keys to uniquely identify each row. In the example below, player is present in each object, so we can merge by player; however, each object has multiple rows with the same player. For example, mergeExample1A has three rows for player A; mergeExample1B has two rows for player A. Thus, when we merge them, the resulting object has many more rows than each respective object (even though neither object has players that the other object does not). Below, we merge data using the dplyr::full_join() function:\n\nCodemergeExample1A &lt;- data.frame(\n  player = c(\"A\",\"A\",\"A\",\"B\",\"B\"),\n  age = c(20,22,24,26,28)\n)\n\nmergeExample1B &lt;- data.frame(\n  player = c(\"A\",\"A\",\"B\",\"B\"),\n  points = c(10,15,20,25)\n)\n\nmergeExample1 &lt;- dplyr::full_join(\n  mergeExample1A,\n  mergeExample1B,\n  by = \"player\")\n\nmergeExample1\n\n\n  \n\n\nCodedim(mergeExample1)\n\n[1] 10  3\n\n\nNote: if the two objects include variables with the same name (apart from the keys), R will not know how you want each to appear in the merged object. So, it will add a suffix (e.g., .x, .y) to each common variable to indicate which object (i.e., object x or object y) the variable came from, where object x is the first object—i.e., the object to which object y (the second object) is merged. In general, apart from the keys, you should not include variables with the same name in two objects to be merged. To prevent this, either remove or rename the shared variable in one of the objects, or include the shared variable as a key. However, as described above, you should include it as a key only if you want to use its values to align the rows from each object. Below is an example of merging two objects with the same variable name (i.e., points) that is not used as a key, using the dplyr::full_join() function.\n\nCodemergeExample2A &lt;- data.frame(\n  player = c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  points = c(20,22,24,26,28)\n)\n\nmergeExample2B &lt;- data.frame(\n  player = c(\"A\",\"B\",\"C\",\"F\"),\n  points = c(10,15,20,25)\n)\n\nmergeExample2 &lt;- dplyr::full_join(\n  mergeExample2A,\n  mergeExample2B,\n  by = \"player\")\n\nmergeExample2\n\n\n  \n\n\n\nWhen two objects are merged that have different formats, the resulting data object inherits the format of the data object that has more levels of nesting. For instance, consider that you want to merge two objects, object A and object B. Object A is in player form and object B is in player-season-week form. When you merge them, the resulting data object will be in player-season-week form. Below, we merge data using the dplyr::full_join() function:\n\nCodemergeExample3A &lt;- data.frame(\n  player = c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  weight = c(225,250,275,300,325)\n)\n\nmergeExample3B &lt;- data.frame(\n  player = c(\"A\",\"A\",\"A\",\"A\",\"B\",\"B\"),\n  season = c(2023,2023,2024,2024,2024,2024),\n  week = c(1,2,1,2,3,4),\n  points = c(10,15,20,25,30,35)\n)\n\nmergeExample3 &lt;- dplyr::full_join(\n  mergeExample3A,\n  mergeExample3B,\n  by = \"player\")\n\nmergeExample3\n\n\n  \n\n\n\n\n3.26.2 Data Before Merging\nHere are the data in the players object:\n\nCodeplayers\n\n\n  \n\n\nCodedim(players)\n\n[1] 12 10\n\n\nThe data are structured in ID form. That is, every row in the dataset is uniquely identified by the variable, ID.\nHere are the data in the fantasyPoints object:\n\nCodefantasyPoints\n\n\n  \n\n\nCodedim(fantasyPoints)\n\n[1] 4 2\n\n\n\n3.26.3 Types of Joins\n\n3.26.3.1 Visual Overview of Join Types\nFigure 3.1 depicts various types of merges/joins. Object x is the circle labeled as x. Object y is the circle labeled as y. The area of overlap in the Venn diagram indicates the rows on the keys that are shared between the two objects (e.g., the same player_id, season, and week). The non-overlapping area indicates the rows on the keys that are unique to each object. The shaded blue area indicates which rows (on the keys) are kept in the merged object from each of the two objects, when using each of the merge types. For instance, a left outer join keeps the shared rows and the rows that are unique to object x, but it drops the rows that are unique to object y.\n\n\n\n\n\nFigure 3.1: Types of Merges/Joins.\n\n\n\n3.26.3.2 Full Outer Join\nA full outer join includes all rows in x or y. It returns columns from x and y. Here is how to merge two data frames using a full outer join (i.e., “full join”; dplyr::full_join()):\n\nCodefullJoinData &lt;- dplyr::full_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nfullJoinData\n\n\n  \n\n\nCodedim(fullJoinData)\n\n[1] 14 11\n\n\n\n3.26.3.3 Left Outer Join\nA left outer join includes all rows in x. It returns columns from x and y. Here is how to merge two data frames using a left outer join (“left join”; dplyr::left_join()):\n\nCodeleftJoinData &lt;- dplyr::left_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nleftJoinData\n\n\n  \n\n\nCodedim(leftJoinData)\n\n[1] 12 11\n\n\n\n3.26.3.4 Right Outer Join\nA right outer join includes all rows in y. It returns columns from x and y. Here is how to merge two data frames using a right outer join (“right join”; dplyr::right_join()):\n\nCoderightJoinData &lt;- dplyr::right_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nrightJoinData\n\n\n  \n\n\nCodedim(rightJoinData)\n\n[1]  4 11\n\n\n\n3.26.3.5 Inner Join\nAn inner join includes all rows that are in both x and y. An inner join will return one row of x for each matching row of y, and can duplicate values of records on either side (left or right) if x and y have more than one matching record. It returns columns from x and y. Here is how to merge two data frames using an inner join (dplyr::inner_join()):\n\nCodeinnerJoinData &lt;- dplyr::inner_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\ninnerJoinData\n\n\n  \n\n\nCodedim(innerJoinData)\n\n[1]  2 11\n\n\n\n3.26.3.6 Semi Join\nA semi join is a filter. A left semi join returns all rows from x with a match in y. That is, it filters out records from x that are not in y. Unlike an inner join, a left semi join will never duplicate rows of x, and it includes columns from only x (not from y). Here is how to merge two data frames using a left semi join (dplyr::semi_join()):\n\nCodesemiJoinData &lt;- dplyr::semi_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nsemiJoinData\n\n\n  \n\n\nCodedim(semiJoinData)\n\n[1]  2 10\n\n\n\n3.26.3.7 Anti Join\nAn anti join is a filter. A left anti join returns all rows from x without a match in y. That is, it filters out records from x that are in y. It returns columns from only x (not from y). Here is how to merge two data frames using a left anti join (dplyr::anti_join()):\n\nCodeantiJoinData &lt;- dplyr::anti_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nantiJoinData\n\n\n  \n\n\nCodedim(antiJoinData)\n\n[1] 10 10\n\n\n\n3.26.3.8 Cross Join\nA cross join combines each row in x with each row in y (dplyr::cross_join()).\n\nCodecrossJoinData &lt;- dplyr::cross_join(\n  players,\n  fantasyPoints)\n\ncrossJoinData\n\n\n  \n\n\nCodedim(crossJoinData)\n\n[1] 48 12",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-longWideForm",
    "href": "getting-started.html#sec-longWideForm",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.27 Long and Wide Form",
    "text": "3.27 Long and Wide Form\nDepending on the analysis, it may be important to restructure the data to be in long or wide form. When the data are in wide form, each player has only one row. When the data are in long form, each player has multiple rows—e.g., a row for each game. The data structure is called wide or long form because a dataset in wide form has more columns and fewer rows (i.e., it appears wider and shorter), whereas a dataset in long form has more rows and fewer columns (i.e., it appears narrower and taller).\nHere are the original data in long form. The data are structured in “player-season-week form”. That is, every row in the dataset is uniquely identified by the combination of variables, ID, season, and week—these are the keys. This is an example of long form, because each player has multiple rows.\n\nCodedataLong &lt;- dplyr::full_join(\n  players %&gt;% dplyr::select(-age),\n  fantasyPoints_weekly,\n  by = c(\"ID\")\n) %&gt;% \n  arrange(ID, season, week)\n\ndataLong\n\n\n  \n\n\nCodedim(dataLong)\n\n[1] 408  12\n\nCodenames(dataLong)\n\n [1] \"ID\"            \"name\"          \"position\"      \"newVar1\"      \n [5] \"newVar2\"       \"factorVar\"     \"numericVar\"    \"integerVar\"   \n [9] \"characterVar\"  \"season\"        \"week\"          \"fantasyPoints\"\n\n\n\n3.27.1 Transform Data from Long to Wide\nBelow, we transform the data from long form to wide form. The long data are in “player-season-week form”. We widen the data by two variables (season and week), using tidyverse, so that the data are now in “player form” (where each row is uniquely identified by the ID variable) using the tidyr::pivot_wider() function:\n\nCodedataWide &lt;- dataLong %&gt;% \n  tidyr::pivot_wider(\n    names_from = c(season, week),\n    names_glue = \"{.value}_{season}_week{week}\",\n    values_from = fantasyPoints)\n\ndataWide\n\n\n  \n\n\nCodedim(dataWide)\n\n[1] 12 43\n\nCodenames(dataWide)\n\n [1] \"ID\"                        \"name\"                     \n [3] \"position\"                  \"newVar1\"                  \n [5] \"newVar2\"                   \"factorVar\"                \n [7] \"numericVar\"                \"integerVar\"               \n [9] \"characterVar\"              \"fantasyPoints_2022_week1\" \n[11] \"fantasyPoints_2022_week2\"  \"fantasyPoints_2022_week3\" \n[13] \"fantasyPoints_2022_week4\"  \"fantasyPoints_2022_week5\" \n[15] \"fantasyPoints_2022_week6\"  \"fantasyPoints_2022_week7\" \n[17] \"fantasyPoints_2022_week8\"  \"fantasyPoints_2022_week9\" \n[19] \"fantasyPoints_2022_week10\" \"fantasyPoints_2022_week11\"\n[21] \"fantasyPoints_2022_week12\" \"fantasyPoints_2022_week13\"\n[23] \"fantasyPoints_2022_week14\" \"fantasyPoints_2022_week15\"\n[25] \"fantasyPoints_2022_week16\" \"fantasyPoints_2022_week17\"\n[27] \"fantasyPoints_2023_week1\"  \"fantasyPoints_2023_week2\" \n[29] \"fantasyPoints_2023_week3\"  \"fantasyPoints_2023_week4\" \n[31] \"fantasyPoints_2023_week5\"  \"fantasyPoints_2023_week6\" \n[33] \"fantasyPoints_2023_week7\"  \"fantasyPoints_2023_week8\" \n[35] \"fantasyPoints_2023_week9\"  \"fantasyPoints_2023_week10\"\n[37] \"fantasyPoints_2023_week11\" \"fantasyPoints_2023_week12\"\n[39] \"fantasyPoints_2023_week13\" \"fantasyPoints_2023_week14\"\n[41] \"fantasyPoints_2023_week15\" \"fantasyPoints_2023_week16\"\n[43] \"fantasyPoints_2023_week17\"\n\n\n\n3.27.2 Transform Data from Wide to Long\nConversely, we can also restructure data from wide to long using the tidyr::pivot_longer() function. Here are the data in long form, after they have been transformed from wide form using tidyverse:\n\nCodedataLong &lt;- dataWide %&gt;% \n  tidyr::pivot_longer(\n    cols = fantasyPoints_2022_week1:fantasyPoints_2023_week17,\n    names_to = c(\"season\", \"week\"),\n    names_pattern = \"fantasyPoints_(.*)_week(.*)\",\n    values_to = \"fantasyPoints\")\n\ndataLong\n\n\n  \n\n\nCodedim(dataLong)\n\n[1] 408  12\n\nCodenames(dataLong)\n\n [1] \"ID\"            \"name\"          \"position\"      \"newVar1\"      \n [5] \"newVar2\"       \"factorVar\"     \"numericVar\"    \"integerVar\"   \n [9] \"characterVar\"  \"season\"        \"week\"          \"fantasyPoints\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-loops",
    "href": "getting-started.html#sec-loops",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.28 Loops",
    "text": "3.28 Loops\nIf you want to perform the same computation multiple times, it can be faster to do it in a loop compared to writing out the same computation many times. For instance, here is a loop that runs from 1 to 12 (the number of players in the players object), incrementing by 1 after each iteration. The loop prints each element of a vector (i.e., the player’s name) and the loop index (i) that indicates where the loop is in terms of its iterations:\n\nCodefor(i in 1:length(players$ID)){\n  print(paste(\"The loop is at index:\", i, sep = \" \"))\n  print(paste(\"My favorite player is:\", players$name[i], sep = \" \"))\n}\n\n[1] \"The loop is at index: 1\"\n[1] \"My favorite player is: Ken Cussion\"\n[1] \"The loop is at index: 2\"\n[1] \"My favorite player is: Ben Sacked\"\n[1] \"The loop is at index: 3\"\n[1] \"My favorite player is: Chuck Downfield\"\n[1] \"The loop is at index: 4\"\n[1] \"My favorite player is: Ron Ingback\"\n[1] \"The loop is at index: 5\"\n[1] \"My favorite player is: Rhonda Ball\"\n[1] \"The loop is at index: 6\"\n[1] \"My favorite player is: Hugo Long\"\n[1] \"The loop is at index: 7\"\n[1] \"My favorite player is: Lionel Scrimmage\"\n[1] \"The loop is at index: 8\"\n[1] \"My favorite player is: Drew Blood\"\n[1] \"The loop is at index: 9\"\n[1] \"My favorite player is: Chase Emdown\"\n[1] \"The loop is at index: 10\"\n[1] \"My favorite player is: Justin Time\"\n[1] \"The loop is at index: 11\"\n[1] \"My favorite player is: Spike D'Ball\"\n[1] \"The loop is at index: 12\"\n[1] \"My favorite player is: Isac Ulooz\"\n\n\nLoops are fine for basic computations, but other approaches (such as the apply() family of functions) can be even faster than loops.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createFunction",
    "href": "getting-started.html#sec-createFunction",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.29 Create a Function",
    "text": "3.29 Create a Function\nNow, let’s put together what we have learned to create a useful function. Functions are useful if you want to perform an operation multiple times, especially if you want to be able to manipulate various settings to make slight modifications each time. Any operation that you want to perform multiple times, you can create a function to accomplish. Use of a function can save you time without needed to retype out all of the code each time. For instance, let’s say you want to convert temperature between Fahrenheit and Celsius, you could create a function to do that. In this case, our function has two arguments: temperature (in degrees) and unit of the original temperature (F for Fahrenheit or C for Celsius, where the default unit is Fahrenheit). That allows us to make the slight modification of which unit is the input temperature and to change the calculation accordingly.\n\nCodeconvert_temperature &lt;- function(temperature, unit = \"F\"){ # specify the arguments and any defaults\n  if(unit == \"F\"){ # if the input temperature(s) in Fahrenheit\n    newtemp &lt;- (temperature - 32) / (9/5)\n  } else if(unit == \"C\"){ # if the input temperature(s) in Celsius\n    newtemp &lt;- (temperature * (9/5)) + 32\n  }\n  \n  return(newtemp) # this is what is returned by the function\n}\n\n\nNow we can use the function to convert temperatures between Fahrenheit and Celsius. A temperature of 32°F is equal to 0°C. A temperature of 0°C is equal to 89.6°F.\n\nCodeconvert_temperature(\n  temperature = 32,\n  unit = \"F\"\n)\n\n[1] 0\n\nCodeconvert_temperature(\n  temperature = 32,\n  unit = \"C\"\n)\n\n[1] 89.6\n\n\nWe can also convert the temperature for a vector of values at once:\n\nCodeconvert_temperature(\n  temperature = c(0, 10, 20, 30, 40, 50),\n  unit = \"F\"\n)\n\n[1] -17.777778 -12.222222  -6.666667  -1.111111   4.444444  10.000000\n\nCodeconvert_temperature(\n  temperature = c(0, 10, 20, 30, 40, 50),\n  unit = \"C\"\n)\n\n[1]  32  50  68  86 104 122\n\n\nBecause the default unit is “F”, we do not need to specify the unit if our input temperatures are in Fahrenheit:\n\nCodeconvert_temperature(\n  c(0, 10, 20, 30, 40, 50)\n)\n\n[1] -17.777778 -12.222222  -6.666667  -1.111111   4.444444  10.000000",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-comment",
    "href": "getting-started.html#sec-comment",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.30 Commenting Code",
    "text": "3.30 Commenting Code\nTo comment your code, using the # sign. Anything on the line that appears after the # sign will be interpreted as a comment and will not be evaluated by R. It is important to comment your code frequently with what you are doing doing and why so that you and others know how to read it.\n\nCodex &lt;- 1 # this line will run\n       # this line will NOT run: x &lt;- 2\n\nx\n\n[1] 1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingStartedSessionInfo",
    "href": "getting-started.html#sec-gettingStartedSessionInfo",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n4.1 Session Info",
    "text": "4.1 Session Info\nAt the end of each chapter in which R code is used, I provide the session information, which describes the system and operating system the code was run on and the versions of each package. That way, if you get different results from me, you can see which session settings differ, to help with reproducibility. If you run the (all of) the exact same code as is provided in the text, in the exact same order, with the exact same setup (platform, operating system, package versions, etc.) and the exact same data, you should get the exact same answer as is in the text. That is the idea of reproducibility—getting the exact same result with the exact same inputs. Reproducibility is crucial for studies to achieve greater confidence in their findings and to ensure better replicability of findings across studies.\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n [9] ggplot2_3.5.2     tidyverse_2.0.0   petersenlab_1.2.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   psych_2.5.6        viridisLite_0.4.2  farver_2.1.2      \n [5] fastmap_1.2.0      digest_0.6.37      rpart_4.1.24       timechange_0.3.0  \n [9] lifecycle_1.0.4    cluster_2.1.8.1    magrittr_2.0.3     compiler_4.5.1    \n[13] rlang_1.1.6        Hmisc_5.2-3        tools_4.5.1        yaml_2.3.10       \n[17] data.table_1.17.8  knitr_1.50         htmlwidgets_1.6.4  mnormt_2.1.1      \n[21] plyr_1.8.9         RColorBrewer_1.1-3 foreign_0.8-90     withr_3.0.2       \n[25] nnet_7.3-20        grid_4.5.1         stats4_4.5.1       lavaan_0.6-19     \n[29] xtable_1.8-4       colorspace_2.1-1   scales_1.4.0       MASS_7.3-65       \n[33] cli_3.6.5          mvtnorm_1.3-3      rmarkdown_2.29     reformulas_0.4.1  \n[37] generics_0.1.4     rstudioapi_0.17.1  reshape2_1.4.4     tzdb_0.5.0        \n[41] minqa_1.2.8        DBI_1.2.3          splines_4.5.1      parallel_4.5.1    \n[45] base64enc_0.1-3    mitools_2.4        vctrs_0.6.5        boot_1.3-31       \n[49] Matrix_1.7-3       jsonlite_2.0.0     hms_1.1.3          Formula_1.2-5     \n[53] htmlTable_2.4.3    glue_1.8.0         nloptr_2.2.1       stringi_1.8.7     \n[57] gtable_0.3.6       quadprog_1.5-8     lme4_1.1-37        pillar_1.11.0     \n[61] htmltools_0.5.8.1  R6_2.6.1           Rdpack_2.6.4       mix_1.0-13        \n[65] evaluate_1.0.4     pbivnorm_0.6.0     lattice_0.22-7     rbibutils_2.3     \n[69] backports_1.5.0    Rcpp_1.1.0         gridExtra_2.3      nlme_3.1-168      \n[73] checkmate_2.3.3    xfun_0.53          pkgconfig_2.0.3   \n\n\n\n\n\n\nAden-Buie, G., Schloerke, B., Allaire, J., & Rossell Hayes, A. (2023). learnr: Interactive tutorials for R. https://doi.org/10.32614/CRAN.package.learnr\n\n\nBryan, J., Hester, J., Robinson, D., Wickham, H., Dervieux, C., & Posit. (2025). Reprex do’s and don’ts. https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nCsárdi, G., Hester, J., Wickham, H., Chang, W., Morgan, M., & Tenenbaum, D. (2024). remotes: R package installation from remote repositories, including GitHub. https://doi.org/10.32614/CRAN.package.remotes\n\n\nPetersen, I. T. (2025). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nR Core Team. (2025). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org\n\n\nSalmon, M. (2018). Where to get help with your R question? https://masalmon.eu/2018/07/22/wheretogethelp/\n\n\nStack Overflow. (2018). How to make a great R reproducible example. https://stackoverflow.com/a/5963610\n\n\nStack Overflow. (2025). How to create a minimal, reproducible example. https://stackoverflow.com/help/minimal-reproducible-example\n\n\nTungate, A., Andersen, D., & Petersen, I. T. (2025). ffanalytics: Scrape data for fantasy football. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nWickham, H. (2023). tidyverse: Easily install and load the Tidyverse. https://doi.org/10.32614/CRAN.package.tidyverse\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#footnotes",
    "href": "getting-started.html#footnotes",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "",
    "text": "Although the petersenlab package (Petersen, 2025) is hosted on CRAN, installing from GitHub will ensure you have the latest version.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "download-football-data.html",
    "href": "download-football-data.html",
    "title": "4  Download and Process NFL Football Data",
    "section": "",
    "text": "4.1 Load Packages\nThis chapter provides an overview of how to download and process a wide range of football and fantasy football-related data. We leverage the data in subsequent chapters for data analysis.\nCodelibrary(\"ffanalytics\") # to install: install.packages(\"remotes\"); remotes::install_github(\"FantasyFootballAnalytics/ffanalytics\")\nlibrary(\"petersenlab\") # to install: install.packages(\"remotes\"); remotes::install_github(\"DevPsyLab/petersenlab\")\nlibrary(\"nflreadr\")\nlibrary(\"nflfastR\")\nlibrary(\"nflseedR\")\nlibrary(\"nfl4th\")\nlibrary(\"nflplotR\")\nlibrary(\"progressr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-dataDictionary",
    "href": "download-football-data.html#sec-dataDictionary",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n4.2 Data Dictionaries of NFL Data",
    "text": "4.2 Data Dictionaries of NFL Data\nData Dictionaries are metadata that describe the meaning of the variables in a dataset. Ho & Carl (2025a) provide Data Dictionaries for the various National Football League (NFL) datasets at the following link: https://nflreadr.nflverse.com/articles/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-downloadFootballDataOverview",
    "href": "download-football-data.html#sec-downloadFootballDataOverview",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n4.3 Types of NFL Data",
    "text": "4.3 Types of NFL Data\nBelow, we provide examples for how to download various types of NFL data using the nflreadr (Ho & Carl, 2024) and nflfastR (Carl & Baldwin, 2024) packages. For additional resources, Congelio (2023) provides a helpful introductory text for working with NFL data in R. We save each data file after downloading it, so we can use the data in subsequent chapters. If you have difficulty downloading the data files using the nflreadr (Ho & Carl, 2024) or nflfastR (Carl & Baldwin, 2024) packages, we also saved the data files so they are publicly available on the Open Science Framework: https://osf.io/z6pg4.\nThis chapter extensively uses merging to process the data for later use. See Section 3.26 for a reminder of how to perform merging, the types of merges, and what you can expect when you merge data objects with different formats. Guidance for how to merge the various NFL-related data files is provided by Sharpe (2020a): https://github.com/nflverse/nfldata/blob/master/DATASETS.md.\n\n4.3.1 Players\n\nCodenfl_players_raw &lt;- progressr::with_progress(\n  nflreadr::load_players())\n\n\n\nCodesave(\n  nfl_players_raw,\n  file = \"./data/nfl_players_raw.RData\"\n)\n\n\nThe nfl_players object is in player form. That is, each row should be uniquely identified by gsis_id. Let’s rearrange the data accordingly by player.\n\nCodenfl_players &lt;- nfl_players_raw %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  arrange(display_name)\n\n\nIn this case, I arrange the data by player using display_name. You could arrange by gsis_id (or any other variable, for that matter), but gsis_id is fairly unintelligible by itself. For instance, here are the first six players by ID according to gsis_id:\n\nCodehead(nfl_players_raw$gsis_id)\n\n[1] \"00-0004866\" \"00-0032889\" \"00-0037845\" \"00-0039793\" \"00-0030228\"\n[6] \"00-0035676\"\n\n\nNote how it is unclear who these players are until you combine this column with other, more relevant information. Thus, I prefer to sort by a variable that is more interpretable, such as a player’s name. Here are the first six players by name according to display_name:\n\nCodehead(nfl_players_raw$display_name)\n\n[1] \"'Omar Ellison\"    \"A'Shawn Robinson\" \"A.J. Arcuri\"      \"A.J. Barner\"     \n[5] \"A.J. Bouye\"       \"A.J. Brown\"      \n\n\nNote how it is more clear, at a glance and without additional information, who these players are.\nIn data analysis, the variable(s) that are used to sort the dataframe are primarily chosen for aesthetic or usability purposes. Many data analysis approaches do not depend on the order of the rows in the data. However, the sorting of the dataframe may influence data wrangling operations. Thus, when performing data wrangling operations it may be prudent to make sure to re-sort the data to the desired format before performing such operations.\nRegardless of which variable(s) are used to sort the dataframe, it is important to know which variable(s) uniquely identify the rows because that will influence approaches to merge the data with other dataframes. For instance, it would be preferable to merge two data objects based on the player ID (gsis_id) rather than the player name (display_name), because multiple players share the same name, which can lead the merge operation not to know which player from the first dataframe goes with which player from the second dataframe when both players have the same name (despite different having different player IDs).\nLet’s check for duplicate player instances:\n\nCodenfl_players %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.1.1 Processing\nLet’s do some data cleanup:\n\nCode# Convert missing values to NA\nnfl_players[nfl_players == \"\"] &lt;- NA\n\n# Drop players with missing values for gsis_id\nnfl_players &lt;- nfl_players %&gt;% \n  filter(!is.na(gsis_id))\n\n\nHere are the variable names:\n\nCodenames(nfl_players) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_players,\n  file = \"./data/nfl_players.RData\"\n)\n\n\n\n4.3.2 Teams\n\nCodenfl_teams_raw &lt;- progressr::with_progress(\n  nflreadr::load_teams(current = TRUE))\n\n\n\nCodesave(\n  nfl_teams_raw,\n  file = \"./data/nfl_teams_raw.RData\"\n)\n\n\nThe nfl_teams object is in team form. That is, each row should be uniquely identified by team_id. Let’s rearrange the data accordingly:\n\nCodenfl_teams &lt;- nfl_teams_raw %&gt;% \n  select(team_id, everything())\n\n\nLet’s check for duplicate team instances:\n\nCodenfl_teams %&gt;% \n  group_by(team_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_teams) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_teams,\n  file = \"./data/nfl_teams.RData\"\n)\n\n\n\n4.3.3 Fantasy Player IDs\nHo & Carl (2025h) provide a Data Dictionary for fantasy player ID data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_playerids.html\n\nCodenfl_playerIDs_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_playerids())\n\n\n\nCodesave(\n  nfl_playerIDs_raw,\n  file = \"./data/nfl_playerIDs_raw.RData\"\n)\n\n\nThe nfl_playerIDs object is in player form. That is, each row should be uniquely identified by mfl_id.\n\nCodenfl_playerIDs &lt;- nfl_playerIDs_raw %&gt;% \n  arrange(name, mfl_id)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_playerIDs %&gt;% \n  group_by(mfl_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_playerIDs %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s do some data processing to help with merging the dataset with other datasets:\n\nCodenfl_playerIDs$name[which(nfl_playerIDs$name == \"Bennett,Michael\")] &lt;- \"Michael Bennett\"\n\nnfl_playerIDs &lt;- nfl_playerIDs %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(name, lowercase = TRUE)\n  )\n\n\nHere are the variable names:\n\nCodenames(nfl_playerIDs) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_playerIDs,\n  file = \"./data/nfl_playerIDs.RData\"\n)\n\n\n\n4.3.4 Player Info\n\n4.3.5 Rosters\nA Data Dictionary for rosters is located at the following links:\n\n\nhttps://nflreadr.nflverse.com/articles/dictionary_rosters.html (Ho & Carl, 2025r)\n\n\nhttps://github.com/nflverse/nfldata/blob/master/DATASETS.md#rosters (Sharpe, 2020c)\n\n\n\nCodenfl_rosters_raw &lt;- progressr::with_progress(\n  nflreadr::load_rosters(seasons = TRUE))\n\nnfl_rosters_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_rosters_weekly(seasons = TRUE))\n\nrosters &lt;- read_csv(\"https://raw.githubusercontent.com/leesharpe/nfldata/master/data/rosters.csv\")\n\n\n\nCodesave(\n  nfl_rosters_raw,\n  rosters,\n  file = \"./data/nfl_rosters_raw.RData\"\n)\n\nsave(\n  nfl_rosters_weekly_raw,\n  file = \"./data/nfl_rosters_weekly_raw.RData\"\n)\n\n\nThe nfl_rosters object is in player-season-team form. That is, each row should be uniquely identified by the combination of gsis_id, season, and team. Let’s rearrange the data accordingly:\n\nCodenfl_rosters &lt;- nfl_rosters_raw %&gt;% \n  left_join(\n    rosters %&gt;% select(playerid, season, team, side, category, games, starts, years, av),\n    by = c(\"season\",\"team\",\"pfr_id\" = \"playerid\"),\n    na_matches = \"never\"\n  ) %&gt;% \n  select(gsis_id, season, team, week, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, team, week)\n\n\nLet’s check for duplicate player-season-team instances:\n\nCodenfl_rosters %&gt;% \n  group_by(gsis_id, season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.5.1 Processing\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for gsis_id\nnfl_rosters &lt;- nfl_rosters %&gt;% \n  filter(!is.na(gsis_id))\n\n# Fill in missing values for a player in their duplicate instances, and then keep only the first of the duplicate instances\nnfl_rosters &lt;- nfl_rosters %&gt;% \n  group_by(gsis_id, season, team) %&gt;% \n  fill(names(.), .direction = \"downup\") %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\n\nLet’s check again for duplicate player-season-team instances:\n\nCodenfl_rosters %&gt;% \n  group_by(gsis_id, season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_rosters_weekly object is in player-season-week form. That is, each row should be uniquely identified by the combination of gsis_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_rosters_weekly &lt;- nfl_rosters_weekly_raw %&gt;% \n  select(gsis_id, season, week, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_rosters_weekly %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for gsis_id\nnfl_rosters_weekly &lt;- nfl_rosters_weekly %&gt;% \n  filter(!is.na(gsis_id))\n\n# Fill in missing values for a player in their duplicate instances, and then keep only the first of the duplicate instances\nnfl_rosters_weekly &lt;- nfl_rosters_weekly %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  fill(names(.), .direction = \"downup\") %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\n\nLet’s check again for duplicate player-season-week instances:\n\nCodenfl_rosters_weekly %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_rosters) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_rosters_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_rosters,\n  file = \"./data/nfl_rosters.RData\"\n)\n\nsave(\n  nfl_rosters_weekly,\n  file = \"./data/nfl_rosters_weekly.RData\"\n)\n\n\n\n4.3.6 Game Schedules\nHo & Carl (2025s) provide a Data Dictionary for game schedules data at the following link: https://nflreadr.nflverse.com/articles/dictionary_schedules.html\n\nCodenfl_schedules_raw &lt;- progressr::with_progress(\n  nflreadr::load_schedules(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_schedules_raw,\n  file = \"./data/nfl_schedules_raw.RData\"\n)\n\n\nThe nfl_schedules object is in game form and in season-week (and -game type) form. That is, each row should be uniquely identified by game_id. Each row should also be uniquely identified by the combination of season and week (and game type).\n\nCodenfl_schedules &lt;- nfl_schedules_raw\n\n\nLet’s check for duplicate game instances:\n\nCodenfl_schedules %&gt;% \n  group_by(game_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_schedules) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_schedules,\n  file = \"./data/nfl_schedules.RData\"\n)\n\n\n\n4.3.7 Standings\nSharpe (2020d) provides a Data Dictionary for standings data at the following link: https://github.com/nflverse/nfldata/blob/master/DATASETS.md#standings\nWe can determine NFL standings based on the nfl_schedules object using the nflseedR (Carl & Sharpe, 2025) package.\n\nCodenfl_standings_raw &lt;- nflseedR::nfl_standings(\n  nfl_schedules %&gt;% filter(!is.na(result))) #read_csv(\"http://www.habitatring.com/standings.csv\"); archived at: https://perma.cc/5QLN-VK7V\n\n\n\nCodesave(\n  nfl_standings_raw,\n  file = \"./data/nfl_standings_raw.RData\"\n)\n\n\nThe nfl_standings object is in season-team form. That is, each row should be uniquely identified by the combination of season and team.\n\nCodenfl_standings &lt;- data.frame(nfl_standings_raw)\n\n\nLet’s check for duplicate season-team instances:\n\nCodenfl_standings %&gt;% \n  group_by(season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_standings) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_standings,\n  file = \"./data/nfl_standings.RData\"\n)\n\n\n\n4.3.8 The Combine\nHo & Carl (2025b) provide a Data Dictionary for data from the NFL Combine at the following link: https://nflreadr.nflverse.com/articles/dictionary_combine.html\n\nCodenfl_combine_raw &lt;- progressr::with_progress(\n  nflreadr::load_combine(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_combine_raw,\n  file = \"./data/nfl_combine_raw.RData\"\n)\n\n\nThe nfl_combine object is in player form. That is, each row should be uniquely identified by the player’s id. However, there is no gsis_id variable to merge it easily with other datasets. Some of the players have other id variables, including pfr_id and cfb_id. Let’s rearrange the data accordingly:\n\nCodenfl_combine &lt;- nfl_combine_raw %&gt;% \n  select(pfr_id, cfb_id, everything()) %&gt;% \n  arrange(season, player_name)\n\n\nLet’s do some data processing to help with merging the dataset with other datasets:\n\nCodenfl_combine &lt;- nfl_combine %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player_name, lowercase = TRUE)\n  )\n\n# First, merge on both pfr_id and cfb_id\nmerged_data1 &lt;- left_join(\n  nfl_combine,\n  nfl_playerIDs %&gt;% select(pfr_id, cfbref_id, gsis_id),\n  by = c(\"pfr_id\", \"cfb_id\" = \"cfbref_id\"),\n  na_matches = \"never\"\n)\n\n# Second, merge on pfr_id\nmerged_data2 &lt;- left_join(\n  nfl_combine,\n  nfl_playerIDs %&gt;% select(pfr_id, merge_name, position, gsis_id),\n  by = c(\"pfr_id\",\"merge_name\",\"pos\" = \"position\"),\n  na_matches = \"never\"\n)\n\n# Third, merge on cfb_id\nmerged_data3 &lt;- left_join(\n  nfl_combine,\n  nfl_playerIDs %&gt;% select(cfbref_id, merge_name, position, gsis_id),\n  by = c(\"cfb_id\" = \"cfbref_id\",\"merge_name\",\"pos\" = \"position\"),\n  na_matches = \"never\"\n)\n\n# Combine gsis_id across merges\nnfl_combine$gsis_id &lt;- coalesce(\n  merged_data1$gsis_id,\n  merged_data2$gsis_id,\n  merged_data3$gsis_id\n  )\n\n# Rearrange the data\nnfl_combine &lt;- nfl_combine %&gt;% \n  select(gsis_id, pfr_id, cfb_id, player_name, everything()) %&gt;% \n  arrange(season, player_name)\n\n\nLet’s check for duplicate gsis_id, pfr_id, and cfb_id instances:\n\nCodenfl_combine %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1, !is.na(gsis_id)) %&gt;% \n  arrange(gsis_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(pfr_id) %&gt;% \n  filter(n() &gt; 1, !is.na(pfr_id)) %&gt;% \n  arrange(pfr_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(cfb_id) %&gt;% \n  filter(n() &gt; 1, !is.na(cfb_id)) %&gt;% \n  arrange(cfb_id) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s do some additional data processing:\n\nCode# Drop Stanford Samuels Jr.\nnfl_combine$gsis_id[which(nfl_combine$cfb_id == \"stanford-samuels-1\")] &lt;- NA\n\nnfl_combine &lt;- nfl_combine %&gt;% \n  separate_wider_delim(\n    ht,\n    names = c(\"feet\", \"inches\"),\n    delim = \"-\") %&gt;%\n  mutate(\n    feet = as.numeric(feet),\n    inches = as.numeric(inches),\n    ht = feet * 12 + inches\n  ) %&gt;%\n  select(-feet, -inches)\n\n\nHowever, these apparent duplicates appear to be different players at different positions:\n\nCodenfl_combine %&gt;% \n  group_by(season, gsis_id, pos) %&gt;% \n  filter(n() &gt; 1, !is.na(gsis_id)) %&gt;% \n  arrange(gsis_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(season, pfr_id, pos) %&gt;% \n  filter(n() &gt; 1, !is.na(pfr_id)) %&gt;% \n  arrange(pfr_id) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_combine %&gt;% \n  group_by(season, cfb_id, pos) %&gt;% \n  filter(n() &gt; 1, !is.na(cfb_id)) %&gt;% \n  arrange(cfb_id) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_combine) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_combine,\n  file = \"./data/nfl_combine.RData\"\n)\n\n\n\n4.3.9 Draft Picks\nHo & Carl (2025e) provide a Data Dictionary for draft picks data at the following link: https://nflreadr.nflverse.com/articles/dictionary_draft_picks.html\n\nCodenfl_draftPicks_raw &lt;- progressr::with_progress(\n  nflreadr::load_draft_picks(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_draftPicks_raw,\n  file = \"./data/nfl_draftPicks_raw.RData\"\n)\n\n\nThe nfl_draftPicks object is in player form. That is, each row should be uniquely identified by gsis_id. Let’s rearrange the data accordingly:\n\nCodenfl_draftPicks &lt;- nfl_draftPicks_raw %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  arrange(pfr_player_name)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_draftPicks %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.9.1 Processing\nLet’s do some data cleanup:\n\nCode# Convert missing values to NA\nnfl_draftPicks[nfl_draftPicks == \"\"] &lt;- NA\n\n# Drop players with missing values for gsis_id\nnfl_draftPicks &lt;- nfl_draftPicks %&gt;% \n  filter(!is.na(gsis_id))\n\n\nLet’s check again for duplicate player instances:\n\nCodenfl_draftPicks %&gt;% \n  group_by(gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_draftPicks) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_draftPicks,\n  file = \"./data/nfl_draftPicks.RData\"\n)\n\n\n\n4.3.10 Draft Values\nSharpe (2020b) provides a Data Dictionary for draft values data at the following link: https://github.com/nflverse/nfldata/blob/master/DATASETS.md#draft_values\n\nCodenfl_draftValues_raw &lt;- read_csv(\"https://raw.githubusercontent.com/leesharpe/nfldata/master/data/draft_values.csv\")\n\n\n\nCodesave(\n  nfl_draftValues_raw,\n  file = \"./data/nfl_draftValues_raw.RData\"\n)\n\n\nThe nfl_draftValues object is in pick form. That is, each row should be uniquely identified by pick. Let’s rearrange the data accordingly:\n\nCodenfl_draftValues &lt;- nfl_draftValues_raw %&gt;% \n  select(pick, everything()) %&gt;% \n  arrange(pick)\n\n\nLet’s check for duplicate pick instances:\n\nCodenfl_draftValues %&gt;% \n  group_by(pick) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_draftValues) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_draftValues,\n  file = \"./data/nfl_draftValues.RData\"\n)\n\n\n\n4.3.11 Depth Charts\nHo & Carl (2025d) provide a Data Dictionary for data from weekly depth charts at the following link: https://nflreadr.nflverse.com/articles/dictionary_depth_charts.html\n\nCodenfl_depthCharts_raw &lt;- progressr::with_progress(\n  nflreadr::load_depth_charts(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_depthCharts_raw,\n  file = \"./data/nfl_depthCharts_raw.RData\"\n)\n\n\nThe nfl_depthCharts object is in player-season-week-position form. That is, each row should be uniquely identified by the combination of gsis_id, season, week, and depth_position. Let’s rearrange the data accordingly:\n\nCodenfl_depthCharts &lt;- nfl_depthCharts_raw %&gt;% \n  select(gsis_id, season, week, depth_position, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, week, depth_position)\n\n\nLet’s check for duplicate player-season-week-position instances:\n\nCodenfl_depthCharts %&gt;% \n  group_by(gsis_id, season, week, depth_position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.11.1 Processing\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for gsis_id\nnfl_depthCharts &lt;- nfl_depthCharts %&gt;% \n  filter(!is.na(gsis_id))\n\n# Fill in missing values for a player in their duplicate instances, and then keep only the first of the duplicate instances\nnfl_depthCharts &lt;- nfl_depthCharts %&gt;% \n  group_by(gsis_id, season, week, depth_position) %&gt;% \n  fill(names(.), .direction = \"downup\") %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup()\n\n\nLet’s check again for duplicate player-season-week-position instances:\n\nCodenfl_depthCharts %&gt;% \n  group_by(gsis_id, season, week, depth_position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_depthCharts) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_depthCharts,\n  file = \"./data/nfl_depthCharts.RData\"\n)\n\n\n\n4.3.12 Trades\nSharpe (2020e) provides a Data Dictionary for NFL trade data at the following link: https://github.com/nflverse/nfldata/blob/master/DATASETS.md#trades\n\nCodenfl_trades_raw &lt;- read_csv(\"https://raw.githubusercontent.com/leesharpe/nfldata/master/data/trades.csv\")\n\n\n\nCodesave(\n  nfl_trades_raw,\n  file = \"./data/nfl_trades_raw.RData\"\n)\n\n\nThe nfl_trades object is in trade-player form. That is, each row should be uniquely identified by the combination of trade_id and pfr_id.\n\nCodenfl_trades &lt;- nfl_trades_raw\n\n\nLet’s check for duplicate trade-player instances:\n\nCodenfl_trades %&gt;%\n  filter(!is.na(pfr_id)) %&gt;% \n  group_by(trade_id, pfr_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_trades) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_trades,\n  file = \"./data/nfl_trades.RData\"\n)\n\n\n\n4.3.13 Play-By-Play Data\nTo download play-by-play data from prior weeks and seasons, we can use the nflreadr::load_pbp() function of the nflreadr package (Ho & Carl, 2024). We add a progress bar using the progressr::with_progress() function from the progressr package (Bengtsson, 2024) because it takes a while to run. Ho & Carl (2025n) provide a Data Dictionary for the play-by-play data at the following link: https://nflreadr.nflverse.com/articles/dictionary_pbp.html\n\n\n\n\n\n\nNote 4.1: Downloading play-by-play data\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_pbp_raw &lt;- progressr::with_progress(\n  nflreadr::load_pbp(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_pbp_raw,\n  file = \"./data/nfl_pbp_raw.RData\"\n)\n\n\nThe nfl_pbp object is in game-drive-play form. That is, each row should be uniquely identified by the combination of game_id, fixed_drive, play_id. Let’s rearrange the data accordingly:\n\nCodenfl_pbp &lt;- nfl_pbp_raw %&gt;% \n  select(game_id, drive, play_id, everything()) %&gt;% \n  arrange(game_id, drive, play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_pbp %&gt;% \n  group_by(game_id, fixed_drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_pbp) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_pbp,\n  file = \"./data/nfl_pbp.RData\"\n)\n\n\nNow, let’s identify the unique seasons of play-by-play data for use in later calculations:\n\nCodeseasons &lt;- unique(nfl_pbp$season)\n\n\n\n4.3.14 4th Down Data\nWe use the nfl4th package (Baldwin, 2023) to download fourth down data.\n\n\n\n\n\n\nNote 4.2: Downloading 4th down data\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_4thdown_raw &lt;- nfl4th::load_4th_pbp(\n  seasons = 2014:nflreadr::most_recent_season())\n\n\n\nCodesave(\n  nfl_4thdown_raw,\n  file = \"./data/nfl_4thdown_raw.RData\"\n)\n\n\nThe nfl_4thdown object is in game-drive-play form. That is, each row should be uniquely identified by the combination of game_id, drive, play_id. Let’s rearrange the data accordingly:\n\nCodenfl_4thdown &lt;- nfl_4thdown_raw %&gt;% \n  select(game_id, drive, play_id, everything()) %&gt;% \n  arrange(game_id, drive, play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_4thdown %&gt;% \n  group_by(game_id, drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_4thdown) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_4thdown,\n  file = \"./data/nfl_4thdown.RData\"\n)\n\n\n\n4.3.15 Participation\nHo & Carl (2025m) provide a Data Dictionary for the participation data at the following link: https://nflreadr.nflverse.com/articles/dictionary_participation.html\n\nCodenfl_participation_raw &lt;- progressr::with_progress(\n  nflreadr::load_participation(\n    seasons = 2016:2023, # participation data are no longer available after 2023\n    include_pbp = TRUE))\n\n\n\nCodesave(\n  nfl_participation_raw,\n  file = \"./data/nfl_participation_raw.RData\"\n)\n\n\nThe nfl_participation object is in game-drive-play form. That is, each row should be uniquely identified by the combination of nflverse_game_id, drive, play_id. Let’s rearrange the data accordingly:\n\nCodenfl_participation &lt;- nfl_participation_raw %&gt;% \n  select(nflverse_game_id, drive, play_id, everything()) %&gt;% \n  arrange(nflverse_game_id, drive, play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_participation %&gt;% \n  group_by(nflverse_game_id, drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_participation) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_participation,\n  file = \"./data/nfl_participation.RData\"\n)\n\n\n\n4.3.16 Historical Actual Player Statistics\n\n4.3.16.1 Season-by-Season Statistics\nWe calculate historical actual season-by-season statistics in Section 4.4.1.1.\n\n4.3.16.2 Career Statistics\nWe calculate historical actual career statistics in Section 4.4.1.2.\n\n4.3.16.3 Week-by-Week Statistics\nWe can download historical week-by-week actual player statistics using the nflreadr::load_player_stats() function from the nflreadr package (Ho & Carl, 2024). Ho & Carl (2025p) provide a Data Dictionary for statistics for offensive players at the following link: https://nflreadr.nflverse.com/articles/dictionary_player_stats.html. Ho & Carl (2025q) provide a Data Dictionary for statistics for defensive players at the following link: https://nflreadr.nflverse.com/articles/dictionary_player_stats_def.html.\n\nCodenfl_actualStats_offense_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"offense\"))\n\nnfl_actualStats_defense_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"defense\"))\n\nnfl_actualStats_kicking_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"kicking\"))\n\n\n\nCodesave(\n  nfl_actualStats_offense_weekly_raw, nfl_actualStats_defense_weekly_raw, nfl_actualStats_kicking_weekly_raw,\n  file = \"./data/nfl_actualStats_position_weekly_raw.RData\"\n)\n\n\nThe nfl_actualStats_weekly objects are in player-season-week form. That is, each row should be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_offense_weekly &lt;- nfl_actualStats_offense_weekly_raw %&gt;% \n  rename(team = recent_team) %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualStats_defense_weekly &lt;- nfl_actualStats_defense_weekly_raw %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualStats_kicking_weekly &lt;- nfl_actualStats_kicking_weekly_raw %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualStats_offense_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_defense_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_kicking_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_actualStats_offense_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualStats_defense_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualStats_kicking_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualStats_offense_weekly, nfl_actualStats_defense_weekly, nfl_actualStats_kicking_weekly,\n  file = \"./data/nfl_actualStats_position_weekly.RData\"\n)\n\n\n\n4.3.17 Injuries\nHo & Carl (2025k) provide a Data Dictionary for injury data at the following link: https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n\nCodenfl_injuries_raw &lt;- progressr::with_progress(\n  nflreadr::load_injuries(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_injuries_raw,\n  file = \"./data/nfl_injuries_raw.RData\"\n)\n\n\nThe nfl_injuries object is in player-season-week form. That is, each row should be uniquely identified by the combination of gsis_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_injuries &lt;- nfl_injuries_raw %&gt;% \n  select(gsis_id, season, week, everything()) %&gt;% \n  arrange(full_name, gsis_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_injuries %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_injuries) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_injuries,\n  file = \"./data/nfl_injuries.RData\"\n)\n\n\n\n4.3.18 Snap Counts\nHo & Carl (2025t) provide a Data Dictionary for snap counts data at the following link: https://nflreadr.nflverse.com/articles/dictionary_snap_counts.html\n\nCodenfl_snapCounts_raw &lt;- progressr::with_progress(\n  nflreadr::load_snap_counts(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_snapCounts_raw,\n  file = \"./data/nfl_snapCounts_raw.RData\"\n)\n\n\nThe nfl_snapCounts object is in game-player form. That is, each row should be uniquely identified by the combination of game_id and pfr_player_id. Let’s rearrange the data accordingly:\n\nCodenfl_snapCounts &lt;- nfl_snapCounts_raw %&gt;% \n  select(game_id, pfr_player_id, everything()) %&gt;% \n  arrange(game_id, pfr_player_id)\n\n\nLet’s check for duplicate game instances:\n\nCodenfl_snapCounts %&gt;% \n  group_by(game_id, pfr_player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_snapCounts) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_snapCounts,\n  file = \"./data/nfl_snapCounts.RData\"\n)\n\n\n\n4.3.19 ESPN QBR\nHo & Carl (2025f) provide a Data Dictionary for ESPN QBR data at the following link: https://nflreadr.nflverse.com/articles/dictionary_espn_qbr.html\n\nCodenfl_espnQBR_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_espn_qbr(\n    seasons = TRUE,\n    summary_type = c(\"season\")))\n\nnfl_espnQBR_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_espn_qbr(\n    seasons = TRUE,\n    summary_type = c(\"week\")))\n\n\n\nCodesave(\n  nfl_espnQBR_seasonal_raw,\n  file = \"./data/nfl_espnQBR_seasonal_raw.RData\"\n)\n\nsave(\n  nfl_espnQBR_weekly_raw,\n  file = \"./data/nfl_espnQBR_weekly_raw.RData\"\n)\n\n\nThe nfl_espnQBR_seasonal object is in player-season-season type form, where season type refers to regular season versus postseason. That is, each row should be uniquely identified by the combination of player_id, season, and season_type. Let’s rearrange the data accordingly:\n\nCodenfl_espnQBR_seasonal &lt;- nfl_espnQBR_seasonal_raw %&gt;% \n  select(player_id, season, season_type, everything()) %&gt;% \n  arrange(name_display, player_id, season, season_type)\n\n\nLet’s check for duplicate player-season-team instances:\n\nCodenfl_espnQBR_seasonal %&gt;% \n  group_by(player_id, season, season_type) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_espnQBR_weekly object is in both game-player form and player-season-season type-week form, where season type refers to regular season versus postseason. That is, each row should be uniquely identified by the combination of gsis_id, season, and week or by the combination of player_id, season, season_type, and week_num. Let’s rearrange the data accordingly:\n\nCodenfl_espnQBR_weekly &lt;- nfl_espnQBR_weekly_raw %&gt;% \n  select(player_id, season, season_type, week_num, everything()) %&gt;% \n  arrange(name_display, player_id, season, season_type, week_num)\n\n\nLet’s check for duplicate game-player or player-season-season type-week instances:\n\nCodenfl_espnQBR_weekly %&gt;% \n  arrange(game_id, player_id) %&gt;% \n  group_by(game_id, player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_espnQBR_weekly %&gt;% \n  arrange(player_id, season, season_type, week_num) %&gt;% \n  group_by(player_id, season, season_type, week_num) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_espnQBR_seasonal) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_espnQBR_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_espnQBR_seasonal,\n  file = \"./data/nfl_espnQBR_seasonal.RData\"\n)\n\nsave(\n  nfl_espnQBR_weekly,\n  file = \"./data/nfl_espnQBR_weekly.RData\"\n)\n\n\n\n4.3.20 NFL Next Gen Stats\nHo & Carl (2025l) provide a Data Dictionary for NFL Next Gen Stats data at the following link: https://nflreadr.nflverse.com/articles/dictionary_nextgen_stats.html\n\nCodenfl_nextGenStats_pass_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"passing\")))\n\nnfl_nextGenStats_rush_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"rushing\")))\n\nnfl_nextGenStats_rec_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"receiving\")))\n\nnfl_nextGenStats_weekly_raw &lt;- bind_rows(\n  nfl_nextGenStats_pass_weekly_raw,\n  nfl_nextGenStats_rush_weekly_raw,\n  nfl_nextGenStats_rec_weekly_raw\n)\n\n\n\nCodesave(\n  nfl_nextGenStats_weekly_raw,\n  file = \"./data/nfl_nextGenStats_weekly_raw.RData\"\n)\n\n\nThe nfl_nextGenStats_weekly object is in player-season-season type-week form, where season type refers to regular season versus postseason. That is, each row should be uniquely identified by the combination of player_gsis_id, season, season_type, and week. Let’s rearrange the data accordingly:\n\nCodenfl_nextGenStats_weekly &lt;- nfl_nextGenStats_weekly_raw %&gt;% \n  select(player_gsis_id, season, season_type, week, everything()) %&gt;% \n  arrange(player_display_name, player_gsis_id, season, season_type)\n\n\nLet’s check for duplicate player-season-season type-week instances:\n\nCodenfl_nextGenStats_weekly %&gt;% \n  group_by(player_gsis_id, season, season_type, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_nextGenStats_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_nextGenStats_weekly,\n  file = \"./data/nfl_nextGenStats_weekly.RData\"\n)\n\n\n\n4.3.21 Advanced Stats from Pro Football Reference\nThere is a Data Dictionary for Pro Football Reference passing data at the following links:\n\n\nhttps://nflreadr.nflverse.com/articles/dictionary_pfr_passing.html (Ho & Carl, 2025o)\n\n\nhttps://www.pro-football-reference.com/about/advanced_stats.htm [Pro Football Reference (2025); archived at https://perma.cc/LKH5-92PQ]\n\nPro Football Reference (2024) provides advanced stats from the 2024 season at the following link: https://www.pro-football-reference.com/years/2024/advanced.htm\n\nCode# Seasonal Data\nnfl_advancedStatsPFR_pass_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"pass\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_rush_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rush\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_rec_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rec\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_def_seasonal_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"def\"),\n    summary_level = c(\"season\")))\n\n# Weekly Data\nnfl_advancedStatsPFR_pass_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"pass\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_rush_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rush\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_rec_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rec\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_def_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"def\"),\n    summary_level = c(\"week\")))\n\n\n\nCodesave(\n  nfl_advancedStatsPFR_pass_seasonal_raw,\n  nfl_advancedStatsPFR_rush_seasonal_raw,\n  nfl_advancedStatsPFR_rec_seasonal_raw,\n  nfl_advancedStatsPFR_def_seasonal_raw,\n  file = \"./data/nfl_advancedStatsPFR_seasonal_raw.RData\"\n)\n\nsave(\n  nfl_advancedStatsPFR_pass_weekly_raw,\n  nfl_advancedStatsPFR_rush_weekly_raw,\n  nfl_advancedStatsPFR_rec_weekly_raw,\n  nfl_advancedStatsPFR_def_weekly_raw,\n  file = \"./data/nfl_advancedStatsPFR_weekly_raw.RData\"\n)\n\n\n\n4.3.21.1 Processing\n\nCode# Clean up player name for merging; name variables based on which data object they're from\n\n## Seasonal Data\nnfl_advancedStatsPFR_pass_seasonal &lt;- nfl_advancedStatsPFR_pass_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".pass\"),\n    -c(pfr_id, merge_name, season, team))\n\nnfl_advancedStatsPFR_rush_seasonal &lt;- nfl_advancedStatsPFR_rush_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  ) %&gt;% \n  rename(\n    team = tm\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rush\"),\n    -c(pfr_id, merge_name, season, team))\n\nnfl_advancedStatsPFR_rec_seasonal &lt;- nfl_advancedStatsPFR_rec_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  ) %&gt;% \n  rename(\n    team = tm\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rec\"),\n    -c(pfr_id, merge_name, season, team))\n\nnfl_advancedStatsPFR_def_seasonal &lt;- nfl_advancedStatsPFR_def_seasonal_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)\n  )  %&gt;% \n  rename(\n    team = tm\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".def\"),\n    -c(pfr_id, merge_name, season, team))\n\n## Weekly Data\nnfl_advancedStatsPFR_pass_weekly &lt;- nfl_advancedStatsPFR_pass_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".pass\"),\n    -c(game_id, pfr_player_id))\n\nnfl_advancedStatsPFR_rush_weekly &lt;- nfl_advancedStatsPFR_rush_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rush\"),\n    -c(game_id, pfr_player_id))\n\nnfl_advancedStatsPFR_rec_weekly &lt;- nfl_advancedStatsPFR_rec_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".rec\"),\n    -c(game_id, pfr_player_id))\n\nnfl_advancedStatsPFR_def_weekly &lt;- nfl_advancedStatsPFR_def_weekly_raw %&gt;% \n  mutate(\n    merge_name = nflreadr::clean_player_names(pfr_player_name, lowercase = TRUE)\n  ) %&gt;% \n  rename_with(\n    ~ paste0(., \".def\"),\n    -c(game_id, pfr_player_id))\n\n## Merge across positions\nnfl_advancedStatsPFR_seasonal_list &lt;- list(\n  nfl_advancedStatsPFR_pass_seasonal,\n  nfl_advancedStatsPFR_rush_seasonal,\n  nfl_advancedStatsPFR_rec_seasonal,\n  nfl_advancedStatsPFR_def_seasonal)\n\nnfl_advancedStatsPFR_weekly_list &lt;- list(\n  nfl_advancedStatsPFR_pass_weekly,\n  nfl_advancedStatsPFR_rush_weekly,\n  nfl_advancedStatsPFR_rec_weekly,\n  nfl_advancedStatsPFR_def_weekly)\n\nnfl_advancedStatsPFR_seasonalByTeam &lt;- nfl_advancedStatsPFR_seasonal_list %&gt;% \n  purrr::reduce(\n    full_join,\n    by = c(\"pfr_id\",\"merge_name\",\"season\",\"team\"))\n\nnfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly_list %&gt;% \n  purrr::reduce(\n    full_join,\n    by = c(\"game_id\",\"pfr_player_id\")) #merge_name\n\n#nfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly_list %&gt;% \n#  purrr::reduce(\n#    full_join,\n#    by = c(\"pfr_player_id\",\"merge_name\",\"season\",\"week\"))\n\nnfl_advancedStatsPFR_seasonalByTeam &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  mutate(\n    pfr_player_name = coalesce(\n      player.pass,\n      player.rush,\n      player.rec,\n      player.def\n    ),\n    age = coalesce(\n      #age.pass,\n      age.rush,\n      age.rec,\n      age.def\n    ),\n    pos = coalesce(\n      #pos.pass,\n      pos.rush,\n      pos.rec,\n      pos.def\n    ),\n    g = coalesce(\n      #g.pass,\n      g.rush,\n      g.rec,\n      g.def\n    ),\n    gs = coalesce(\n      #gs.pass,\n      gs.rush,\n      gs.rec,\n      gs.def\n    )\n  ) %&gt;% \n  select(-c(\n    starts_with(\"player.\"),\n    starts_with(\"age.\"),\n    starts_with(\"pos.\"),\n    starts_with(\"g.\"),\n    starts_with(\"gs.\")))\n\nnfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly %&gt;% \n  mutate(\n    pfr_player_name = coalesce(\n      pfr_player_name.pass,\n      pfr_player_name.rush,\n      pfr_player_name.rec,\n      pfr_player_name.def\n    ),\n    season = coalesce(\n      season.pass,\n      season.rush,\n      season.rec,\n      season.def\n    ),\n    week = coalesce(\n      week.pass,\n      week.rush,\n      week.rec,\n      week.def\n    ),\n    team = coalesce(\n      team.pass,\n      team.rush,\n      team.rec,\n      team.def\n    ),\n    merge_name = coalesce(\n      merge_name.pass,\n      merge_name.rush,\n      merge_name.rec,\n      merge_name.def\n    ),\n    pfr_game_id = coalesce(\n      pfr_game_id.pass,\n      pfr_game_id.rush,\n      pfr_game_id.rec,\n      pfr_game_id.def\n    ),\n    game_type = coalesce(\n      game_type.pass,\n      game_type.rush,\n      game_type.rec,\n      game_type.def\n    ),\n    opponent = coalesce(\n      opponent.pass,\n      opponent.rush,\n      opponent.rec,\n      opponent.def\n    )\n  ) %&gt;% \n  select(-c(\n    starts_with(\"pfr_player_name.\"),\n    starts_with(\"season.\"),\n    starts_with(\"week.\"),\n    starts_with(\"team.\"),\n    starts_with(\"merge_name.\"),\n    starts_with(\"pfr_game_id.\"),\n    starts_with(\"game_type.\"),\n    starts_with(\"opponent.\"),\n    ))\n\n\nThe nfl_advancedStatsPFR_seasonalByTeam object is in player-season-team form. That is, each row should be uniquely identified by the combination of pfr_id, season, and team. Let’s rearrange the data accordingly:\n\nCodenfl_advancedStatsPFR_seasonalByTeam &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  select(pfr_id, season, team, pfr_player_name, everything()) %&gt;% \n  arrange(pfr_player_name, pfr_id, season, team)\n\n\nLet’s check for duplicate player-season-team instances:\n\nCodenfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  group_by(pfr_id, season, team) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nAggregate variables within each pass/rush/rec/def object by team for seasonal data (so seasonal data are in player-season form, not player-season-team form). Depending on the variable, aggregation was performed using a sum, weighted mean (weighted by the number of games played for each team), or a recomputed percentage.\n\nCodepfrVars &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  select(pass_attempts.pass:m_tkl_percent.def, g, gs) %&gt;% \n  names()\n\nweightedAverageVars &lt;- c(\n  \"pocket_time.pass\",\n  \"ybc_att.rush\",\"yac_att.rush\",\n  \"ybc_r.rec\",\"yac_r.rec\",\"adot.rec\",\"rat.rec\",\n  \"yds_cmp.def\",\"yds_tgt.def\",\"dadot.def\",\"m_tkl_percent.def\",\"rat.def\"\n)\n\nrecomputeVars &lt;- c(\n  \"drop_pct.pass\", # drops.pass / pass_attempts.pass\n  \"bad_throw_pct.pass\", # bad_throws.pass / pass_attempts.pass\n  \"on_tgt_pct.pass\", # on_tgt_throws.pass / pass_attempts.pass\n  \"pressure_pct.pass\", # times_pressured.pass / pass_attempts.pass\n  \"drop_percent.rec\", # drop.rec / tgt.rec\n  \"rec_br.rec\", # rec.rec / brk_tkl.rec\n  \"cmp_percent.def\" # cmp.def / tgt.def\n)\n\nsumVars &lt;- pfrVars[pfrVars %ni% c(\n  weightedAverageVars, recomputeVars,\n  \"merge_name\", \"loaded.pass\", \"loaded.rush\", \"loaded.rec\", \"loaded.def\")]\n\nnfl_advancedStatsPFR_seasonal &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  group_by(pfr_id, merge_name, season) %&gt;% \n  summarise(\n    across(all_of(weightedAverageVars), ~ weighted.mean(.x, w = g, na.rm = TRUE)),\n    across(all_of(sumVars), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\") %&gt;% \n  mutate(\n    drop_pct.pass = drops.pass / pass_attempts.pass,\n    bad_throw_pct.pass = bad_throws.pass / pass_attempts.pass,\n    on_tgt_pct.pass = on_tgt_throws.pass / pass_attempts.pass,\n    pressure_pct.pass = times_pressured.pass / pass_attempts.pass,\n    drop_percent.rec = drop.rec / tgt.rec,\n    rec_br.rec = drop.rec / tgt.rec,\n    cmp_percent.def = cmp.def / tgt.def\n  )\n\n# Merge with other player info\nnfl_advancedStatsPFR_seasonalByTeam_1stTeam &lt;- nfl_advancedStatsPFR_seasonalByTeam %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  slice(1)\n\nnfl_advancedStatsPFR_seasonalByTeam_1stTeam_mergeVars &lt;- nfl_advancedStatsPFR_seasonalByTeam_1stTeam %&gt;% \n  select(pfr_id, season, team, pfr_player_name, age, pos) #, g, gs\n\nnfl_advancedStatsPFR_seasonal &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  left_join(\n    nfl_advancedStatsPFR_seasonalByTeam_1stTeam_mergeVars,\n    by = c(\"pfr_id\",\"season\")\n  ) %&gt;% \n  select(\n    pfr_id, season, pfr_player_name, pos, age, team, g, gs,\n    contains(\".pass\"), contains(\".rush\"), contains(\".rec\"), contains(\".def\"),\n    everything())\n\n\nLet’s check for duplicate player-season instances:\n\nCodenfl_advancedStatsPFR_seasonal %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_advancedStatsPFR_weekly object is in both game-player form and player-season-week form. That is, each row should be uniquely identified by the combination of pfr_player_id, season, and week or by the combination of pfr_player_id, season, game_type, and week. Let’s rearrange the data accordingly:\n\nCodenfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly %&gt;% \n  select(pfr_player_id, season, week, game_type, game_id, pfr_player_name, everything()) %&gt;% \n  arrange(pfr_player_name, pfr_player_id, season, week)\n\n\nLet’s check for duplicate game-player or player-season-week instances:\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(game_id, pfr_player_id) %&gt;% \n  group_by(game_id, pfr_player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(pfr_player_id, season, week) %&gt;% \n  group_by(pfr_player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nMerge with gsis_id for merging with other datasets:\n\nCode# Prepare data for merging\nnfl_advancedStatsPFR_weekly &lt;- nfl_advancedStatsPFR_weekly %&gt;% \n  rename(pfr_id = pfr_player_id)\n\n# Identify duplicates\nnfl_advancedStatsPFR_seasonal %&gt;% \n  select(pfr_id, season, age) %&gt;% \n  na.omit() %&gt;% \n  unique() %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  arrange(pfr_id, season) %&gt;% \n  head()\n\n\n  \n\n\nCode# Merge seasonal data with the player IDs\nnfl_advancedStatsPFR_seasonal &lt;- left_join(\n  nfl_advancedStatsPFR_seasonal,\n  nfl_playerIDs %&gt;% \n    filter(!is.na(pfr_id)) %&gt;% \n    filter(gsis_id != \"00-0039137\") %&gt;% # drop DL Byron Young, keep OLB Byron Young\n    select(pfr_id, gsis_id) %&gt;% \n    unique(),\n  by = \"pfr_id\"\n)\n\n# Merge weekly data with the player IDs\nnfl_advancedStatsPFR_weekly &lt;- left_join(\n  nfl_advancedStatsPFR_weekly,\n  nfl_playerIDs %&gt;% \n    filter(!is.na(pfr_id)) %&gt;% \n    filter(gsis_id != \"00-0039137\") %&gt;% # drop DL Byron Young, keep OLB Byron Young\n    select(pfr_id, gsis_id) %&gt;% \n    unique(),\n  by = \"pfr_id\"\n)\n\n# Remove distinct players who were given the same `pfr_id` (to allow merging)\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0035665\" & nfl_advancedStatsPFR_seasonal$pos %in% c(\"LB\",\"LILB\",\"RILB\"))] &lt;- NA # drop LB David Young, keep DB David Young\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0035665\" & nfl_advancedStatsPFR_weekly$team %in% c(\"TEN\",\"MIA\"))] &lt;- NA # drop LB David Young, keep DB David Young\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0035292\" & nfl_advancedStatsPFR_seasonal$pos %in% c(\"LB\",\"LILB\",\"RILB\"))] &lt;- NA # drop LB David Young, keep DB David Young\nnfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0035292\" & nfl_advancedStatsPFR_weekly$team %in% c(\"TEN\",\"MIA\"))] &lt;- NA # drop LB David Young, keep DB David Young\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0033894\" & nfl_advancedStatsPFR_seasonal$pos == \"DB\")] &lt;- NA # drop S Marcus Williams, keep DB David Young\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0033894\" & nfl_advancedStatsPFR_weekly$pos == \"DB\")] &lt;- NA # drop S Marcus Williams\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0038407\" & nfl_advancedStatsPFR_seasonal$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0038407\" & nfl_advancedStatsPFR_weekly$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0037106\" & nfl_advancedStatsPFR_seasonal$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0037106\" & nfl_advancedStatsPFR_weekly$pos == \"DB\")] &lt;- NA # drop DB Jaylon Jones, keep CB Jaylon Jones\n\nnfl_advancedStatsPFR_seasonal$gsis_id[which(nfl_advancedStatsPFR_seasonal$gsis_id == \"00-0038549\" & nfl_advancedStatsPFR_seasonal$pos == \"WR\")] &lt;- NA # drop WR DJ TUrner, keep CB DJ Turner\n#nfl_advancedStatsPFR_weekly$gsis_id[which(nfl_advancedStatsPFR_weekly$gsis_id == \"00-0038549\" & nfl_advancedStatsPFR_weekly$pos == \"WR\")] &lt;- NA # drop WR DJ TUrner, keep CB DJ Turner\n\n\nNow, each row of the nfl_advancedStatsPFR_seasonal object should be uniquely identified by the combination of gsis_id (or pfr_id), and season. Each row of the nfl_advancedStatsPFR_weekly object should be uniquely identified by the combination of gsis_id (or pfr_id), season, and week or by the combination of gsis_id (or pfr_id), season, game_type, and week.\nLet’s check again for duplicate game-player or player-season-week instances:\n\nCode# Based on gsis_id\nnfl_advancedStatsPFR_seasonal %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  group_by(gsis_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  arrange(game_id, gsis_id) %&gt;% \n  group_by(game_id, gsis_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  select(gsis_id, everything()) %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  arrange(gsis_id, season, week) %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCode# Based on pfr_id\nnfl_advancedStatsPFR_seasonal %&gt;% \n  group_by(pfr_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(game_id, pfr_id) %&gt;% \n  group_by(game_id, pfr_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_weekly %&gt;% \n  arrange(pfr_id, season, week) %&gt;% \n  group_by(pfr_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_advancedStatsPFR_seasonal) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_advancedStatsPFR_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_advancedStatsPFR_seasonal,\n  file = \"./data/nfl_advancedStatsPFR_seasonal.RData\"\n)\n\nsave(\n  nfl_advancedStatsPFR_weekly,\n  file = \"./data/nfl_advancedStatsPFR_weekly.RData\"\n)\n\n\n\n4.3.22 Player Contracts\nHo & Carl (2025c) provide a Data Dictionary for player contracts data at the following link: https://nflreadr.nflverse.com/articles/dictionary_contracts.html\n\nCodenfl_playerContracts_raw &lt;- progressr::with_progress(\n  nflreadr::load_contracts())\n\n\n\nCodesave(\n  nfl_playerContracts_raw,\n  file = \"./data/nfl_playerContracts_raw.RData\"\n)\n\n\nThe nfl_playerContracts object is in player-year-team-value form. That is, each row should be uniquely identified by the combination of otc_id, year_signed, team, and value. Let’s rearrange the data accordingly:\n\nCodenfl_playerContracts &lt;- nfl_playerContracts_raw %&gt;% \n  select(otc_id, year_signed, team, everything()) %&gt;% \n  arrange(player, otc_id, year_signed, team)\n\n\nLet’s check for duplicate player-year-team-value instances:\n\nCodenfl_playerContracts %&gt;% \n  group_by(otc_id, year_signed, team, value) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_playerContracts) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_playerContracts,\n  file = \"./data/nfl_playerContracts.RData\"\n)\n\n\n\n4.3.23 FTN Charting Data\nHo & Carl (2025j) provide a Data Dictionary for FTN Charting data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ftn_charting.html\n\nCodenfl_ftnCharting_raw &lt;- progressr::with_progress(\n  nflreadr::load_ftn_charting(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_ftnCharting_raw,\n  file = \"./data/nfl_ftnCharting_raw.RData\"\n)\n\n\nThe nfl_ftnCharting object is in game-play form. That is, each row should be uniquely identified by the combination of nflverse_game_id and play_id. Let’s rearrange the data accordingly:\n\nCodenfl_ftnCharting &lt;- nfl_ftnCharting_raw %&gt;% \n  select(nflverse_game_id, nflverse_play_id, everything()) %&gt;% \n  arrange(nflverse_game_id, nflverse_play_id)\n\n\nLet’s check for duplicate game-drive-play instances:\n\nCodenfl_ftnCharting %&gt;% \n  group_by(nflverse_game_id, nflverse_play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_ftnCharting) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_ftnCharting,\n  file = \"./data/nfl_ftnCharting.RData\"\n)\n\n\n\n4.3.24 FantasyPros Rankings\nHo & Carl (2025i) provide a Data Dictionary for FantasyPros ranking data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_rankings.html\n\nCode#nfl_rankings_raw &lt;- progressr::with_progress( # currently throws error\n#  nflreadr::load_ff_rankings(type = \"all\"))\n\nnfl_rankings_draft_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_rankings(type = \"draft\"))\n\nnfl_rankings_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_rankings(type = \"week\"))\n\n\n\nCodesave(\n  nfl_rankings_draft_raw,\n  file = \"./data/nfl_rankings_draft_raw.RData\"\n)\n\nsave(\n  nfl_rankings_weekly_raw,\n  file = \"./data/nfl_rankings_weekly_raw.RData\"\n)\n\n\nThe nfl_rankings_draft object is in player-page_type form. That is, each row should be uniquely identified by the player’s id. Let’s rearrange the data accordingly:\n\nCodenfl_rankings_draft &lt;- nfl_rankings_draft_raw %&gt;% \n  select(id, page_type, player, pos, team, everything()) %&gt;% \n  arrange(player, id, pos, page_type)\n\n\nLet’s check for duplicate player-page_type instances:\n\nCodenfl_rankings_draft %&gt;% \n  group_by(id, page_type) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_rankings_weekly object is in player-page form. That is, each row should be uniquely identified by fantasypros_id and page. Let’s rearrange the data accordingly:\n\nCodenfl_rankings_weekly &lt;- nfl_rankings_weekly_raw %&gt;% \n  select(fantasypros_id, page, player_name, pos, team, everything()) %&gt;% \n  arrange(player_name, fantasypros_id, page, pos)\n\n\nLet’s check for duplicate player-page instances:\n\nCodenfl_rankings_weekly %&gt;% \n  group_by(fantasypros_id, page) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_rankings_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_rankings_draft,\n  file = \"./data/nfl_rankings_draft.RData\"\n)\n\nsave(\n  nfl_rankings_weekly,\n  file = \"./data/nfl_rankings_weekly.RData\"\n)\n\n\n\n4.3.25 Expected Fantasy Points\nHo & Carl (2025g) provide a Data Dictionary for expected fantasy points data at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_opportunity.html\n\nCodenfl_expectedFantasyPoints_weekly_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"weekly\",\n    model_version = \"latest\"\n  ))\n\nnfl_expectedFantasyPoints_pass_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"pbp_pass\",\n    model_version = \"latest\"\n  ))\n\nnfl_expectedFantasyPoints_rush_raw &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"pbp_rush\",\n    model_version = \"latest\"\n  ))\n\n\n\nCodenfl_expectedFantasyPoints_weekly_raw &lt;- nflreadr::load_ff_opportunity(\n  seasons = TRUE,\n  stat_type = \"weekly\",\n  model_version = \"latest\"\n  )\n\nnfl_expectedFantasyPoints_pass_raw &lt;- nflreadr::load_ff_opportunity(\n  seasons = TRUE,\n  stat_type = \"pbp_pass\",\n  model_version = \"latest\"\n  )\n\nnfl_expectedFantasyPoints_rush_raw &lt;- nflreadr::load_ff_opportunity(\n  seasons = TRUE,\n  stat_type = \"pbp_rush\",\n  model_version = \"latest\"\n  )\n\n\n\nCodesave(\n  nfl_expectedFantasyPoints_weekly_raw,\n  file = \"./data/nfl_expectedFantasyPoints_weekly_raw.RData\"\n)\n\nsave(\n  nfl_expectedFantasyPoints_pass_raw,\n  nfl_expectedFantasyPoints_rush_raw,\n  file = \"./data/nfl_expectedFantasyPoints_pbp_raw.RData\"\n)\n\n\n\nCodenfl_expectedFantasyPoints_pbp_list &lt;- list(\n  nfl_expectedFantasyPoints_pass_raw,\n  nfl_expectedFantasyPoints_rush_raw)\n\nnfl_expectedFantasyPoints_pbp &lt;- full_join(\n  nfl_expectedFantasyPoints_pass_raw,\n  nfl_expectedFantasyPoints_rush_raw,\n  by = c(\"game_id\",\"fixed_drive\",\"play_id\"),\n  suffix = c(\".pass\", \".rush\")\n)\n\n\nThe nfl_expectedFantasyPoints_weekly object is in game-player form and in player-season-week form. That is, each row should be uniquely identified by the combination of game_id and player_id. Each row should also be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_expectedFantasyPoints_weekly &lt;- nfl_expectedFantasyPoints_weekly_raw %&gt;% \n  select(player_id, season, week, game_id, full_name, posteam, position, everything()) %&gt;% \n  arrange(full_name, player_id, season, week)\n\n\nLet’s check for duplicate game-player instances and player-season-week instances:\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(game_id, player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.25.1 Processing\nLet’s do some data cleanup:\n\nCode# Drop players with missing values for player_id\nnfl_expectedFantasyPoints_weekly &lt;- nfl_expectedFantasyPoints_weekly %&gt;% \n  filter(!is.na(player_id))\n\n\nLet’s check again for duplicate game-player instances and season-week-player instances:\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(game_id, player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_expectedFantasyPoints_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nThe nfl_expectedFantasyPoints_pbp object is in game-drive-play form. That is, each row should be uniquely identified by the combination of game_id, fixed_drive, and play_id. Let’s rearrange the data accordingly:\n\nCodenfl_expectedFantasyPoints_pbp &lt;- nfl_expectedFantasyPoints_pbp %&gt;% \n  select(game_id, fixed_drive, play_id, everything()) %&gt;% \n  arrange(game_id, fixed_drive, play_id)\n\n\nLet’s check for duplicate game-player instances and season-week-player instances:\n\nCodenfl_expectedFantasyPoints_pbp %&gt;% \n  group_by(game_id, fixed_drive, play_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_expectedFantasyPoints_pbp) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_expectedFantasyPoints_weekly,\n  file = \"./data/nfl_expectedFantasyPoints_weekly.RData\"\n)\n\nsave(\n  nfl_expectedFantasyPoints_pbp,\n  file = \"./data/nfl_expectedFantasyPoints_pbp.RData\"\n)\n\n\n\n4.3.26 Fantasy Football Projections\n\n4.3.26.1 Download Players’ Projections\nYou can download fantasy football projections in R using the ffanalytics::scrape_data() function of the ffanalytics package (Tungate et al., 2025). Alternatively, if you prefer a graphical user interface, you can also download fantasy football projections using the Fantasy Football Analytics web application (depicted in Figure 4.1): https://apps.fantasyfootballanalytics.net.\n\n\n\n\n\nFigure 4.1: Fantasy Football Projections Available From Fantasy Football Analytics: https://apps.fantasyfootballanalytics.net.\n\n\n\n\n\n\n\n\nNote 4.3: Downloading players’ seasonal projections\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCode# Seasonal Projections\nplayers_projections_seasonal_raw &lt;- ffanalytics::scrape_data(\n  season = NULL, # NULL grabs the current season\n  week = 0) # 0 grabs seasonal projections\n\n# Weekly Projections\nplayers_projections_weekly_raw &lt;- ffanalytics::scrape_data(\n  season = NULL, # NULL grabs the current season\n  week = NULL) # NULL grabs the current week\n\n\n\nCodesave(\n  players_projections_seasonal_raw,\n  file = \"./data/players_projections_seasonal_raw.RData\"\n)\n\nsave(\n  players_projections_weekly_raw,\n  file = \"./data/players_projections_weekly_raw.RData\"\n)\n\n\nThe data file is saved in the project repository and can be loaded using the following command:\n\nCodeload(file = \"./data/players_projections_seasonal_raw.RData\")\nload(file = \"./data/players_projections_weekly_raw.RData\")\n\n\nThe players_projections_seasonal_raw and players_projections_weekly_raw object is in player-position-projection source form. That is, each row should be uniquely identified by the combination of id, pos and data_src. Each row should also be uniquely identified by the combination of player, pos, and data_src.\nLet’s check for duplicate player-position-projection source instances:\n\nCodeplayers_projections_seasonal_raw %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_seasonal_raw %&gt;% \n  bind_rows() %&gt;% \n  select(player, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_weekly_raw %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_weekly_raw %&gt;% \n  bind_rows() %&gt;% \n  select(player, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.26.2 Specify League Scoring Settings\nFirst, create a scoring object using the default scoring object:\n\nCodescoring_obj_default &lt;- ffanalytics::scoring\n\n\nView the default scoring settings:\n\nCodescoring_obj_default\n\n$pass\n$pass$pass_att\n[1] 0\n\n$pass$pass_comp\n[1] 0\n\n$pass$pass_inc\n[1] 0\n\n$pass$pass_yds\n[1] 0.04\n\n$pass$pass_tds\n[1] 4\n\n$pass$pass_int\n[1] -3\n\n$pass$pass_40_yds\n[1] 0\n\n$pass$pass_300_yds\n[1] 0\n\n$pass$pass_350_yds\n[1] 0\n\n$pass$pass_400_yds\n[1] 0\n\n\n$rush\n$rush$all_pos\n[1] TRUE\n\n$rush$rush_yds\n[1] 0.1\n\n$rush$rush_att\n[1] 0\n\n$rush$rush_40_yds\n[1] 0\n\n$rush$rush_tds\n[1] 6\n\n$rush$rush_100_yds\n[1] 0\n\n$rush$rush_150_yds\n[1] 0\n\n$rush$rush_200_yds\n[1] 0\n\n\n$rec\n$rec$all_pos\n[1] TRUE\n\n$rec$rec\n[1] 0\n\n$rec$rec_yds\n[1] 0.1\n\n$rec$rec_tds\n[1] 6\n\n$rec$rec_40_yds\n[1] 0\n\n$rec$rec_100_yds\n[1] 0\n\n$rec$rec_150_yds\n[1] 0\n\n$rec$rec_200_yds\n[1] 0\n\n\n$misc\n$misc$all_pos\n[1] TRUE\n\n$misc$fumbles_lost\n[1] -3\n\n$misc$fumbles_total\n[1] 0\n\n$misc$sacks\n[1] 0\n\n$misc$two_pts\n[1] 2\n\n\n$kick\n$kick$xp\n[1] 1\n\n$kick$fg_0019\n[1] 3\n\n$kick$fg_2029\n[1] 3\n\n$kick$fg_3039\n[1] 3\n\n$kick$fg_4049\n[1] 4\n\n$kick$fg_50\n[1] 5\n\n$kick$fg_miss\n[1] 0\n\n\n$ret\n$ret$all_pos\n[1] TRUE\n\n$ret$return_tds\n[1] 6\n\n$ret$return_yds\n[1] 0\n\n\n$idp\n$idp$all_pos\n[1] TRUE\n\n$idp$idp_solo\n[1] 1\n\n$idp$idp_asst\n[1] 0.5\n\n$idp$idp_sack\n[1] 2\n\n$idp$idp_int\n[1] 3\n\n$idp$idp_fum_force\n[1] 3\n\n$idp$idp_fum_rec\n[1] 2\n\n$idp$idp_pd\n[1] 1\n\n$idp$idp_td\n[1] 6\n\n$idp$idp_safety\n[1] 2\n\n\n$dst\n$dst$dst_fum_rec\n[1] 2\n\n$dst$dst_int\n[1] 2\n\n$dst$dst_safety\n[1] 2\n\n$dst$dst_sacks\n[1] 1\n\n$dst$dst_td\n[1] 6\n\n$dst$dst_blk\n[1] 1.5\n\n$dst$dst_ret_yds\n[1] 0\n\n$dst$dst_pts_allowed\n[1] 0\n\n\n$pts_bracket\n$pts_bracket[[1]]\n$pts_bracket[[1]]$threshold\n[1] 0\n\n$pts_bracket[[1]]$points\n[1] 10\n\n\n$pts_bracket[[2]]\n$pts_bracket[[2]]$threshold\n[1] 6\n\n$pts_bracket[[2]]$points\n[1] 7\n\n\n$pts_bracket[[3]]\n$pts_bracket[[3]]$threshold\n[1] 20\n\n$pts_bracket[[3]]$points\n[1] 4\n\n\n$pts_bracket[[4]]\n$pts_bracket[[4]]$threshold\n[1] 34\n\n$pts_bracket[[4]]$points\n[1] 0\n\n\n$pts_bracket[[5]]\n$pts_bracket[[5]]$threshold\n[1] 99\n\n$pts_bracket[[5]]$points\n[1] -4\n\n\nNow, modify the scoring settings to match your league settings. Below, we use the scoring settings for fantasy leagues on NFL.com, which happen to be point-per-reception leagues (i.e., PPR leagues):\n\nCodescoring_obj &lt;- scoring_obj_default\n\n# Offense\nscoring_obj$pass$pass_int &lt;- -2\nscoring_obj$rec$rec &lt;- 1\nscoring_obj$misc$fumbles_lost &lt;- -2\n\n# Kickers\nscoring_obj$kick$fg_4049 &lt;- 3\n\n# Defense/Special Teams\nscoring_obj$pts_bracket &lt;- list(\n  list(threshold = 0, points = 10),\n  list(threshold = 6, points = 7),\n  list(threshold = 13, points = 4),\n  list(threshold = 20, points = 1),\n  list(threshold = 27, points = 0),\n  list(threshold = 34, points = -1),\n  list(threshold = 99, points = -4)\n)\n\n\nView our scoring settings:\n\nCodescoring_obj\n\n$pass\n$pass$pass_att\n[1] 0\n\n$pass$pass_comp\n[1] 0\n\n$pass$pass_inc\n[1] 0\n\n$pass$pass_yds\n[1] 0.04\n\n$pass$pass_tds\n[1] 4\n\n$pass$pass_int\n[1] -2\n\n$pass$pass_40_yds\n[1] 0\n\n$pass$pass_300_yds\n[1] 0\n\n$pass$pass_350_yds\n[1] 0\n\n$pass$pass_400_yds\n[1] 0\n\n\n$rush\n$rush$all_pos\n[1] TRUE\n\n$rush$rush_yds\n[1] 0.1\n\n$rush$rush_att\n[1] 0\n\n$rush$rush_40_yds\n[1] 0\n\n$rush$rush_tds\n[1] 6\n\n$rush$rush_100_yds\n[1] 0\n\n$rush$rush_150_yds\n[1] 0\n\n$rush$rush_200_yds\n[1] 0\n\n\n$rec\n$rec$all_pos\n[1] TRUE\n\n$rec$rec\n[1] 1\n\n$rec$rec_yds\n[1] 0.1\n\n$rec$rec_tds\n[1] 6\n\n$rec$rec_40_yds\n[1] 0\n\n$rec$rec_100_yds\n[1] 0\n\n$rec$rec_150_yds\n[1] 0\n\n$rec$rec_200_yds\n[1] 0\n\n\n$misc\n$misc$all_pos\n[1] TRUE\n\n$misc$fumbles_lost\n[1] -2\n\n$misc$fumbles_total\n[1] 0\n\n$misc$sacks\n[1] 0\n\n$misc$two_pts\n[1] 2\n\n\n$kick\n$kick$xp\n[1] 1\n\n$kick$fg_0019\n[1] 3\n\n$kick$fg_2029\n[1] 3\n\n$kick$fg_3039\n[1] 3\n\n$kick$fg_4049\n[1] 3\n\n$kick$fg_50\n[1] 5\n\n$kick$fg_miss\n[1] 0\n\n\n$ret\n$ret$all_pos\n[1] TRUE\n\n$ret$return_tds\n[1] 6\n\n$ret$return_yds\n[1] 0\n\n\n$idp\n$idp$all_pos\n[1] TRUE\n\n$idp$idp_solo\n[1] 1\n\n$idp$idp_asst\n[1] 0.5\n\n$idp$idp_sack\n[1] 2\n\n$idp$idp_int\n[1] 3\n\n$idp$idp_fum_force\n[1] 3\n\n$idp$idp_fum_rec\n[1] 2\n\n$idp$idp_pd\n[1] 1\n\n$idp$idp_td\n[1] 6\n\n$idp$idp_safety\n[1] 2\n\n\n$dst\n$dst$dst_fum_rec\n[1] 2\n\n$dst$dst_int\n[1] 2\n\n$dst$dst_safety\n[1] 2\n\n$dst$dst_sacks\n[1] 1\n\n$dst$dst_td\n[1] 6\n\n$dst$dst_blk\n[1] 1.5\n\n$dst$dst_ret_yds\n[1] 0\n\n$dst$dst_pts_allowed\n[1] 0\n\n\n$pts_bracket\n$pts_bracket[[1]]\n$pts_bracket[[1]]$threshold\n[1] 0\n\n$pts_bracket[[1]]$points\n[1] 10\n\n\n$pts_bracket[[2]]\n$pts_bracket[[2]]$threshold\n[1] 6\n\n$pts_bracket[[2]]$points\n[1] 7\n\n\n$pts_bracket[[3]]\n$pts_bracket[[3]]$threshold\n[1] 13\n\n$pts_bracket[[3]]$points\n[1] 4\n\n\n$pts_bracket[[4]]\n$pts_bracket[[4]]$threshold\n[1] 20\n\n$pts_bracket[[4]]$points\n[1] 1\n\n\n$pts_bracket[[5]]\n$pts_bracket[[5]]$threshold\n[1] 27\n\n$pts_bracket[[5]]$points\n[1] 0\n\n\n$pts_bracket[[6]]\n$pts_bracket[[6]]$threshold\n[1] 34\n\n$pts_bracket[[6]]$points\n[1] -1\n\n\n$pts_bracket[[7]]\n$pts_bracket[[7]]$threshold\n[1] 99\n\n$pts_bracket[[7]]$points\n[1] -4\n\n\n\n4.3.26.3 Calculate Projected Points\nCalculate projected points by source:\n\nCodeplayers_projectedPoints_seasonal &lt;- ffanalytics:::impute_and_score_sources(\n  data_result = players_projections_seasonal_raw,\n  scoring_rules = scoring_obj)\n\nplayers_projectedPoints_weekly &lt;- ffanalytics:::impute_and_score_sources(\n  data_result = players_projections_weekly_raw,\n  scoring_rules = scoring_obj)\n\n\nCalculate projected statistics and points, averaged across sources:\n\nCode# Seasonal Projections\nplayers_projectedStatsAverage_seasonal &lt;- ffanalytics::projections_table(\n  players_projections_seasonal_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = TRUE)\n\nplayers_projectedPointsAverage_seasonal &lt;- ffanalytics::projections_table(\n  players_projections_seasonal_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = FALSE)\n\n# Weekly Projections\nplayers_projectedStatsAverage_weekly &lt;- ffanalytics::projections_table(\n  players_projections_weekly_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = TRUE)\n\nplayers_projectedPointsAverage_weekly &lt;- ffanalytics::projections_table(\n  players_projections_weekly_raw,\n  scoring_rules = scoring_obj,\n  return_raw_stats = FALSE)\n\n\nThe players_projectedPoints_seasonal, players_projectedStatsAverage_seasonal, players_projectedPointsAverage_seasonal, players_projectedPoints_weekly, players_projectedStatsAverage_weekly, and players_projectedPointsAverage_weekly objects are in player-average type-position form. That is, each row should be uniquely identified by the combination of id, avg_type and pos (or position).\nLet’s check for duplicate player-position-projection source instances:\n\nCodeplayers_projectedPoints_seasonal %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedStatsAverage_seasonal %&gt;% \n  group_by(id, avg_type, position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedPointsAverage_seasonal %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedPoints_weekly %&gt;% \n  bind_rows() %&gt;% \n  select(id, pos, data_src, everything()) %&gt;%\n  arrange(player, id, pos, data_src) %&gt;% \n  group_by(id, pos, data_src) %&gt;% \n  filter(n() &gt; 1, !is.na(id)) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedStatsAverage_weekly %&gt;% \n  group_by(id, avg_type, position) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projectedPointsAverage_weekly %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s merge the two averaged projected statistics and points objects:\n\nCodeplayers_projections_seasonal_average &lt;- full_join(\n  players_projectedPointsAverage_seasonal,\n  players_projectedStatsAverage_seasonal,\n  by = c(\"id\",\"avg_type\",\"pos\" = \"position\")\n)\n\nplayers_projections_weekly_average &lt;- full_join(\n  players_projectedPointsAverage_weekly,\n  players_projectedStatsAverage_weekly,\n  by = c(\"id\",\"avg_type\",\"pos\" = \"position\")\n)\n\n\nLet’s again check for duplicate player-position-projection source instances:\n\nCodeplayers_projections_seasonal_average %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeplayers_projections_weekly_average %&gt;% \n  group_by(id, avg_type, pos) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n4.3.26.4 Add Additional Player Information\n\nCodeplayers_projections_seasonal_average &lt;- players_projections_seasonal_average %&gt;% \n  add_ecr() %&gt;% \n  add_adp() %&gt;% \n  add_aav() %&gt;%\n  add_uncertainty() %&gt;% \n  add_player_info()\n\nplayers_projections_weekly_average &lt;- players_projections_weekly_average %&gt;% \n  add_ecr() %&gt;% \n  #add_uncertainty() %&gt;% # currently throws an error\n  add_player_info()\n\n\nHere are the variable names:\n\nCodenames(dplyr::bind_rows(players_projectedPoints_seasonal)) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(players_projections_seasonal_average) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(dplyr::bind_rows(players_projectedPoints_weekly)) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(players_projections_weekly_average) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  players_projectedPoints_seasonal, players_projections_seasonal_average,\n  file = \"./data/players_projectedPoints_seasonal.RData\"\n)\n\nsave(\n  players_projectedPoints_weekly, players_projections_weekly_average,\n  file = \"./data/players_projectedPoints_weekly.RData\"\n)\n\n\nThe data file is saved in the project repository and can be loaded using the following command:\n\nCodeload(file = \"./data/players_projectedPoints_seasonal.RData\")\nload(file = \"./data/players_projectedPoints_weekly.RData\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-calculations",
    "href": "download-football-data.html#sec-calculations",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n4.4 Calculations",
    "text": "4.4 Calculations\n\n4.4.1 Historical Actual Player Statistics\nIn addition to week-by-week actual player statistics, we can also compute historical actual player statistics as a function of different timeframes, including season-by-season and career statistics.\n\n4.4.1.1 Season-by-Season Statistics\nFirst, we can compute the players’ season-by-season statistics using the nflfastR (Carl & Baldwin, 2024) package.\n\n\n\n\n\n\nNote 4.4: Downloading season-by-season statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_actualStats_seasonal_player_raw &lt;- nflfastR::calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  season_type = \"REG\")\n\nnfl_actualStats_seasonal_team_raw &lt;- nflfastR::calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  stat_type = \"team\",\n  season_type = \"REG\")\n\nnfl_actualStats_seasonal_player_inclPost_raw &lt;- nflfastR::calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  season_type = \"REG+POST\")\n\nnfl_actualStats_seasonal_team_inclPost_raw &lt;- nflfastR::calculate_stats(\n  seasons = TRUE,\n  summary_level = \"season\",\n  stat_type = \"team\",\n  season_type = \"REG+POST\")\n\n\n\nCodesave(\n  nfl_actualStats_seasonal_player_raw, nfl_actualStats_seasonal_team_raw,\n  nfl_actualStats_seasonal_player_inclPost_raw, nfl_actualStats_seasonal_team_inclPost_raw,\n  file = \"./data/nfl_actualStats_seasonal_raw.RData\"\n)\n\n\nA Data Dictionary for the variables is available in the nfl_stats_variables object that is returned when running the nflfastR::calculate_stats() function:\n\nCodenfl_stats_variables\n\n\n  \n\n\n\nThe nfl_actualStats_seasonal_player_raw object is in player-season form. That is, each row should be uniquely identified by the combination of player_id and season. The nfl_actualStats_seasonal_team_raw object is in team-season form. That is, each row should be uniquely identified by the combination of team and season. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_seasonal_player &lt;- nfl_actualStats_seasonal_player_raw %&gt;% \n  rename(team = recent_team) %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nnfl_actualStats_seasonal_team &lt;- nfl_actualStats_seasonal_team_raw %&gt;% \n  select(season, team, everything()) %&gt;% \n  arrange(season, team)\n\nnfl_actualStats_seasonal_player_inclPost &lt;- nfl_actualStats_seasonal_player_inclPost_raw %&gt;% \n  rename(team = recent_team) %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nnfl_actualStats_seasonal_team_inclPost &lt;- nfl_actualStats_seasonal_team_inclPost_raw %&gt;% \n  select(season, team, everything()) %&gt;% \n  arrange(season, team)\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_actualStats_seasonal_player %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_seasonal_team %&gt;% \n  group_by(team, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_seasonal_player_inclPost %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_seasonal_team_inclPost %&gt;% \n  group_by(team, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s remove infinite (implausible) values:\n\nCodenfl_actualStats_seasonal_player[sapply(nfl_actualStats_seasonal_player, is.infinite)] &lt;- NA\nnfl_actualStats_seasonal_team[sapply(nfl_actualStats_seasonal_team, is.infinite)] &lt;- NA\nnfl_actualStats_seasonal_player_inclPost[sapply(nfl_actualStats_seasonal_player_inclPost, is.infinite)] &lt;- NA\nnfl_actualStats_seasonal_team_inclPost[sapply(nfl_actualStats_seasonal_team_inclPost, is.infinite)] &lt;- NA\n\n\nHere are the variable names:\n\nCodenames(nfl_actualStats_seasonal_player) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualStats_seasonal_team) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualStats_seasonal_player_inclPost) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualStats_seasonal_team_inclPost) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualStats_seasonal_player, nfl_actualStats_seasonal_team,\n  nfl_actualStats_seasonal_player_inclPost, nfl_actualStats_seasonal_team_inclPost,\n  file = \"./data/nfl_actualStats_seasonal.RData\"\n)\n\n\n\n4.4.1.2 Career Statistics\nSecond, we can compute the players’ career statistics using the nflfastR::calculate_stats() function from the nflfastR package (Carl & Baldwin, 2024).\n\nCode# Compute Player Stats by Career\nplayerVars &lt;- nfl_actualStats_seasonal_player %&gt;% \n  select(games:fantasy_points_ppr) %&gt;% \n  names()\n\nweightedAverageVars_byGames &lt;- c(\n  \"passing_epa\",\"rushing_epa\",\"receiving_epa\",\n  \"target_share\",\"air_yards_share\"\n)\n\nweightedAverageVars_byAttempts &lt;- c(\n  \"passing_cpoe\"\n)\n\nweightedAverageVars_byTargets &lt;- c(\n  \"wopr\"\n)\n\nconcatenateVars &lt;- c(\n  \"fg_made_list\",\"fg_missed_list\",\"fg_blocked_list\",\"gwfg_distance_list\"\n)\n\nrecomputeVars &lt;- c(\n  \"pacr\", # passing_yards / passing_air_yards\n  \"racr\", # receiving_yards / receiving_air_yards\n  \"fg_pct\", # fg_made / fg_att\n  \"pat_pct\" # pat_made / pat_att\n)\n\nsumVars &lt;- playerVars[playerVars %ni% c(\n  weightedAverageVars_byGames, weightedAverageVars_byAttempts, weightedAverageVars_byTargets, concatenateVars, recomputeVars)]\n\nnfl_actualStats_career_player &lt;- nfl_actualStats_seasonal_player %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(\n    across(all_of(weightedAverageVars_byGames), ~ weighted.mean(.x, w = games, na.rm = TRUE)),\n    across(all_of(weightedAverageVars_byAttempts), ~ weighted.mean(.x, w = attempts, na.rm = TRUE)),\n    across(all_of(weightedAverageVars_byTargets), ~ weighted.mean(.x, w = targets, na.rm = TRUE)),\n    across(all_of(concatenateVars), ~ paste(.x[!is.na(.x)], collapse = \";\")),\n    across(all_of(sumVars), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\") %&gt;% \n  mutate(\n    pacr = passing_yards / passing_air_yards,\n    racr = receiving_yards / receiving_air_yards,\n    fg_pct = fg_made / fg_att,\n    pat_pct = pat_made / pat_att\n  )\n\nnfl_actualStats_career_player$fg_made_list[which(nfl_actualStats_career_player$fg_made_list == \"\")] &lt;- NA\nnfl_actualStats_career_player$fg_missed_list[which(nfl_actualStats_career_player$fg_missed_list == \"\")] &lt;- NA\nnfl_actualStats_career_player$fg_blocked_list[which(nfl_actualStats_career_player$fg_blocked_list == \"\")] &lt;- NA\nnfl_actualStats_career_player$gwfg_distance_list[which(nfl_actualStats_career_player$gwfg_distance_list == \"\")] &lt;- NA\n\nnfl_actualStats_career_player_inclPost &lt;- nfl_actualStats_seasonal_player_inclPost %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(\n    across(all_of(weightedAverageVars_byGames), ~ weighted.mean(.x, w = games, na.rm = TRUE)),\n    across(all_of(weightedAverageVars_byAttempts), ~ weighted.mean(.x, w = attempts, na.rm = TRUE)),\n    across(all_of(weightedAverageVars_byTargets), ~ weighted.mean(.x, w = targets, na.rm = TRUE)),\n    across(all_of(concatenateVars), ~ paste(.x[!is.na(.x)], collapse = \";\")),\n    across(all_of(sumVars), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\") %&gt;% \n  mutate(\n    pacr = passing_yards / passing_air_yards,\n    racr = receiving_yards / receiving_air_yards,\n    fg_pct = fg_made / fg_att,\n    pat_pct = pat_made / pat_att\n  )\n\nnfl_actualStats_career_player_inclPost$fg_made_list[which(nfl_actualStats_career_player_inclPost$fg_made_list == \"\")] &lt;- NA\nnfl_actualStats_career_player_inclPost$fg_missed_list[which(nfl_actualStats_career_player_inclPost$fg_missed_list == \"\")] &lt;- NA\nnfl_actualStats_career_player_inclPost$fg_blocked_list[which(nfl_actualStats_career_player_inclPost$fg_blocked_list == \"\")] &lt;- NA\nnfl_actualStats_career_player_inclPost$gwfg_distance_list[which(nfl_actualStats_career_player_inclPost$gwfg_distance_list == \"\")] &lt;- NA\n\nnfl_actualStats_career_player[sapply(nfl_actualStats_career_player, is.infinite)] &lt;- NA\nnfl_actualStats_career_player_inclPost[sapply(nfl_actualStats_career_player_inclPost, is.infinite)] &lt;- NA\n\n\nLet’s check for duplicate player instances:\n\nCodenfl_actualStats_career_player %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_career_player_inclPost %&gt;% \n  group_by(player_id) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_actualStats_career_player) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualStats_career_player_inclPost) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualStats_career_player, nfl_actualStats_career_player_inclPost,\n  file = \"./data/nfl_actualStats_career.RData\"\n)\n\n\n\n4.4.1.3 Week-by-Week Statistics\nWe already load players’ week-by-week statistics above using the nflreadr::load_player_stats() function from the nflreadr (Ho & Carl, 2024) package. Nevertheless, we could compute players’ weekly statistics from the play-by-play data using the nflfastR::calculate_stats() function from the nflfastR (Carl & Baldwin, 2024) package:\n\n\n\n\n\n\nNote 4.5: Downloading week-by-week statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_actualStats_weekly_player_raw &lt;- nflfastR::calculate_stats(\n  seasons = TRUE,\n  summary_level = \"week\")\n\nnfl_actualStats_weekly_team_raw &lt;- nflfastR::calculate_stats(\n  seasons = TRUE,\n  summary_level = \"week\",\n  stat_type = \"team\")\n\n\n\nCodesave(\n  nfl_actualStats_weekly_player_raw, nfl_actualStats_weekly_team_raw,\n  file = \"./data/nfl_actualStats_weekly_raw.RData\"\n)\n\n\nThe nfl_actualStats_weekly objects are in player-season-week form. That is, each row should be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodenfl_actualStats_weekly_player &lt;- nfl_actualStats_weekly_player_raw %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualStats_weekly_team &lt;- nfl_actualStats_weekly_team_raw %&gt;% \n  select(season, week, team, everything()) %&gt;% \n  arrange(season, week, team)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualStats_weekly_player %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualStats_weekly_team %&gt;% \n  group_by(team, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(nfl_actualStats_weekly_player) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualStats_weekly_team) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualStats_weekly_player, nfl_actualStats_weekly_team,\n  file = \"./data/nfl_actualStats_weekly.RData\"\n)\n\n\n\n4.4.2 Historical Actual Fantasy Points\nSpecify scoring settings:\n\n4.4.2.1 Week-by-Week\n\nCodenfl_actualFantasyPoints_player_weekly_seasonalList &lt;- list()\nnfl_actualFantasyPoints_dst_weekly_seasonalList &lt;- list()\n\n\n\n\n\n\n\n\nNote 4.6: Calculating players’ week-by-week fantasy points\n\n\n\nNote: the following code takes a while to run.\n\n\nWe set the seasons variable in Section 4.3.13.\n\nCode#nfl_actualFantasyPoints_weekly_raw &lt;- ffanalytics:::actual_points_scoring(\n#  season = 2023,\n#  summary_level = c(\"week\"),\n#  stat_type = c(\"player\", \"dst\", \"team\"),\n#  season_type = c(\"REG\", \"POST\", \"REG+POST\"),\n#  scoring_rules = scoring_obj,\n#  vor_baseline = NULL,\n#  rename_colums = FALSE\n#)\n\npb &lt;- txtProgressBar(\n  min = 0,\n  max = length(seasons),\n  style = 3)\n\nfor(i in 1:length(seasons)){\n  # Compute actual statistics by season\n  nfl_actualFantasyPoints_player_weekly_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"week\"),\n      stat_type = c(\"player\"),\n      #season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_dst_weekly_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"week\"),\n      stat_type = c(\"dst\"),\n      #season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_player_weekly_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualFantasyPoints_dst_weekly_seasonalList[[i]]$season &lt;- seasons[i]\n  \n  print(\n    paste(\"Completed computing actual fantasy points for season: \", seasons[i], sep = \"\"))\n  \n  # Update the progress bar\n  setTxtProgressBar(pb, i)\n}\n\n# Close the progress bar\nclose(pb)\n\nnfl_actualFantasyPoints_player_weekly_raw &lt;- nfl_actualFantasyPoints_player_weekly_seasonalList %&gt;% \n  dplyr::bind_rows()\n\nnfl_actualFantasyPoints_dst_weekly_raw &lt;- nfl_actualFantasyPoints_dst_weekly_seasonalList %&gt;% \n  dplyr::bind_rows()\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_weekly_raw, nfl_actualFantasyPoints_dst_weekly_raw,\n  file = \"./data/nfl_actualFantasyPoints_weekly_raw.RData\"\n)\n\n\nThe nfl_actualFantasyPoints_weekly objects are in player-season-week (or team-season-week) form. That is, each row should be uniquely identified by the combination of player_id, season, and week (or team-season-week). Let’s rearrange the data accordingly:\n\nCodenfl_actualFantasyPoints_player_weekly &lt;- nfl_actualFantasyPoints_player_weekly_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(player_id, season, week, everything()) %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\nnfl_actualFantasyPoints_dst_weekly &lt;- nfl_actualFantasyPoints_dst_weekly_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(season, week, team, everything()) %&gt;% \n  arrange(season, week, team)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualFantasyPoints_player_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualFantasyPoints_dst_weekly %&gt;% \n  group_by(team, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s remove infinite (implausible) values:\n\nCodenfl_actualFantasyPoints_player_weekly_raw[sapply(nfl_actualFantasyPoints_player_weekly_raw, is.infinite)] &lt;- NA\nnfl_actualFantasyPoints_dst_weekly_raw[sapply(nfl_actualFantasyPoints_dst_weekly_raw, is.infinite)] &lt;- NA\n\n\nHere are the variable names:\n\nCodenames(nfl_actualFantasyPoints_player_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualFantasyPoints_dst_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_weekly, nfl_actualFantasyPoints_dst_weekly,\n  file = \"./data/nfl_actualFantasyPoints_weekly.RData\"\n)\n\n\n\n4.4.2.2 Season-by-Season\n\nCodenfl_actualFantasyPoints_player_seasonal_seasonalList &lt;- list()\nnfl_actualFantasyPoints_dst_seasonal_seasonalList &lt;- list()\n\n\n\n\n\n\n\n\nNote 4.7: Calculating players’ season-by-season fantasy points\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCode#nfl_actualFantasyPoints_seasonal_raw &lt;- ffanalytics:::actual_points_scoring(\n#  season = 2023,\n#  summary_level = c(\"season\"),\n#  stat_type = c(\"player\", \"dst\", \"team\"),\n#  season_type = c(\"REG\"),\n#  scoring_rules = scoring_obj,\n#  vor_baseline = NULL,\n#  rename_colums = TRUE\n#)\n\npb &lt;- txtProgressBar(\n  min = 0,\n  max = length(seasons),\n  style = 3)\n\nfor(i in 1:length(seasons)){\n  # Compute actual statistics by season\n  nfl_actualFantasyPoints_player_seasonal_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"season\"),\n      stat_type = c(\"player\"),\n      season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_dst_seasonal_seasonalList[[i]] &lt;- \n    ffanalytics:::actual_points_scoring(\n      season = seasons[i],\n      summary_level = c(\"season\"),\n      stat_type = c(\"dst\"),\n      season_type = c(\"REG\"),\n      scoring_rules = scoring_obj,\n      vor_baseline = NULL,\n      rename_colums = FALSE)\n  \n  nfl_actualFantasyPoints_player_seasonal_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualFantasyPoints_dst_seasonal_seasonalList[[i]]$season &lt;- seasons[i]\n  \n  print(\n    paste(\"Completed computing actual fantasy points for season: \", seasons[i], sep = \"\"))\n  \n  # Update the progress bar\n  setTxtProgressBar(pb, i)\n}\n\n# Close the progress bar\nclose(pb)\n\nnfl_actualFantasyPoints_player_seasonal_raw &lt;- nfl_actualFantasyPoints_player_seasonal_seasonalList %&gt;% \n  dplyr::bind_rows()\n\nnfl_actualFantasyPoints_dst_seasonal_raw &lt;- nfl_actualFantasyPoints_dst_seasonal_seasonalList %&gt;% \n  dplyr::bind_rows()\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_seasonal_raw, nfl_actualFantasyPoints_dst_seasonal_raw,\n  file = \"./data/nfl_actualFantasyPoints_seasonal_raw.RData\"\n)\n\n\nThe nfl_actualFantasyPoints_seasonal objects are in player-season (or team-season) form. That is, each row should be uniquely identified by the combination of player_id and season (or team-season). Let’s rearrange the data accordingly:\n\nCodenfl_actualFantasyPoints_player_seasonal &lt;- nfl_actualFantasyPoints_player_seasonal_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\nnfl_actualFantasyPoints_dst_seasonal &lt;- nfl_actualFantasyPoints_dst_seasonal_raw %&gt;% \n  rename(\n    fantasyPoints = raw_points,\n    team = recent_team) %&gt;% \n  select(season, team, everything()) %&gt;% \n  arrange(season, team)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodenfl_actualFantasyPoints_player_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_actualFantasyPoints_dst_seasonal %&gt;% \n  group_by(team, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nLet’s remove infinite (implausible) values:\n\nCodenfl_actualFantasyPoints_player_seasonal[sapply(nfl_actualFantasyPoints_player_seasonal, is.infinite)] &lt;- NA\nnfl_actualFantasyPoints_dst_seasonal[sapply(nfl_actualFantasyPoints_dst_seasonal, is.infinite)] &lt;- NA\n\n\nHere are the variable names:\n\nCodenames(nfl_actualFantasyPoints_player_seasonal) %&gt;% as.data.frame()\n\n\n  \n\n\nCodenames(nfl_actualFantasyPoints_dst_seasonal) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_seasonal, nfl_actualFantasyPoints_dst_seasonal,\n  file = \"./data/nfl_actualFantasyPoints_seasonal.RData\"\n)\n\n\n\n4.4.2.3 Career\n\nCodenfl_actualFantasyPoints_player_career_raw &lt;- nfl_actualFantasyPoints_player_seasonal %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(fantasyPoints = sum(fantasyPoints, na.rm = TRUE))\n\nnfl_actualFantasyPoints_player_career_raw &lt;- full_join(\n  nfl_actualFantasyPoints_player_career_raw,\n  nfl_players %&gt;% select(gsis_id, display_name, first_name, last_name, position_group, position),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_career_raw,\n  file = \"./data/nfl_actualFantasyPoints_career_raw.RData\"\n)\n\n\nLet’s rearrange the data:\n\nCodenfl_actualFantasyPoints_player_career &lt;- nfl_actualFantasyPoints_player_career_raw %&gt;% \n  select(player_id, display_name, first_name, last_name, position_group, position, fantasyPoints)\n\n\nHere are the variable names:\n\nCodenames(nfl_actualFantasyPoints_player_career) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCodesave(\n  nfl_actualFantasyPoints_player_career,\n  file = \"./data/nfl_actualFantasyPoints_career.RData\"\n)\n\n\n\n4.4.3 Player Age and Experience\n\n4.4.3.1 Weekly\nWe calculate the player’s age based on the difference between dates using the lubridate package (Spinu et al., 2024):\n\nCode# Reshape from wide to long format\nnfl_actualFantasyPoints_player_weekly_long &lt;- nfl_actualFantasyPoints_player_weekly %&gt;% \n  tidyr::pivot_longer(\n    cols = c(team, opponent_team),\n    names_to = \"role\",\n    values_to = \"team\")\n\n# Perform separate inner join operations for the home_team and away_team\nnfl_actualFantasyPoints_player_weekly_home &lt;- dplyr::inner_join(\n  nfl_actualFantasyPoints_player_weekly_long,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"home_team\")) %&gt;% \n  mutate(home_away = \"home_team\")\n\nnfl_actualFantasyPoints_player_weekly_away &lt;- dplyr::inner_join(\n  nfl_actualFantasyPoints_player_weekly_long,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"away_team\")) %&gt;% \n  mutate(home_away = \"away_team\")\n\n# Combine the results of the join operations\nnfl_actualFantasyPoints_player_weekly_schedules_long &lt;- dplyr::bind_rows(\n  nfl_actualFantasyPoints_player_weekly_home,\n  nfl_actualFantasyPoints_player_weekly_away)\n\n# Reshape from long to wide\nplayer_game_gameday &lt;- nfl_actualFantasyPoints_player_weekly_schedules_long %&gt;%\n  dplyr::distinct(player_id, season, week, game_id, home_away, team, gameday) %&gt;% #, .keep_all = TRUE\n  tidyr::pivot_wider(\n    names_from = home_away,\n    values_from = team)\n\n# Merge player birthdate and the game date\nplayer_game_birthdate_gameday &lt;- dplyr::left_join(\n  player_game_gameday,\n  unique(nfl_players[,c(\"gsis_id\",\"birth_date\")]),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\nplayer_game_birthdate_gameday$birth_date &lt;- lubridate::ymd(player_game_birthdate_gameday$birth_date)\nplayer_game_birthdate_gameday$gameday &lt;- lubridate::ymd(player_game_birthdate_gameday$gameday)\n\n# Calculate player's age for a given week as the difference between their birthdate and the game date\nplayer_game_birthdate_gameday$age &lt;- lubridate::interval(\n  start = player_game_birthdate_gameday$birth_date,\n  end = player_game_birthdate_gameday$gameday\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\n# Merge with Pro Football Reference Data on Player Age by Season\nplayer_game_birthdate_gameday &lt;- player_game_birthdate_gameday %&gt;% \n  dplyr::left_join(\n    nfl_advancedStatsPFR_seasonal %&gt;% filter(!is.na(gsis_id), !is.na(season), !is.na(age)) %&gt;% select(gsis_id, season, age) %&gt;% unique(),\n    by = c(\"player_id\" = \"gsis_id\", \"season\")\n  )\n\n# Set age as first non-missing value from calculation above or from PFR\nplayer_game_birthdate_gameday &lt;- player_game_birthdate_gameday %&gt;% \n  mutate(age = coalesce(age.x, age.y)) %&gt;% \n  select(-age.x, -age.y)\n\n# Calculate ageCentered and ageCenteredQuadratic\nplayer_game_birthdate_gameday$ageCentered20 &lt;- player_game_birthdate_gameday$age - 20\nplayer_game_birthdate_gameday$ageCentered20Quadratic &lt;- player_game_birthdate_gameday$ageCentered20 ^ 2\n\n# Merge with player info\nplayer_age &lt;- dplyr::left_join(\n  player_game_birthdate_gameday,\n  nfl_players %&gt;% select(-birth_date, -team_abbr, - team_seq),\n  by = c(\"player_id\" = \"gsis_id\"))\n\n# Add game_id to weekly stats to facilitate merging\nnfl_actualFantasyPoints_player_weekly &lt;- nfl_actualFantasyPoints_player_weekly %&gt;% \n  dplyr::left_join(\n    player_age[,c(\"season\",\"week\",\"player_id\",\"game_id\")],\n    by = c(\"season\",\"week\",\"player_id\"))\n\n# Merge with player weekly stats\nplayer_stats_weekly &lt;- dplyr::full_join(\n  player_age %&gt;% select(-position, -position_group),\n  nfl_actualFantasyPoints_player_weekly,\n  by = c(\"season\",\"week\",\"player_id\",\"game_id\"))\n\nplayer_stats_weekly$total_years_of_experience &lt;- as.integer(player_stats_weekly$years_of_experience)\n\nplayer_stats_weekly$years_of_experience &lt;- NULL\n\ndistinct_seasons &lt;- player_stats_weekly %&gt;%\n  dplyr::select(player_id, season) %&gt;%\n  dplyr::distinct() %&gt;% \n  dplyr::left_join(\n    nfl_players[,c(\"gsis_id\",\"years_of_experience\")],\n    by = c(\"player_id\" = \"gsis_id\")\n  ) %&gt;% \n  dplyr::mutate(total_years_of_experience = as.integer(years_of_experience)) %&gt;% \n  dplyr::select(-years_of_experience)\n\nyears_of_experience &lt;- distinct_seasons %&gt;% \n  dplyr::arrange(player_id, -season) %&gt;% \n  dplyr::group_by(player_id) %&gt;%\n  dplyr::mutate(years_of_experience = first(total_years_of_experience) - (row_number() - 1)) %&gt;%\n  dplyr::ungroup()\n\nyears_of_experience$years_of_experience[which(years_of_experience$years_of_experience &lt; 0)] &lt;- 0\n\nplayer_stats_weekly &lt;- player_stats_weekly %&gt;% \n  dplyr::left_join(\n    years_of_experience[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\n\nThe player_stats_weekly objects are in player-season-week form. That is, each row should be uniquely identified by the combination of player_id, season, and week. Let’s rearrange the data accordingly:\n\nCodeplayer_stats_weekly &lt;- player_stats_weekly %&gt;% \n  arrange(player_display_name, player_id, season, week)\n\n\nLet’s check for duplicate player-season-week instances:\n\nCodeplayer_stats_weekly %&gt;% \n  group_by(player_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(player_stats_weekly) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCode# Save data\nsave(\n  player_stats_weekly,\n  file = \"./data/player_stats_weekly.RData\"\n)\n\n\n\n4.4.3.2 Seasonal\n\nCode# Merge player info with seasonal stats\nplayer_stats_seasonal &lt;- dplyr::full_join(\n  nfl_actualFantasyPoints_player_seasonal,\n  nfl_players %&gt;% select(-position, -position_group, -team_abbr, - team_seq),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\n# Calculate age\nseason_startdate &lt;- nfl_schedules %&gt;% \n  dplyr::group_by(season) %&gt;% \n  dplyr::summarise(startdate = min(gameday, na.rm = TRUE))\n\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  dplyr::left_join(\n    season_startdate,\n    by = \"season\"\n  )\n\nplayer_stats_seasonal$age &lt;- lubridate::interval(\n  start = player_stats_seasonal$birth_date,\n  end = player_stats_seasonal$startdate\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\n# Merge with Pro Football Reference Data on Player Age by Season\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  dplyr::left_join(\n    nfl_advancedStatsPFR_seasonal %&gt;% filter(!is.na(gsis_id), !is.na(season), !is.na(age)) %&gt;% select(gsis_id, season, age) %&gt;% unique(),\n    by = c(\"player_id\" = \"gsis_id\", \"season\")\n  )\n\n# Set age as first non-missing value from calculation above or from PFR\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  mutate(age = coalesce(age.x, age.y)) %&gt;% \n  select(-age.x, -age.y)\n\n# Calculate ageCentered and ageCenteredQuadratic\nplayer_stats_seasonal$ageCentered20 &lt;- player_stats_seasonal$age - 20\nplayer_stats_seasonal$ageCentered20Quadratic &lt;- player_stats_seasonal$ageCentered20 ^ 2\n\n# Years of experience\nplayer_stats_seasonal$years_of_experience &lt;- NULL\n\nplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  dplyr::left_join(\n    years_of_experience[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\n\nThe player_stats_seasonal objects are in player-season form. That is, each row should be uniquely identified by the combination of player_id and season. Let’s rearrange the data accordingly:\n\nCodeplayer_stats_seasonal &lt;- player_stats_seasonal %&gt;% \n  select(player_id, season, everything()) %&gt;% \n  arrange(player_display_name, player_id, season)\n\n\nLet’s check for duplicate player-season instances:\n\nCodeplayer_stats_seasonal %&gt;% \n  group_by(player_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the variable names:\n\nCodenames(player_stats_seasonal) %&gt;% as.data.frame()\n\n\n  \n\n\n\n\nCode# Save data\nsave(\n  player_stats_seasonal,\n  file = \"./data/player_stats_seasonal.RData\"\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "download-football-data.html#sec-downloadFootballDataSessionInfo",
    "href": "download-football-data.html#sec-downloadFootballDataSessionInfo",
    "title": "4  Download and Process NFL Football Data",
    "section": "\n5.1 Session Info",
    "text": "5.1 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_1.0.0           stringr_1.5.1           dplyr_1.1.4            \n [4] purrr_1.1.0             readr_2.1.5             tidyr_1.3.1            \n [7] tibble_3.3.0            ggplot2_3.5.2           tidyverse_2.0.0        \n[10] lubridate_1.9.4         progressr_0.15.1        nflplotR_1.4.0         \n[13] nfl4th_1.0.4            nflseedR_2.0.1          nflfastR_5.1.0         \n[16] nflreadr_1.4.1          petersenlab_1.2.0       ffanalytics_3.1.10.0000\n\nloaded via a namespace (and not attached):\n  [1] Rdpack_2.6.4       DBI_1.2.3          mnormt_2.1.1      \n  [4] gridExtra_2.3      httr2_1.2.1        readxl_1.4.5      \n  [7] rlang_1.1.6        magrittr_2.0.3     furrr_0.3.1       \n [10] compiler_4.5.1     mgcv_1.9-3         vctrs_0.6.5       \n [13] reshape2_1.4.4     quadprog_1.5-8     rvest_1.0.4       \n [16] pkgconfig_2.0.3    fastmap_1.2.0      backports_1.5.0   \n [19] pbivnorm_0.6.0     promises_1.3.3     rmarkdown_2.29    \n [22] tzdb_0.5.0         ps_1.9.1           nloptr_2.2.1      \n [25] xfun_0.53          cachem_1.1.0       jsonlite_2.0.0    \n [28] later_1.4.3        rrapply_1.2.7      psych_2.5.6       \n [31] parallel_4.5.1     lavaan_0.6-19      cluster_2.1.8.1   \n [34] R6_2.6.1           stringi_1.8.7      RColorBrewer_1.1-3\n [37] parallelly_1.45.1  boot_1.3-31        rpart_4.1.24      \n [40] cellranger_1.1.0   xgboost_1.7.11.1   Rcpp_1.1.0        \n [43] knitr_1.50         base64enc_0.1-3    timechange_0.3.0  \n [46] Matrix_1.7-3       splines_4.5.1      nnet_7.3-20       \n [49] tidyselect_1.2.1   rstudioapi_0.17.1  yaml_2.3.10       \n [52] codetools_0.2-20   websocket_1.4.4    curl_7.0.0        \n [55] processx_3.8.6     listenv_0.9.1      lattice_0.22-7    \n [58] plyr_1.8.9         withr_3.0.2        evaluate_1.0.4    \n [61] foreign_0.8-90     future_1.67.0      xml2_1.4.0        \n [64] pillar_1.11.0      checkmate_2.3.3    stats4_4.5.1      \n [67] reformulas_0.4.1   generics_0.1.4     chromote_0.5.1    \n [70] mix_1.0-13         hms_1.1.3          scales_1.4.0      \n [73] minqa_1.2.8        globals_0.18.0     xtable_1.8-4      \n [76] glue_1.8.0         Hmisc_5.2-3        tools_4.5.1       \n [79] data.table_1.17.8  lme4_1.1-37        gsubfn_0.7        \n [82] mvtnorm_1.3-3      grid_4.5.1         mitools_2.4       \n [85] rbibutils_2.3      colorspace_2.1-1   nlme_3.1-168      \n [88] htmlTable_2.4.3    proto_1.0.0        Formula_1.2-5     \n [91] cli_3.6.5          rappdirs_0.3.3     viridisLite_0.4.2 \n [94] gt_1.0.0           gtable_0.3.6       fastrmodels_2.0.0 \n [97] digest_0.6.37      htmlwidgets_1.6.4  farver_2.1.2      \n[100] memoise_2.0.1      htmltools_0.5.8.1  lifecycle_1.0.4   \n[103] httr_1.4.7         MASS_7.3-65       \n\n\n\n\n\n\nBaldwin, B. (2023). nfl4th: Functions to calculate optimal fourth down decisions in the National Football League. https://doi.org/10.32614/CRAN.package.nfl4th\n\n\nBengtsson, H. (2024). progressr: An inclusive, unifying API for progress updates. https://doi.org/10.32614/CRAN.package.progressr\n\n\nCarl, S., & Baldwin, B. (2024). nflfastR: Functions to efficiently access NFL play by play data. https://doi.org/10.32614/CRAN.package.nflfastR\n\n\nCarl, S., & Sharpe, L. (2025). nflseedR: Functions to efficiently simulate and evaluate NFL seasons. https://doi.org/10.32614/CRAN.package.nflseedR\n\n\nCongelio, B. J. (2023). Introduction to NFL analytics with R. CRC Press. https://bradcongelio.com/nfl-analytics-with-r-book\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nHo, T., & Carl, S. (2024). nflreadr: Download nflverse data. https://doi.org/10.32614/CRAN.package.nflreadr\n\n\nHo, T., & Carl, S. (2025a). Articles. https://nflreadr.nflverse.com/articles/index.html\n\n\nHo, T., & Carl, S. (2025b). Data dictionary - combine. https://nflreadr.nflverse.com/articles/dictionary_combine.html\n\n\nHo, T., & Carl, S. (2025c). Data dictionary - contracts. https://nflreadr.nflverse.com/articles/dictionary_contracts.html\n\n\nHo, T., & Carl, S. (2025d). Data dictionary - depth charts. https://nflreadr.nflverse.com/articles/dictionary_depth_charts.html\n\n\nHo, T., & Carl, S. (2025e). Data dictionary - draft picks. https://nflreadr.nflverse.com/articles/dictionary_draft_picks.html\n\n\nHo, T., & Carl, S. (2025f). Data dictionary - ESPN QBR. https://nflreadr.nflverse.com/articles/dictionary_espn_qbr.html\n\n\nHo, T., & Carl, S. (2025g). Data dictionary - FF opportunity. https://nflreadr.nflverse.com/articles/dictionary_ff_opportunity.html\n\n\nHo, T., & Carl, S. (2025h). Data dictionary - FF player IDs. https://nflreadr.nflverse.com/articles/dictionary_ff_playerids.html\n\n\nHo, T., & Carl, S. (2025i). Data dictionary - FF rankings. https://nflreadr.nflverse.com/articles/dictionary_ff_rankings.html\n\n\nHo, T., & Carl, S. (2025j). Data dictionary - FTN charting. https://nflreadr.nflverse.com/articles/dictionary_ftn_charting.html\n\n\nHo, T., & Carl, S. (2025k). Data dictionary - injuries. https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n\n\nHo, T., & Carl, S. (2025l). Data dictionary - next gen stats. https://nflreadr.nflverse.com/articles/dictionary_nextgen_stats.html\n\n\nHo, T., & Carl, S. (2025m). Data dictionary - participation. https://nflreadr.nflverse.com/articles/dictionary_participation.html\n\n\nHo, T., & Carl, S. (2025n). Data dictionary - PBP. https://nflreadr.nflverse.com/articles/dictionary_pbp.html\n\n\nHo, T., & Carl, S. (2025o). Data dictionary - PFR passing. https://nflreadr.nflverse.com/articles/dictionary_pfr_passing.html\n\n\nHo, T., & Carl, S. (2025p). Data dictionary - player stats. https://nflreadr.nflverse.com/articles/dictionary_player_stats.html\n\n\nHo, T., & Carl, S. (2025q). Data dictionary - player stats defense. https://nflreadr.nflverse.com/articles/dictionary_player_stats_def.html\n\n\nHo, T., & Carl, S. (2025r). Data dictionary - rosters. https://nflreadr.nflverse.com/articles/dictionary_rosters.html\n\n\nHo, T., & Carl, S. (2025s). Data dictionary - schedules. https://nflreadr.nflverse.com/articles/dictionary_schedules.html\n\n\nHo, T., & Carl, S. (2025t). Data dictionary - snap counts. https://nflreadr.nflverse.com/articles/dictionary_snap_counts.html\n\n\nPro Football Reference. (2024). 2024 NFL advanced stats. https://www.pro-football-reference.com/years/2024/advanced.htm\n\n\nPro Football Reference. (2025). About our advanced stats. https://www.pro-football-reference.com/about/advanced_stats.htm\n\n\nSharpe, L. (2020a). NFL data sets. https://github.com/nflverse/nfldata/blob/master/DATASETS.md\n\n\nSharpe, L. (2020b). NFL data sets - draft values. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#draft_values\n\n\nSharpe, L. (2020c). NFL data sets - rosters. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#rosters\n\n\nSharpe, L. (2020d). NFL data sets - standings. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#standings\n\n\nSharpe, L. (2020e). NFL data sets - trades. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#trades\n\n\nSpinu, V., Grolemund, G., & Wickham, H. (2024). lubridate: Make dealing with dates a little easier. https://doi.org/10.32614/CRAN.package.lubridate\n\n\nTungate, A., Andersen, D., & Petersen, I. T. (2025). ffanalytics: Scrape data for fantasy football. https://github.com/FantasyFootballAnalytics/ffanalytics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Download and Process NFL Football Data</span>"
    ]
  },
  {
    "objectID": "data-visualization.html",
    "href": "data-visualization.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1 Getting Started\nThis chapter provides an overview of principles of data visualization and how to implement them using R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationGettingStarted",
    "href": "data-visualization.html#sec-dataVisualizationGettingStarted",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1.1 Load Packages\n\nCodelibrary(\"nflplotR\")\nlibrary(\"plotly\")\nlibrary(\"gghighlight\")\nlibrary(\"ggridges\")\nlibrary(\"ggExtra\")\nlibrary(\"patchwork\")\nlibrary(\"tidyverse\")\n\n\n\n5.1.2 Load Data\n\nCodeload(file = \"./data/nfl_pbp.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationOverview",
    "href": "data-visualization.html#sec-dataVisualizationOverview",
    "title": "5  Data Visualization",
    "section": "\n5.2 Overview",
    "text": "5.2 Overview\nGraphics serve an important way of efficiently communicating findings to diverse audiences. Effective graphics can help people with less technical knowledge better understand your findings. As noted by Tanney (2021), it is important to communicate your work clearly to stakeholders (e.g., coaches, companies, or prospective clients); if you cannot explain your work clearly, you cannot expect a stakeholder to “buy in” to what you are selling.\n\n5.2.1 Principles of Graphic Design\nWhen designing graphics, it is important to understand general principles of graphic design. Adobe Express (2020) describes important principles about graphic design at the following link: https://www.adobe.com/express/learn/blog/8-basic-design-principles-to-help-you-create-better-graphics (archived at https://perma.cc/29P9-NNSK). The important principles include:\n\n\nFocus on alignment.\nUse hierarchy to help focus your design.\nLeverage contrast to accentuate important design elements.\nUse repetition to your advantage.\nConsider proximity when organizing your graphic elements.\nMake sure your designs have balance.\nOptimize color to support your design.\nLeave negative space.\n\n\n\n5.2.2 Principles of Data Visualization\nData visualization involves graphic design in a particular domain—the visualization of data (numeric-derived information). Schwabish (2021) describes five principles in data visualization:\n\n\nShow the data.\nReduce the clutter.\nIntegrate the graphics and text.\nAvoid the spaghetti chart.\nStart with gray.\n\n\n“Showing the data” involves showing the data that matter the most. “Reducing the clutter” involves removing non-data things that obscure the data—for example, extraneous gridlines, tick marks, data markers (e.g., symbols to distinguish between series), and complex shadings (e.g., textured or filled gradients). “Integrating the graphics and text” involves using headline titles, clear and useful labels (instead of legends), and helpful annotations. Headline or newspaper-like titles are titles that are succinct with active phasing and that indicate the take-away message (e.g., “Quarterbacks Threw Fewer Touchdowns in 2024 than in Previous Years”). In terms of labels, Schwabish (2021) advocates to label the data directly instead of using a legend. In terms of helpful annotations, you can provide additional text that helps explain the data (e.g., peaks or valleys, outliers, or other variations that deserve explanation), including how to interpret the chart. “Avoiding the spaghetti chart” means avoiding packed charts with too much information that makes them difficult to interpret. Spaghetti charts are lines with many lines that, make the plot look like a bunch of spaghetti. However, Schwabish (2021) also advocates against using charts of other types that are complicated and difficult to interpret due to too much information, such as complicated maps or bar plots with too many colors, icons, or bars. If there are too many lines or series, Schwabish (2021) advocates breaking it up into multiple charts (i.e., facets, trellis charts, or small multiples). An example of faceted charts is depicted in Figure 5.48. “Starting with gray” refers to the idea of using gray as the default color for most lines/points/bars, so that you can use a color to highlight the lines/points/bars of interest. In addition, as noted by Schwabish (2021), it is important to treat data as objectively as possible and not to present figures in a biased way as to mislead.\nIn his classic book, Tufte (2001) states that effective data visualizations should follow principles of graphical excellence and integrity. He notes that “Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.” (p. 51). That is, data visualizations should seek to maximize the data-to-ink ratio (within reason), and should spend less space on “fluff” (i.e., non-data things that can be erased without losing meaning, such as grid lines, redundancies, etc.). This is consistent with Schwabish’s (2021) principles of showing the data and reducing the clutter. Tufte (2001) describes six principles of graphical integrity:\n\n\nThe representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the numerical quantities represented.\nClear, detailed, and thorough labeling should be used to defeat graphic distortion and ambiguity. Write out explanations of the data on the graphic itself. Label important events in the data.\nShow data variation, not design variation.\nIn time-series displays of money, deflated and standardized units of monetary measurement are nearly always better than nominal units.\nThe number of information-carrying (variable) dimensions depicted should not exceed the number of dimensions in the data.\nGraphics must not quote data out of context.\n\n— Tufte (2001, p. 77)\n\nTufte (2001) also provides recommendations for friendly, accessible graphics, including:\n\nspell words out (rather than using abbreviations)\nhave words run from left to write (including the y-axis title)\ninclude little messages to help explain the data\nplace labels on the graphic so no legend is needed\navoid elaborately encoded shadings, cross-hatching, and colors\navoid “chartjunk”—i.e., unnecessary or distracting elements (e.g., excessive decoration, overly complex graphics, graphical effects, and irrelevant information such as moiré vibration, heavy grids, and self-promoting graphs) that do not improve viewers’ understanding of the data\nif colors are used, use colors that are distinguishable by color-deficient and color-blind viewers (red–green is a common form of color-blindness)\nuse type (i.e., of the text) that is clear, precise, and modest\nuse text that is upper-and-lower case, not all capitals\n\nAn example figure that applies these principles of data visualization is in Figure 5.1.\n\nCodeconfidenceLevel &lt;- .95 # for 95% confidence interval\n\nplayer_stats_seasonal_offense_summary &lt;- player_stats_seasonal %&gt;%\n  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  group_by(position_group) %&gt;%\n  summarise(\n    n = sum(!is.na(fantasyPoints)),\n    mean = mean(fantasyPoints, na.rm = TRUE),\n    sd = sd(fantasyPoints, na.rm = TRUE)\n  ) %&gt;%\n  mutate(se = sd/sqrt(n)) %&gt;%\n  mutate(\n    ci_lower = mean - qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se,\n    ci_upper = mean + qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se,\n    positionLabel = case_match(\n      position_group,\n      \"QB\" ~ \"Quarterback\",\n      \"RB\" ~ \"Running Back\",\n      \"WR\" ~ \"Wide Receiver\",\n      \"TE\" ~ \"Tight End\"\n      )\n  )\n\nggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = positionLabel,\n    y = mean,\n    fill = positionLabel\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  geom_errorbar(\n    aes(\n      ymin = ci_lower,\n      ymax = ci_upper),\n    width = 0.2,\n    color = \"black\"\n  ) +\n  gghighlight::gghighlight(\n    positionLabel == \"Quarterback\",\n    label_key = positionLabel) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Quarterbacks Score More Fantasy Points than Other Positions\"\n  ) +\n  annotate(\n    \"segment\",\n    x = 3.5,\n    xend = 3.2,\n    y = 70,\n    yend = 35,\n    color = \"blue\",\n    linewidth = 1.5,\n    alpha = 0.6,\n    arrow = arrow()) +\n  annotate(\n    \"text\",\n    x = 2.75,\n    y = 75,\n    label = \"Tight Ends score fewer fantasy\\npoints than other positions\",\n    hjust = 0) + # left-justify\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.1: Example Figure that Applies the Principles of Data Visualization.\n\n\n\n\n\n5.2.3 Creating Data Visualizations in R\n\nThe Data Visualization Catalogue, from Data to Viz, and Data Viz Project provide examples of various types of plots depending on one’s goal: https://datavizcatalogue.com/search.html, https://www.data-to-viz.com, and https://datavizproject.com. The R Graph Gallery provides examples of various types of plots and how to create them in R: https://r-graph-gallery.com. Books on data visualization in R include ggplot2: Elegant Graphics for Data Analysis (Wickham, 2024) and R Graphics Cookbook: Practical Recipes for Visualizing Data (Chang, 2018). In this chapter, we will examine how to create statistical graphics to visualize data. We will create the plots using the ggplot2 package (Wickham et al., 2024; Wickham, 2024). When creating plots in ggplot2 with multiple points or lines (e.g., multiple players or levels of a predictor variable), it is easiest to do so with the data in long form (as opposed to wide form).\nA key principle of graphic design and data visualization is the importance of contrast. Each visual component (e.g., line) that is important to see should be easy to distinguish. For instance, you can highlight lines or points of interest to draw people’s attention to the target of interest (Schwabish, 2021). For examples of highlighting in figures, see Figures 5.44 (Section 5.6.1) and 14.6.\nIt is also important to use color schemes with distinguishable colors. Good color schemes for sequential, diverging, and qualitative (i.e., categorical) data are provided by ColorBrewer (https://colorbrewer2.org) and are available using the ggplot2::scale_color_brewer() and ggplot2::scale_fill_brewer() functions of the ggplot2 package (Wickham et al., 2024), as demonstrated in Figure 5.47 (Section 5.7.2). ColorBrewer allows users to select colorblind safe, print friendly, and photocopy safe palettes. Good colorblind-friendly color schemes for binned (_b), continuous (_c), and discrete (i.e., categorical; _d) scales are available from the viridis palette using the ggplot2::scale_color_viridis_X() and ggplot2::scale_fill_viridis_X() functions of the ggplot2 package (Wickham et al., 2024) (where X is b, c, or d, depending on whether you want a binned, continuous, discrete scale). There are a variety of resources for color schemes that are accessible to color-blind viewers:\n\n\nhttps://www.datylon.com/blog/data-visualization-for-colorblind-readers [Kilin (2022); archived at https://perma.cc/7VTA-Y8YS]\nthe viridis package (Garnier, 2024; Garnier et al., 2024)\n\n\nhttps://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html (archived at https://perma.cc/NK9K-HL7L)\n\n\n\nOkabe-Ito color palette [Okabe & Ito (2008); archived at https://perma.cc/VD2E-XZZU]\nNuñez et al. (2018)\n\nThe Okabe-Ito color palette (Okabe & Ito, 2008) can be specified with the following colors:\n\nCodepalette_OkabeIto &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#999999\")\n\npalette_OkabeIto_black &lt;- c(\n  \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n  \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#000000\")\n\n\nThe Okabe-Ito color palette (Okabe & Ito, 2008) is depicted in Figures 5.2 and 5.3. You can see a pie chart of colors using the graphics::pie() function:\n\nCodepie(\n  rep(1, 8),\n  col = palette_OkabeIto)\n\n\n\n\n\n\nFigure 5.2: Okabe-Ito Color Palette With Gray.\n\n\n\n\n\nCodepie(\n  rep(1, 8),\n  col = palette_OkabeIto_black)\n\n\n\n\n\n\nFigure 5.3: Okabe-Ito Color Palette With Black.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-ggplot2Syntax",
    "href": "data-visualization.html#sec-ggplot2Syntax",
    "title": "5  Data Visualization",
    "section": "\n5.3 ggplot2 Syntax",
    "text": "5.3 ggplot2 Syntax\n\n5.3.1 General Syntax\nThe ggplot2 package (Wickham et al., 2024) is based on the grammar of graphics (Wickham, 2024), which is the principle that you can build any graph from three components: 1) the data, 2) the coordinate system (and aesthetic attributes), and 3) the geometric objects (“geoms”)—visual marks that represent data points (e.g., points, lines, and bars). The general syntax for plots created using the ggplot2 package (Wickham et al., 2024) is:\n\nCodeggplot2::ggplot(\n  data = INSERT, # 1) data\n  mapping = aes(\n    x = variableName1, # name of variable on x-axis\n    y = variableName2) # name of variable on y-axis\n) +\n  coord_cartesian( # 2) coordinate system\n    xlim = c(NA, NA), # set limits (min and max) of x-axis\n    ylim = c(NA, NA)) + # set limits (min and max) of y-axis\n  INSERT_GEOM_FUNCTIONS # 3) geom(s)\n\n\nThe ggplot2::coord_cartesian() function can be used to specify the axis limits; however, this function is not required and the plot will apply sensible limits by default. In addition, there are optional settings you can specify or modify, such as axis titles, axis labels, faceting, and the theme.\nHere is a template including these potential modifications:\n\nCodeggplot2::ggplot(\n  data = INSERT, # 1) data\n  mapping = aes(\n    x = variableName1, # name of variable on x-axis\n    y = variableName2) # name of variable on y-axis\n) +\n  coord_cartesian( # 2) coordinate system\n    xlim = c(NA, NA), # set limits (min and max) of x-axis\n    ylim = c(NA, NA)) + # set limits (min and max) of y-axis\n  INSERT_GEOM_FUNCTIONS + # 3) geom(s)\n  scale_x_continuous(\n    breaks = INSERT # specify x-axis labels\n  ) +\n  scale_y_continuous(\n    breaks = INSERT # specify y-axis labels\n  ) +\n  labs( # add plot labels\n    x = \"INSERT\", # x-axis title\n    y = \"INSERT\", # y-axis title\n    title = \"INSERT\", # plot title\n    subtitle = \"INSERT\", # plot subtitle\n    caption = \"Add a caption below plot\", # plot caption\n    alt = \"Add alt text to the plot\" # plot alt-text\n  ) +\n  INSERT_THEME_NAME + # apply theme\n  facet_wrap(vars(INSERT_VARIABLES_TO_FACET)) # facet by one or more variables\n\n\nYou can layer multiple geoms in the same plot, layer by layer. You can have a different data set and aesthetics associated with each geom, by specifying the data and mapping arguments for the function of that particular geom. For an example of a plot that uses different data sets for different geoms, see Figure 11.32 (Section 11.13.1).\n\n5.3.2 Geometric Objects\nHere is a list of common geometric objects (“geoms”) available in ggplot2 (there are many others):\n\n\nhistogram: ggplot2::geom_histogram()\n\n\ndensity plot: ggplot2::geom_density()\n\n\nbox-and-whisker plot: ggplot2::geom_boxplot()\n\n\nviolin plot: ggplot2::geom_violin()\n\n\nbar plot: ggplot2::geom_bar()\n\n\nscatterplot: ggplot2::geom_point()\n\n\nrug plot: ggplot2::geom_rug()\n\n\nline chart: ggplot2::geom_line()\n\nstatistical summary (e.g., best-fit line): ggplot2::geom_smooth()\n\n\n5.3.3 Color\nIn ggplot2, you can specify the color of a geom using one of two parameters: 1) fill and 2) color. The fill parameter specifies the color inside the geom. The color parameter specifies the outline/border color of the geom.\nThe colors to use in the fill and color parameters can be specified by name (as in Figure 5.4), hexadecimal (HEX) codes (as in Figure 5.5), values in red-green-blue (RGB) space (as in Figure 5.6), or palettes (e.g., ColorBrewer or viridis; as in Figures 5.9 and 5.10).\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_histogram(\n    color = \"black\",\n    fill = \"forestgreen\"\n  )\n\n\n\n\n\n\nFigure 5.4: Specifying the Color/Fill by Name.\n\n\n\n\nYou can see a full list of color names available using the grDevices::colors() function:\n\nCodecolors()\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"        \n  [4] \"antiquewhite1\"        \"antiquewhite2\"        \"antiquewhite3\"       \n  [7] \"antiquewhite4\"        \"aquamarine\"           \"aquamarine1\"         \n [10] \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"         \n [13] \"azure\"                \"azure1\"               \"azure2\"              \n [16] \"azure3\"               \"azure4\"               \"beige\"               \n [19] \"bisque\"               \"bisque1\"              \"bisque2\"             \n [22] \"bisque3\"              \"bisque4\"              \"black\"               \n [25] \"blanchedalmond\"       \"blue\"                 \"blue1\"               \n [28] \"blue2\"                \"blue3\"                \"blue4\"               \n [31] \"blueviolet\"           \"brown\"                \"brown1\"              \n [34] \"brown2\"               \"brown3\"               \"brown4\"              \n [37] \"burlywood\"            \"burlywood1\"           \"burlywood2\"          \n [40] \"burlywood3\"           \"burlywood4\"           \"cadetblue\"           \n [43] \"cadetblue1\"           \"cadetblue2\"           \"cadetblue3\"          \n [46] \"cadetblue4\"           \"chartreuse\"           \"chartreuse1\"         \n [49] \"chartreuse2\"          \"chartreuse3\"          \"chartreuse4\"         \n [52] \"chocolate\"            \"chocolate1\"           \"chocolate2\"          \n [55] \"chocolate3\"           \"chocolate4\"           \"coral\"               \n [58] \"coral1\"               \"coral2\"               \"coral3\"              \n [61] \"coral4\"               \"cornflowerblue\"       \"cornsilk\"            \n [64] \"cornsilk1\"            \"cornsilk2\"            \"cornsilk3\"           \n [67] \"cornsilk4\"            \"cyan\"                 \"cyan1\"               \n [70] \"cyan2\"                \"cyan3\"                \"cyan4\"               \n [73] \"darkblue\"             \"darkcyan\"             \"darkgoldenrod\"       \n [76] \"darkgoldenrod1\"       \"darkgoldenrod2\"       \"darkgoldenrod3\"      \n [79] \"darkgoldenrod4\"       \"darkgray\"             \"darkgreen\"           \n [82] \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [85] \"darkolivegreen\"       \"darkolivegreen1\"      \"darkolivegreen2\"     \n [88] \"darkolivegreen3\"      \"darkolivegreen4\"      \"darkorange\"          \n [91] \"darkorange1\"          \"darkorange2\"          \"darkorange3\"         \n [94] \"darkorange4\"          \"darkorchid\"           \"darkorchid1\"         \n [97] \"darkorchid2\"          \"darkorchid3\"          \"darkorchid4\"         \n[100] \"darkred\"              \"darksalmon\"           \"darkseagreen\"        \n[103] \"darkseagreen1\"        \"darkseagreen2\"        \"darkseagreen3\"       \n[106] \"darkseagreen4\"        \"darkslateblue\"        \"darkslategray\"       \n[109] \"darkslategray1\"       \"darkslategray2\"       \"darkslategray3\"      \n[112] \"darkslategray4\"       \"darkslategrey\"        \"darkturquoise\"       \n[115] \"darkviolet\"           \"deeppink\"             \"deeppink1\"           \n[118] \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[121] \"deepskyblue\"          \"deepskyblue1\"         \"deepskyblue2\"        \n[124] \"deepskyblue3\"         \"deepskyblue4\"         \"dimgray\"             \n[127] \"dimgrey\"              \"dodgerblue\"           \"dodgerblue1\"         \n[130] \"dodgerblue2\"          \"dodgerblue3\"          \"dodgerblue4\"         \n[133] \"firebrick\"            \"firebrick1\"           \"firebrick2\"          \n[136] \"firebrick3\"           \"firebrick4\"           \"floralwhite\"         \n[139] \"forestgreen\"          \"gainsboro\"            \"ghostwhite\"          \n[142] \"gold\"                 \"gold1\"                \"gold2\"               \n[145] \"gold3\"                \"gold4\"                \"goldenrod\"           \n[148] \"goldenrod1\"           \"goldenrod2\"           \"goldenrod3\"          \n[151] \"goldenrod4\"           \"gray\"                 \"gray0\"               \n[154] \"gray1\"                \"gray2\"                \"gray3\"               \n[157] \"gray4\"                \"gray5\"                \"gray6\"               \n[160] \"gray7\"                \"gray8\"                \"gray9\"               \n[163] \"gray10\"               \"gray11\"               \"gray12\"              \n[166] \"gray13\"               \"gray14\"               \"gray15\"              \n[169] \"gray16\"               \"gray17\"               \"gray18\"              \n[172] \"gray19\"               \"gray20\"               \"gray21\"              \n[175] \"gray22\"               \"gray23\"               \"gray24\"              \n[178] \"gray25\"               \"gray26\"               \"gray27\"              \n[181] \"gray28\"               \"gray29\"               \"gray30\"              \n[184] \"gray31\"               \"gray32\"               \"gray33\"              \n[187] \"gray34\"               \"gray35\"               \"gray36\"              \n[190] \"gray37\"               \"gray38\"               \"gray39\"              \n[193] \"gray40\"               \"gray41\"               \"gray42\"              \n[196] \"gray43\"               \"gray44\"               \"gray45\"              \n[199] \"gray46\"               \"gray47\"               \"gray48\"              \n[202] \"gray49\"               \"gray50\"               \"gray51\"              \n[205] \"gray52\"               \"gray53\"               \"gray54\"              \n[208] \"gray55\"               \"gray56\"               \"gray57\"              \n[211] \"gray58\"               \"gray59\"               \"gray60\"              \n[214] \"gray61\"               \"gray62\"               \"gray63\"              \n[217] \"gray64\"               \"gray65\"               \"gray66\"              \n[220] \"gray67\"               \"gray68\"               \"gray69\"              \n[223] \"gray70\"               \"gray71\"               \"gray72\"              \n[226] \"gray73\"               \"gray74\"               \"gray75\"              \n[229] \"gray76\"               \"gray77\"               \"gray78\"              \n[232] \"gray79\"               \"gray80\"               \"gray81\"              \n[235] \"gray82\"               \"gray83\"               \"gray84\"              \n[238] \"gray85\"               \"gray86\"               \"gray87\"              \n[241] \"gray88\"               \"gray89\"               \"gray90\"              \n[244] \"gray91\"               \"gray92\"               \"gray93\"              \n[247] \"gray94\"               \"gray95\"               \"gray96\"              \n[250] \"gray97\"               \"gray98\"               \"gray99\"              \n[253] \"gray100\"              \"green\"                \"green1\"              \n[256] \"green2\"               \"green3\"               \"green4\"              \n[259] \"greenyellow\"          \"grey\"                 \"grey0\"               \n[262] \"grey1\"                \"grey2\"                \"grey3\"               \n[265] \"grey4\"                \"grey5\"                \"grey6\"               \n[268] \"grey7\"                \"grey8\"                \"grey9\"               \n[271] \"grey10\"               \"grey11\"               \"grey12\"              \n[274] \"grey13\"               \"grey14\"               \"grey15\"              \n[277] \"grey16\"               \"grey17\"               \"grey18\"              \n[280] \"grey19\"               \"grey20\"               \"grey21\"              \n[283] \"grey22\"               \"grey23\"               \"grey24\"              \n[286] \"grey25\"               \"grey26\"               \"grey27\"              \n[289] \"grey28\"               \"grey29\"               \"grey30\"              \n[292] \"grey31\"               \"grey32\"               \"grey33\"              \n[295] \"grey34\"               \"grey35\"               \"grey36\"              \n[298] \"grey37\"               \"grey38\"               \"grey39\"              \n[301] \"grey40\"               \"grey41\"               \"grey42\"              \n[304] \"grey43\"               \"grey44\"               \"grey45\"              \n[307] \"grey46\"               \"grey47\"               \"grey48\"              \n[310] \"grey49\"               \"grey50\"               \"grey51\"              \n[313] \"grey52\"               \"grey53\"               \"grey54\"              \n[316] \"grey55\"               \"grey56\"               \"grey57\"              \n[319] \"grey58\"               \"grey59\"               \"grey60\"              \n[322] \"grey61\"               \"grey62\"               \"grey63\"              \n[325] \"grey64\"               \"grey65\"               \"grey66\"              \n[328] \"grey67\"               \"grey68\"               \"grey69\"              \n[331] \"grey70\"               \"grey71\"               \"grey72\"              \n[334] \"grey73\"               \"grey74\"               \"grey75\"              \n[337] \"grey76\"               \"grey77\"               \"grey78\"              \n[340] \"grey79\"               \"grey80\"               \"grey81\"              \n[343] \"grey82\"               \"grey83\"               \"grey84\"              \n[346] \"grey85\"               \"grey86\"               \"grey87\"              \n[349] \"grey88\"               \"grey89\"               \"grey90\"              \n[352] \"grey91\"               \"grey92\"               \"grey93\"              \n[355] \"grey94\"               \"grey95\"               \"grey96\"              \n[358] \"grey97\"               \"grey98\"               \"grey99\"              \n[361] \"grey100\"              \"honeydew\"             \"honeydew1\"           \n[364] \"honeydew2\"            \"honeydew3\"            \"honeydew4\"           \n[367] \"hotpink\"              \"hotpink1\"             \"hotpink2\"            \n[370] \"hotpink3\"             \"hotpink4\"             \"indianred\"           \n[373] \"indianred1\"           \"indianred2\"           \"indianred3\"          \n[376] \"indianred4\"           \"ivory\"                \"ivory1\"              \n[379] \"ivory2\"               \"ivory3\"               \"ivory4\"              \n[382] \"khaki\"                \"khaki1\"               \"khaki2\"              \n[385] \"khaki3\"               \"khaki4\"               \"lavender\"            \n[388] \"lavenderblush\"        \"lavenderblush1\"       \"lavenderblush2\"      \n[391] \"lavenderblush3\"       \"lavenderblush4\"       \"lawngreen\"           \n[394] \"lemonchiffon\"         \"lemonchiffon1\"        \"lemonchiffon2\"       \n[397] \"lemonchiffon3\"        \"lemonchiffon4\"        \"lightblue\"           \n[400] \"lightblue1\"           \"lightblue2\"           \"lightblue3\"          \n[403] \"lightblue4\"           \"lightcoral\"           \"lightcyan\"           \n[406] \"lightcyan1\"           \"lightcyan2\"           \"lightcyan3\"          \n[409] \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"     \n[412] \"lightgoldenrod2\"      \"lightgoldenrod3\"      \"lightgoldenrod4\"     \n[415] \"lightgoldenrodyellow\" \"lightgray\"            \"lightgreen\"          \n[418] \"lightgrey\"            \"lightpink\"            \"lightpink1\"          \n[421] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"          \n[424] \"lightsalmon\"          \"lightsalmon1\"         \"lightsalmon2\"        \n[427] \"lightsalmon3\"         \"lightsalmon4\"         \"lightseagreen\"       \n[430] \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"       \n[433] \"lightskyblue3\"        \"lightskyblue4\"        \"lightslateblue\"      \n[436] \"lightslategray\"       \"lightslategrey\"       \"lightsteelblue\"      \n[439] \"lightsteelblue1\"      \"lightsteelblue2\"      \"lightsteelblue3\"     \n[442] \"lightsteelblue4\"      \"lightyellow\"          \"lightyellow1\"        \n[445] \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"        \n[448] \"limegreen\"            \"linen\"                \"magenta\"             \n[451] \"magenta1\"             \"magenta2\"             \"magenta3\"            \n[454] \"magenta4\"             \"maroon\"               \"maroon1\"             \n[457] \"maroon2\"              \"maroon3\"              \"maroon4\"             \n[460] \"mediumaquamarine\"     \"mediumblue\"           \"mediumorchid\"        \n[463] \"mediumorchid1\"        \"mediumorchid2\"        \"mediumorchid3\"       \n[466] \"mediumorchid4\"        \"mediumpurple\"         \"mediumpurple1\"       \n[469] \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"       \n[472] \"mediumseagreen\"       \"mediumslateblue\"      \"mediumspringgreen\"   \n[475] \"mediumturquoise\"      \"mediumvioletred\"      \"midnightblue\"        \n[478] \"mintcream\"            \"mistyrose\"            \"mistyrose1\"          \n[481] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"          \n[484] \"moccasin\"             \"navajowhite\"          \"navajowhite1\"        \n[487] \"navajowhite2\"         \"navajowhite3\"         \"navajowhite4\"        \n[490] \"navy\"                 \"navyblue\"             \"oldlace\"             \n[493] \"olivedrab\"            \"olivedrab1\"           \"olivedrab2\"          \n[496] \"olivedrab3\"           \"olivedrab4\"           \"orange\"              \n[499] \"orange1\"              \"orange2\"              \"orange3\"             \n[502] \"orange4\"              \"orangered\"            \"orangered1\"          \n[505] \"orangered2\"           \"orangered3\"           \"orangered4\"          \n[508] \"orchid\"               \"orchid1\"              \"orchid2\"             \n[511] \"orchid3\"              \"orchid4\"              \"palegoldenrod\"       \n[514] \"palegreen\"            \"palegreen1\"           \"palegreen2\"          \n[517] \"palegreen3\"           \"palegreen4\"           \"paleturquoise\"       \n[520] \"paleturquoise1\"       \"paleturquoise2\"       \"paleturquoise3\"      \n[523] \"paleturquoise4\"       \"palevioletred\"        \"palevioletred1\"      \n[526] \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"      \n[529] \"papayawhip\"           \"peachpuff\"            \"peachpuff1\"          \n[532] \"peachpuff2\"           \"peachpuff3\"           \"peachpuff4\"          \n[535] \"peru\"                 \"pink\"                 \"pink1\"               \n[538] \"pink2\"                \"pink3\"                \"pink4\"               \n[541] \"plum\"                 \"plum1\"                \"plum2\"               \n[544] \"plum3\"                \"plum4\"                \"powderblue\"          \n[547] \"purple\"               \"purple1\"              \"purple2\"             \n[550] \"purple3\"              \"purple4\"              \"red\"                 \n[553] \"red1\"                 \"red2\"                 \"red3\"                \n[556] \"red4\"                 \"rosybrown\"            \"rosybrown1\"          \n[559] \"rosybrown2\"           \"rosybrown3\"           \"rosybrown4\"          \n[562] \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[565] \"royalblue3\"           \"royalblue4\"           \"saddlebrown\"         \n[568] \"salmon\"               \"salmon1\"              \"salmon2\"             \n[571] \"salmon3\"              \"salmon4\"              \"sandybrown\"          \n[574] \"seagreen\"             \"seagreen1\"            \"seagreen2\"           \n[577] \"seagreen3\"            \"seagreen4\"            \"seashell\"            \n[580] \"seashell1\"            \"seashell2\"            \"seashell3\"           \n[583] \"seashell4\"            \"sienna\"               \"sienna1\"             \n[586] \"sienna2\"              \"sienna3\"              \"sienna4\"             \n[589] \"skyblue\"              \"skyblue1\"             \"skyblue2\"            \n[592] \"skyblue3\"             \"skyblue4\"             \"slateblue\"           \n[595] \"slateblue1\"           \"slateblue2\"           \"slateblue3\"          \n[598] \"slateblue4\"           \"slategray\"            \"slategray1\"          \n[601] \"slategray2\"           \"slategray3\"           \"slategray4\"          \n[604] \"slategrey\"            \"snow\"                 \"snow1\"               \n[607] \"snow2\"                \"snow3\"                \"snow4\"               \n[610] \"springgreen\"          \"springgreen1\"         \"springgreen2\"        \n[613] \"springgreen3\"         \"springgreen4\"         \"steelblue\"           \n[616] \"steelblue1\"           \"steelblue2\"           \"steelblue3\"          \n[619] \"steelblue4\"           \"tan\"                  \"tan1\"                \n[622] \"tan2\"                 \"tan3\"                 \"tan4\"                \n[625] \"thistle\"              \"thistle1\"             \"thistle2\"            \n[628] \"thistle3\"             \"thistle4\"             \"tomato\"              \n[631] \"tomato1\"              \"tomato2\"              \"tomato3\"             \n[634] \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[637] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"          \n[640] \"violet\"               \"violetred\"            \"violetred1\"          \n[643] \"violetred2\"           \"violetred3\"           \"violetred4\"          \n[646] \"wheat\"                \"wheat1\"               \"wheat2\"              \n[649] \"wheat3\"               \"wheat4\"               \"whitesmoke\"          \n[652] \"yellow\"               \"yellow1\"              \"yellow2\"             \n[655] \"yellow3\"              \"yellow4\"              \"yellowgreen\"         \n\n\nHowever, you can use any color by specifying its HEX code, as in Figure 5.5.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    fill = \"#228B22\"\n  )\n\n\n\n\n\n\nFigure 5.5: Specifying the Color/Fill by Hexadecimal Code.\n\n\n\n\nYou can use any color by specifying its values in red-green-blue (RGB) color space using the grDevices::rgb() function, as in Figure 5.6.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    color = rgb(\n      red = 0,\n      green = 0,\n      blue = 0,\n      maxColorValue = 255\n    ),\n    fill = rgb(\n      red = 34,\n      green = 139,\n      blue = 34,\n      maxColorValue = 255\n    )\n  )\n\n\n\n\n\n\nFigure 5.6: Specifying the Color/Fill by Value in Red-Green-Blue (RGB) Space.\n\n\n\n\nWe can allow the colors to differ by group by specifying the fill parameter in the mapping aesthetics, as in Figure 5.7.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_density(\n    color = \"black\",\n    alpha = 0.7\n  )\n\n\n\n\n\n\nFigure 5.7: Specifying Color By Group.\n\n\n\n\nWe can specify multiple colors manually using the ggplot2::scale_fill_manual() function, as in Figure 5.8.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_density(\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"QB\" = \"red\", # or could use hex, rgb, etc.\n      \"RB\" = \"blue\", # or could use hex, rgb, etc.\n      \"WR\" = \"green\", # or could use hex, rgb, etc.\n      \"TE\" = \"yellow\") # or could use hex, rgb, etc.\n    )\n\n\n\n\n\n\nFigure 5.8: Specifying Multiple Colors Manually.\n\n\n\n\nWe can also specify the colors using a palette, such as by the ColorBrewer palette using the ggplot2::scale_fill_brewer() function (as in Figure 5.9) or the viridis palette using the ggplot2::scale_fill_viridis_d() function (as in Figure 5.10).\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_density(\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  scale_fill_brewer(palette = \"Set1\") # see ?scale_fill_brewer for list of available palettes\n\n\n\n\n\n\nFigure 5.9: Specifying Multiple Colors Using the ColorBrewer Palette.\n\n\n\n\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_density(\n    color = \"black\",\n    alpha = 0.7\n  ) +\n  scale_fill_viridis_d(option = \"viridis\") # see ?scale_fill_viridis_d for list of available palettes\n\n\n\n\n\n\nFigure 5.10: Specifying Multiple Colors Using the Viridis Palette.\n\n\n\n\n\n5.3.4 Line Type\nCommon line types include “solid” lines, “dashed” lines, and “dotted” lines. A full list of line types available is provided here: https://ggplot2.tidyverse.org/articles/ggplot2-specs.html (archived at https://perma.cc/N9ED-6LUS). To specify the line type, you can use the linetype parameter, as in Figure 5.11.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_display_name)\n) +\n  geom_line(\n    linetype = \"dashed\"\n  )\n\n\n\n\n\n\nFigure 5.11: Specifying Line Type.\n\n\n\n\nWe can allow the line type to differ by group by specifying the linetype parameter in the mapping aesthetics, as in Figure 5.12.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\",\"Peyton Manning\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    linetype = player_display_name)\n) +\n  geom_line()\n\n\n\n\n\n\nFigure 5.12: Specifying Line Type By Group.\n\n\n\n\nWe can specify the line types manually using the ggplot2::scale_linetype_manual() function as in Figure 5.13.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\",\"Peyton Manning\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_display_name,\n    linetype = player_display_name)\n) +\n  geom_line() +\n  scale_linetype_manual(\n    values = c(\n      \"Tom Brady\" = \"solid\",\n      \"Peyton Manning\" = \"dashed\")\n    )\n\n\n\n\n\n\nFigure 5.13: Specifying Line Type By Group Manually.\n\n\n\n\n\n5.3.5 Line Width\nTo specify the line width, you can use the linewidth parameter, as in Figure 5.14.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_display_name)\n) +\n  geom_line(\n    linewidth = 4\n  )\n\n\n\n\n\n\nFigure 5.14: Specifying Line Width.\n\n\n\n\n\n5.3.6 Point Shape\nCommon point shapes include circles, triangles, squares, etc. A full list of point shapes available is provided here: https://ggplot2.tidyverse.org/articles/ggplot2-specs.html (archived at https://perma.cc/N9ED-6LUS) and is in Figure 5.15.\n\n\n\n\n\nFigure 5.15: Available Point Shapes in ggplot2.\n\n\nTo specify the point shape, you can use the shape parameter, as in Figure 5.16.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_display_name)\n) +\n  geom_point(\n    shape = 2\n  )\n\n\n\n\n\n\nFigure 5.16: Specifying Point Shape.\n\n\n\n\nWe can allow the point shape to differ by group by specifying the shape parameter in the mapping aesthetics, as in Figure 5.17.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\",\"Peyton Manning\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    shape = player_display_name)\n) +\n  geom_point()\n\n\n\n\n\n\nFigure 5.17: Specifying Point Shape By Group.\n\n\n\n\nWe can specify the point shape manually using the ggplot2::scale_shape_manual() function as in Figure 5.18.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\",\"Peyton Manning\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    shape = player_display_name)\n) +\n  geom_point() +\n  scale_shape_manual(\n    values = c(\n      \"Tom Brady\" = 1,\n      \"Peyton Manning\" = 3)\n    )\n\n\n\n\n\n\nFigure 5.18: Specifying Point Shape By Group Manually.\n\n\n\n\n\n5.3.7 Size of Points and Text\nTo specify the size of points and text, you can use the size and base_size parameters, respectively, as in Figure 5.19.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_display_name)\n) +\n  geom_point(\n    size = 2 # size of points\n  ) +\n  theme_classic(\n    base_size = 20 # font size\n  )\n\n\n\n\n\n\nFigure 5.19: Specifying Size of Points and Text.\n\n\n\n\nYou can also specify the font size separately for particular elements, as in Figure 5.20.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name %in% c(\"Tom Brady\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_display_name)\n) +\n  geom_point() +\n  theme(\n    axis.title.x = element_text(size = 30), # font size for x-axis title\n    axis.title.y = element_text(size = 30), # font size for y-axis title\n    axis.text.x  = element_text(size = 12), # font size for x-axis labels\n    axis.text.y  = element_text(size = 12)  # font size for y-axis labels\n  )\n\n\n\n\n\n\nFigure 5.20: Specifying Size of Text Manually for Particular Elements.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-univariateDistribution",
    "href": "data-visualization.html#sec-univariateDistribution",
    "title": "5  Data Visualization",
    "section": "\n5.4 Univariate Distribution",
    "text": "5.4 Univariate Distribution\nScherer (2021) describes various ways of visualizing univariate distributions; see here (archived at https://perma.cc/EEJ8-LND2).\n\n5.4.1 Histogram\nA histogram of fantasy points is depicted in Figure 5.21.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Count\",\n    title = \"Histogram of Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.21: Histogram of Fantasy Points.\n\n\n\n\n\n5.4.2 Density Plot\nA histogram of fantasy points is depicted in Figure 5.22.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    fill = position_group)\n) +\n  geom_density(alpha = 0.7) + # add transparency\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    fill = \"Position\",\n    title = \"Density Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.22: Density Plot of Fantasy Points by Position.\n\n\n\n\n\n5.4.3 Density Plot with Histogram and Rug Plot\nA density plot of fantasy points with a histogram and rug plot is depicted in Figure 5.23.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Fantasy Points with Histogram and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.23: Density Plot with Histogram and Rug Plot.\n\n\n\n\n\n5.4.4 Histogram with Overlaid Density and Rug Plot\nA histogram of fantasy points with an overlaid density and rug plot is depicted in Figure 5.24.\n\nCodebinWidth &lt;- 15\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    fill = \"#0099F8\",\n    binwidth = binWidth\n  ) +\n  geom_density(\n    aes(y = after_stat(count) * binWidth),\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Count\",\n    title = \"Histogram of Fantasy Points with Overlaid Density and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.24: Histogram with Overlaid Density and Rug Plot.\n\n\n\n\n\n5.4.5 Box-and-Whisker Plot\nIn a box-and-whisker plot, the box is created using the 1st and 3rd quartiles (i.e., the 25th and 75th percentiles, respectively). The length of the box is equal to the interquartile range, which is calculated as: \\(\\text{IQR} = Q_3 - Q_1\\), where \\(Q_3\\) and \\(Q_1\\) are the third and first quartiles, respectively. The line in the middle of the box is located at the median (i.e., the 2nd quartile or 50th percentile). The whiskers commonly extend \\(1.5 \\times \\text{IQR}\\) units from the box. That is, the upper whisker is commonly located at \\(1.5 \\times \\text{IQR}\\) units above the third quartile. The lower whisker is commonly located at \\(1.5 \\times \\text{IQR}\\) units below the first quartile. The points represent extreme values (i.e., outliers) that are outside the whiskers.\nA box-and-whisker plot of fantasy points is depicted in Figure 5.25.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = fantasyPoints,\n    fill = position_group)\n) +\n  geom_boxplot(staplewidth = 0.25) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Box-and-Whisker Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.25: Box-and-Whisker Plot.\n\n\n\n\n\n5.4.6 Violin Plot\nA violin plot of fantasy points is depicted in Figure 5.26.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = fantasyPoints,\n    fill = position_group)\n) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Violin Plot of Fantasy Points by Position\",\n    subtitle = \"Lines represent the 25th, 50th, and 75th quantiles\"\n  ) +\n  theme_classic() + \n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.26: Violin Plot. Lines represent the 25th, 50th, and 75th quantiles.\n\n\n\n\n\n5.4.7 Ridgeline Plot\nA ridgeline plot of fantasy points is depicted in Figure 5.27 using the ggridges package (Wilke, 2024).\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_group,\n    group = position_group,\n    fill = position_group)\n) +\n  ggridges::geom_density_ridges(\n    rel_min_height = 0.0085, # remove trailing tails\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Position\",\n    title = \"Ridgeline Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.27: Ridgeline Plot.\n\n\n\n\nWe can add lines at the quartiles, as depicted in Figure 5.28.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_group,\n    group = position_group,\n    fill = factor(after_stat(quantile)))\n) +\n  ggridges::stat_density_ridges(\n    rel_min_height = 0.0085, # remove trailing tails\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = 4,\n    quantile_lines = TRUE\n  ) +\n   scale_fill_viridis_d() + # use viridis color scheme\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Position\",\n    title = \"Ridgeline Plot of Fantasy Points by Position\",\n    subtitle = \"Vertical lines represent the 25th, 50th, and 75th quantiles\",\n    fill = \"Quartile\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.28: Ridgeline Plot. Lines represent the 25th, 50th, and 75th quantiles.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-scatterplot",
    "href": "data-visualization.html#sec-scatterplot",
    "title": "5  Data Visualization",
    "section": "\n5.5 Scatterplot",
    "text": "5.5 Scatterplot\nAs a tutorial, we walk through some of the (many) modifications that can be made to create an advanced, customized plot in ggplot2.\nFirst, we prepare the data:\n\nCode# Subset Data\nrb_seasonal &lt;- player_stats_seasonal %&gt;% \n  filter(position_group == \"RB\")\n\n\n\n5.5.1 Base Layer\nSecond, we create the base layer of the plot using the ggplot2::ggplot() function of the ggplot2 package (Wickham et al., 2024), as in Figure 5.29. We specify the data object and the variables in the data object that are associated with the x- and y-axes:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal, # specify data object\n  aes(\n    x = age, # specify variable on x-axis\n    y = rushing_yards)) # specify variable on y-axis\n\n\n\n\n\n\nFigure 5.29: Base Plot.\n\n\n\n\n\n5.5.2 Add Points\nThird, we create a scatterplot using the ggplot2::geom_point() function from the ggplot2 package (Wickham et al., 2024), as in Figure 5.30:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() # add points for scatterplot\n\n\n\n\n\n\nFigure 5.30: Scatterplot.\n\n\n\n\n\n5.5.3 Best-Fit Line\nFourth, we add a linear best-fit line using the geom_smooth(), as in Figure 5.31:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # add linear best-fit line\n\n\n\n\n\n\nFigure 5.31: Scatterplot with Linear Best-Fit Line.\n\n\n\n\nWe could also estimate a quadratic polynomial best-fit line, as in Figure 5.32:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    formula = y ~ poly(x, 2)) # add quadratic best-fit line\n\n\n\n\n\n\nFigure 5.32: Scatterplot with Quadratic Best-Fit Line.\n\n\n\n\nOr, we could estimate a smooth best-fit line using locally estimated scatterplot smoothing (LOESS) to allow for any form of nonlinearity, as in Figure 5.33:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") # add smooth best-fit (LOESS) line\n\n\n\n\n\n\nFigure 5.33: Scatterplot with Best-Fit Line Using Locally Estimated Scatterplot Smoothing (LOESS).\n\n\n\n\nBy default, the best-fit line is based on a generalized additive model, which allows for nonlinearity, as in Figure 5.34:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() # add GAM best-fit line; same as specifying method = \"gam\"\n\n\n\n\n\n\nFigure 5.34: Scatterplot with Best-Fit Line from Generalized Additive Model.\n\n\n\n\n\n5.5.4 Modify Axes\nThen, we can change the axes, as in Figure 5.35:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40), # set limits of x-axis\n    ylim = c(0,NA), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5) # specify x-axis labels\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250) # specify y-axis labels\n  )\n\n\n\n\n\n\nFigure 5.35: Scatterplot with Modified Axes.\n\n\n\n\n\n5.5.5 Plot Labels\nThen, we can add plot labels, as in Figure 5.36:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs( # add plot labels\n    x = \"Running Back's Age (years)\", # x-axis title\n    y = \"Rushing Yards (Season)\", # y-axis title\n    title = \"NFL Rushing Yards (Season) by Player Age\", # plot title\n    subtitle = \"(Among Running Backs)\" # plot subtitle\n  )\n\n\n\n\n\n\nFigure 5.36: Scatterplot with Plot Labels.\n\n\n\n\n\n5.5.6 Theme\nThen, we can use a theme such as the classic theme (theme_classic()) to make it more visually presentable, as in Figure 5.37:\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  ) +\n  theme_classic() # use the classic theme\n\n\n\n\n\n\nFigure 5.37: Scatterplot with Classic Theme.\n\n\n\n\nOr, we could use a different theme, such as the dark theme (theme_dark()) in Figure 5.38. For a list of themes available in ggplot2, see here: https://ggplot2-book.org/themes#sec-themes (Wickham, 2024).\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  ) +\n  theme_dark() # use the dark theme\n\n\n\n\n\n\nFigure 5.38: Scatterplot with Dark Theme.\n\n\n\n\n\n5.5.7 Interactive\nAfter creating our plot, we can make the plot interactive using the plotly::ggplotly() function from the plotly package (Sievert, 2020; Sievert et al., 2024), as in Figure 5.39.\n\nCodeplot_ypcByPlayerAge &lt;- ggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season)) + # add season for mouse over tooltip\n  geom_smooth() +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(plot_ypcByPlayerAge)\n\n\n\n\n\n\nFigure 5.39: Interactive Scatterplot Using Plotly.\n\n\n\n\n5.5.8 Scatterplot with Rug Plot\nWe can also add a rug plot, as in Figure 5.40.\n\nCodeggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_yards)) +\n  geom_point() +\n  geom_smooth() +\n  geom_rug(\n    color = \"#800000\",\n    alpha = 0.2) +\n  coord_cartesian(\n    xlim = c(20,40),\n    ylim = c(0,NA)) +\n  scale_x_continuous(\n    breaks = seq(from = 20, to = 40, by = 5)\n  ) +\n  scale_y_continuous(\n    breaks = seq(from = 0, to = 2500, by = 250)\n  ) +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards (Season)\",\n    title = \"NFL Rushing Yards (Season) by Player Age\",\n    subtitle = \"(Among Running Backs)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 5.40: Scatterplot With Rug Plot.\n\n\n\n\n\n5.5.9 Marginal Density/Histogram\nWe can also add a marginal density/histogram, as in Figure 5.41 using the ggExtra package (Attali & Baker, 2023):\n\nCodeggExtra::ggMarginal(\n  plot_ypcByPlayerAge,\n  type = \"densigram\")\n\n\n\n\n\n\nFigure 5.41: Scatterplot With Marginal Density/Histogram.\n\n\n\n\nOr we can create the plots manually, as in Figure 5.42 using the patchwork package (Pedersen, 2024):\n\nCodedens1 &lt;- ggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age)) +\n  coord_cartesian(\n    xlim = c(20,40),\n    expand = FALSE) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  theme_void()\n\ndens2 &lt;- ggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = rushing_yards)) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  theme_void() +\n  coord_flip(\n    ylim = c(0, NA),\n    expand = FALSE)\n\ndens1 + patchwork::plot_spacer() + plot_ypcByPlayerAge + dens2 + \n  patchwork::plot_layout(\n    ncol = 2, \n    nrow = 2, \n    widths = c(4, 1),\n    heights = c(1, 4)\n  )\n\n\n\n\n\n\nFigure 5.42: Scatterplot With Marginal Density/Histogram.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-lineChart",
    "href": "data-visualization.html#sec-lineChart",
    "title": "5  Data Visualization",
    "section": "\n5.6 Line Chart",
    "text": "5.6 Line Chart\nA bar plot of Tom Brady’s fantasy points by season is depicted in Figure 5.43.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(player_display_name == \"Tom Brady\"),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints\n    )\n) +\n  geom_line(\n    linewidth = 1.5,\n    color = \"blue\"\n  ) +\n  labs(\n    x = \"Season\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Tom Brady's Fantasy Points by Season\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.43: Line Chart.\n\n\n\n\n\n5.6.1 With Highlighting\nWe use the gghighlight package (Yutani, 2023) to highlight particular elements:\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% \n    filter(position_group %in% c(\"QB\")),\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    group = player_id,\n    color = player_display_name)\n) +\n  geom_line(linewidth = 2) +\n  gghighlight::gghighlight(\n    player_display_name == \"Tom Brady\",\n    label_key = player_display_name,\n    unhighlighted_params = list(linewidth = 0.5)) +\n  labs(\n    x = \"Season\",\n    y = \"Fantasy Points\",\n    title = \"Fantasy Points by Season and Player\",\n    subtitle = \"(Tom Brady in Red)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.44: Line Chart with Highlighting.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-barPlot",
    "href": "data-visualization.html#sec-barPlot",
    "title": "5  Data Visualization",
    "section": "\n5.7 Bar Plot",
    "text": "5.7 Bar Plot\nTo create a bar plot, we first compute summary statistics:\n\nCodeconfidenceLevel &lt;- .95 # for 95% confidence interval\n\nplayer_stats_seasonal_offense_summary &lt;- player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  group_by(position_group) %&gt;%\n  summarise( \n    n = sum(!is.na(fantasyPoints)),\n    mean = mean(fantasyPoints, na.rm = TRUE),\n    sd = sd(fantasyPoints, na.rm = TRUE)\n  ) %&gt;%\n  mutate(se = sd/sqrt(n)) %&gt;%\n  mutate(\n    ci_lower = mean - qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se,\n    ci_upper = mean + qt(p = 1 - (1 - confidenceLevel) / 2, df = n - 1) * se\n  )\n\n\nThe summary statistics are in Table 5.1.\n\nCodeplayer_stats_seasonal_offense_summary\n\n\nTable 5.1: Table of Summary Statistics.\n\n\n\n  \n\n\n\n\n\n\nA bar plot of fantasy points by position is depicted in Figure 5.45.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary,\n  mapping = aes(\n    x = position_group,\n    y = mean,\n    fill = position_group\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.45: Bar Plot.\n\n\n\n\n\n5.7.1 With Error Bars\nBased on the summary statistics in Table 5.1, we create a bar plot with bars representing the 95% confidence interval in Figure 5.46.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = mean,\n    fill = position_group\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  geom_errorbar(\n    aes(\n      ymin = ci_lower,\n      ymax = ci_upper),\n    width = 0.2,\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.46: Bar Plot with Bars Representing the 95% Confidence Interval.\n\n\n\n\n\n5.7.2 Modified Color Scheme\nWe can also modify the color scheme, as in Figure 5.47\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_summary %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = position_group,\n    y = mean,\n    fill = position_group\n    )\n) +\n  geom_bar(\n    stat = \"identity\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  geom_errorbar(\n    aes(\n      ymin = ci_lower,\n      ymax = ci_upper),\n    width = 0.2,\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Position\",\n    y = \"Fantasy Points\",\n    title = \"Bar Plot of Fantasy Points by Position\"\n  ) +\n  theme_classic() + \n  theme(legend.position = \"none\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 5.47: Bar Plot with Bars Representing the 95% Confidence Interval.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-faceting",
    "href": "data-visualization.html#sec-faceting",
    "title": "5  Data Visualization",
    "section": "\n5.8 Faceting",
    "text": "5.8 Faceting\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% \n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  aes(\n    x = age,\n    y = fantasyPoints)) +\n  geom_point() +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  labs(\n    x = \"Player's Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age\"\n  ) +\n  theme_bw() +\n  facet_wrap(vars(position_group)) # facet by position_group\n\n\n\n\n\n\nFigure 5.48: Faceted Scatterplot.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-scatterplotExamples",
    "href": "data-visualization.html#sec-scatterplotExamples",
    "title": "5  Data Visualization",
    "section": "\n5.9 Examples",
    "text": "5.9 Examples\n\n5.9.1 Players\n\n5.9.1.1 Running Back Performance By Player Age\n\nCode# Prepare Data\nrushing_attempts &lt;- nfl_pbp %&gt;% \n  dplyr::filter(season_type == \"REG\") %&gt;% \n  dplyr::filter(\n    rush == 1,\n    rush_attempt == 1,\n    qb_scramble == 0,\n    qb_dropback == 0,\n    !is.na(rushing_yards))\n\nrb_yardsPerCarry &lt;- rushing_attempts %&gt;% \n  dplyr::group_by(rusher_id, season) %&gt;% \n  dplyr::summarise(\n    ypc = mean(rushing_yards, na.rm = TRUE),\n    rush_attempts = n(),\n    .groups = \"drop\") %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(\n    player_stats_seasonal,\n    by = c(\"rusher_id\" = \"player_id\", \"season\")\n  ) %&gt;% \n  dplyr::filter(\n    position_group == \"RB\",\n    rush_attempts &gt;= 50)\n\n\n\n5.9.1.1.1 Rushing Yards Per Carry\nRushing yards per carry over the course of the season is depicted as a function of the Running Back’s age in Figure 5.49.\n\nCodeplot_ypcByPlayerAge2 &lt;- ggplot2::ggplot(\n  data = rb_yardsPerCarry,\n  aes(\n    x = age,\n    y = ypc)) +\n  geom_point(\n    aes(\n      text = player_display_name,\n      label = season)) +\n  geom_smooth() +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards Per Carry (Season)\",\n    title = \"NFL Rushing Yards Per Carry (Season) by Player Age\",\n    subtitle = \"(minimum 50 rushing attempts)\"\n  ) +\n  theme_classic()\n\nggplotly(plot_ypcByPlayerAge2)\n\n\n\n\n\n\nFigure 5.49: Rushing Yards Per Carry (Season) by Player Age.\n\n\n\n\n5.9.1.1.2 Rushing EPA Per Season\nRushing expected points added (EPA) over the course of the season is depicted as a function of the Running Back’s age in Figure 5.50.\n\nCodeplot_rushEPAbyPlayerAge &lt;- ggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_epa)) +\n  geom_point(\n    aes(\n      text = player_display_name,\n      label = season)) +\n  geom_smooth() +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing EPA (Season)\",\n    title = \"NFL Rushing Expected Points Added (Season) by Player Age\"\n  ) +\n  theme_classic()\n\nggplotly(plot_rushEPAbyPlayerAge)\n\n\n\n\n\n\nFigure 5.50: Rushing Expected Points Added (Season) by Player Age.\n\n\n\n\n5.9.2 Teams\n\n5.9.2.1 Defensive and Offensive EPA per Play\nExpected points added (EPA) per play by the team with possession.\n\nCodepbp_regularSeason &lt;- nfl_pbp %&gt;% \n  dplyr::filter(\n    season == 2024,\n    season_type == \"REG\") %&gt;%\n  dplyr::filter(!is.na(posteam) & (rush == 1 | pass == 1))\n\nepa_offense &lt;- pbp_regularSeason %&gt;%\n  dplyr::group_by(team = posteam) %&gt;%\n  dplyr::summarise(off_epa = mean(epa, na.rm = TRUE))\n\nepa_defense &lt;- pbp_regularSeason %&gt;%\n  dplyr::group_by(team = defteam) %&gt;%\n  dplyr::summarise(def_epa = mean(epa, na.rm = TRUE))\n\nepa_combined &lt;- epa_offense %&gt;%\n  dplyr::inner_join(\n    epa_defense,\n    by = \"team\")\n\n\nDefensive EPA per play during the 2024 NFL season is depicted as a function of offensive EPA per play in Figure 5.51 using the nflplotR package (Carl, 2024).\n\nCodeggplot2::ggplot(\n  data = epa_combined,\n  aes(\n    x = off_epa,\n    y = def_epa)) +\n  nflplotR::geom_mean_lines(\n    aes(\n      x0 = off_epa,\n      y0 = def_epa)) +\n  nflplotR::geom_nfl_logos(\n    aes(\n      team_abbr = team),\n      width = 0.065,\n      alpha = 0.7) +\n  labs(\n    x = \"Offense EPA/play\",\n    y = \"Defense EPA/play\",\n    title = \"2024 NFL Offensive and Defensive EPA per Play\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) + # horizontal y-axis title\n  scale_y_reverse()\n\n\n\n\n\n\nFigure 5.51: 2024 NFL Offensive and Defensive EPA Per Play.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationConclusion",
    "href": "data-visualization.html#sec-dataVisualizationConclusion",
    "title": "5  Data Visualization",
    "section": "\n5.10 Conclusion",
    "text": "5.10 Conclusion\nThis chapter provided an overview of principles of data visualization and how to implement them using R. Key principles of data visualization include: ensuring adequate contrast, highlighting key elements of interest (and using gray for other elements), reducing clutter, and adding helpful titles and annotations. We provided examples of data visualizations, including histograms, density plots, rug plots, box-and-whisker plots, violin plots, ridgeline plots, scatterplots, line charts, and bar plots.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationSessionInfo",
    "href": "data-visualization.html#sec-dataVisualizationSessionInfo",
    "title": "5  Data Visualization",
    "section": "\n5.11 Session Info",
    "text": "5.11 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n [9] tidyverse_2.0.0   patchwork_1.3.2   ggExtra_0.10.1    ggridges_0.5.6   \n[13] gghighlight_0.5.0 plotly_4.11.0     ggplot2_3.5.2     nflplotR_1.4.0   \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       xfun_0.53          htmlwidgets_1.6.4  ggrepel_0.9.6     \n [5] lattice_0.22-7     tzdb_0.5.0         vctrs_0.6.5        tools_4.5.1       \n [9] crosstalk_1.2.1    generics_0.1.4     pkgconfig_2.0.3    Matrix_1.7-3      \n[13] data.table_1.17.8  RColorBrewer_1.1-3 gt_1.0.0           lifecycle_1.0.4   \n[17] compiler_4.5.1     farver_2.1.2       httpuv_1.6.16      htmltools_0.5.8.1 \n[21] yaml_2.3.10        lazyeval_0.2.2     later_1.4.3        pillar_1.11.0     \n[25] cachem_1.1.0       magick_2.8.7       nlme_3.1-168       mime_0.13         \n[29] tidyselect_1.2.1   digest_0.6.37      stringi_1.8.7      labeling_0.4.3    \n[33] splines_4.5.1      fastmap_1.2.0      grid_4.5.1         cli_3.6.5         \n[37] magrittr_2.0.3     withr_3.0.2        nflreadr_1.4.1     scales_1.4.0      \n[41] promises_1.3.3     ggpath_1.0.2       timechange_0.3.0   rmarkdown_2.29    \n[45] httr_1.4.7         hms_1.1.3          memoise_2.0.1      shiny_1.11.1      \n[49] evaluate_1.0.4     knitr_1.50         miniUI_0.1.2       viridisLite_0.4.2 \n[53] mgcv_1.9-3         rlang_1.1.6        Rcpp_1.1.0         xtable_1.8-4      \n[57] glue_1.8.0         xml2_1.4.0         jsonlite_2.0.0     R6_2.6.1          \n\n\n\n\n\n\nAdobe Express. (2020). 8 basic design principles to help you make awesome graphics. https://www.adobe.com/express/learn/blog/8-basic-design-principles-to-help-you-create-better-graphics\n\n\nAttali, D., & Baker, C. (2023). ggExtra: Add marginal histograms to ggplot2, and more ggplot2 enhancements. https://doi.org/10.32614/CRAN.package.ggExtra\n\n\nCarl, S. (2024). nflplotR: NFL logo plots in ggplot2 and gt. https://doi.org/10.32614/CRAN.package.nflplotR\n\n\nChang, W. (2018). R graphics cookbook: Practical recipes for visualizing data (2nd ed.). O’Reilly Media. https://r-graphics.org\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGarnier, S. (2024). viridis: Colorblind-friendly color maps for R. https://doi.org/10.32614/CRAN.package.viridis\n\n\nGarnier, S., Ross, N., Rudis, B., Sciaini, M., Camargo, A. P., & Scherer, C. (2024). viridis(Lite) - colorblind-friendly color maps for R. https://doi.org/10.5281/zenodo.4679423\n\n\nKilin, I. (2022). The best charts for color blind viewers. https://www.datylon.com/blog/data-visualization-for-colorblind-readers\n\n\nNuñez, J. R., Anderton, C. R., & Renslow, R. S. (2018). Optimizing colormaps with consideration for color vision deficiency to enable accurate interpretation of scientific data. PLOS ONE, 13(7), e0199239. https://doi.org/10.1371/journal.pone.0199239\n\n\nOkabe, M., & Ito, K. (2008). Color universal design (CUD)- how to make figures and presentations that are friendly to colorblind people. https://jfly.uni-koeln.de/color/\n\n\nPedersen, T. L. (2024). patchwork: The composer of plots. https://doi.org/10.32614/CRAN.package.patchwork\n\n\nScherer, C. (2021). Beyond bar and box plots. https://z3tt.github.io/beyond-bar-and-box-plots\n\n\nSchwabish, J. (2021). Better data visualizations: A guide for scholars, researchers, and wonks. Columbia University Press. https://doi.org/10.7312/schw19310\n\n\nSievert, C. (2020). Interactive web-based data visualization with R, plotly, and shiny. Chapman; Hall/CRC. https://plotly-r.com\n\n\nSievert, C., Parmer, C., Hocking, T., Chamberlain, S., Ram, K., Corvellec, M., & Despouy, P. (2024). plotly: Create interactive web graphics via plotly.js. https://doi.org/10.32614/CRAN.package.plotly\n\n\nTanney, M. (2021). R in sports analytics. https://www.youtube.com/watch?v=1zCDWtNEucI\n\n\nTufte, E. R. (2001). The visual display of quantitative information. Graphics Press.\n\n\nWickham, H. (2024). ggplot2: Elegant graphics for data analysis (3rd ed.). Springer. https://ggplot2-book.org\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., Dunnington, D., & van den Brand, T. (2024). ggplot2: Create elegant data visualisations using the grammar of graphics. https://doi.org/10.32614/CRAN.package.ggplot2\n\n\nWilke, C. O. (2024). ggridges: Ridgeline plots in ggplot2. https://doi.org/10.32614/CRAN.package.ggridges\n\n\nYutani, H. (2023). gghighlight: Highlight lines and points in ggplot2. https://doi.org/10.32614/CRAN.package.gghighlight",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html",
    "href": "player-evaluation.html",
    "title": "6  Player Evaluation",
    "section": "",
    "text": "6.1 Getting Started\nThis chapter provides an overview of some of the many factors in player evaluation. Ultimately, player evaluation involves prediction—predicting how well the player will do in future games.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-playerEvaluationGettingStarted",
    "href": "player-evaluation.html#sec-playerEvaluationGettingStarted",
    "title": "6  Player Evaluation",
    "section": "",
    "text": "6.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#overview",
    "href": "player-evaluation.html#overview",
    "title": "6  Player Evaluation",
    "section": "\n6.2 Overview",
    "text": "6.2 Overview\nEvaluating players for fantasy football could be thought of as similar to the process of evaluating companies when picking stocks to buy. You want to evaluate and compare various assets so that you get the assets with the best value.\nThere are various domains of criteria we can consider when evaluating a football player’s fantasy prospects. Potential domains to consider include:\n\nathletic profile\nhistorical performance\nhealth\nage and career stage\nsituational factors\nmatchups\ncognitive and motivational factors\nfantasy value\n\nThe discussion that follows is based on my and others’ impressions of some of the characteristics that may be valuable to consider when evaluating players. However, the extent to which any factor is actually relevant for predicting future performance is an empirical question and should be evaluated empirically.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAthleticProfile",
    "href": "player-evaluation.html#sec-evalAthleticProfile",
    "title": "6  Player Evaluation",
    "section": "\n6.3 Athletic Profile",
    "text": "6.3 Athletic Profile\nFactors related to a player’s athletic profile include factors such as:\n\nbody shape\n\nheight\nweight\nhand size\nwing span (arm length)\n\n\nbody function\n\nagility\nstrength\nspeed\nacceleration/explosiveness\njumping ability\n\n\n\nIn terms of body shape, we might consider a player’s height, weight, hand size, and wing span (arm length). Height allows players to see over opponents and to reach balls higher in the air. Thus, greater height is particularly valuable for Quarterbacks and Wide Receivers. Heavier players are tougher to budge and to tackle. Greater weight is particularly valuable for Linemen, Fullbacks, and Tight Ends, but it can also be valuable—to a deree—for Quarterbacks, Running Backs, and Wide Receivers. Hand size and wing span is particularly valuable for people catching the ball; thus, a larger hand size and longer wing span are particularly valuable for Wide Receivers and Tight Ends.\nIn terms of body function, we can consider a player’s agility, strength, speed, acceleration/explosiveness, and jumping ability. For Wide Receivers, speed, explosiveness, and jumping ability are particularly valuable. For Running Backs, agility, strength, speed, and explosiveness are particularly valuable.\nMany aspects of a player’s athletic profile, including tests of speed (40-yard dash), strength (bench press), agility (20-yard shuttle run; three cone drill), and jumping ability (vertical jump; broad jump) are available from the National Football League (NFL) Combine, which is especially relevant for evaluating rookies. We demonstrate how to import data from the NFL Combine in Section 4.3.8. There are also calculators that integrate information about body shape and information from the NFL Combine to determine a player’s relative athletic score (RAS) for their position: https://ras.football/ras-calculator/ and https://www.nfl.com/combine/iq/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-skill",
    "href": "player-evaluation.html#sec-skill",
    "title": "6  Player Evaluation",
    "section": "\n6.4 Skill",
    "text": "6.4 Skill\nWhen scouting players, scouts consider not only the player’s athletic profile, but also their position-relevant skill. For instance, how good are they are reading the defense, passing the ball, running routes, catching balls, making defenders miss tackles, taking care of the ball, consistency, etc. Scouting and evaluating skill is a complicated endeavor, and even the professional scouts frequently make mistakes in their evaluations and predictions. You can certainly read skill evaluations about various players; however, unlike metrics of athletic profile, we do not have direct access to the player’s underlying skill. Some may say, “You know it when you see it.” But, this is not particularly useful when trying to identify players who are undervalued or overvalued—because the skill evaluations are likely already “baked into” a player’s projections. Because we do not have direct access to a player’s skill, we tend to rely on indirect metrics of their ability, such as historical performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalHistoricalPerformance",
    "href": "player-evaluation.html#sec-evalHistoricalPerformance",
    "title": "6  Player Evaluation",
    "section": "\n6.5 Historical Performance",
    "text": "6.5 Historical Performance\n\n6.5.1 Overview\n\n“The best predictor of future behavior is past behavior.” – Unknown\n\n\n“Past performance does not guarantee future results.” – A common disclaimer about investments.\n\nFactors relating to historical performance to consider could include:\n\nperformance in college\n\ndraft position\n\n\nperformance in the NFL\nefficiency\nconsistency\n\nIt is important to consider a player’s past performance. However, the extent to which historical performance may predict future performance may depend on many factors such as (a) the similarity of the prior situation to the current situation, (b) how long ago the prior situation was, and (c) the extent to which the player (or situation) has changed in the interim. For rookies, the player does not have prior seasons of performance in the NFL to draw upon. Thus, when evaluating rookies, it can be helpful to consider their performance in college or in their prior leagues. However, there are large differences between the situation in college and the situation in the NFL, so prior success in college may not portend future success in the NFL. An indicator that intends to be prognostic of future performance, and that accounts for past performance, is a player’s draft position—that is, how early (or late) was a player selected in the NFL Draft. The earlier a player was selected in the NFL Draft, the greater likelihood that the player will perform well; however, this is somewhat countered by the fact that the teams with the highest draft picks tend to be the worst based on the prior season’s record.\nFor players who have played in the NFL, past performance becomes more relevant because, presumably, the prior situation is more similar (than was their situation in college) to their current situation. Nevertheless, as described below, lots of things change from game to game and season to season, and such changes are important to monitor because they can render prior situations less relevant to the player’s current situation. Thus, it is important not to rely just on a player’s historical performance from last season. Nevertheless, historical performance is one of the best indicators we have.\nWe demonstrate how to import historical player statistics in Section 4.3.16. We demonstrate how to calculate historical player statistics in Section 4.4.1. We demonstrate how to calculate historical fantasy points in Section 4.4.2.\n\n6.5.1.1 Changes in Situational Factors\nLots of things change from game to game and season to season: injuries, coaches, coaching strategies, teammates, etc. Just because a player performed well or poorly in a given game or season does not necessarily mean that they will perform similarly in subsequent games/seasons. Thus, it is crucial to consider the player’s current situation—and what has changed since the prior game or season. Consider what has changed in terms of situational factors:\n\nIs the player on a new team?\nDo they have a new head coach or coordinator?\nHow will the team’s offensive scheme change?\nDoes the team have a better offensive line?\nDoes the team have better receiving targets?\nWill the player’s position change on the depth chart?\nWill there be more competition for targets?\nIs there a greater level of competition by opposing teams?\n\nThere is often greater uncertainty in selecting a player who has changed teams or whose situation has greatly changed, but such risk can be highly rewarded when the player’s new team or situation is better suited to the player. If a player underachieved on a given team, they may perform better for another team.\n\n6.5.2 Efficiency\nIn addition to how many fantasy points a player scores in terms of historical performance, we also care about efficiency and consistency. How efficient were they given the number of opportunities they had? This is important to consider because different players have different opportunities. For example, some Running Backs have more opportunity than others (e.g., more carries), so comparing players merely on rushing yards would be misleading. If they were relatively more efficient, they will likely score more points than many of their peers when given more opportunities. If they were relatively inefficient, their capacity to score fantasy points may be more dependent on touches/opportunities. Efficiency might be operationalized by indicators such as yards per passing attempt, yards per rushing attempt, yards per target, yards per reception, yards per route run, etc.\n\n6.5.3 Consistency\nIn terms of consistency, how consistent was the player from game to game and from season to season? For instance, we could examine the standard deviations of players’ fantasy points across games in a given season. However, the standard deviation tends to be upwardly biased as the mean increases. So, we can account for the player’s mean fantasy points per game by dividing their game-to-game standard deviation of fantasy points (\\(s_x\\) or \\(\\sigma_x\\)) by their mean fantasy points across games (\\(\\bar{x}\\) or \\(\\mu_x\\)). This is known as the coefficient of variation (CV), which is provided in Equation 6.1.\n\\[\n\\text{CV} = \\frac{s_x}{\\bar{x}}\n\\tag{6.1}\\]\nPlayers with a lower standard deviation and a lower coefficient of variation (of fantasy points across games) are more consistent. In the example below, Player 2 might be preferable to Player 1 because Player 2 is more consistent; Player 1 is more “boom-or-bust.” Despite showing a similar mean of fantasy points across weeks, Player 2 shows a smaller week-to-week standard deviation and coefficient of variation.\n\nCodeset.seed(1)\n\nplayerScoresByWeek &lt;- data.frame(\n  player1_scores = rnorm(17, mean = 20, sd = 7),\n  player2_scores = rnorm(17, mean = 20, sd = 4),\n  player3_scores = rnorm(17, mean = 10, sd = 4),\n  player4_scores = rnorm(17, mean = 10, sd = 1)\n)\n\nconsistencyData &lt;- data.frame(t(playerScoresByWeek))\n\nweekNames &lt;- paste(\"week\", 1:17, sep = \"\")\n\nnames(consistencyData) &lt;- weekNames\nrow.names(consistencyData) &lt;- NULL\n\nconsistencyData$mean &lt;- rowMeans(consistencyData[,weekNames])\nconsistencyData$sd &lt;- apply(consistencyData, 1, sd)\nconsistencyData$cv &lt;- consistencyData$sd / consistencyData$mean\n\nconsistencyData$player &lt;- c(1, 2, 3, 4)\n\nconsistencyData &lt;- consistencyData %&gt;% \n  select(player, mean, sd, cv, week1:week17)\n\nround(consistencyData, 2)\n\n\n  \n\n\n\nHowever, another perspective is to embrace the chaos and uncertainty that is part of fantasy football; one is better positioned to win a given week if you have one or a few players that “go off”—i.e., that score lots of points. Thus, starting boom-or-bust players can make it more likely that you win in the “boom weeks”; however, this may also mean having some “bust weeks”.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalHealth",
    "href": "player-evaluation.html#sec-evalHealth",
    "title": "6  Player Evaluation",
    "section": "\n6.6 Health",
    "text": "6.6 Health\nHealth-related factors to consider include:\n\ncurrent injury status\ninjury history\n\nIt is also important to consider a player’s past and current health status. In terms of a player’s current health status, it is important to consider whether they are injured or are playing at less than 100% of their typical health. In terms of a player’s prior health status, one can consider their injury history, including the frequency and severity of injuries and their prognosis.\nWe demonstrate how to import injury reports in Section 4.3.17.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAvailability",
    "href": "player-evaluation.html#sec-evalAvailability",
    "title": "6  Player Evaluation",
    "section": "\n6.7 Availability",
    "text": "6.7 Availability\nIn addition to injuries, other factors can affect a player’s availability, including contract hold-outs or league suspensions, for instance, due to violating the league’s conduct policy. Some players may be “holding out”, which means that they may refuse to play until they sign a more desirable contract with the team (e.g., for more money or for a longer-term contract).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAgeCareerStage",
    "href": "player-evaluation.html#sec-evalAgeCareerStage",
    "title": "6  Player Evaluation",
    "section": "\n6.8 Age and Career Stage",
    "text": "6.8 Age and Career Stage\nAge and career stage-related factors include:\n\nage\nexperience\ntouches\n\nA player’s age is relevant because of important age-related changes in a player’s speed, ability to recover from injury, etc. A player’s experience is relevant because players develop knowledge and skills with greater experience. A player’s prior touches/usage is also relevant, because it speaks to how many hits a player may have taken. For players who take more hits, it may be more likely that their bodies “break down” sooner.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalSituation",
    "href": "player-evaluation.html#sec-evalSituation",
    "title": "6  Player Evaluation",
    "section": "\n6.9 Situational Factors",
    "text": "6.9 Situational Factors\nSituational factors one could consider include:\n\nteam quality\n\nquality of the team’s offense\nquality of the team’s defense\n\n\nrole on team\nteammates\nopportunity and usage\n\nsnap count\ntouches/targets\nred zone usage\n\n\n\nFootball is a team sport. A player is embedded within a broader team context; it is important to consider the strength of their team context insofar as it may support— or detract from—a player’s performance. For instance, for a Quarterback, it is important to consider how strong the pass blocking is from the Offensive Line. Will they have enough time to throw the ball, or will they be constantly under pressure to be sacked? It is also important to consider the strength of the pass catchers—the Wide Receivers and Tight Ends. For a Running Back, it is important to consider how strong the run blocking is from the Offensive Line. For a Wide Receiver, it is important to consider how strong the pass blocking is, and how strong the Quarterback is. In general, offensive players benefit from being on stronger offenses, so it can make sense to draft players (onto your fantasy team) that are on strong offenses. In addition, offensive players can also benefit from being on teams with weaker defenses because that may force the offense to score more points.\nIt is also important to consider a player’s role on the team. Is the player a starter or a backup? Related to this, it is important to consider the strength of one’s teammates. For a given Running Back, if a teammate is better at running the ball, this may take away from how much the player sees the field. For a given Wide Receiver, if a teammate is better at catching the ball, this may take some targets away from the player. However, the team’s top defensive back is often matched up against the team’s top Wide Receiver. So, if the team’s top Wide Receiver is matched up against a particularly strong Defensive Back, the second- and third-best Wide Receivers may more targets than usual.\nIt is also important to consider a player’s opportunity, usage, and volume, which are influenced by many factors, including the skill of the player, the skill of their teammates, the role of the player on the team, the coaching style, the strategy of the opposing team, game scripts, etc. In terms of the player’s opportunity and usage, how many snaps do they get? How many routes do they run? How many touches and/or targets do they receive? Being on the field for more snaps, running more routes, and receiving more touches and/or targets means that the player has more opportunities to score fantasy points. What is the depth of their targets? Are they receiving short passes or long passes? Players who receive longer passes may have an opportunity for more fantasy points but also may be more volatile (i.e., less consistent). Are they targeted in the red zone? Red zone targets are more likely to lead to touchdown scoring opportunities, which are particularly valuable in fantasy football.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalMatchups",
    "href": "player-evaluation.html#sec-evalMatchups",
    "title": "6  Player Evaluation",
    "section": "\n6.10 Matchups",
    "text": "6.10 Matchups\nMatchup-related factors to consider include:\n\nstrength of schedule\nweekly matchup\n\nAnother aspect to consider is how challenging their matchup(s) and strength of schedule is. For a Quarterback, it is valuable to consider how strong the opponent’s passing defense is. For a Running Back, how strong is the running defense? For a Wide Receiver, how strong is the passing defense and the Defensive Back that is likely to be assigned to guard them?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalCogMotivational",
    "href": "player-evaluation.html#sec-evalCogMotivational",
    "title": "6  Player Evaluation",
    "section": "\n6.11 Cognitive and Motivational Factors",
    "text": "6.11 Cognitive and Motivational Factors\nOther factors to consider include cognitive and motivational factors. Some coaches refer to these as the “X Factor” or “the intangibles.” However, just as any other construct in psychology, we can devise ways to operationalize them. Insofar as they are observable, they are measurable.\nCognitive and motivational factors one could consider include:\n\nreaction time\nknowledge and intelligence\nwork ethic and mental toughness\nincentives\n\ncontract performance incentives\nwhether they are in a contract year\n\n\n\nA player’s knowledge, intelligence, and reaction time can help them gain an upper-hand even when they may not be the fastest or strongest. The Wonderlic test was formerly used to assess a player’s cognitive abilities [O’Connell (2022); archived at https://web.archive.org/web/20220303015018/https://www.nytimes.com/2022/03/02/sports/football/nfl-wonderlic-test.html]. A player’s work ethic and mental toughness may help them be resilient and persevere in the face of challenges. Contact-related incentives may lead a player to put forth greater effort. For instance, a contract may have a performance incentive that provides a player greater compensation if they achieve a particular performance milestone (e.g., receiving yards). Another potential incentive is if a player is in what is called their “contract year” (i.e., the last year of their current contract). If a player is in the last year of their current contract, they have an incentive to perform well so they can get re-signed to a new contract.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-fantasyValue",
    "href": "player-evaluation.html#sec-fantasyValue",
    "title": "6  Player Evaluation",
    "section": "\n6.12 Fantasy Value",
    "text": "6.12 Fantasy Value\n\n6.12.1 Sources From Which to Evaluate Fantasy Value\nThere are several sources that one can draw upon to evaluate a player’s fantasy value:\n\nexpert or aggregated rankings\nlayperson rankings\n\nplayers’ Average Draft Position (ADP) in other league snake drafts\n\nplayers’ Average Auction Value (AAV) in other league auction drafts\n\n\n\nexpert or aggregated projections\n\n\n6.12.1.1 Expert Fantasy Rankings\nFantasy rankings (by so-called “experts”) are provided by many sources. To reduce some of the bias due to a given source, some services aggregate projections across sources, consistent with a “wisdom of the crowd” approach. FantasyPros aggregates fantasy rankings across sources. Fantasy Football Analytics creates fantasy rankings from projections that are aggregated across sources (see the webapp here: https://apps.fantasyfootballanalytics.net). There are also sites that aggregate rankings from both fantasy football analysts and from sportsbooks, which are based on Vegas lines and how people bet: https://www.evsharps.com/ranks?format=std.\n\n6.12.1.2 Layperson Fantasy Rankings: ADP and AAV\nAverage Draft Position (ADP) and Average Auction Value (AAV), are based on league drafts, mostly composed of everyday people. ADP is based on snake drafts, whereas AAV is based on auction drafts. Thus, ADP and AAV are consistent with a “wisdom of the crowd” approach, and I refer to them as forms of rankings by laypeople. ADP data are provided by FantasyPros. AAV data are also provided by FantasyPros.\n\n6.12.1.3 Projections\nProjections are provided by various sources. Projections (and rankings, for that matter) are a bit of a black box. It is often unclear how they were derived by a particular source. That is, it is unclear how much of the projection was based on statistical analysis versus conjecture.\nTo reduce some of the bias due to a given source, some services aggregate projections across sources, consistent with a “wisdom of the crowd” approach. Projections that are aggregated across sources are provided by Fantasy Football Analytics (see the webapp here: https://apps.fantasyfootballanalytics.net) and by FantasyPros. There are also sites that aggregate projections from both fantasy football analysts and from sportsbooks, which are based on Vegas lines and how people bet: https://www.evsharps.com/ranks?format=std. Moreover, sites also provide season-long player futures for the over/under betting lines: https://www.rotowire.com/betting/nfl/player-futures.php; https://www.evsharps.com/futures. In addition, sites also provide weekly projections based on Vegas lines: https://vegasprojections.com; https://www.actionnetwork.com/nfl/props; https://tools.32beatwriters.com; https://the-odds-api.com.\nWe demonstrate how to download fantasy football projections in Section 4.3.26.\n\n6.12.1.4 Benefits of Using Projections Rather than Rankings\nIt is important to keep in mind that rankings, ADP, and AAV are specific to roster and scoring settings of a particular league. For instance, in point-per-reception (PPR) leagues, players who catch lots of passes (Wide Receivers, Tight Ends, and some Running Backs) are valued more highly. As another example, Quarterbacks are valued more highly in 2-Quarterback leagues. Thus, if using rankings, ADP, or AAV, it is important to find ones from leagues that mirror—as closely as possible—your league settings.\nProjected statistics (e.g., projected passing touchdowns) are agnostic to league settings and can thus be used to generate league-specific fantasy projections and rankings. Thus, projected statistics may be more useful than rankings because they can be used to generate rankings for your particular league settings. For instance, if you know how many touchdowns, yards, and interceptions a Quarterback is a projected to throw (in addition to any other relevant categories for the player, e.g., rushing yards and touchdowns), you can calculate how many fantasy points the Quarterback is expected to gain in your league (or in any league). Thus, you can calculate ranking from projections, but you cannot reverse engineer projections from rankings. Moreover, projections have been shown to be more accurate than rankings [Petersen (2016); archived at https://perma.cc/KEZ9-WRB6].\n\n6.12.2 Indices to Evaluate Fantasy Value\nBased on the sources above (rankings, ADP, AAV, and projections), we can derive multiple indices to evaluate fantasy value. There are many potential indices that can be worthwhile to consider, including a player’s:\n\ndropoff\nvalue over replacement player (VORP)\nuncertainty\n\n\n6.12.2.1 Dropoff\nA player’s dropoff is the difference between (a) the player’s projected points and (b) the projected points of the next-best player at that position.\n\n6.12.2.2 Value Over Replacement Player\nBecause players from some positions (e.g., Quarterbacks) tend to score more points than players from other positions (e.g., Wide Receivers), it would be inadvisable to compare players across different positions based on projected points. In order to more fairly compare players across positions, we can consider a player’s value over a typical replacement player at that position (shortened to “value over replacement player”). A player’s value over a replacement player (VORP) is the difference between (a) a player’s projected fantasy points and (b) the fantasy points that you would be expected to get from a typical bench player at that position. Thus, VORP provides an index of how much added value a player provides.\n\n6.12.2.3 Uncertainty\nA player’s uncertainty is how much variability there is in projections or rankings for a given player across sources. For instance, consider a scenario where three experts provide ratings about two players, Player A and Player B. Player A is projected to score 300, 310, and 290 points by experts 1, 2, and 3, respectively. Player B is projected to score 400, 300, and 200 points by experts 1, 2, and 3, respectively. In this case, both players are (on average) projected to score the same number of points (300).\n\nCodeexampleData &lt;- data.frame(\n  player = c(rep(\"A\", 3), rep(\"B\", 3)),\n  expert = c(1:3, 1:3),\n  projectedPoints = c(300, 310, 290, 400, 300, 200)\n)\n\nplayerA_mean &lt;- mean(exampleData$projectedPoints[which(exampleData$player == \"A\")])\nplayerB_mean &lt;- mean(exampleData$projectedPoints[which(exampleData$player == \"B\")])\n\nplayerA_sd &lt;- sd(exampleData$projectedPoints[which(exampleData$player == \"A\")])\nplayerB_sd &lt;- sd(exampleData$projectedPoints[which(exampleData$player == \"B\")])\n\nplayerA_cv &lt;- playerA_mean / playerA_sd\nplayerB_cv &lt;- playerB_mean / playerB_sd\n\n\n\nCodeplayerA_mean\n\n[1] 300\n\nCodeplayerB_mean\n\n[1] 300\n\n\nHowever, the players differ considerably in their uncertainty (i.e., the source-to-source variability in their projections), as operationalized with the standard deviation and coefficient of variation of projected points across sources for a given player.\n\nCodeplayerA_sd\n\n[1] 10\n\nCodeplayerB_sd\n\n[1] 100\n\nCodeplayerA_cv\n\n[1] 30\n\nCodeplayerB_cv\n\n[1] 3\n\n\nHere is a depiction of a density plot of projected points for a player with a low, medium, and high uncertainty:\n\nCodeplayerA &lt;- rnorm(1000000, mean = 150, sd = 5)\nplayerB &lt;- rnorm(1000000, mean = 150, sd = 15)\nplayerC &lt;- rnorm(1000000, mean = 150, sd = 30)\n\nmydata &lt;- data.frame(playerA, playerB, playerC)\n\nmydata_long &lt;- mydata %&gt;% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"player\",\n    values_to = \"points\"\n  ) %&gt;% \n  mutate(\n    name = case_match(\n      player,\n      \"playerA\" ~ \"Player A\",\n      \"playerB\" ~ \"Player B\",\n      \"playerC\" ~ \"Player C\",\n    )\n  )\n\nggplot2::ggplot(\n  data = mydata_long,\n  ggplot2::aes(\n    x = points,\n    fill = name\n  )\n) +\n  ggplot2::geom_density(alpha = .3) + \n  ggplot2::labs(\n    x = \"Players' Projected Points\",\n    title = \"Density Plot of Projected Points for Three Players\"\n  ) +\n  ggplot2::theme_classic() +\n  ggplot2::theme(legend.title = element_blank())\n\n\n\n\n\n\nFigure 6.1: Density Plot of Projected Points for Three Players\n\n\n\n\nUncertainty is not necessarily a bad characteristic of a player’s projected points. It just means we have less confidence about how the player may be expected to perform. Thus, players with greater uncertainty are risky and tend to have a higher upside (or ceiling) and a lower downside (or floor). For this reason, identifying players with a high uncertainty can be useful for identifying potential “sleepers”—i.e., late-round draft picks who outperform expectations [Petersen (2014); archived at https://perma.cc/K6LX-85A4].",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-signsSamples",
    "href": "player-evaluation.html#sec-signsSamples",
    "title": "6  Player Evaluation",
    "section": "\n6.13 Signs Versus Samples",
    "text": "6.13 Signs Versus Samples\nWhen thinking about the most important domains to assess for evaluating players and predicting their performance, it is important to distinguish between signs and samples (Den Hartigh et al., 2018). Signs are indicators of underlying states. The signs approach to assessment involves assessing processes that may predict performance, where the emphasis is on what the sign indicates about the underlying attribute, rather than on the specific behavior itself. In football, the signs approach might involve measuring skills using separate tests where players’ skills are tested separately (i.e., in isolation from other skills) in controlled settings. For instance, the NFL Combine takes a signs approach to assessment, in which players have a separate test to assess for speed (e.g., 40-yard dash) and jumping ability (e.g., vertical jump). The signs approach to assessment assumes that the various skills assessed (e.g., being fast, being able to jump high, having strong endurance, being motivated) have high predictive validity for the target behavior [i.e., future performance in games; Den Hartigh et al. (2018)].\nSamples reflect behaviors that are close to the behavior of interest. The samples approach to asesssment tries to assess the person’s performance in a behavior, situation, and context that are as reflective as possible of the kinds of behaviors, situations, and contexts that the person would have to perform in for the position or occupation. In the samples approach, the behavior itself is the main emphasis, rather than an underlying attribute. In football, the samples approach might involve measuring the player’s performance during games or game-like situations. For instance, to assess the skills of a Wide Receiver, you might observe—during games or game-like situations—how well they are able to catch passes when closely guarded or double-teamed, or how well they are able to catch poorly thrown passes.\nIn general, samples tend to be stronger than signs for predicting player performance in sports (Den Hartigh et al., 2018). For instance, compared to tests of speed, power, and agility at the NFL Combine, collegiate performance is a stronger predictor of performance in the NFL (Lyons et al., 2011). That is, previous sports performance is the best predictor of future performance (for a review, see Den Hartigh et al., 2018).\nAccording to the theoretical perspective to sports performance known as the ecological dynamics approach, successful performance in sports involves the coordination of multiple, intertwined skills that are contextually embedded (Den Hartigh et al., 2018). For instance, a Wide Receiver needs to coordinate skills in speed, route running, jumping, and good hands to create space from defenders and to catch poorly thrown passes in a game. The ecological dynamics approach considers the important interaction of the player, task, and environment. Thus, to assess players in a way that is likely to be most predictive of their future performance, it is important for the assessment to retain the player–task–environment interaction (Den Hartigh et al., 2018). For example, it would be valuable to use assessments from games or game-like situations in which players must perform tasks that leverage multiple, intertwined skills and that are similar to the tasks you want to predict. The extent to which the assessments are more reflective of games or game-like situations, we consider the assessment to have better ecological validity.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalIntegration",
    "href": "player-evaluation.html#sec-evalIntegration",
    "title": "6  Player Evaluation",
    "section": "\n6.14 Putting it Altogether",
    "text": "6.14 Putting it Altogether\nAfter performing an evaluation of the relevant domain(s) for a given player, one must integrate the evaluation information across domains to make a judgment about a player’s overall value. When considering how much weight to give to each of various factors, it is important to evaluate how much predictive validity each factor has for predicting a player’s successful performance in the NFL. Then, one can weight each variable according to its predictive validity using an actuarial approach. Actuarial approaches are described in a later chapter, in Section 15.2.2. For now, suffice it to say that actuarial approaches leverage statistical formulas, as opposed to using judgment alone. As described in Chapter 14, people’s judgment—including judgments by experts (e.g., professional scouts and coaches)—tends to be riddled with biases (Den Hartigh et al., 2018). Moreover, experts tend to disagree in terms of their judgments/predictions about players (Den Hartigh et al., 2018). In addition, there are benefits to leveraging multiple perpectives, consistent with the “wisdom of the crowd”, as described in Section 26.3. In general, as noted in Section 6.13, give more weight to samples of relevant behavior—such as past performance in games—than to signs such as NFL Combine metrics (e.g., bench press, 40-yard dash, etc.). Where you can, use assessments from games or game-like situations in which players must perform tasks that leverage multiple, intertwined skills and that are similar to the tasks you want to predict.\nWhen thinking about a player’s value, it can be worth thinking of a player’s upside and a player’s downside. Players that are more consistent may show higher downside but a lower upside. Younger, less experienced players may show a higher upside but a lower downside.\nThe extent to which you prioritize a higher upside versus a higher downside may depend on many factors. For instance, when drafting players, you may prioritize drafting players with the highest downside (i.e., the safest players), whereas you may draft sleepers (i.e., players with higher upside) for your bench. When choosing which players to start in a given week, if you are predicted to beat a team handily, it may make sense to start the players with the highest downside. By contrast, if you are predicted to lose to a team by a good margin, it may make sense to start the players with the highest upside.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-playerEvaluationConclusion",
    "href": "player-evaluation.html#sec-playerEvaluationConclusion",
    "title": "6  Player Evaluation",
    "section": "\n6.15 Conclusion",
    "text": "6.15 Conclusion\nIn conclusion, there are many factors to consider when evaluating players. You will have less direct access to some factors (e.g., cognitive and motivational factors) than others (e.g., historical performance). When considering how much weight to give to each of various factors, it is important to evaluate how much predictive validity each factor has for predicting a player’s successful performance, and to weight it accordingly. In general, samples tend to be stronger than signs for predicting player performance in sports. For instance, compared to tests of speed, power, and agility at the NFL Combine, collegiate performance is a stronger predictor of performance in the NFL. That is, previous sports performance is the best predictor of future performance. When thinking about a player’s value, it can be worth thinking their potential upside and downside. Whether you value a player with higher upside or higher downside may depend on many factors such as, when drafting, whether you are seeking a starter or bench player, and, when in-season, whether you are projected to win by a large margin in your matchpup.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-playerEvaluationSessionInfo",
    "href": "player-evaluation.html#sec-playerEvaluationSessionInfo",
    "title": "6  Player Evaluation",
    "section": "\n6.16 Session Info",
    "text": "6.16 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.1.0     readr_2.1.5     tidyr_1.3.1     tibble_3.3.0   \n [9] ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_2.0.0     compiler_4.5.1     tidyselect_1.2.1  \n [5] scales_1.4.0       yaml_2.3.10        fastmap_1.2.0      R6_2.6.1          \n [9] labeling_0.4.3     generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4 \n[13] pillar_1.11.0      RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6       \n[17] stringi_1.8.7      xfun_0.53          timechange_0.3.0   cli_3.6.5         \n[21] withr_3.0.2        magrittr_2.0.3     digest_0.6.37      grid_4.5.1        \n[25] hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5        evaluate_1.0.4    \n[29] glue_1.8.0         farver_2.1.2       rmarkdown_2.29     tools_4.5.1       \n[33] pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J. (2011). On the predictive efficiency of past performance and physical ability: The case of the National Football League. Human Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218\n\n\nO’Connell, R. (2022). Without the Wonderlic, the N.F.L. finds other ways to test football I.Q. https://www.nytimes.com/2022/03/02/sports/football/nfl-wonderlic-test.html\n\n\nPetersen, I. T. (2014). Identify sleepers in fantasy football using statistics and wisdom of the crowd. https://fantasyfootballanalytics.net/2014/06/identify-sleepers-using-wisdom-crowd.html\n\n\nPetersen, I. T. (2016). Which are more accurate: Fantasy football rankings or projections? https://fantasyfootballanalytics.net/2016/04/accuracy-of-rankings-vs-projections.html",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "draft.html",
    "href": "draft.html",
    "title": "7  The Fantasy Draft",
    "section": "",
    "text": "7.1 Getting Started\nThis chapter provides an overview of the major types of drafts in fantasy football and various draft strategies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftGettingStarted",
    "href": "draft.html#sec-draftGettingStarted",
    "title": "7  The Fantasy Draft",
    "section": "",
    "text": "7.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")\nlibrary(\"Rglpk\")\n\n\n\n7.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/players_projections_seasonal.RData\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftTypes",
    "href": "draft.html#sec-draftTypes",
    "title": "7  The Fantasy Draft",
    "section": "\n7.2 Types of Fantasy Drafts",
    "text": "7.2 Types of Fantasy Drafts\nThere are several types of drafts in fantasy football. The most common types of drafts are snake drafts and auction drafts.\n\n7.2.1 Snake Draft\nIn a snake draft, the participants (i.e., managers) are assigned a draft order. In the first round, the managers draft in that order. In the second round, the managers draft in reverse order. It continues to “snake” in this way, round after round, so that the person who has the first pick in a given round has the last pick in the next round, and whoever has the last pick in a given round has the first pick in the next round.\n\n7.2.2 Auction Draft\nIn an auction draft, the managers are assigned a nomination order and there is a salary cap (e.g., $200). The first manager chooses which player to nominate. Then, the managers bid on that player like in an auction. In order to bid, the manager must raise the price by at least $1. If two managers want to obtain the same player, they may continue to raise the amount until one manager backs out and is no longer to bid by raising the price. The highest bidder wins (i.e., drafts) that player. Then, the second manager nominates a player, and the managers bid on that player. This process repeats until all teams have drafted their allotment of players.\n\n7.2.3 Comparison\nSnake drafts are more common than auction drafts. Snake drafts tend to be quicker than auction drafts. However, auction drafts are more fair than snake drafts. In an auction draft, unlike a snake draft, all players are available to all teams. For instance, in a snake draft, the first 9 players drafted are unavailable to the 10th pick of the first round. So, if you have the 10th pick and want the top-ranked player, this player would not be available to you in the snake draft. However, in the auction draft, every player is available to every manager, so long as the manager is able and willing to bid enough.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftStrategy",
    "href": "draft.html#sec-draftStrategy",
    "title": "7  The Fantasy Draft",
    "section": "\n7.3 Draft Strategy",
    "text": "7.3 Draft Strategy\n\n7.3.1 Overview\nThere is no one “right” draft strategy. As noted by Lee & Liu (2022) in their analysis of fantasy drafts, the effectiveness of any draft strategy depends on the strategies of the other managers in the league. Sometimes it works best to “zig” when everyone else is “zagging”. For instance, Lee & Liu (2022) found that the most successful draft strategies in terms of roster composition (i.e., the number of players for each position) were not the most common draft strategies. For instance, if you notice that everyone else is drafting Wide Receivers, this may mean that other managers are over-valuing Wide Receivers, and this could be a nice opportunity to draft a Running Back for good value.\nIn general, you will first want to generate the rankings you will use to select which players to prioritize. You may generate your rankings based one or more of the following:\n\nyour evaluation of players and of their situation\n\nIt may make sense to prioritize players, in particular Running Backs, Wide Receivers, Quarterbacks, and Tight Ends, who are are on strong offenses.\n\n\nexpert or aggregated rankings\n\nlayperson rankings\n\nplayers’ Average Draft Position (ADP) in other league drafts (for snake drafts)\nplayers’ Average Auction Value (AAV) in other league drafts (for auction drafts)\n\n\nexpert or aggregated projections\nindices derived from rankings and projections\n\nAs described in Section 26.3, there can be benefits of leveraging the wisdom of the crowd by using rankings or projections that are averaged across many people and perspectives. Section 6.12.1 describes where to obtain aggregated rankings, aggregated projections, ADP, and AAV data. As described in Chapter 15, there are also benefits to using the actuarial approach to prediction rather than (merely) using judgment.\nIt is not sufficient to compare players in terms of projected points because some positions have more depth than other positions. Some positions show positional scarcity—that is, a limited number of high performing players.\nAn important concept in the draft is “dropoff”, which is described in Section 6.12.2.1. Dropoff at a given position, is the difference—in terms of projected fantasy points—between (a) the best available player remaining at that position and (b) the second-best available player remaining at that position. If there is a bigger dropoff at a given position, there may be greater value in drafting the top player from that position. For instance, consider the following scenario: “Quarterback A” is projected to score 325 points, and “Quarterback B” is projected to score 320 points. “Tight End A” is projected to score 230 points, and “Tight End B” is projected to score 150 points. In this example, there is a much greater dropoff for Tight Ends than there is for Quarterbacks. Thus, even though “Quarterback A” is projected to score more points than “Tight End A”, “Tight End A” may be more valuable, relatively, because there is still a good Quarterback available if someone else drafts “Quarterback A”. In general, Kickers and Defenses tend to have the lowest dropoff (i.e., the lowest expected drop in fantasy points) by positional rank, so it makes sense to draft Kickers and Defenses late in the draft (Lee & Liu, 2022). Defenses, in particular, appear to be among the least predictable of the positions (Lee & Liu, 2022).\nAnother important concept is a player’s value over a typical replacement player at that position (shortened to “value over replacement player”; VORP), which is described in Section 6.12.2.2. A player’s value over a typical replacement player provides a way to more fairly compare (and thus rank) players across different positions.\nAnother important concept is a player’s uncertainty, which is described in Section 6.12.2.3.\nIn both snake and auction draft formats, your goal is to draft the team whose weekly starting lineup scores the most points and thus the collection of players with the greatest VORP. For your starting lineup, it may make sense—especially with your earliest selections—when comparing two players with equivalent VORP, to prioritize players with higher consistency and lower uncertainty, because they may be considered “safer” with a higher floor. However, when drafting players for your bench, it make make more sense to prioritize high-risk, high reward players with greater uncertainty, because they may have a higher ceiling. Players with a higher ceiling have a potential to be “sleepers”—players who are valued low (i.e., with a high ADP or low AAV) and who outperform their valuation. Note that, although players with greater uncertainty are high-risk, high-reward players, selecting this kind of a player for your bench (i.e., in a late round or for a small cost) is a lower risk selection, because you have less to lose with later/lower-cost picks. That is, even though the player is higher risk, selecting a higher risk player for your bench is a lower risk decision.\nThe Spurs in the National Basketball Association (NBA) were well-reputed for excelling in this draft strategy [Ryan (2013); archived at https://perma.cc/X7NW-WZC6]. They frequently used their second-round picks to draft high-risk, high-reward players. Sometimes, the secound round pick was a bust, but they have little to lose with a failed second round pick. Other times, their second round picks—including Willie Anderson, DeJuan Blair, Goran Dragic, Luis Scola, and Manu Ginóbili—greatly outperformed expectations. Thanks, in part, to this draft strategy, the team showed strong extended success for nearly three decades from 1989 through the late-2010s.\nAnother approach is to embrace variability and increase the variability of a team’s outcomes by stacking players from the same team (Lee & Liu, 2022).\nHowever, the draft strategies to achieve the “optimal lineup” differ between snake versus auction drafts.\nOne factor that is not included above is whether a player is on your favorite team. Managers commonly like to draft players on their favorite teams (e.g., Cowboys, Eagles). That is fine—fantasy football is a game. Do what is fun for you. However, if your goal is to select the best players, leave your allegiances at the door. Selecting players based on their playing for your favorite team—rather than based on performance—is a form of cognitive bias called in-group bias.\nIn general, the most consequential decisions tend to be those made early in the draft regarding the top projected players (Lee & Liu, 2022). That is, the earlier selections tend to have the greatest impact on the success of one’s fantasy season. So, make sure to spend time in your draft preparation to identify the players you want to select early in the draft. Moreover, some teams like to hedge their bet on their top Running Backs in the case that the player were to get injured, by drafting the player’s backup, a strategy known as handcuffing. However, there is not strong evidence that handcuffing leads to better outcomes (Lee & Liu, 2022).\nIn general, Lee & Liu (2022) found that teams that drafted more Running Backs and Wide Receivers tended to outperform other teams.\n\n7.3.2 Snake Draft\nIn general, your goal is to draft the team whose weekly starting lineup has the greatest VORP. Consequently, you are often looking to pick the player with the highest VORP at a given selection, while keeping in mind (a) the dropoff of players at other positions and (b) which players may be available at subsequent picks so that you do not sacrifice too much later value with a given selection. For instance, if a particular Quarterback has a slightly higher VORP than a particular Running Back, but the Quarterback is likely to be available at the manager’s next pick but the Running Back is likely to be unavailable at their next pick, it might make more sense to draft the Running Back. Another way to draft players who are a good value at a given pick is to identify players who have fallen past their ADP [Harstad (2023); archived at https://perma.cc/9MAC-PUGL]. For instance, if you are on pick 30, drafting a player who has fallen past their ADP might mean drafting a player whose ADP is 22.\nHerding behavior is common in snake drafts. As described in Section 26.3, herding occurs when people align their behavior with others. For instance, Lee & Liu (2022) found evidence that when Quarterbacks, Kickers, and Defenses were drafted by one team, subsequent teams were more likely to draft a player of that position. The same was not the case for Running Backs, Wide Receivers, and Tight Ends.\nFantasyFootballAnalytics.net provides tools to help you win your snake draft: https://fantasyfootballanalytics.net/win-your-snake-draft. For draft tools for snake drafts, see the Fantasy Football Analytics web application (depicted in Figure 7.1): https://apps.fantasyfootballanalytics.net. The web application includes a free tool that allows users to specify their custom league settings to calculate projected points that are customized to their league and helps users select which players to draft that maximize the players’ value over replacement players (VORP).\n\n\n\n\n\nFigure 7.1: Free Snake Draft Tool From Fantasy Football Analytics: https://apps.fantasyfootballanalytics.net.\n\n\n\n7.3.3 Auction Draft\nAccording to an analysis by the Harvard Sports Analysis Collective, the majority of the manager’s salary cap should be spent on the starting lineup, and you should spend less on bench players [Chakravarthy (2012); archived at https://perma.cc/P7RX-92UU]. This is known as the “stars and scrubs” draft strategy. Based on the analysis, the author recommended applying a 10% premium to the top players and a 10% discount to the lower-tiered players. The idea behind the approach is that a player on your bench does not contribute to the team’s points and, thus, most players drafted to your bench do not contribute much to the team’s points throughout the season. That said, bench players can be important in the case of a starter’s injury or under-performance. So, it is recommended to draft starters with lower uncertainty who are safer. In contrast to your starting lineup, you may look to draft players on your bench who have greater uncertainty for their high reward potential in a low-risk selection given the lower price.\nAn alternative to the “stars and scrubs” approach is to wait to draft more “high-value” players after other managers have over-paid for players. In any case, having some small amount of cap left over toward the end of the draft can help you draft good value players to fill out your bench spots for cheap (e.g., $2). Having some depth can help you offset the risk of injuries and the possibility that some of your starters may underperform their expectation, which is quite likely given how challenging it is to predict fantasy performance.\nFantasyFootballAnalytics.net provides tools to help you win your auction draft (https://fantasyfootballanalytics.net/how-to-win-your-auction-draft) and daily fantasy sports (DFS) leagues (https://fantasyfootballanalytics.net/how-to/dfsoptimizerhelp). For draft and player selection tools for auction drafts and DFS leagues, see the Fantasy Football Analytics web application (depicted in Figure 7.2): https://apps.fantasyfootballanalytics.net. The web application includes a free tool that allows users to specify their custom league settings to calculate projected points that are customized to their league, and to identify the optimal lineup given their league settings and salary cap.\n\n\n\n\n\nFigure 7.2: Free Auction Draft Lineup Optimizer From Fantasy Football Analytics: https://apps.fantasyfootballanalytics.net.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftAnalysis",
    "href": "draft.html#sec-draftAnalysis",
    "title": "7  The Fantasy Draft",
    "section": "\n7.4 Draft Analysis",
    "text": "7.4 Draft Analysis\n\n7.4.1 Fantasy Points by (End-of-Season) Position Rank and Position\n\nCodefantasyPointsByPositionRank &lt;- player_stats_seasonal %&gt;% \n  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\",\"K\",\"DL\",\"LB\",\"DB\")) %&gt;% \n  group_by(position_group, season) %&gt;% \n  arrange(position_group, season, -fantasyPoints) %&gt;% \n  mutate(positionRank = row_number())\n\n\nA plot of fantasy points by end-of-season position rank and position is depicted in Figure 7.3.\n\nCodeggplot(\n  data = fantasyPointsByPositionRank %&gt;% filter(positionRank &lt;= 200),\n  mapping = aes(\n    x = positionRank,\n    y = fantasyPoints\n  )\n) + \n  geom_point(\n    alpha = 0.03\n  ) +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) + # don't add space between axes and data\n  labs(\n    x = \"End-of-Season Position Rank\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by End-of-Season Position Rank\"\n  ) +\n  theme_bw() +\n  facet_wrap(vars(position_group)) # facet by position_group\n\n\n\n\n\n\nFigure 7.3: Fantasy Points by End-of-Season Position Rank and Position.\n\n\n\n\nIn Figure 7.4, we overlay the smoothed best-fit line for each position onto the same graphing area for direct comparison.\n\nCodepalette_OkabeIto_black &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#000000\")\n\nggplot(\n  data = fantasyPointsByPositionRank %&gt;% filter(positionRank &lt;= 200),\n  mapping = aes(\n    x = positionRank,\n    y = fantasyPoints,\n    color = position_group\n  )\n) + \n  geom_smooth(\n    linewidth = 1.5,\n    se = FALSE) +\n  coord_cartesian(\n    ylim = c(0,NA), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  scale_color_manual(\n    values = palette_OkabeIto_black\n  ) +\n  labs(\n    x = \"End-of-Season Position Rank\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by End-of-Season Position Rank\",\n    color = \"Position Group\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 7.4: Fantasy Points by End-of-Season Position Rank and Position.\n\n\n\n\nIn Figure 7.5, we limit the position rank to ranks less than or equal to 30, for easier comparison of positions’ comparative dropoff at the top position ranks.\n\nCodeggplot(\n  data = fantasyPointsByPositionRank %&gt;% filter(positionRank &lt;= 30),\n  mapping = aes(\n    x = positionRank,\n    y = fantasyPoints,\n    color = position_group\n  )\n) + \n  geom_smooth(\n    linewidth = 1.5,\n    se = FALSE) +\n  coord_cartesian(\n    ylim = c(0,425), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  scale_color_manual(\n    values = palette_OkabeIto_black\n  ) +\n  labs(\n    x = \"End-of-Season Position Rank\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by End-of-Season Position Rank\",\n    subtitle = \"Position Rank \\u2264 30\", # \\u2264: &lt;= symbol\n    color = \"Position Group\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 7.5: Fantasy Points by End-of-Season Position Rank and Position.\n\n\n\n\nRunning Backs show the fastest intial dropoff (i.e., steepest negative slope), followed by Tight Ends, Quarterbacks, Wide Receivers, and then defensive positions. So, identifying the top-scoring Running Back can be of great benefit. After the top 10 or so players at each position, Quarterbacks show a steeper dropoff than Running Backs, Tight Ends, and Wide Receivers.\nHowever, the style of gameplay has changed. Figure 7.6 depicts the same data since 2020.\n\nCodeggplot(\n  data = fantasyPointsByPositionRank %&gt;% filter(positionRank &lt;= 30 & season &gt;= 2020),\n  mapping = aes(\n    x = positionRank,\n    y = fantasyPoints,\n    color = position_group\n  )\n) + \n  geom_smooth(\n    linewidth = 1.5,\n    se = FALSE) +\n  coord_cartesian(\n    ylim = c(0,425), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  scale_color_manual(\n    values = palette_OkabeIto_black\n  ) +\n  labs(\n    x = \"End-of-Season Position Rank\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by End-of-Season Position Rank\",\n    subtitle = \"Position Rank \\u2264 30, Since 2020\", # \\u2264: &lt;= symbol\n    color = \"Position Group\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 7.6: Fantasy Points by End-of-Season Position Rank and Position.\n\n\n\n\nSince 2020, although Running Backs show the fastest initial dropoff, Quarterbacks and Wide Receivers have appeared to overtake them in terms of fantasy points. Quarterbacks continue to show the steepest dropoff after the top 10 or so players at each position.\nFigure 7.7 depicts the mean fantasy points of the top 10 players at each position by season.\n\nCodemeanFantasyPointsTop10ByPosition &lt;- fantasyPointsByPositionRank %&gt;% \n  filter(positionRank &lt;= 10) %&gt;% \n  summarise(\n    fantasyPoints = mean(fantasyPoints, na.rm = TRUE),\n    .groups = \"drop_last\"\n  )\n\nggplot(\n  data = meanFantasyPointsTop10ByPosition,\n  mapping = aes(\n    x = season,\n    y = fantasyPoints,\n    color = position_group\n  )\n) + \n  geom_line(\n    linewidth = 1.5) +\n  coord_cartesian(\n    ylim = c(0,425), # set limits of y-axis\n    expand = FALSE) + # don't add space between axes and data\n  scale_color_manual(\n    values = palette_OkabeIto_black\n  ) +\n  labs(\n    x = \"Season\",\n    y = \"Mean Fantasy Points\",\n    title = \"Mean Fantasy Points of Top 10 Players at Each Position by Season\",\n    color = \"Position Group\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 7.7: Mean Fantasy Points of Top 10 Players at Each Position by Season.\n\n\n\n\nThe trend suggests that, since 2000, Quarterbacks have increased in fantasy points scored, and Running Backs have decreased in fantasy points scored.\nKey takeaways include that, based on their steep initial dropoff, it can be of great benefit (in terms of a comparative advantage against your opponents) to have a top Running Back or Tight End. The challenge, of course, is identifying—in advance—which of the players is going to be the top-ranked player at that position. It also seems important to have a top-10 Quarterback, because Quarterbacks show the steepest dropoff after position rank of ~10. In general, given their tendency to score lots of points and to show a relatively steep dropoff, it seems crucial to have strong Quarterbacks, Wide Receivers, and Running Backs, with the highest priority likely going to Running Backs and Wide Receivers.\n\n7.4.2 Lineup Optimization\nFor auction drafts and daily fantasy sports (DFS) leagues, a key goal is to select the best lineup or team within the given cap. For instance, in an auction drafts, you may have a $200 cap and your goal is to draft the best team that you can with the $200. Using a mixed-integer programming solver, we can identify the lineup with the most projected points given a particular salary cap. For projected points for a player, we use their average projected points across all available projection sources. For a player’s estimated cost, we use their Average Auction Value (AAV), rounded up to the nearest integer.\nWe first subset and prepare the data so we can perform the lineup optimization.\n\nCodemostRecentProjections &lt;- players_projections_seasonal_average_merged %&gt;% \n  filter(season == max(season, na.rm = TRUE)) %&gt;% \n  filter(pos %in% c(\"QB\",\"RB\",\"WR\",\"TE\",\"K\",\"DST\")) %&gt;% \n  filter(avg_type == \"average\")\n\n# Convert NAs for fantasy points to zero\nmostRecentProjections$points[which(is.na(mostRecentProjections$points))] &lt;- 0\n\n# Convert NA for AAV to $1\nmostRecentProjections$aav[which(is.na(mostRecentProjections$aav))] &lt;- 1\n\n# Round up AAV to nearest integer\nmostRecentProjections$cost &lt;- ceiling(mostRecentProjections$aav)\n\nmostRecentProjections$name &lt;- paste(mostRecentProjections$first_name, mostRecentProjections$last_name)\n\n\nNext, we set up the constraints. Our example is adapted from https://stackoverflow.com/questions/15147398/optimize-value-with-linear-or-non-linear-constraints-in-r. Our constraints were:\n\n9 total players of the following number at each of following positions:\n\n1 Quarterback\n2 Running Backs\n2 Wide Receivers\n1 Tight End\n1 Flex: RB/WR/TE\n1 K\n1 DST\n\n\nsalary cap of $200.\n\n\nCode# number of players\nnum.players &lt;- nrow(mostRecentProjections)\n\n# 1 for every player\ntotal_players &lt;- rep(1, num.players)\n\n# the variables are booleans\nvar.types &lt;- rep(\"B\", num.players)\n\n# the constraints\nA &lt;- rbind(\n  as.numeric(mostRecentProjections$pos == \"QB\"),                # num QB\n  as.numeric(mostRecentProjections$pos == \"RB\"),                # num RB\n  as.numeric(mostRecentProjections$pos == \"WR\"),                # num WR\n  as.numeric(mostRecentProjections$pos == \"TE\"),                # num TE\n  as.numeric(mostRecentProjections$pos == \"K\"),                 # num K\n  as.numeric(mostRecentProjections$pos == \"DST\"),               # num DST\n  as.numeric(mostRecentProjections$pos %in% c(\"RB\",\"WR\",\"TE\")), # num flex\n  total_players,\n  mostRecentProjections$cost)                                   # player cost\n\ndir &lt;- c(\n  \"==\",\n  \"&gt;=\",\n  \"&gt;=\",\n  \"&gt;=\",\n  \"==\",\n  \"==\",\n  \"==\",\n  \"==\",\n  \"&lt;=\")\n\nb &lt;- c(\n  1,   # 1 QB\n  2,   # at least 2 RBs\n  2,   # at least 2 WRs\n  1,   # at least 1 TE\n  1,   # 1 K\n  1,   # 1 DST\n  6,   # 1 RB/WR/TE flex (i.e., 6 total RB/WR/TEs)\n  9,   # 9 total players\n  200) # salary cap of $200\n\n\nThe optimizer thus seeks to identify the lineup with the most projected points that meets the above constraints (within a salary cap of $200). We perform the optimization using the Rglpk::Rglpk_solve_LP() function of the Rglpk package (Theussl & Hornik, 2024).\n\nCodesol &lt;- Rglpk::Rglpk_solve_LP(\n  obj = mostRecentProjections$points,\n  mat = A,\n  dir = dir,\n  rhs = b,\n  types = var.types,\n  max = TRUE)\n\n\nHere is the solution obtained by the solver:\n\nCodesol\n\n$optimum\n[1] 2141.228\n\n$solution\n  [1] 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[260] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n[297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[334] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[371] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[408] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[445] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[482] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[519] 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[556] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[593] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[630] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[667] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[704] 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[741] 0 0 0 0\n\n$status\n[1] 0\n\n$solution_dual\n[1] NA\n\n$auxiliary\n$auxiliary$primal\n[1]   1   2   3   1   1   1   6   9 200\n\n$auxiliary$dual\n[1] NA\n\n\n$sensitivity_report\n[1] NA\n\n\nA zero for status indicates that it found a solution:\n\nCodesol$status\n\n[1] 0\n\n\nHere is the maximum value obtained in terms of the sum of projected points for the optimal lineup:\n\nCodesol$optimum\n\n[1] 2141.228\n\n\nHere is a vector indicating whether or not a given player was selected for the optimal lineup (0 = no; 1 = yes):\n\nCodesol$solution\n\n  [1] 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[260] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n[297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[334] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[371] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[408] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[445] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[482] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[519] 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[556] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[593] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[630] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[667] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[704] 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[741] 0 0 0 0\n\n\nHere is the optimal lineup that maximized the sum of projected points within the salary cap:\n\nCodemostRecentProjections[which(sol$solution == 1), c(\"name\",\"pos\",\"cost\",\"points\")]\n\n\n  \n\n\n\nFor a web application that includes a lineup optimizer for selecting the optimal lineup for auction drafts and for DFS leagues, see the Fantasy Football Analytics web application: https://apps.fantasyfootballanalytics.net. The web application allows users to specify their custom league settings, select players to draft or exclude, and exclude risky players, among other features.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftConclusion",
    "href": "draft.html#sec-draftConclusion",
    "title": "7  The Fantasy Draft",
    "section": "\n7.5 Conclusion",
    "text": "7.5 Conclusion\nThe two major draft types are snake drafts and auction drafts. Auction drafts take longer than snake drafts but are more fair because every player is available to every manager, so long as the manager is able and willing to bid enough. There is no one right draft strategy. In general, it is helpful to draft players with the best value over other players at their position.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftSessionInfo",
    "href": "draft.html#sec-draftSessionInfo",
    "title": "7  The Fantasy Draft",
    "section": "\n7.6 Session Info",
    "text": "7.6 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] Rglpk_0.6-5.1   slam_0.1-55     lubridate_1.9.4 forcats_1.0.0  \n [5] stringr_1.5.1   dplyr_1.1.4     purrr_1.1.0     readr_2.1.5    \n [9] tidyr_1.3.1     tibble_3.3.0    ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] Matrix_1.7-3       gtable_0.3.6       jsonlite_2.0.0     compiler_4.5.1    \n [5] tidyselect_1.2.1   splines_4.5.1      scales_1.4.0       yaml_2.3.10       \n [9] fastmap_1.2.0      lattice_0.22-7     R6_2.6.1           labeling_0.4.3    \n[13] generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4  pillar_1.11.0     \n[17] RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6        stringi_1.8.7     \n[21] xfun_0.53          timechange_0.3.0   cli_3.6.5          mgcv_1.9-3        \n[25] withr_3.0.2        magrittr_2.0.3     digest_0.6.37      grid_4.5.1        \n[29] hms_1.1.3          nlme_3.1-168       lifecycle_1.0.4    vctrs_0.6.5       \n[33] evaluate_1.0.4     glue_1.8.0         farver_2.1.2       rmarkdown_2.29    \n[37] tools_4.5.1        pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\n\n\n\nChakravarthy, P. (2012). Optimizing draft strategies in fantasy football. https://harvardsportsanalysis.wordpress.com/wp-content/uploads/2012/04/fantasyfootballdraftanalysis1.pdf\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nHarstad, A. (2023). The best draft strategy for 2023 (and every other year). https://www.footballguys.com/article/2023-draft-adp-fallers\n\n\nLee, M. D., & Liu, S. (2022). Drafting strategies in fantasy football: A study of competitive sequential human decision making. Judgment and Decision Making, 17(4), 691–719. https://doi.org/10.1017/S1930297500008901\n\n\nRyan, J. (2013). Beating the NBA draft: Does any team outperform expectations? https://harvardsportsanalysis.org/2013/11/beating-the-nba-draft-does-any-team-outperform-expectations\n\n\nTheussl, S., & Hornik, K. (2024). Rglpk: R/GNU linear programming kit interface. https://doi.org/10.32614/CRAN.package.Rglpk",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "research-methods.html",
    "href": "research-methods.html",
    "title": "8  Research Methods",
    "section": "",
    "text": "8.1 Getting Started\nThis chapter provides an overview of key concepts in research methods. These considerations are crucial for designing studies and for interpreting findings from research studies.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsGettingStarted",
    "href": "research-methods.html#sec-researchMethodsGettingStarted",
    "title": "8  Research Methods",
    "section": "",
    "text": "8.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-generalWorkflowStudy",
    "href": "research-methods.html#sec-generalWorkflowStudy",
    "title": "8  Research Methods",
    "section": "\n8.2 General Workflow",
    "text": "8.2 General Workflow\nHere is a general workflow for a study:\n\nFormulate your research questions, hypotheses, resulting predictions, and exploratory questions\n\nObtain approval from an institutional review board (IRB) to conduct the research\n\nIn the United States, analyzing publicly available data from nflverse is not considered human subjects research because the work is not considered to involve human subjects—the data involve publicly available information that is not sensitive (i.e., it does not pose risks to players’ rights, privacy, or welfare), and the work does not involve intervention, interaction with the players, private information, or biospecimens.\n\n\nDesign the study to test your research questions\nCollect the data you need to test your research questions\nAnalyze the data to test your research questions\nCommunicate the findings to others (e.g., papers, posters, talks, news articles, interviews, blog posts)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-sampleVsPopulation",
    "href": "research-methods.html#sec-sampleVsPopulation",
    "title": "8  Research Methods",
    "section": "\n8.3 Sample vs Population",
    "text": "8.3 Sample vs Population\nIn research, it is important to distinguish between the sample and the target population. The target population is who you want your study’s findings to generalize to. For instance, if we want our findings to lead to inferences we can draw regarding all current NFL players, then NFL players are our target population. However, despite our best efforts to recruit all NFL players into our study, we may not succeed in doing that. The participants (i.e., people or players) who we successfully recruit to be in our study represent our sample. The number of participants in the study is our sample size.\nIt is rare for the sample to include all people who are in the target population. It can be costly to recruit large samples, and many potential participants may decline to participate for a variety of reasons (insufficient time, lack of interest in the study, distrust of scientists, etc.). Thus, our goals are (a) to recruit as many people from the population as possible and (b) for the sample to be as representative of the population as possible.\nFor increasing the representativeness of the sample (with respect to the population), we might conduct a random sample, in which each person in the population (i.e., each NFL player) has equal likelihood of being selected. For instance, we might randomly select 250 players to recruit to the study. True random samples, though strong in aspiration, are difficult and costly to achieve. In reality, many researchers conduct convenience sampling. A convenience sample is recruited because it is convenient (i.e., less costly and time-consuming).\nFor instance, many studies examine college students—in part, because they are easy to recruit. If our target population is NFL players but we are unable to recruit NFL players into our study, we could easily recruit a large sample of college students. Although the convenience sample may afford a very large sample, the college student sample may not be representative of the target population (NFL players). Thus, the findings in our study may not generalize to NFL players—that is, what we learn in college students may not apply in the same way among NFL players. For instance, if we learn that consumption of sports drinks (compared to drinking only water) improves running speed among college students, that may not be the case among NFL players.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-hypothesisVsPrediction",
    "href": "research-methods.html#sec-hypothesisVsPrediction",
    "title": "8  Research Methods",
    "section": "\n8.4 Research Questions, Hypotheses, and Predictions",
    "text": "8.4 Research Questions, Hypotheses, and Predictions\nA research question is a question that the investigator (you!) wants to know the answer to. For example, a research question might be: “Does consumption of sports drink improve player performance?” A hypothesis is a proposed explanation; it specifies the causal relation between processes. A prediction is “the expected result of a test that is derived, by deduction, from a hypothesis or theory” [Eastwell (2014), p. 17; archived at https://perma.cc/8EX4-8JYN]. Here is an example of a hypothesis and the resulting prediction:\n\nThe present study evaluates whether consumption of sports drink improves player performance. I hypothesize that consumption of sports drink leads football players to perform better in games because of greater endurance owing to restoration of electrolytes. If the hypothesis is true, I predict that players who consume sports drink during a game will score more fantasy points than players who do not consume sports drink during the game.\n\nResearch questions that you do not have hypotheses and predictions for are considered exploratory questions. Both hypothesis-driven work and exploration are important parts of science. Just be honest about which one you are pursuing for each question. Secretly hypothesizing after the results are known in the Introduction section of a paper (known as SHARKing) is problematic (Hollenbeck & Wright, 2017). Conducting many exploratory analyses is potentially more prone to false-positive results than conducting a single confirmatory (i.e., hypothesis-driven) test. Thus, presenting exploratory work as confirmatory is misrepresenting the likelihood that your results are true and can contribute to the replication crisis—the failure of many findings from many studies to replicate upon subsequent investigation. By contrast, it is fine to transparently hypothesize after the results are known in the Disucssion section of a paper (known as THARKing), because this can provide potential explanations for the findings that can be evaluated in subsequent analyses or studies (Hollenbeck & Wright, 2017).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchDesigns",
    "href": "research-methods.html#sec-researchDesigns",
    "title": "8  Research Methods",
    "section": "\n8.5 Research Designs",
    "text": "8.5 Research Designs\nThere are three broad types of research designs:\n\nexperiment\ncorrelational/observational study\ncase study\n\n\n8.5.1 Experiment\nIn an experiment, there are one or more things (i.e., variables) that we manipulate to see how the manipulation influences the process of interest. The variable that we manipulate is the independent variable. By contrast, the dependent variable is the variable that we evaluate to determine whether it was influenced by the manipulation (i.e., by the independent variable). Besides the independent and dependent variables, the researcher attempts to hold everything else constant through processes including standardization and random assignment. Standardization involves using the same procedures to assess each participant, so that scores can be fairly compared across participants (and groups). Random assignment involves randomly assigning participants to conditions of the independent variable, so the people in each condition are comparable and do not differ systematically. However, not all things are feasible or ethical to manipulate. For instance, it would not be ethical to randomly assign some players to receive a head trauma. In addition, it would not be feasible to manipulate the weather of a game or the performance of players.\n\n8.5.1.1 Intervention Study\nAn intervention study is a study that involves some modification (e.g., a treatment) with the intent to improve people’s standing on the dependent variable (e.g., depression). Some intervention studies have a control group, whereas intervention studies do not. Inclusion of a control group is valuable; without a control group, you do not know whether any apparent gains in the treatment condition were due to the treatment per se versus just the mere passage of time, regression effects, or other things that were going on in the participants’ lives. An intervention that includes random assignment (e.g., to the intervention or control group) is an experiment. A randomized controlled trial (RCT) is an example of an experiment because it is an intervention with random assignment.\nFor instance, we may be interested to evaluate whether players perform better (e.g., run faster) if they drink a sports drink compared when they drink only water. Our hypothesis might be that players will be expected to perform better when they drink a sports drink (compared to when they drink only water), for the reasons specified in Section 8.4. To this this research question and hypothesis, we might conduct an experiment by randomly assigning some players during practice to receive a sports drink and some players to receive only water. In this case, our independent variable is whether the player receives a sports drink. Our dependent variable might be their 40-yard dash time during practice.\n\n8.5.2 Correlational/Observational Study\nIn a correlational (aka observational) study, we do not manipulate a variable to see how the manipulation influences another variable. Instead, we examine how two variables, a predictor and an outcome variable, are associated. The hypothesized cause is called the predictor variable. The hypothesized effect is called the outcome variable. In this way, the predictor variable is similar to the independent variable, and the outcome variable is similar to the dependent variable. However, unlike the independent and dependent variables in an experiment, the predictor and outcome variables in a correlational study are not manipulated.\nFor instance, to use a correlational study to test the possibility that players who drink sports drinks perform better than players who drink only water, we could examine whether the players who drink sports drinks during a game score more fantasy points than players who drink only water during the game. In this case, our predictor variable is whether the players drinks sports drinks during a game. Our outcome variable is the number of fantasy points the player scored.\n\n8.5.2.1 Correlation Does Not Imply Causation\nAs the maxim goes, “correlation does not imply causation”—just because two variables are associated does not necessarily mean that they are causally related.\nJust because X is associated with Y does not mean that X causes Y. Consider that you find an association between variables X and Y.\nThere are several reasons why you might observe an association between X and Y:\n\n\nX causes Y\n\n\nY causes X\n\n\nX and Y are bidirectional: X causes Y and Y causes X\n\na third variable (i.e., confound), Z, influences both X and Y\n\nthe association between X and Y is spurious\n\nFor instance, one possibility is that the association we observed reflects our hypothesis that X causes Y, as depicted in Figure 8.1. That is, consumption of more sports drink may improve players’ performance.\n\n\n\n\n\nFigure 8.1: Hypothesized Causal Effect Based on an Observed Association Between X and Y, Such That X Causes Y. From Petersen (2024) and Petersen (2025).\n\n\nHowever, a second possibility is that the association reflects the opposite direction of effect, where Y actually causes X, as depicted in Figure 8.2. For instance, greater performance may lead players to drink more sports drink (rather than the reverse).\n\n\n\n\n\nFigure 8.2: Reverse (Opposite) Direction of Effect From the Hypothesized Effect, Where Y Causes X. From Petersen (2024) and Petersen (2025).\n\n\nA third possibility is that the association reflects a bidirectional effect, where X causes Y and Y causes X, as depicted in Figure 8.3. For instance, consumption of more sports drink may improve players’ performance, and greater performance in turn may lead players to drink more sports drink.\n\n\n\n\n\nFigure 8.3: Bidirectional Effect Between X and Y, such that X causes Y and Y causes X. From Petersen (2024) and Petersen (2025).\n\n\nA fourth possibility is that the association could reflect the influence of a third variable. If a third variable is a common cause of each and accounts for their association, it is a confound. An observed association between X and Y could reflect a confound—i.e., a cause (Z) that influences both X and Y, which explains why X and Y are correlated even though they are not causally related. A third variable confound that is a common cause of both X and Y is depicted in Figure 8.4. For instance, it may not be that sport drink consumption per se influences player performance; rather, it may be that players who are more intelligent or have more financial resources tend to drink more sports drinks and also tend to perform better. In this case, intelligence or financial resources may be a confound that influences both sports drink consumption and player performance, but sports drink consumptions—though correlated with player performance—does not influence player performance.\nFor another example, consider that ice cream sales are associated with shark attacks. It is unlikely that more people eating ice creams leads to shark attacks. There is a likely a third variable—heat waves—that is a confound because it influences both ice cream sales and shark attacks and explains their association.\n\n\n\n\n\nFigure 8.4: Confounded Association Between X and Y due to a Common Cause, Z. From Petersen (2024) and Petersen (2025).\n\n\nLastly, the association might be spurious (pure coincidence). It might just reflect random variation (i.e., chance), and that when tested on an independent sample, what appeared as an association in the original dataset may not hold when testing the association in a new dataset. Vigen (2024) provides a collection of spurious correlations: https://www.tylervigen.com/spurious-correlations (archived at https://perma.cc/L443-JPRN). As Vigen (2024) notes, spurious correlations may be particularly common in the context of outliers (extreme values), a small sample size, and when performing data dredging (or p-hacking)—i.e., examining the associations among many, many variables.\n\n8.5.3 Case Study\nIn a case study, we assess a small sample of individuals (commonly only one person or a few people), often with rich qualitative information. Themes may be coded from the qualitative information, which may help inform inferences about whether some process may have played a role in influencing the outcome of interest. The inferences are then drawn in a subjective, qualitative way. Testimonials and anecdotes are examples are case studies.\nFor instance, to use a case study to evalute the possibilty that players who drink sports drinks perform better than players who drink only water, we could conduct an in-depth interview with a player. In the interview, we might ask the player how they performed in games with versus without a sports drink and have them discuss whether they believe the sports drink improved their performance (and if so, how). Then, based on the player’s responses, we might code the responses to extract themes and to make a qualitative judgement of whether or not the player likely performed better during games in which they had a sports drink.\n\n8.5.4 Other Features of the Research Design\n\n8.5.4.1 Number of Timepoints\nIn addition to whether the research design is an experiment, correlational/observational study, or a case study, a research design can also have one or multiple timepoints. The differing number of timepoints allow studies to be characterized as one of the following:\n\ncross-sectional\nlongitudinal\n\n\n8.5.4.1.1 Cross-Sectional\nA cross-sectional study is a study with one timepoint.\nFor instance, in a cross-sectional study evaluating whether having a sports drink improves player performance, we might assess players’ drinking behavior and performance during only game 1.\nCross-sectional studies are more common than longitudinal studies because cross-sectional studies are less costly and time-consuming. They can provide a helpful starting point to test findings more rigorously in subsequent longitudinal studies.\n\n8.5.4.1.2 Longitudinal Design\nA longitudinal study is a study with more than one timepoint. When the same measures are assessed at each of multiple timepoints, we refer to this as a “repeated measures” design.\nIn a longitudinal study evaluating whether having a sports drink improves player performance, we might assess players’ drinking behavior and performance during each game of the season, and possibly across multiple seasons.\nLongitudinal studies are less common than cross-sectional studies because longitudinal studies are more costly and time-consuming. Nevertheless, longitudinal studies can allow us test our hypotheses more rigorously, because they can allow us to test whether changes in the predictor/indepdnent variable leads to changes in the outcome/dependent variable. Thus, compared to cross-sectional studies, longitudinal studies can provide greater confidence in causal inferences.\n\n8.5.4.2 Within- or Between-Subject\nA research design can also be within-subject, between-subject, or both. A study can involve both within-subject and between-subject comparisons if one predictor/independent variable is within-subject and another predictor/independent variable is between-subject.\n\n8.5.4.2.1 Within-Subject Design\nA within-subject design is one in which each participant (i.e., person or player) receives multiple levels of the independent variable (or predictor).\nFor instance, in an experiment evaluating whether having a sports drink improves player performance, we might assign players to drink the sports drink in the first half of the game and to drink only water in the second half of the game. Or we could assign some of the players to drink sports drink in the first half and water in the second half, and assign the other players to drink water in the first half and sports drink in the second half.\nIn a correlational study evaluating whether having a sports drink improves player performance, we might evaluate how within-person changes in sports drink consumption are associated with within-person changes in performance. That is, we could evaluate, when a given player has a sports drink (or more sports drinks), do they perform better than when the same individual has only water (or fewer sports drinks)?\nWithin-subject designs tend to have greater statistical power than between-subject designs. However, within-subject designs often have carryover effects. For instance, consider the study in which we assign players to drink only water in the first and third quarters and to drink sports drink in the second and fourth quarters (an A-B-A-B design). Drinking sports drink in the second quarter could increase how much hydration a player has throughout the rest of the game, which could lead to altered performance in the third and fourth quarters that is not due to what they drink in third and fourth quarters.\n\n8.5.4.2.2 Between-Subject Design\nA between-subject design is one in which each participant (i.e., person or player) receives only one level of the independent variable.\nFor instance, in an experiment evaluating whether having a sports drink improves player performance, we might assign some players to drink the sports drink but the other players to drink only water.\nIn a correlational study evaluating whether having a sports drink improves player performance, we might evaluate whether people who drink sports drinks tend to perform better than players who drink only water. Or, we could evaluate whether players who drink more sports drinks perform better than players who drink fewer sports drinks (i.e., whether the number of sports drinks consumed during a game is correlated with player performance).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchDesignValidity",
    "href": "research-methods.html#sec-researchDesignValidity",
    "title": "8  Research Methods",
    "section": "\n8.6 Research Design Validity",
    "text": "8.6 Research Design Validity\nResearch design validity involves the accuracy of inferences from a study. There are three types of research design validity:\n\ninternal validity\nexternal validity\nconclusion validity\n\n\n8.6.1 Internal Validity\nInternal validity is the extent to which we can be confident that the associations identified in the study are causal.\n\n8.6.2 External Validity\nExternal validity is the extent to which we can be confident that findings from the study play out similarly in the real world—that is, the findings generalize to the target population.\n\n8.6.3 Tradeoffs Between Internal and External Validity\nThere is a tradeoff between internal and external validity—a single research design cannot have both high internal and high external validity. Each study and design has weaknesses. Some research designs are better suited for making causal inferences, whereas other designs tend to be better suited for making inferences that generalize to the real world. The research design that is best suited to making causal inferences is an experiment because it is the design in which the researcher has the greatest control over the variables. Thus, experiments tend to have higher internal validity than other research designs. However, by manipulating one variable and holding everything else constant, the research takes place in a very standardized fashion that can become like studying a process in a vacuum. So, even if a process is theoretically causal in a vacuum, it may act differently in the real world when it interacts with other processes.\nCorrelational designs have greater capacity for external validity than experimental designs because the participants can be observed in their natural environments to evaluate how variables are related in the real world. However, the greater external validity comes at a cost of lower internal validity. Correlational designs are not well-positioned to make causal inferences. Correlational studies can account for potential confounds using covariates or for the reverse direction of effect using longitudinal designs, but the researcher has less control over the variables than in an experiment.\nAs the internal validity of a study’s design increases, its external validity tends to decrease. The greater control we have over variables (and, therefore, have greater confidence about causal inferences), the lower the likelihood that the findings reflect what happens in the real world because it is studying things in a metaphorical vacuum. Because no single research design can have both high internal and external validity, scientific inquiry needs a combination of many different research designs so we can be more confident in our inferences—experimental designs for making causal inferences and correlational designs for making inferences that are more likely to reflect the real world.\nCase studies, because they have smaller sample sizes and inferences drawn in a subjective, qualitative way, tend to have lower external validity than both experimental and correlational studies. Case studies also tend to have lower internal validity because they have less control over variables, and thus fail to remove the possibility of illusory correlations, potential confounds, or the reverse direction of effect. Thus, case studies are among the weakest forms of evidence. Nevertheless, case studies can still be useful for generating hypotheses that can then be tested empirically with a larger sample in experimental or correlational studies.\n\n8.6.4 Conclusion Validity\nConclusion validity is the extent to which a study’s conclusions are reasonable about the association among variables based on the data. That is, were the correct statistical analyses performed, and are the interpretations of the findings from those analyses correct?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-mediationModeration",
    "href": "research-methods.html#sec-mediationModeration",
    "title": "8  Research Methods",
    "section": "\n8.7 Mediation vs Moderation",
    "text": "8.7 Mediation vs Moderation\nBoth types of effects involve (at least) three variables:\n\nAn independent/predictor variable, which will be labeled as X.\nA dependent/outcome variable, which will be labeled as Y.\nThe mediator or moderator variable, which will be labeled as M.\n\nA mnemonic to help remember the difference between mediation and moderation is in Figure 8.5.\n\n\n\n\n\nFigure 8.5: Mediation Versus Moderation Mnemonic.\n\n\n\n8.7.1 Mediation\n\n8.7.1.1 Overview\nMediation is a causal chain of events, where one variable (a mediator variable) at least partially explains (or accounts for) the association between two other variables (the predictor variable and the outcome variable). In mediation, a predictor (X) leads to a mediator (M), which leads to an outcome (Y). Mediation answers the question of, “Why (or how) does X influence Y?” A mediator (M) is a variable that helps explain the association between two other variables, and it answers the question of why/how X influences Y. That is, the mediator is the variable that helps explain how/why X is related to Y. In other words, you can think of the mediator as the mechanism that helps explain why X has an impact on Y. The association between X and Y gets smaller when accounting for M. Visually this can be written as in Figure 8.6:\n\n\n\n\n\nFigure 8.6: Mediation.\n\n\nwhere X is causing M, which in turn is causing Y. In other words, X leads to M, and M leads to Y.\nFor instance, if we determine that consuming sports drinks improves player performance, we may want to know how/why. That is, what is the mechanism that leads consumption of sports drinks to improve player performance? We might hypothesize that consumption of sports drink helps increase a player’s hydration, which in turn will improve the player’s performance. In this case, increased hydration mediates (i.e., helps explain or account for) the effect of the sports drink consumption on improved player performance.\nQuestion: Why/how does sports drink consumpion lead players to perform better?\nAnswer: increased hydration\nAs a picture, we can draw this assocation as in Figure 8.7:\n\n\n\n\n\nFigure 8.7: Mediation Example.\n\n\n\n8.7.1.2 Types of Mediation\n\n8.7.1.2.1 Full Mediation\nWhen one mechanism fully accounts for the effect of the predictor variable on the outcome variable, this is known as full mediation, as depicted in Figure 13.15:\n\n\n\n\n\nFigure 8.8: Full Mediation.\n\n\n\n8.7.1.2.2 Partial Mediation\nWhen a single process partially—but does not fully—accounts for the effect of the predictor variable on the outcome variable; this is known as partial mediation and is depicted in Figure 13.16:\n\n\n\n\n\nFigure 8.9: Partial Mediation.\n\n\n\n8.7.1.2.3 Multiple Mediators\nIn addition, there can be multiple mediators/mechanisms that account for the effect of a predictor variable on an outcome variable, as depicted in Figure 8.10:\n\n\n\n\n\nFigure 8.10: Multiple Mediators.\n\n\n\n8.7.2 Moderation (i.e., Interaction)\n\n8.7.2.1 Overview\nModeration (sometimes called an “interaction”), on the other hand, occurs when there is a variable or condition (M; called a “moderator”) that changes the association between X and Y. That is, the effect of the predictor variable on the outcome variable differs at different levels of the moderator variable. In these cases, X and M work together to have an effect on Y; here X does not have a direct effect on M. Moderation answers the question of, “For whom does X influence Y?” If X influences Y more strongly for some people or in some circumstances, we would say that there is an interaction such that the effect of X on Y depends on M, as depicted in Figure 8.11:\n\n\n\n\n\nFigure 8.11: Moderation.\n\n\nFor example, if the effect of consuming sports drinks on player performance differs for Quarterbacks and Wide Receivers, the interaction could be depicted in Figures 8.12 and 8.13:\n\n\n\n\n\nFigure 8.12: Moderation Example: Path Diagram.\n\n\n\n\n\n\n\nFigure 8.13: Moderation Example: Interaction Graph.\n\n\nAn interaction can be identified visually by non-parallel lines at different levels of the moderator. In this example, the player’s position moderates the effect consuming sports drinks on player performance. In particular, there is a strong positive association between consuming sports drinks and player performance for Wide Receivers (as evidenced by the upward slope of the best-fit regression line), whereas there is no association between consuming sports drinks and player performance for Quarterbacks (as evidenced by the flat line).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-levelsOfMeasurement",
    "href": "research-methods.html#sec-levelsOfMeasurement",
    "title": "8  Research Methods",
    "section": "\n8.8 Levels of Measurement",
    "text": "8.8 Levels of Measurement\nIt is important to know the levels of measurement of your data, because the level(s) of measurement of your data constrain the types of comparisons and analyses that you can meaningfully perform. There are four levels of measurement that any variable can have:\n\nnominal\nordinal\ninterval\nratio\n\nEach is described below:\n\n8.8.1 Nominal\nA variable is considered nominal if it is composed of qualitative classifications. You cannot meaningfully evaluate whether one number in the variable is larger than another number in the variable because higher numbers do not reflect higher levels of the concept. Examples of nominal variables include:\n\nsex (e.g., 1 = male; 2 = female)\nrace (e.g., 1 = American Indian; 2 = Asian; 3 = Black; 4 = Pacific Islander; 5 = White)\nethnicity (e.g., 0 = Non-Hispanic/Latino; 1 = Hispanic/Latino)\nzip code\njersey number\n\nA football player’s jersey number is an example of a nominal variable. A jersey number of 7 is not higher on whatever concept of interest compared to a jersey number of 6.\nTo examine the central tendency of a nominal variable, you can determine the mode, but you cannot calculate a mean or median.\n\n8.8.2 Ordinal\nA variable is considered ordinal if the classifications are ordered. However, ordinal variables do not have equally spaced intervals. Examples of ordinal intervals include:\n\nlikert response scales (e.g., 1 = strongly disagree; 2 = disagree; 3 = neutral; 4 = agree; 5 = strongly agree)\neducational attainment (e.g., 1 = no formal education; 2 = elementary school; 3 = middle school; 4 = high school; 5 = college; 6 = graduate degree)\nacademic grades on A–F scale (e.g., 1 = A; 2 = B; 3 = C; 4 = D; 5 = F)\nplayer rank (1 = 1st; 2 = 2nd; 3 = 3rd, etc.)\n\nA football player’s fantasy rank is an example of an ordinal variable. A player with a fantasy rank of 1 has a higher rank than a player with a rank of 2, but it is not known how far apart each player is—i.e., the intervals do not all reflect the same distance. For instance, the distance between the top-ranked player and the 2nd-best player might be 30 points, whereas the distance between the 2nd-best player and the 3rd-best player might be 2 points.\nTo examine the central tendency of ordinal data, the median and mode are most appropriate; however, the mean may be used (unlike for nominal data).\n\n8.8.3 Interval\nA variable is considered interval if the classifications are ordered (similar to ordinal data) and have equally spaced intervals (unlike ordinal data). However, interval variables do not have a meaningful zero that reflects absence. Examples of interval data include:\n\ntemperature on the Fahrenheit or Celsius scale\ntime of day\n\nFor instance, the temperature difference between 80 and 90 degrees Fahrenheit is the same as the temperature difference between 90 and 100 degrees Fahrenheit. However, 0 degrees Fahrenheit does not reflect absence of temperature/heat.\nInterval data can be meaningfully added or subtracted. For instance, if a game starts at 4 pm and ends at 7 pm, you know the game lasted 3 hours (\\(7 - 4 = 3\\)). However, interval data cannot be meaningfully multiplied or divided. For instance, 100 degrees Fahrenheit is not twice as hot as 50 degrees Fahrenheit.\nTo examine the central tendency of interval data, you can compute the mean, median, or mode.\n\n8.8.4 Ratio\nA variable is considered ratio if the classifications are ordered (similar to ordinal data), have equally spaced intervals (like interval data), and have an absolute zero point that reflects absence of the concept. Examples of ratio data include:\n\ntemperature on the Kelvin scale\nheight\nweight\nage\ndistance\nspeed\nvolume\ntime elapsed\nincome\nstock price\nyears of formal education\npoints in football\n\nFor instance, points in football has order, equally spaced intervals, and an absolute zero—a team cannot score less than zero points, and zero points reflects absence of points (though it could be argued to be interval data because zero points does not reflect absence of skill.)\nRatio data can be meaningfully added, subtracted, multiplied, or divided. A player who weighs 350 pounds weighs twice as much as someone who weighs 175 pounds.\nTo examine the central tendency of ratio data, you can compute the mean, median, or mode.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-psychometrics",
    "href": "research-methods.html#sec-psychometrics",
    "title": "8  Research Methods",
    "section": "\n8.9 Psychometrics",
    "text": "8.9 Psychometrics\nBelow, I provide brief discussions of various aspects of measurement reliability and validity. For more information on these and other aspects of psychometrics, see Petersen (2024) and Petersen (2025).\n\n8.9.1 Measurement Reliability\nThe reliability of a measure’s scores deals with the consistency of measurement. This book focuses on the following types of reliability:\n\ntest–retest reliability\ninter-rater reliability\nintra-rater reliability\ninternal consistency\nparallel-forms reliability\n\nFor more information on these and other aspects of reliability, see https://isaactpetersen.github.io/Principles-Psychological-Assessment/reliability.html (Petersen, 2024, 2025). Reliability is important to consider because random measurement error (i.e., unreliability) weakens the associations between variables (Goodwin & Leech, 2006; Schmidt & Hunter, 1996).\n\n8.9.1.1 Test–Retest Reliability\nTest–retest reliability evaluates the consistency of scores across time. For a construct that is expected to be stable across time (e.g., hand size in adults), we would expect our measurements to be consistent across time. The consistency of scores across time can be examined in terms of relative or absolute test–retest reliability. Relative test–retest reliability—i.e., the consistency of individual differences across time—is commonly evaluated using the coefficient of stability (i.e., the Pearson correlation coefficient). Absolute test–retest reliability—i.e., the absolute consistency of people’s scores across time—is commonly evaluated using the coefficient of repeatability and the coefficient of agreement (i.e., intraclass correlation coefficient).\n\n8.9.1.2 Inter-Rater Reliability\nInter-rater reliability evaluates the consistency of scores across raters. For instance, if we have a strong measure for assessing college players’ aptitude to succeed in the NFL, the measure should yield a similar score for a given player regardless of which (trained) rater (e.g., coach or talent scout) uses it to rate the player. The consistency of scores across raters is commonly evaluated using the intraclass correlation coefficient (for continuous variables) and Cohen’s kappa (\\(\\kappa\\); for categorical variables).\n\n8.9.1.3 Intra-Rater Reliability\nIntra-rater reliability evaluates the consistency of scores within a given rater. If we have a strong measure for assessing college players’ aptitude to succeed in the NFL, the measure should yield a similar score for a given player from the same (trained) rater (e.g., coach or talent scout) each time they rate the same player (assuming the player’s aptitude has not changed). The consistency of scores within raters can be evaluated using similar approaches as those evaluating inter-rater reliability.\n\n8.9.1.4 Internal Consistency\nInternal consistency evaluates the consistency of scores across items within a measure. If we develop a strong questionnaire measure to assess a college players’ aptitude to succeed in the NFL, the scores should be relatively consistent across items. The consistency of scores across items within a measure is commonly evaluated using Cronbach’s alpha (\\(\\alpha\\)) or McDonald’s omega (\\(\\omega\\)).\n\n8.9.1.5 Parallel-Forms Reliability\nParallel-forms reliability evaluates the consistency of scores across different but equivalent forms of a measure. If we develop two equivalent versions of the Wonderlic Contemporary Cognitive Ability Test (Form A and Form B) so that players sitting next to each other do not receive the same items, we would expect a player’s score on Form A would be similar to their score on Form B. Parallel-forms reliability is is commonly evaluated using the coefficient of equivalence (i.e., the Pearson correlation coefficient).\n\n8.9.2 Measurement Validity\nThe validity of a measure’s scores deals with the accuracy of measurement. This book focuses on the following types of validity:\n\nface validity\ncontent validity\n\ncriterion-related validity\n\nconcurrent (criterion-related) validity\npredictive (criterion-related) validity\n\n\nconstruct validity\nconvergent validity\ndiscriminant validity\nincremental validity\necological validity\n\nFor more information on these and other aspects of validity, see https://isaactpetersen.github.io/Principles-Psychological-Assessment/validity.html (Petersen, 2024, 2025).\n\n8.9.2.1 Face Validity\nFace validity evaluates the extent to which a measure “looks like” (on its face) it assesses the construct of interest. For instance, if a measure is developed to assess aptitude of Wide Receivers for the position, it would be considered to have face validity if everyday (lay) people believe that it assesses aptitude for being a successful Wide Receiver.\n\n8.9.2.2 Content Validity\nContent validity evaluates the extent to which the measure assesses the full breadth of the content, as determined by context experts. For the measure to have content validity, it should not have gaps (missing content facets) or intrusions (facets of other constructs). For instance, a strong measure for assessing a player’s aptitude to succeed in the NFL might need to include a player’s speed, strength, size, lateral quickness, etc. If the measure is missing their speed, this would be a content gap. If the measure assesses a construct-irrelevant facet (e.g., their attractiveness), this would be a content intrusion.\n\n8.9.2.3 Criterion-Related Validity\nCriterion-related validity evaluates the extent to which the measure’s scores are related to meaningful variables of interest. Criterion-related validity is commonly evaluated using a Pearson correlation or some form of regression.\nThere are two types of criterion-related validity:\n\nconcurrent (criterion-related) validity\npredictive (criterion-related) validity\n\n\n8.9.2.3.1 Concurrent (Criterion-Related) Validity\nConcurrent criterion-related validity (aka concurrent validity) evaluates the extent to which the measure’s scores are related to meaningful variables of interest assessed at the same point in time. That is, concurrent validity could evaluate whether current player statistics (e.g., passing yards) are associated with their fantasy points.\n\n8.9.2.3.2 Predictive (Criterion-Related) Validity\nPredictive criterion-related validity (aks predictive validity) evaluates the extent to which the measure’s scores are related to meaningful variables of interest that are assessed at a later point in time. For example, predictive validity could evaluate whether scores on the measure we developed to assess a player’s aptitude to succeed in the NFL predicts later performance in the NFL.\n\n8.9.2.4 Construct Validity\nConstruct validity evaluates the extent to which the measure’s scores accurately assess the construct of interest. If we develop a measure with intent to assess aptitude for being a successful Running Back, and it appears to more accurately assess aptitude for being a successful Wide Receiver, then our measure has poor construct validity for assessing aptitude for being a successful Running Back. Construct validity subsumes convergent and discriminant validity, in addition to all of the other forms of measurement validity.\n\n8.9.2.5 Convergent Validity\nConvergent validity evaluates the extent to which the measure’s scores are related to other measures of the same construct. For instance, if we develop a new measure to assess intelligence, its scores should be related to scores from other measures designed to assess intelligence (e.g., Wonderlic Contemporary Cognitive Ability Test).\n\n8.9.2.6 Discriminant Validity\nDiscriminant validity evaluates the extent to which the measure’s scores are unrelated to measures of the different constructs. For instance, if we develop a new measure to assess intelligence, its scores should be less strongly associated with measures of other constructs (e.g., measures of happiness).\n\n8.9.2.7 Incremental Validity\nIncremental validity evaluates the extent to which the measure’s scores provide an increase in predictive accuracy compared to other information that is easily and cheaply available. That is, in order to be useful, a strong measure should tell us something that we did not already know. For instance, if we develop a strong measure of intelligence, it should result in increased predictive accuracy (for success in the NFL) compared to when just relying on the Wonderlic Contemporary Cognitive Ability Test.\n\n8.9.2.8 Ecological Validity\nEcological validity evaluates the extent to which the measures’ scores are indicative of the behavior of a person in the natural environment. For instance, measures of a players’ speed during a game has higher ecological validity (and is more predictive of their performance) than their speed during the NFL Combine (Lyons et al., 2011). For instance, compared to tests of speed, power, and agility at the NFL Combine, collegiate performance is a stronger predictor of performance in the NFL (Lyons et al., 2011). That is, previous sports performance is the best predictor of future performance (for a review, see Den Hartigh et al., 2018).\n\n8.9.3 Reliability vs Validity\nReliability and validity are different but related. Reliability refers to the consistency of scores, whereas accuracy refers to the accuracy of scores. Validity depends on reliability. Reliability is necessary—but insufficient for—validity. That is, consistency is necessary—but insufficient for—accuracy. As depicted in Figure 8.14, a measure can be no more valid than it is reliable. A measure can be consistent but inaccurate; however, a measure cannot be accurate but inconsistent.\n\n\n\n\n\nFigure 8.14: Reliability Versus Validity. From Petersen (2024) and Petersen (2025).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsConclusion",
    "href": "research-methods.html#sec-researchMethodsConclusion",
    "title": "8  Research Methods",
    "section": "\n8.10 Conclusion",
    "text": "8.10 Conclusion\nThere are various types of research designs. Each type of research design differs in the extent to which it supports the ability to draw causal inferences (internal validity) versus the extent to which it supports the ability to identify processes that generalize to the real-world (external validity). In addition, it is important to understand the distinction between sample and population, and the distinction between mediation and moderation. It is also important to consider the levels of measurement used because they constrain the types of analyses that may be performed. In addition, it is important to consider the psychometrics of measurements, including multiple aspects of reliability (consistency) and validity (accuracy).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsSessionInfo",
    "href": "research-methods.html#sec-researchMethodsSessionInfo",
    "title": "8  Research Methods",
    "section": "\n8.11 Session Info",
    "text": "8.11 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 yaml_2.3.10       rmarkdown_2.29   \n [9] knitr_1.50        jsonlite_2.0.0    xfun_0.53         digest_0.6.37    \n[13] rlang_1.1.6       evaluate_1.0.4   \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nEastwell, P. (2014). Understanding hypotheses, predictions, laws, and theories. Science Education Review, 13(1), 16–21. https://eric.ed.gov/?id=EJ1057150\n\n\nGoodwin, L. D., & Leech, N. L. (2006). Understanding correlation: Factors that affect the size of r. The Journal of Experimental Education, 74(3), 249–266. https://doi.org/10.3200/JEXE.74.3.249-266\n\n\nHollenbeck, J. R., & Wright, P. M. (2017). Harking, sharking, and tharking: Making the case for post hoc analysis of scientific data. Journal of Management, 43(1), 5–18. https://doi.org/10.1177/0149206316679487\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J. (2011). On the predictive efficiency of past performance and physical ability: The case of the National Football League. Human Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nSchmidt, F. L., & Hunter, J. E. (1996). Measurement error in psychological research: Lessons from 26 research scenarios. Psychological Methods, 1(2), 199–223. https://doi.org/10.1037/1082-989X.1.2.199\n\n\nVigen, T. (2024). spurious correlations: correlation is not causation. https://www.tylervigen.com/spurious-correlations",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html",
    "href": "basic-statistics.html",
    "title": "9  Basic Statistics",
    "section": "",
    "text": "9.1 Getting Started\nThis chapter provides an overview of foundational statistical concepts.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsGettingStarted",
    "href": "basic-statistics.html#sec-basicStatsGettingStarted",
    "title": "9  Basic Statistics",
    "section": "",
    "text": "9.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"DescTools\")\nlibrary(\"pwr\")\nlibrary(\"pwrss\")\nlibrary(\"WebPower\")\nlibrary(\"grid\")\nlibrary(\"psych\")\nlibrary(\"SimDesign\")\nlibrary(\"tidyverse\")\n\n\n\n9.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_seasonal.RData object in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-descriptiveStatistics",
    "href": "basic-statistics.html#sec-descriptiveStatistics",
    "title": "9  Basic Statistics",
    "section": "\n9.2 Descriptive Statistics",
    "text": "9.2 Descriptive Statistics\nDescriptive statistics are used to describe data. For instance, they may be used to describe the center, spread, or shape of the data. There are various indices of each.\n\n9.2.1 Center\nIndices to describe the center (central tendency) of a variable’s data include:\n\nmean (aka “average”) (i.e., \\(M\\) or \\(\\mu\\))\nmedian\nHodges-Lehmann statistic (aka pseudomedian)\nmode\nweighted mean\nweighted median\n\nThe mean of \\(x\\) (written as: \\(\\bar{x}\\) or \\(\\mu_x\\)) is calculated as in Equation 9.1:\n\\[\n\\bar{x} = \\frac{\\sum x_i}{n} = \\frac{x_1 + x_2 + ... + x_n}{n}\n\\tag{9.1}\\]\n\nCodeexampleValues &lt;- c(0, 0, 10, 15, 20, 30, 1000)\nexampleValues_mean &lt;- mean(exampleValues)\n\n\nThat is, to compute the mean, sum all of the values and divide by the number of values (\\(n\\)). One issue with the mean is that it is sensitive to extreme (outlying) values. For instance, the mean of the values of 0, 0, 10, 15, 20, 30, and 1000 is 153.57.\n\nCodeexampleValues_median &lt;- median(exampleValues)\n\n\nThe median is determined as the value at the 50th percentile (i.e., the value that is higher than 50% of the values and is lower than the other 50% of values). Compared to the mean, the median is less influenced by outliers. The median of the values of 0, 0, 10, 15, 20, 30, and 1000 is 15.\n\nCodeexampleValues_pseudomedian &lt;- DescTools::HodgesLehmann(exampleValues)\n\n\nThe Hodges-Lehmann statistic (aka pseudomedian) is computed as the median of all pairwise means, and it is also robust to outliers. The pseudomedian can be computed using the DescTools::HodgesLehmann() function of the DescTools package (Signorell, 2025). The pseudomedian of the values of 0, 0, 10, 15, 20, 30, and 1000 is 15. The pseudomedian is an example of a robust average—an index of the center of a distribution that is less sensitive to outliers.\n\nCodeexampleValues_mode &lt;- petersenlab::Mode(exampleValues)\n\n\nThe mode is the most common/frequent value. The mode of the values of 0, 0, 10, 15, 20, 30, and 1000 is 0. The petersenlab package (Petersen, 2025a) contains the petersenlab::Mode() function for computing the mode of a set of data.\nIf you want to give some values more weight to others, you can calculate a weighted mean and a weighted median (or other quantile), while assigning a weight to each value. The petersenlab package (Petersen, 2025a) contains various functions for computing the weighted median (i.e., a weighted quantile at the 0.5 quantile, which is equivalent to the 50th percentile) based on Akinshin (2023). Because some projections are outliers, we use a trimmed version of the weighted Harrell-Davis quantile estimator for greater robustness.\nBelow is R code to estimate each:\n\nmean (base::mean()):\n\n\nCodemean(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 50.47589\n\n\n\nmedian (stats::median()):\n\n\nCodemedian(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 29.3\n\n\n\nHodges-Lehmann statistic (aka pseudomedian; DescTools::HodgesLehmann()):\n\n\nCodeDescTools::HodgesLehmann(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 39.15\n\n\n\nmode (petersenlab::Mode()):\n\n\nCodepetersenlab::Mode(player_stats_seasonal$fantasyPoints)\n\n[1] 1\n\n\n\nweighted mean (stats::weighted.mean()):\n\n\nCodeweighted.mean(\n  player_stats_seasonal$fantasyPoints,\n  weights = sample( # randomly generate weights (could specify them manually)\n    x = 1:3,\n    size = length(player_stats_seasonal$fantasyPoints),\n    replace = TRUE),\n  na.rm = TRUE)\n\n[1] 50.47589\n\n\n\nweighted median (petersenlab::wthdquantile()):\n\n\nCodepetersenlab::wthdquantile(\n  player_stats_seasonal$fantasyPoints,\n  w = sample( # randomly generate weights (could specify them manually)\n    x = 1:3,\n    size = length(player_stats_seasonal$fantasyPoints),\n    replace = TRUE),\n  probs = 0.5)\n\n[1] 29.27097\n\n\n\n9.2.2 Spread\nIndices to describe the spread (variability or dispersion) of a variable’s data include:\n\nstandard deviation (\\(s\\) or \\(\\sigma\\))\nvariance (\\(s^2\\) or \\(\\sigma^2\\))\nrange\nminimum and maximum (or some other quantiles)\nweighted quantiles\ninterquartile range (IQR)\nmedian absolute deviation\ncoefficient of variation\n\nThe (sample) variance of \\(x\\) (written as: \\(s^2\\)) is calculated as in Equation 9.2:\n\\[\ns^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1}\n\\tag{9.2}\\]\nwhere \\(x_i\\) is each data point, \\(\\bar{x}\\) is the mean of \\(x\\), and \\(n\\) is the number of data points. The population variance divides by \\(n\\) rather than \\(n-1\\) and is written as \\(\\sigma^2\\).\nThe (sample) standard deviation of \\(x\\) (written as: \\(s\\)) is calculated as in Equation 9.3:\n\\[\ns = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\tag{9.3}\\]\nThe population standard deviation divides by \\(n\\) rather than \\(n-1\\) and is written as \\(\\sigma\\).\nThe range is calculated of \\(x\\) is calculated as in Equation 9.4:\n\\[\n\\text{range} = \\text{maximum} - \\text{minimum}\n\\tag{9.4}\\]\nThe interquartile range (IQR) is calculated as in Equation 9.5:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\tag{9.5}\\]\nwhere \\(Q_3\\) is the score at the third quartile (i.e., 75th percentile), and \\(Q_1\\) is the score at the first quartile (i.e., 25th percentile).\nThe median absolute deviation (MAD) is the median of all deviations from the median, and is calculated as in Equation 9.6:\n\\[\n\\text{MAD} = \\text{median}(|x_i - \\tilde{x}|)\n\\tag{9.6}\\]\nwhere \\(\\tilde{x}\\) is the median of \\(x\\). Compared to the standard deviation, the median absolute deviation is more robust to outliers.\nThe coefficient of variation (CV) accounts for the fact that variables with larger means also tend to have larger standard deviations; it is calculated as the standard deviation divided by the mean, as in Equation 6.1.\nIf you want to give some values more weight to others, you can calculate weighted quantiles, while assigning a weight to each value. The petersenlab package (Petersen, 2025a) contains various functions for computing weighted quantiles based on Akinshin (2023). Because some projections are outliers, we use a trimmed version of the weighted Harrell-Davis quantile estimator for greater robustness.\nBelow is R code to estimate each:\n\nstandard deviation (stats::sd()):\n\n\nCodesd(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 61.26828\n\n\n\nvariance (stats::var()):\n\n\nCodevar(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 3753.803\n\n\n\nrange (base::range()):\n\n\nCoderange(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1]  -7.28 485.10\n\nCodemax(player_stats_seasonal$fantasyPoints, na.rm = TRUE) - min(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 492.38\n\n\n\nminimum (base::min()) and maximum (base::max()):\n\n\nCodemin(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] -7.28\n\nCodemax(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 485.1\n\n\n\nquantiles (in this case, the 10th and 90th quantiles; stats::quantile()):\n\n\nCodequantile(player_stats_seasonal$fantasyPoints, c(0.10, 0.90), na.rm = TRUE)\n\n  10%   90% \n  2.0 120.2 \n\n\n\nweighted quantiles (in this case, the 10th and 90th quantiles; petersenlab::wthdquantile()):\n\n\nCodepetersenlab::wthdquantile(\n  player_stats_seasonal$fantasyPoints,\n  w = sample( # randomly generate weights (could specify them manually)\n    x = 1:3,\n    size = length(player_stats_seasonal$fantasyPoints),\n    replace = TRUE),\n  probs = c(0.10, 0.90))\n\n[1]   2.0000 120.7394\n\n\n\ninterquartile range (IQR; stats::IQR()):\n\n\nCodeIQR(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 61.48\n\n\n\nmedian absolute deviation (stats::mad()):\n\n\nCodemad(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 36.76848\n\n\n\ncoefficient of variation:\n\n\nCodesd(player_stats_seasonal$fantasyPoints, na.rm = TRUE) / mean(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 1.213813\n\n\n\n9.2.3 Shape\nIndices to describe the shape of a variable’s data include:\n\nskewness\nkurtosis\n\nThe skewness represents the degree of asymmetry of a distribution around its mean. Positive skewness (right-skewed) reflects a longer or heavier right-tailed distribution, whereas negative skewness (left-skewed) reflects a longer or heavier left-tailed distribution. Fantasy points tend to be positively skewed.\nThe kurtosis reflects the extent of extreme (outlying) values in a distribution relative to a normal distribution (or bell curve). A mesokurtic distribution (with a kurtosis value near zero) reflects a normal amount of tailedness. Positive kurtosis values reflect a leptokurtic distribution, where there are heavier tails and a sharper peak than a normal distribution, indicating more frequent extreme values. Negative kurtosis values reflect a platykurtic distribution, where there are lighter tails and a flatter peak than a normal distribution, indicating fewer extreme values. Fantasy points tend to have a leptokurtic distribution.\nBelow is R code to estimate each using the psych::skew() and psych::kurtosis() functions of the psych package (Revelle, 2025):\n\nskewness (psych::skew()):\n\n\nCodepsych::skew(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 2.234608\n\n\n\nkurtosis (psych::kurtosi()):\n\n\nCodepsych::kurtosi(player_stats_seasonal$fantasyPoints, na.rm = TRUE)\n\n[1] 6.068551\n\n\n\n9.2.4 Combination\nTo estimate multiple indices of center, spread, and shape of the data, you can use the following code:\n\nCodepsych::describe(player_stats_seasonal[\"fantasyPoints\"])\n\n\n  \n\n\nCodeplayer_stats_seasonal %&gt;% \n  dplyr::select(age, years_of_experience, fantasyPoints) %&gt;% \n  dplyr::summarise(across(\n      everything(),\n      .fns = list(\n        n = ~ length(na.omit(.)),\n        missingness = ~ mean(is.na(.)) * 100,\n        M = ~ mean(., na.rm = TRUE),\n        SD = ~ sd(., na.rm = TRUE),\n        min = ~ min(., na.rm = TRUE),\n        max = ~ max(., na.rm = TRUE),\n        q10 = ~ quantile(., .10, na.rm = TRUE), # 10th quantile\n        q90 = ~ quantile(., .90, na.rm = TRUE), # 90th quantile\n        range = ~ max(., na.rm = TRUE) - min(., na.rm = TRUE),\n        IQR = ~ IQR(., na.rm = TRUE),\n        MAD = ~ mad(., na.rm = TRUE),\n        CV = ~ sd(., na.rm = TRUE) / mean(., na.rm = TRUE),\n        median = ~ median(., na.rm = TRUE),\n        pseudomedian = ~ DescTools::HodgesLehmann(., na.rm = TRUE),\n        mode = ~ petersenlab::Mode(., multipleModes = \"mean\"),\n        skewness = ~ psych::skew(., na.rm = TRUE),\n        kurtosis = ~ psych::kurtosi(., na.rm = TRUE)),\n      .names = \"{.col}.{.fn}\")) %&gt;%\n    tidyr::pivot_longer(\n      cols = everything(),\n      names_to = c(\"variable\",\"index\"),\n      names_sep = \"\\\\.\") %&gt;% \n    tidyr::pivot_wider(\n      names_from = index,\n      values_from = value)\n\n\n  \n\n\n\n\n9.2.5 Interactive Visualization\nYou can use the following interactive graph to visualize various distributions according to a specified center (mean), spread (standard deviation), and shape (skewness and kurtosis). Try varying the standard deviation, skewness, and kurtosis to see the impact of each. The interactive graph was created using the shiny (Chang et al., 2024) and PearsonDS (Becker & Klößner, 2025) packages. The source code for interactive graph is here: https://github.com/isaactpetersen/DistributionVisualizer.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-scoresAndScales",
    "href": "basic-statistics.html#sec-scoresAndScales",
    "title": "9  Basic Statistics",
    "section": "\n9.3 Scores and Scales",
    "text": "9.3 Scores and Scales\nThere are many different types of scores and scales. This book focuses on raw scores and z scores. For information on other scores and scales, including percentile ranks, T scores, standard scores, scaled scores, and stanine scores, see here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/scoresScales.html#scoreTransformation (Petersen, 2025b).\n\n9.3.1 Raw Scores\nRaw scores are the original data on the original metric. Thus, raw scores are considered unstandardized. For example, raw scores that represent the players’ age may range from 20 to 40. Raw scores depend on the construct and unit; thus raw scores may not be comparable across variables.\n\n9.3.2 z Scores\nz scores have a mean of zero and a standard deviation of one. z scores are frequently used to render scores across variables more comparable. Thus, z scores are considered a form of a standardized score.\nz scores are calculated using Equation 9.7:\n\\[\nz = \\frac{x - \\bar{x}}{s_x}\n\\tag{9.7}\\]\nwhere \\(x\\) is the observed score, \\(\\bar{x}\\) is the mean observed score, and \\(s_x\\) is the standard deviation of the observed scores.\nYou can easily convert a variable to a z score using the base::scale() function:\n\nCodescale(variable)\n\n\nWith a standard normal curve, 68% of scores fall within one standard deviation of the mean. 95% of scores fall within two standard deviations of the mean. 99.7% of scores fall within three standard deviations of the mean.\nThe area under a normal curve within one standard deviation of the mean is calculated below using the stats::pnorm() function, which calculates the cumulative density function for a normal curve.\n\nCodestdDeviations &lt;- 1\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.6826895\n\n\nThe area under a normal curve within one standard deviation of the mean is depicted in Figure 9.1.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 9.1: Density of Standard Normal Distribution. The blue region represents the area within one standard deviation of the mean.\n\n\n\n\nThe area under a normal curve within two standard deviations of the mean is calculated below:\n\nCodestdDeviations &lt;- 2\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.9544997\n\n\nThe area under a normal curve within two standard deviations of the mean is depicted in Figure 9.2.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 9.2: Density of Standard Normal Distribution. The blue region represents the area within two standard deviations of the mean.\n\n\n\n\nThe area under a normal curve within three standard deviations of the mean is calculated below:\n\nCodestdDeviations &lt;- 3\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.9973002\n\n\nThe area under a normal curve within three standard deviations of the mean is depicted in Figure 9.3.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 9.3: Density of Standard Normal Distribution. The blue region represents the area within three standard deviations of the mean.\n\n\n\n\nIf you want to determine the z score associated with a particular percentile in a normal distribution, you can use the stats::qnorm() function. For instance, the z score associated with the 37th percentile is:\n\nCodeqnorm(.37)\n\n[1] -0.3318533",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-inferentialStatistics",
    "href": "basic-statistics.html#sec-inferentialStatistics",
    "title": "9  Basic Statistics",
    "section": "\n9.4 Inferential Statistics",
    "text": "9.4 Inferential Statistics\nInferential statistics are used to draw inferences regarding whether there is (a) a difference in level on variable across groups or (b) an association between variables. For instance, inferential statistics may be used to evaluate whether Quarterbacks tend to have longer careers compared to Running Backs. Or, they could be used to evaluate whether number of carries is associated with injury likelihood. To apply inferential statistics, we make use of the null hypothesis (\\(H_0\\)) and the alternative hypothesis (\\(H_1\\)).\n\n9.4.1 Null Hypothesis Significance Testing\nTo draw statistical inferences, the frequentist statistics paradigm leverages null hypothesis significance testing. Frequentist statistics is the most widely used statistical paradigm. However, frequentist statistics is not the only statistical paradigm. Other statistical paradigms exist, including Bayesian statistics, which is based on Bayes’ theorem. This chapter focuses on the frequentist approach to hypothesis testing, known as null hypothesis significance testing. We discuss Bayesian statistics in Chapter 16.\n\n9.4.1.1 Null Hypothesis (\\(H_0\\))\nWhen testing whether there are differences in level across groups on a variable of interest, the null hypothesis (\\(H_0\\)) is that there is no difference in level across groups. For instance, when testing whether Quarterbacks tend to have longer careers compared to Running Backs, the null hypothesis (\\(H_0\\)) is that Quarterbacks do not systematically differ from Running Backs in the length of their career.\nWhen testing whether there is an association between variables, the null hypothesis (\\(H_0\\)) is that there is no association between the variables. For instance, when testing whether number of carries is associated with injury likelihood, the null hypothesis (\\(H_0\\)) is that there is no association between number of carries and injury likelihood.\n\n9.4.1.2 Alternative Hypothesis (\\(H_1\\))\nThe alternative hypothesis (\\(H_1\\)) is the researcher’s hypothesis that they want to evaluate. An alternative hypothesis (\\(H_1\\)) might be directional (i.e., one-sided) or non-directional (i.e., two-sided).\nDirectional hypotheses specify a particular direction, such as which group will have larger scores or which direction (positive or negative) two variables will be associated. Examples of directional hypotheses include:\n\nQuarterbacks have longer careers compared to Running Backs\nNumber of carries is positively associated with injury likelihood\n\nNon-directional hypotheses do not specify a particular direction. For instance, non-directional hypotheses may state that two groups differ but do not specify which group will have larger scores. Or, non-directional hypotheses may state that two variables are associated but do not state what the sign is of the association—i.e., positive or negative. Examples of non-directional hypotheses include:\n\nQuarterbacks differ in the length of their careers compared to Running Backs\nNumber of carries is associated with injury likelihood\n\n9.4.1.3 Statistical Significance\nStatistical significance is the reliability (i.e., consistency) of an effect, and it is evaluated with the p-value. The p-value does not represent the probability that you observed the result by chance. The p-value represents a conditional probability—it examines the probability of one event given another event. In particular, the p-value evaluates the likelihood that you would detect a result as at least as extreme as the one observed (in terms of the magnitude of the difference or of the association) given that the null hypothesis (\\(H_0\\)) is true.\nThis can be expressed in conditional probability notation, \\(P(A | B)\\), which is the probability (likelihood) of event A occurring given that event B occurred (or given condition B).\nThe conditional probability notation for a left-tailed directional test (i.e., Quarterbacks have shorter careers than Running Backs; or number of carries is negatively associated with injury likelihood) is in Equation 9.8.\n\\[\np\\text{-value} = P(T \\le t | H_0)\n\\tag{9.8}\\]\nwhere \\(T\\) is the test statistic of interest (e.g., the distribution of \\(t\\)-, \\(r-\\), or \\(F\\) values, depending on the test) and \\(t\\) is the observed test statistic (e.g., \\(t\\)-, \\(r-\\), or \\(F\\)-coefficient, depending on the test).\nThe conditional probability notation for a right-tailed directional test (i.e., Quarterbacks have longer careers than Running Backs; or number of carries is positively associated with injury likelihood) is in Equation 9.9.\n\\[\np\\text{-value} = P(T \\ge t | H_0)\n\\tag{9.9}\\]\nThe conditional probability notation for a two-tailed non-directional test (i.e., Quarterbacks differ in the length of their careers compared to Running Backs; or number of carries is associated with injury likelihood) is in Equation 9.10.\n\\[\np\\text{-value} = 2 \\times \\text{min}(P(T \\le t | H_0), P(T \\ge t | H_0))\n\\tag{9.10}\\]\nwhere min(a, b) is the smaller number of a and b.\nIf the distribution of the test statistic is symmetric around zero, the p-value for the two-tailed non-directional test simplifies to Equation 9.11.\n\\[\np\\text{-value} = 2 \\times P(T \\ge |t| | H_0)\n\\tag{9.11}\\]\nNevertheless, to be conservative (i.e., to avoid false positive/Type I errors), many researchers use two-tailed p-values regardless whether their hypothesis is one- or two-tailed.\nFor a test of group differences, the p-value evaluates the likelihood that you would observe a difference as large or larger than the one you observed between the groups if there were no systematic difference between the groups in the population, as depicted in Figure 9.4. For instance, when evaluating whether Quarterbacks have longer careers than Running Backs, and you observed a mean difference of 0.03 years, the p-value evaluates the likelihood that you would observe a difference as large or larger than 0.03 years between the groups if, in truth among all Quarterbacks and Running Backs in the NFL, Quarterbacks do not differ from Running Backs in terms of the length of their career.\nCodeset.seed(52242)\n\nnObserved &lt;- 1000\nnPopulation &lt;- 1000000\n\nobservedGroups &lt;- data.frame(\n  score = c(rnorm(nObserved, mean = 47, sd = 3), rnorm(nObserved, mean = 52, sd = 3)),\n  group = as.factor(c(rep(\"Group 1\", nObserved), rep(\"Group 2\", nObserved)))\n)\n\npopulationGroups &lt;- data.frame(\n  score = c(rnorm(nPopulation, mean = 50, sd = 3.03), rnorm(nPopulation, mean = 50, sd = 3)),\n  group = as.factor(c(rep(\"Group 1\", nPopulation), rep(\"Group 2\", nPopulation)))\n)\n\nggplot2::ggplot(\n  data = observedGroups,\n  mapping = aes(\n    x = score,\n    fill = group,\n    color = group\n  )\n) +\n  geom_density(alpha = 0.5) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_fill_manual(values = c(\"red\",\"blue\")) +\n  geom_vline(xintercept = mean(observedGroups$score[which(observedGroups$group == \"Group 1\")])) +\n  geom_vline(xintercept = mean(observedGroups$score[which(observedGroups$group == \"Group 2\")])) +\n  ggplot2::labs(\n    x = \"Score\",\n    y = \"Frequency\",\n    title = \"What is the probability my data would look like this...\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    #plot.title.position = \"plot\"\n    legend.position = \"inside\",\n    legend.margin = margin(0, 0, 0, 0),\n    legend.justification.top = \"left\",\n    legend.justification.left = \"top\",\n    legend.justification.bottom = \"right\",\n    legend.justification.inside = c(1, 1),\n    legend.location = \"plot\")\n\nggplot2::ggplot(\n  data = populationGroups,\n  mapping = aes(\n    x = score,\n    fill = group,\n    color = group\n  )\n) +\n  geom_density(alpha = 0.5) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_fill_manual(values = c(\"red\",\"blue\")) +\n  geom_vline(xintercept = mean(populationGroups$score[which(populationGroups$group == \"Group 1\")])) +\n  geom_vline(xintercept = mean(populationGroups$score[which(populationGroups$group == \"Group 2\")])) +\n  ggplot2::labs(\n    x = \"Score\",\n    y = \"Frequency\",\n    title = \"...if in the population, the groups were really this:\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    #plot.title.position = \"plot\",\n    legend.position = \"inside\",\n    legend.margin = margin(0, 0, 0, 0),\n    legend.justification.top = \"left\",\n    legend.justification.left = \"top\",\n    legend.justification.bottom = \"right\",\n    legend.justification.inside = c(1, 1),\n    legend.location = \"plot\")\n\n\n\n\n\n\n\n\n\n(a) What is the probability my data would look like this…\n\n\n\n\n\n\n\n\n\n(b) …if in the population, the groups were really this?\n\n\n\n\n\n\nFigure 9.4: Interpretation of p-Values When Examining The Differences Between Groups. The vertical black lines reflect the group means.\n\n\nFor a test of whether two variables are associated, the p-value evaluates the likelihood that you would observe an association as strong or stronger than the one you observed if there were no actual association between the variables in the population, as depicted in Figure 9.5. For instance, when evaluating whether number of carries is positively associated with injury likelihood, and you observed a correlation coefficient of \\(r = .25\\) between number of carries and injury likelihood, the p-value evaluates the likelihood that you would observe a correlation as strong or stronger than \\(r = .25\\) between the variables if, in truth among all NFL Running Backs, number of carries is not associated with injury likelihood.\nCodeset.seed(52242)\n\nobservedCorrelation &lt;- 0.9\n\ncorrelations &lt;- data.frame(criterion = rnorm(2000))\ncorrelations$sample &lt;- NA\ncorrelations$sample[1:100] &lt;- complement(correlations$criterion[1:100], observedCorrelation)\ncorrelations$population &lt;- complement(correlations$criterion, 0)\n\nggplot2::ggplot(\n  data = correlations,\n  mapping = aes(\n    x = sample,\n    y = criterion\n  )\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(\n    limits = c(-3.5,3)\n  ) +\n  annotate(\n    x = 0,\n    y = 4,\n    label = paste(\"italic(r) != \", 0, sep = \"\"),\n    parse = TRUE,\n    geom = \"text\",\n    size = 7) + \n  labs(\n    x = \"Predictor Variable\",\n    y = \"Outcome Variable\",\n    title = \"What is the probability my data would look like this...\"\n  ) +\n  theme_classic(\n    base_size = 16) +\n  theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\nggplot2::ggplot(\n  data = correlations,\n  mapping = aes(\n    x = population,\n    y = criterion\n  )\n) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  scale_x_continuous(\n    limits = c(-2.5,2.5)\n  ) +\n  annotate(\n    x = 0,\n    y = 4,\n    label = paste(\"italic(r) == '\", \"0.00\", \"'\", sep = \"\"),\n    parse = TRUE,\n    geom = \"text\",\n    size = 7) + \n  labs(\n    x = \"Predictor Variable\",\n    y = \"Outcome Variable\",\n    title = \"...if in the population, the association was really this:\"\n  ) +\n  theme_classic(\n    base_size = 16) +\n  theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\n(a) What is the probability my data would look like this…\n\n\n\n\n\n\n\n\n\n(b) …if in the population, the association was really this?\n\n\n\n\n\n\nFigure 9.5: Interpretation of p-Values When Examining The Association Between Variables.\n\n\nUsing what is called null-hypothesis significance testing (NHST), we consider an effect to be statistically significant if the p-value is less than some threshold, called the alpha level. In science, we typically want to be conservative because a false positive (i.e., Type I error) is considered more problematic than a false negative (i.e., Type II error). That is, we would rather say an effect does not exist when it really does than to say an effect does exist when it really does not. Thus, we typically set the alpha level to a low value, commonly .05. Then, we would consider an effect to be statistically significant if the p-value is less than .05. That is, there is a small chance (5%; or 1 in 20 times) that we would observe an effect at least as extreme as the effect observed, if the null hypothesis were true. So, you might expect around 5% of tests where the null hypothesis is true to be statistically significant just by chance. We could lower the rate of Type II (i.e., false negative) errors—i.e., we could detect more effects—if we set the alpha level to a higher value (e.g., .10); however, raising the alpha level would raise the possibility of Type I (false positive) errors.\nIf the p-value is less than .05, we reject the null hypothesis (\\(H_0\\)) that there was no difference or association. Thus, we conclude that there was a statistically significant (non-zero) difference or association. If the p-value is greater than .05, we fail to reject the null hypothesis; the difference/association was not statistically significant. Thus, we do not have confidence that there was a difference or association. However, we do not accept the null hypothesis; it could be there we did not observe an effect because we did not have adequate power to detect the effect—e.g., if the effect size was small, the data were noisy, and the sample size was small and/or unrepresentative.\nThere are four general possibilities of decision making outcomes when performing null-hypothesis significance testing:\n\nWe (correctly) reject the null hypothesis when it is in fact false (\\(1 - \\beta\\)). This is a true positive. For instance, we may correctly determine that Quarterbacks have longer careers than Running Backs.\nWe (correctly) fail to reject the null hypothesis when it is in fact true (\\(1 - \\alpha\\)). This is a true negative. For instance, we may correctly determine that Quarterbacks do not have longer careers than Running Backs.\nWe (incorrectly) reject the null hypothesis when it is in fact true (\\(\\alpha\\)). This is a false positive. When performing null hypothesis testing, a false positive is known as a Type I error. For instance, we may incorrectly determine that Quarterbacks have longer careers than Running Backs when, in fact, Quarterbacks and Running Backs do not differ in their career length.\nWe (incorrectly) fail to reject the null hypothesis when it is in fact false (\\(\\beta\\)). This is a false negative. When performing null hypothesis testing, a false negative is known as a Type II error. For instance, we may incorrectly determine that Quarterbacks and Running Backs do not differ in their career length when, in fact, Quarterbacks have longer careers than Running Backs.\n\nA two-by-two confusion matrix for null-hypothesis significance testing is in Figure 9.6.\n\n\n\n\n\nFigure 9.6: A Two-by-Two Confusion Matrix for Null-Hypothesis Significance Testing.\n\n\nIn statistics, power is the probability of detecting an effect, if, in fact, the effect exists. Otherwise said, power is the probability of rejecting the null hypothesis, if, in fact, the null hypothesis is false. Power is influenced by several variables:\n\nthe sample size (N): the larger the N, the greater the power\n\nfor group comparisons, the power depends on the sample size of each group\n\n\nthe effect size: the larger the effect, the greater the power\n\nfor group comparisons, larger effect sizes reflect:\n\nlarger between-group variance, and\nsmaller within-group variance (i.e., strong measurement precision, i.e., reliability)\n\n\n\n\nthe alpha level: the researcher specifies the alpha level (though it is typically set at .05); the higher the alpha level, the greater the power; however, the higher we set the alpha level, the higher the likelihood of Type I errors (false positives)\none- versus two-tailed tests: one-tailed tests have higher power than two-tailed tests\n\nwithin-subject versus between-subject comparisons: within-subject designs tend to have greater power than between-subject designs\n\n\nA plot of statistical power is in Figure 9.7, as adapted from Magnusson (2013) (archived at https://perma.cc/FG3J-85L6).\n\nCodem1 &lt;- 0  # mu H0\nsd1 &lt;- 1.5 # sigma H0\nm2 &lt;- 3.5 # mu HA\nsd2 &lt;- 1.5 # sigma HA\n \nz_crit &lt;- qnorm(1-(0.05/2), m1, sd1)\n \n# set length of tails\nmin1 &lt;- m1-sd1*4\nmax1 &lt;- m1+sd1*4\nmin2 &lt;- m2-sd2*4\nmax2 &lt;- m2+sd2*4\n# create x sequence\nx &lt;- seq(min(min1,min2), max(max1, max2), .01)\n# generate normal dist #1\ny1 &lt;- dnorm(x, m1, sd1)\n# put in data frame\ndf1 &lt;- data.frame(\"x\" = x, \"y\" = y1)\n# generate normal dist #2\ny2 &lt;- dnorm(x, m2, sd2)\n# put in data frame\ndf2 &lt;- data.frame(\"x\" = x, \"y\" = y2)\n \n# Alpha polygon\ny.poly &lt;- pmin(y1,y2)\npoly1 &lt;- data.frame(x=x, y=y.poly)\npoly1 &lt;- poly1[poly1$x &gt;= z_crit, ]\npoly1 &lt;- rbind(poly1, c(z_crit, 0))  # add lower-left corner\n \n# Beta polygon\npoly2 &lt;- df2\npoly2 &lt;- poly2[poly2$x &lt;= z_crit,]\npoly2 &lt;- rbind(poly2, c(z_crit, 0))  # add lower-left corner\n \n# power polygon; 1-beta\npoly3 &lt;- df2\npoly3 &lt;- poly3[poly3$x &gt;= z_crit,]\npoly3 &lt;- rbind(poly3, c(z_crit, 0))  # add lower-left corner\n \n# combine polygons\npoly1$id &lt;- 3 # alpha, give it the highest number to make it the top layer\npoly2$id &lt;- 2 # beta\npoly3$id &lt;- 1 # power; 1 - beta\npoly &lt;- rbind(poly1, poly2, poly3)\npoly$id &lt;- factor(poly$id,  labels = c(\"power\", \"beta\", \"alpha\"))\n\n# plot with ggplot2\nggplot(poly, aes(x,y, fill=id, group=id)) +\n  geom_polygon(show.legend=F, alpha=I(8/10)) +\n  # add line for treatment group\n  geom_line(data=df1, aes(x, y, color = \"H0\", group = NULL, fill = NULL), linewidth = 1.5, show_guide = FALSE) + \n  # add line for treatment group. These lines could be combined into one dataframe.\n  geom_line(data=df2, aes(color = \"HA\", group = NULL, fill = NULL), linewidth = 1.5, show_guide = FALSE) +\n  # add vlines for z_crit\n  geom_vline(xintercept = z_crit, linewidth=1, linetype=\"dashed\") +\n  # change colors\n  scale_color_manual(\n    \"Group\",\n    values = c(\"HA\" = \"#981e0b\", \"H0\" = \"black\")) +\n  scale_fill_manual(\"test\", values= c(\"alpha\" = \"#0d6374\", \"beta\" = \"#be805e\", \"power\"=\"#7cecee\")) +\n  # beta arrow\n  annotate(\"segment\", x = 0.1, y = 0.045, xend = 1.3, yend = 0.01, arrow = grid::arrow(length = unit(0.3, \"cm\")), linewidth = 1) +\n  annotate(\"text\", label=\"beta\", x=0, y=0.05, parse=T, size=8) +\n  # alpha arrow\n  annotate(\"segment\", x = 4, y = 0.043, xend = 3.4, yend = 0.01, arrow = grid::arrow(length = unit(0.3, \"cm\")), linewidth = 1) +\n  annotate(\"text\", label=\"frac(alpha,2)\", x=4.2, y=0.05, parse=T, size=8) +\n  # power arrow\n  annotate(\"segment\", x = 6, y = 0.2, xend = 4.5, yend = 0.15, arrow = grid::arrow(length = unit(0.3, \"cm\")), linewidth = 1) +\n  annotate(\"text\", label = expression(paste(1-beta, \"  (\\\"power\\\")\")), x = 6.1, y = 0.21, parse = TRUE, size = 8) +\n  # H_0 title\n  annotate(\"text\", label = \"H[0]\", x = m1, y = 0.28, parse = TRUE, size = 8) +\n  # H_a title\n  annotate(\"text\", label = \"H[1]\", x = m2, y = 0.28, parse = TRUE, size = 8) +\n  ggtitle(\"Statistical Power\") +\n  # remove some elements\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill=\"white\"),\n    panel.border = element_blank(),\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=22))\n\n\n\n\n\n\nFigure 9.7: Statistical Power (Adapted from Kristoffer Magnusson: https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics; archived at https://perma.cc/FG3J-85L6). The dashed line represents the critical value or threshold.\n\n\n\n\nInteractive visualizations by Kristoffer Magnusson on p-values and null-hypothesis significance testing are below:\n\n\nhttps://rpsychologist.com/pvalue [Magnusson (2021); archived at https://perma.cc/JP9F-9ZVY]\n\nhttps://rpsychologist.com/d3/pdist [Magnusson (2015); archived at https://perma.cc/BE96-8LSJ]\n\nhttps://rpsychologist.com/d3/nhst [Magnusson (2014); archived at https://perma.cc/ZU9A-37F3]\n\nTwelve misconceptions about p-values (Goodman, 2008) are in Table 9.1.\n\n\nTable 9.1: Twelve Misconceptions About p-Values from Goodman (2008). Goodman also provides a discussion about why each statement is false.\n\n\n\n\n\n\n\nNumber\nMisconception\n\n\n\n1\nIf \\(p = .05\\), the null hypothesis has only a 5% chance of being true.\n\n\n2\nA nonsignificant difference (eg, \\(p &gt; .05\\)) means there is no difference between groups.\n\n\n3\nA statistically significant finding is clinically important.\n\n\n4\nStudies with \\(p\\)-values on opposite sides of .05 are conflicting.\n\n\n5\nStudies with the same \\(p\\)-value provide the same evidence against the null hypothesis.\n\n\n6\n\n\\(p = .05\\) means that we have observed data that would occur only 5% of the time under the null hypothesis.\n\n\n7\n\n\\(p = .05\\) and \\(p &lt; .05\\) mean the same thing.\n\n\n8\n\n\\(p\\)-values are properly written as inequalities (e.g., “\\(p \\le .05\\)” when \\(p = .015\\)).\n\n\n9\n\n\\(p = .05\\) means that if you reject the null hypothesis, the probability of a Type I error is only 5%.\n\n\n10\nWith a \\(p = .05\\) threshold for significance, the chance of a Type I error will be 5%.\n\n\n11\nYou should use a one-sided \\(p\\)-value when you don’t care about a result in one direction, or a difference in that direction is impossible.\n\n\n12\nA scientific conclusion or treatment policy should be based on whether or not the \\(p\\)-value is significant.\n\n\n\n\n\n\nThat is, the p-value is not:\n\nthe probability that the effect was due to chance\nthe probability that the null hypothesis is true\nthe size of the effect\nthe importance of the effect\nwhether the effect is true, real, or causal\n\nStatistical significance involves the consistency of an effect/association/difference; it suggests that the association/difference is reliably non-zero. However, just because something is statistically significant does not mean that it is important. For instance, consider that we discover that players who consume sports drink before a game tend to perform better than players who do not (\\(p &lt; .05\\)). However, what if consumption of sports drinks is associated with an average improvement of 0.002 points per game. A small effect such as this might be detectable with a large sample size. This effect would be considered to be reliable/consistent because it is statistically significant. However, such an effect is so small that it results in differences that are not practically important. Thus, in addition to statistical significance, it is also important to consider practical significance.\n\n9.4.2 Practical Significance\nPractical significance deals with how large or important the effect/association/difference is. It is based on the magnitude of the effect, called the effect size. Effect size can be quantified in various ways including:\n\nCohen’s \\(d\\)\n\nStandardized regression coefficient (beta; \\(\\beta\\))\nCorrelation coefficient (\\(r\\))\nCohen’s \\(\\omega\\) (omega)\nCohen’s \\(f\\)\n\nCohen’s \\(f^2\\)\n\nCoefficient of determination (\\(R^2\\))\nEta squared (\\(\\eta^2\\))\nPartial eta squared (\\(\\eta_p^2\\))\n\n\n9.4.2.1 Cohen’s \\(d\\)\n\nCohen’s \\(d\\) is commonly used for evaluating the magnitude of group differences. Cohen’s \\(d\\) is calculated as in Equation 9.12:\n\\[\n\\begin{aligned}\n  d &= \\frac{\\text{mean difference}}{\\text{pooled standard deviation}} \\\\\n   &= \\frac{\\bar{x_1} - \\bar{x_2}}{s} \\\\\n\\end{aligned}\n\\tag{9.12}\\]\nwhere \\(\\bar{x_1}\\) and \\(\\bar{x_2}\\) is the mean for group 1 and group 2, respectively, and:\n\\[\ns = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\tag{9.13}\\]\nwhere \\(n_1\\) and \\(n_2\\) is the sample size of group 1 and group 2, respectively, and \\(s_1\\) and \\(s_2\\) is the standard deviation of group 1 and group 2, respectively.\nCohen’s \\(d\\) is interpreted as the mean of group 1 was _ standard deviation units [larger/smaller] than the mean of group 2.\n\n9.4.2.2 Standardized Regression Coefficient (Beta; \\(\\beta\\))\nThe standardized regression coefficient (beta; \\(\\beta\\)) is used in multiple regression, and is calculated as in Equation 9.14:\n\\[\n\\beta_x = B_x \\times \\frac{s_x}{s_y}\n\\tag{9.14}\\]\nwhere \\(B_x\\) is the unstandardized regression coefficient of the predictor variable \\(x\\) in predicting the outcome variable \\(y\\), \\(s_x\\) is the standard deviation of \\(x\\), and \\(s_y\\) is the standard deviation of \\(y\\). The standardized regression coefficient is interpreted such that, for every 1 standard deviation increase in \\(x\\), there was a _ standard deviation [increase/decrease] in \\(y\\).\n\n9.4.2.3 Correlation Coefficient (\\(r\\))\nThe formula for the correlation coefficient is in Equation 9.22.\n\n9.4.2.4 Cohen’s \\(\\omega\\)\n\nCohen’s \\(\\omega\\) is used in chi-square tests, and is calculated as in Equation 9.15:\n\\[\n\\omega = \\sqrt{\\frac{\\chi^2}{N} - \\frac{df}{N}}\n\\tag{9.15}\\]\nwhere \\(\\chi^2\\) is the chi-square statistic from the test, \\(N\\) is the sample size, and \\(df\\) is the degrees of freedom.\n\n9.4.2.5 Cohen’s \\(f\\)\n\nCohen’s \\(f\\) is commonly used in ANOVA, and is calculated as in Equation 9.16:\n\\[\n\\begin{aligned}\n  f &= \\sqrt{\\frac{R^2}{1 - R^2}} \\\\\n    &= \\sqrt{\\frac{\\eta^2}{1 - \\eta^2}}\n\\end{aligned}\n\\tag{9.16}\\]\n\n9.4.2.6 Cohen’s \\(f^2\\)\n\nCohen’s \\(f^2\\) is commonly used in regression, and is calculated as in Equation 9.17:\n\\[\n\\begin{aligned}\n  f^2 &= \\frac{R^2}{1 - R^2} \\\\\n      &= \\frac{\\eta^2}{1 - \\eta^2}\n\\end{aligned}\n\\tag{9.17}\\]\nTo calculate the effect size of a particular predictor, you can calculate \\(\\Delta f^2\\) as in Equation 9.18:\n\\[\n\\begin{aligned}\n  \\Delta f^2 &= \\frac{R^2_{\\text{model}} - R^2_{\\text{reduced}}}{1 - R^2_{\\text{model}}} \\\\\n             &= \\frac{\\eta^2_{\\text{model}} - \\eta^2_{\\text{reduced}}}{1 - \\eta^2_{\\text{model}}}\n\\end{aligned}\n\\tag{9.18}\\]\nwhere \\(R^2_{\\text{model}}\\) is the \\(R^2\\) of the model with the predictor variable of interest and \\(R^2_{\\text{reduced}}\\) is the \\(R^2\\) of the model without the predictor variable of interest.\n\n9.4.2.7 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome variable that is explained by the predictor variable(s). \\(R^2\\) is commonly used in regression, and is calculated as in Equation 9.19:\n\\[\n\\begin{aligned}\n  R^2 &= 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} \\\\\n      &= 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{\\text{sum of squared residuals}}{\\text{total sum of squares}} \\\\\n      &= \\frac{f^2}{1 + f^2} \\\\\n      &= \\eta^2 \\\\\n      &= \\frac{\\text{variance explained in }y}{\\text{total variance in }y}\n\\end{aligned}\n\\tag{9.19}\\]\nwhere \\(y\\) is the outcome variable, \\(y_i\\) is the observed value of the outcome variable for the \\(i\\)th observation, \\(\\hat{y}_i\\) is the model predicted value for the \\(i\\)th observation, and \\(\\bar{y}\\) is the mean of the observed values of the outcome variable. The total sum of squares is an index of the total variation in the outcome variable.\nThe coefficient of determination typically ranges from 0 to 1, but it can be negative. A negative coefficient of determination indicates that the predicted values are less accurate than using the mean of the actual values as the predicted values (i.e., predicting the mean of the observed values for every case).\n\n9.4.2.8 Eta Squared (\\(\\eta^2\\)) and Partial Eta Squared (\\(\\eta_p^2\\))\nLike \\(R^2\\), eta squared (\\(\\eta^2\\)) reflects the proportion of variance in the dependent variable that is explained by the independent variable(s). \\(\\eta^2\\) is commonly used in ANOVA, and is calculated as in Equation 9.20:\n\\[\n\\begin{aligned}\n  \\eta^2 &= \\frac{SS_{\\text{effect}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{\\text{sum of squared residuals}}{\\text{total sum of squares}} \\\\\n      &= \\frac{f^2}{1 + f^2} \\\\\n      &= R^2\n\\end{aligned}\n\\tag{9.20}\\]\nwhere \\(SS_{\\text{effect}}\\) is the sum of squares for the effect of interest and \\(SS_{\\text{total}}\\) is the total sum of squares.\nPartial eta squared (\\(\\eta_p^2\\)) reflects the proportion of variance in the dependent variable that is explained by the independent variable while controlling for the other independent variables. \\(\\eta_p^2\\) is commonly used in ANOVA, and is calculated as in Equation 9.21:\n\\[\n\\eta_p^2 = \\frac{SS_{\\text{effect}}}{SS_{\\text{effect}} + SS_{\\text{error}}}\n\\tag{9.21}\\]\nwhere \\(SS_{\\text{effect}}\\) is the sum of squares for the effect of interest and \\(SS_{\\text{error}}\\) is the sum of squares for the residual error term.\n\n9.4.2.9 Effect Size Thresholds\nEffect size thresholds (Cohen, 1988; McGrath & Meyer, 2006) for small, medium, and large effect sizes are in Table 9.2.\n\n\nTable 9.2: Effect Size Thresholds for Small, Medium, and Large Effect Sizes.\n\n\n\n\n\n\n\n\n\nEffect Size Index\nSmall\nMedium\nLarge\n\n\n\nCohen’s \\(d\\)\n\n\\(\\ge |.20|\\)\n\\(\\ge |.50|\\)\n\\(\\ge |.80|\\)\n\n\nStandardized regression coefficient (beta; \\(\\beta\\))\n\\(\\ge |.10|\\)\n\\(\\ge |.24|\\)\n\\(\\ge |.37|\\)\n\n\nCorrelation coefficient (\\(r\\))\n\\(\\ge |.10|\\)\n\\(\\ge |.24|\\)\n\\(\\ge |.37|\\)\n\n\nCohen’s \\(\\omega\\)\n\n\\(\\ge .10\\)\n\\(\\ge .30\\)\n\\(\\ge .50\\)\n\n\nCohen’s \\(f\\)\n\n\\(\\ge .10\\)\n\\(\\ge .25\\)\n\\(\\ge .40\\)\n\n\nCohen’s \\(f^2\\)\n\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .16\\)\n\n\nCoefficient of determination (\\(R^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)\n\n\nEta squared (\\(\\eta^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)\n\n\nPartial eta squared (\\(\\eta_p^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-statisticalDecisionTree",
    "href": "basic-statistics.html#sec-statisticalDecisionTree",
    "title": "9  Basic Statistics",
    "section": "\n9.5 Statistical Decision Tree",
    "text": "9.5 Statistical Decision Tree\nA statistical decision tree is a flowchart or decision tree that depicts which statistical test to use given the purpose of analysis, the type of data, etc. An example statistical decision tree is depicted in Figure 9.8.\n\n\n\n\n\nFigure 9.8: A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R. & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including the addition of several statistical tests. Note: “Interval” as a level of measurement includes data with an “interval” or higher level of measurement; thus, it also includes data with a “ratio” level of measurement.\n\n\nThis statistical decision tree can be generally summarized such that associations are examined with the correlation/regression family, and differences are examined with the t-test/ANOVA family, as depicted in Figure 9.9.\n\n\n\n\n\nFigure 9.9: Summary of A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure.\n\n\nHowever, many statistical tests can be re-formulated in a regression framework, as in Figure 9.10.\n\n\n\n\n\nFigure 9.10: A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure, Re-Formulated in a Regression Framework. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R. & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including re-formulating the tests in a regression framework.\n\n\nBoth associations and differences can be examined with the regression family, which greatly simplifies our summary of the statistical decision tree, as depicted in Figure 9.11.\n\n\n\n\n\nFigure 9.11: Summary of A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure.\n\n\nThus, in most cases, the regression framework can be used to examine most questions regarding associations between variables or differences between groups. Indeed, t-test, ANOVA, and regression are all special cases of the general linear model, which in turn, is a special case of mixed models without random effects (Brauer & Curtin, 2018).\nJackson-Wood (2017) provides an online, interactive statistical decision tree to help you decide which statistical analysis to use: https://www.statsflowchart.co.uk",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-statisticalTests",
    "href": "basic-statistics.html#sec-statisticalTests",
    "title": "9  Basic Statistics",
    "section": "\n9.6 Statistical Tests",
    "text": "9.6 Statistical Tests\n\n9.6.1 t-Test\nThere are several t-tests:\n\none-sample t-test\ntwo-samples t-test\n\nindependent samples t-test\npaired samples t-test\n\n\n\nA one-sample t-test is used to evaluate whether a sample mean differs systematically from a particular value. The null hypothesis is that the sample mean does not differ systematically from the pre-specified value. The alternative hypothesis is that the sample mean differs systematically from the pre-specified value. For instance, let’s say you want to test out a new draft strategy. You could participate in a mock draft and draft players using the new strategy. Then, you could use a one-sample t-test to evaluate whether your new draft strategy yields players with more projected points than the average of players’ projected points for other teams.\nTwo-samples t-tests are used to test for differences between scores of two groups. If the two groups are independent, the independent samples t-test is used. If the two groups involve paired samples, the paired samples t-test is used. The null hypothesis is that the mean of group 1 does not differ systematically from the mean of group 2. The alternative hypothesis is that the mean of group 1 differs systematically from the mean of group 2. For instance, you could use an independent-samples t-test if you want to examine whether Quarterbacks tend to have have longer careers than Running Backs. By contrast, you could use a paired samples t-test if you want to examine whether Quarterbacks tend to score more points in the second year of their contract compared to their rookie year, because the same subjects were assessed twice (i.e., a within-subject design).\n\n9.6.2 Analysis of Variance\nAnalysis of variance (ANOVA) allows examining whether groups differ systematically as a function of one or more factors. There are multiple variants of ANOVA:\n\none-way ANOVA\nfactorial ANOVA\nrepeated measures ANOVA (RM-ANOVA)\nmultivariate ANOVA (MANOVA)\n\nLike two-samples t-tests, ANOVA allows examining whether groups differ as a function of an independent variable. However, unlike a t-test, ANOVA allows examining multiple independent variables and more than two groups. The null hypothesis is that the the groups’ mean value does not differ systematically. The alternative hypothesis is that the groups’ mean value differs systematically.\nA one-way ANOVA examines whether two or more groups differ as a function of an independent variable. For instance, you could use a one-way ANOVA to evaluate if you want to evaluate whether multiple positions differ in their length of career. Factorial ANOVA examines whether two or more groups differ as a function of multiple independent variables. For instance, you could use factorial ANOVA to evaluate whether one’s length of career depends on one’s position and weight. Repeated measures ANOVA examines whether scores differ across repeated measures (e.g., across time) for the same participants. For instance, you could use repeated-measures ANOVA to evaluate whether rookies score more points as the season progresses. Multivariate ANOVA examines whether multiple dependent variables differ as a function of one or more factor(s). For instance, you could use MANOVA to evaluate whether one’s contract length and pay differ as a function of one’s position.\nANOVA has key limitations. ANOVA cannot handle continuous independent or predictor variables that vary within units (Brauer & Curtin, 2018). In addition, ANOVA poorly handles missing data—it uses listwise deletion and thus excludes participants with missing values on any of the independent or dependent variables (Brauer & Curtin, 2018).\n\n9.6.3 Correlation\nCorrelation examines the association between a predictor and outcome variable. The null hypothesis is that the the two variables are not associated. The alternative hypothesis is that the two variables are associated.\nThe Pearson correlation coefficient (\\(r\\)) is calculated as in Equation 9.22:\n\\[\n\\begin{aligned}\n  r &= \\frac{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}\n{\\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n  && \\quad \\text{the numerator is the covariance; the denominator is the standard deviations} \\\\\n    &= \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}\n{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n  && \\quad \\text{$n-1$ cancels out in numerator and denominator (typical formula for Pearson correlation)} \\\\\n    &= \\frac{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{s_x \\cdot s_y}\n  && \\quad \\text{adding back $n-1$, replacing denominator from line 1 with standard deviation symbols} \\\\\n    &= \\frac{\\text{Cov}(x, y)}{s_x \\cdot s_y}\n  && \\quad \\text{replacing numerator with covariance notation} \\\\\n    &= \\frac{\\sum z_x z_y}{n - 1}\n\\end{aligned}\n\\tag{9.22}\\]\nwhere \\(x\\) is the predictor variable, \\(y\\) is the outcome variable, \\(\\text{Cov}(x, y)\\) is the covariance between \\(x\\) and \\(y\\), \\(s_x\\) is the standard deviation of \\(x\\), \\(s_y\\) is the standard deviation of \\(y\\), \\(n\\) is the sample size, \\(z_x\\) is the z scores for \\(x\\), and \\(z_y\\) is the z scores for \\(y\\).\n\n9.6.4 (Multiple) Regression\nRegression, like correlation, examines the association between a predictor and outcome variable. However, unlike correlation, regression allows multiple predictor variables.\nRegression with a single predictor takes the form in Equation 11.2. A regression line is depicted in Figure 11.29. Multiple regression (i.e., regression with multiple predictors) takes the form in Equation 11.3.\nThe null hypothesis is that the the predictor variable(s) are not associated with the outcome variable. The alternative hypothesis is that the predictor variable(s) are associated with the outcome variable.\n\n9.6.5 Chi-Square Test\nThere are two primary types of chi-square tests:\n\nchi-square goodness-of-fit test\nchi-square test for association (aka test of independence)\n\nThe chi-square goodness-of-fit test evaluates whether a set of categorical data came from a specified distribution. The null hypothesis is that the data came from the specified distribution. The alternative hypothesis is that the data did not come from the specified distribution.\nThe chi-square test for association evaluates whether two categorical variables are associated. The null hypothesis is that the two variables are not associated. The alternative hypothesis is that the two variables are associated.\n\n9.6.6 Formulating Statistical Tests in Terms of Partitioned Variance\nMany statistical tests can be formulated in terms of partitioned variance.\nFor instance, the t statistic from the independent-samples t-test and the F statistic from ANOVA can be thought of as the ratio of between-group variance to within-group variance, as in Equation 9.23:\n\\[\nt \\text{ or } F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\n\\tag{9.23}\\]\nThe correlation coefficient can be thought of as the ratio of shared variance (i.e., covariance) to total variance, as in Equation 9.24:\n\\[\nr = \\frac{\\text{shared variance}}{\\text{total variance}}\n\\tag{9.24}\\]\nThe coefficient of determination (\\(R^2\\)) is the proportion of variance in the outcome variable that is explained by the predictor variables. \\(\\eta^2\\) is the proportion of variance in the dependent variable that is explained by the independent variables. The coefficient of determination and \\(\\eta^2\\) can be expressed as the ratio of variance explained in the outcome or dependent variable to the total variance in the outcome or dependent variable, as in Equation 9.25:\n\\[\nR^2 \\text{ or } \\eta^2 = \\frac{\\text{variance explained in the outcome variable}}{\\text{total variance in the outcome variable}}\n\\tag{9.25}\\]\n\n9.6.7 Critical Value\nThe critical value is the test value for a given test, above which the effect is considered to be statistically significant. The critical value for statistical significance for each test can be determined based on the degrees of freedom and alpha level. The degrees of freedom (df) refer to the number of values in the calculation of a test statistic that are free to vary.\n\nCodealpha &lt;- .05\nN &lt;- 200\nnGroup1 &lt;- 150\nnGroup2 &lt;- 150\nnumGroups &lt;- 4\nnumLevelsFactorA &lt;- 3\nnumLevelsFactorB &lt;- 4\nnumMeasurements &lt;- 4\nnumPredictors &lt;- 5\nnumCategories &lt;- 6\nnumRows &lt;- 5\nnumColumns &lt;- 2\n\n\n\n9.6.7.1 One-Sample t-Test\nFor a one-sample t-test, the degrees of freedom is in Equation 9.26:\n\\[\ndf = N - 1\n\\tag{9.26}\\]\nwhere \\(N\\) is sample size.\n\nCodedf_oneSampleTtest &lt;- N - 1\n\n\nOne-tailed test (stats::qt()):\n\nCodeqt(1 - alpha, df_oneSampleTtest)\n\n[1] 1.652547\n\n\nTwo-tailed test (stats::qt()):\n\nCodeqt(1 - alpha/2, df_oneSampleTtest)\n\n[1] 1.971957\n\n\n\n9.6.7.2 Independent-Samples t-Test\nFor an independent-samples t-test, the degrees of freedom is in Equation 9.27:\n\\[\ndf = n_1 + n_2 - 2\n\\tag{9.27}\\]\nwhere \\(n_1\\) is the sample size of group 1 and \\(n_2\\) is the sample size of group 2.\n\nCodedf_independentSamplesTtest &lt;- nGroup1 + nGroup2 - 2\n\n\nOne-tailed test (stats::qt()):\n\nCodeqt(1 - alpha, df_independentSamplesTtest)\n\n[1] 1.649983\n\n\nTwo-tailed test (stats::qt()):\n\nCodeqt(1 - alpha/2, df_independentSamplesTtest)\n\n[1] 1.967957\n\n\n\n9.6.7.3 Paired-Samples t-Test\nFor a paired-samples t-test, the degrees of freedom is in Equation 9.28:\n\\[\ndf = N - 1\n\\tag{9.28}\\]\nwhere \\(N\\) is sample size (i.e., the number of paired observations).\n\nCodedf_pairedSamplesTtest &lt;- N - 1\n\n\nOne-tailed test (stats::qt()):\n\nCodeqt(1 - alpha, df_pairedSamplesTtest)\n\n[1] 1.652547\n\n\nTwo-tailed test (stats::qt()):\n\nCodeqt(1 - alpha/2, df_pairedSamplesTtest)\n\n[1] 1.971957\n\n\n\n9.6.7.4 One-Way ANOVA\nFor a one-way ANOVA, the degrees of freedom is in Equation 9.29:\n\\[\n\\begin{aligned}\n  df_\\text{between} &= g - 1 \\\\\n  df_\\text{within} &= N - g\n\\end{aligned}\n\\tag{9.29}\\]\nwhere \\(N\\) is sample size and \\(g\\) is the number of groups.\n\nCodedf_betweenOneWayANOVA &lt;- numGroups - 1\ndf_withinOneWayANOVA &lt;- N - numGroups\n\n\nOne-tailed test (stats::qf()):\n\nCodeqf(1 - alpha, df_betweenOneWayANOVA, df_withinOneWayANOVA)\n\n[1] 2.650677\n\n\nTwo-tailed test (stats::qf()):\n\nCodeqf(1 - alpha/2, df_betweenOneWayANOVA, df_withinOneWayANOVA)\n\n[1] 3.183378\n\n\n\n9.6.7.5 Factorial ANOVA\nFor a factorial two-way ANOVA, the degrees of freedom is in Equation 9.30:\n\\[\n\\begin{aligned}\n  df_\\text{Factor A} &= a - 1 \\\\\n  df_\\text{Factor B} &= b - 1 \\\\\n  df_\\text{Interaction} &= (a - 1)(b - 1) \\\\\n  df_\\text{error} &= ab(N - 1)\n\\end{aligned}\n\\tag{9.30}\\]\nwhere \\(N\\) is sample size, \\(a\\) is the number of levels for factor A, and \\(b\\) is the number of levels for factor B.\n\nCodedf_factorA &lt;- numLevelsFactorA - 1\ndf_factorB &lt;- numLevelsFactorB - 1\ndf_interaction &lt;- df_factorA * df_factorB\ndf_error &lt;- numLevelsFactorA * numLevelsFactorB * (N - 1)\n\n\nFactor A (one-tailed test; stats::qf()):\n\nCodeqf(1 - alpha, df_factorA, df_error)\n\n[1] 2.999494\n\n\nFactor B (one-tailed test; stats::qf()):\n\nCodeqf(1 - alpha, df_factorB, df_error)\n\n[1] 2.608629\n\n\nInteraction (one-tailed test; stats::qf()):\n\nCodeqf(1 - alpha, df_interaction, df_error)\n\n[1] 2.102376\n\n\nFactor A (two-tailed test; stats::qf()):\n\nCodeqf(1 - alpha/2, df_factorA, df_error)\n\n[1] 3.694584\n\n\nFactor B (two-tailed test; stats::qf()):\n\nCodeqf(1 - alpha/2, df_factorB, df_error)\n\n[1] 3.121587\n\n\nInteraction (two-tailed test; stats::qf()):\n\nCodeqf(1 - alpha/2, df_interaction, df_error)\n\n[1] 2.413504\n\n\n\n9.6.7.6 Repeated Measures ANOVA\nFor a repeated measures ANOVA, the degrees of freedom is in Equation 9.31:\n\\[\n\\begin{aligned}\n  df_1 &= T - 1 \\\\\n  df_2 &= (T - 1)(N - 1)\n\\end{aligned}\n\\tag{9.31}\\]\nwhere \\(N\\) is sample size and \\(T\\) is the number of measurements (i.e., the number of levels of the within-person factor: e.g., timepoints or conditions).\n\nCodedf1_RMANOVA &lt;- numMeasurements - 1\ndf2_RMANOVA &lt;- (numMeasurements - 1) * (N - 1)\n\n\nOne-tailed test (stats::qf()):\n\nCodeqf(1 - alpha, df1_RMANOVA, df2_RMANOVA)\n\n[1] 2.619828\n\n\nTwo-tailed test (stats::qf()):\n\nCodeqf(1 - alpha/2, df1_RMANOVA, df2_RMANOVA)\n\n[1] 3.138017\n\n\n\n9.6.7.7 Correlation\nFor a correlation, the degrees of freedom is in Equation 9.32:\n\\[\ndf = N - 2\n\\tag{9.32}\\]\nwhere \\(N\\) is sample size.\n\nCodedf_correlation &lt;- N - 2\n\n\nOne-tailed test (stats::qt()):\n\nCodeqt(1 - alpha, df_correlation)\n\n[1] 1.652586\n\n\nTwo-tailed test (stats::qt()):\n\nCodeqt(1 - alpha/2, df_correlation)\n\n[1] 1.972017\n\n\n\n9.6.7.8 Multiple Regression\nFor multiple regression, the degrees of freedom is in Equation 9.33:\n\\[\n\\begin{aligned}\n  df_1 &= p \\\\\n  df_2 &= N - p - 1\n\\end{aligned}\n\\tag{9.33}\\]\nwhere \\(N\\) is sample size and \\(p\\) is the number of predictors.\n\nCodedf1_regression &lt;- numPredictors\ndf2_regression &lt;- N - numPredictors - 1\n\n\nOne-tailed test (stats::qf()):\n\nCodeqf(1 - alpha, df1_regression, df2_regression)\n\n[1] 2.260647\n\n\nTwo-tailed test (stats::qf()):\n\nCodeqf(1 - alpha/2, df1_regression, df2_regression)\n\n[1] 2.63243\n\n\n\n9.6.7.9 Chi-Square Goodness-of-Fit Test\nFor the chi-square goodness-of-fit test, the degrees of freedom is in Equation 9.34:\n\\[\ndf = c - 1\n\\tag{9.34}\\]\nwhere \\(c\\) is the number of categories.\n\nCodedf_chisquareGOF &lt;- numCategories - 1\n\n\nOne-tailed test (stats::qchisq()):\n\nCodeqchisq(1 - alpha, df_chisquareGOF)\n\n[1] 11.0705\n\n\nTwo-tailed test (stats::qchisq()):\n\nCodeqchisq(1 - alpha/2, df_chisquareGOF)\n\n[1] 12.8325\n\n\n\n9.6.7.10 Chi-Square Test for Association\nFor the chi-square test for association, the degrees of freedom is in Equation 9.35:\n\\[\ndf = (r - 1) \\times (c - 1)\n\\tag{9.35}\\]\nwhere \\(r\\) is the number of rows in the contingency table and \\(c\\) is the number of columns in the contingency table.\n\nCodedf_chisquareAssociation &lt;- (numRows - 1) * (numColumns - 1)\n\n\nOne-tailed test (stats::qchisq()):\n\nCodeqchisq(1 - alpha, df_chisquareAssociation)\n\n[1] 9.487729\n\n\nTwo-tailed test (stats::qchisq()):\n\nCodeqchisq(1 - alpha/2, df_chisquareAssociation)\n\n[1] 11.14329\n\n\n\n9.6.8 Statistical Power\nAs described above, statistical power is the probability of detecting an effect, if, in fact, the effect exists. Statistical power for a given test can be calculated based on three factors:\n\neffect size\nsample size\nalpha level\n\nKnowing any three of the following, you can calculate the fourth: statistical power, effect size, sample size, and alpha level. Below is R code for calculating power for each of various statistical tests (i.e., a power analysis). We use the pwr (Champely, 2020) and pwrss (Bulus, 2023) packages to estimate statistical power. For free point-and-click software for calculating statistical power, see G*Power: https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html\n\nCodepower &lt;- .8\neffectSize_d &lt;- .5\neffectSize_r &lt;- .24\neffectSize_beta &lt;- .24\neffectSize_f &lt;- .25\neffectSize_fSquared &lt;- .06\neffectSize_omega &lt;- .3\n\n\nWhen designing a study, it is important to consider power and the sample size needed to detect the hypothesized effect size. If your sample size is too small and you do not detect an effect (i.e., \\(p &gt; .05\\)), you do not know whether your failure to detect the effect was because a) the effect does not exist, or b) the effect exists but you did not have enough power to detect it.\n\n9.6.8.1 One-Sample t-Test\nSolving for statistical power achieved (given effect size, sample size, and alpha level; pwr::pwr.t.test()):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9999998\n    alternative = two.sided\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 200\n              d = 0.1990655\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n9.6.8.2 Independent-Samples t-Test\n\n9.6.8.2.1 Balanced Group Sizes\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level; pwr::pwr.t.test()):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9987689\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 200\n              d = 0.2808267\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n9.6.8.2.2 Unbalanced Group Sizes\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level; pwr::pwr.t2n.test()):\n\nCodepwr::pwr.t2n.test(\n  n1 = nGroup1,\n  n2 = nGroup2,\n  d = effectSize_d,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 150\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9907677\n    alternative = two.sided\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  power = power,\n  n1 = nGroup1,\n  d = effectSize_d,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 40.22483\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  power = power,\n  n1 = nGroup1,\n  n2 = nGroup2,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 150\n              d = 0.3245459\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n9.6.8.3 Paired-Samples t-Test\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level; pwr::pwr.t.test()):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9999998\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 200\n              d = 0.1990655\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n9.6.8.4 One-Way ANOVA\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level; pwr::pwr.anova.test()):\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999962\n\nNOTE: n is number in each group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level; pwr::pwr.anova.test()):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level; pwr::pwr.anova.test()):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.117038\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n9.6.8.4.1 Simulation\nThe power analysis code above assumes the groups are of equal size (i.e., a balanced design). If the design is unbalanced (i.e., there are different numbers of participants in each group), it may be necessary to conduct a power analysis via a simulation. Below is an example of evaluating the statistical power for detecting an effect unbalanced designs via simulation using the SimDesign (Chalmers, 2025; Chalmers & Adkins, 2020) package. We discuss simulation more in Chapter 24.\n\n9.6.8.4.1.1 1. Design Matrix\nFirst, we set up the design matrix using the SimDesign::createDesign() function:\n\nCodeDesign &lt;- SimDesign::createDesign(\n  nGroup1 = c(10),\n  nGroup2 = c(25, 50, 100),\n  f       = c(.10, .25, .40), # small, medium, large effect sizes\n  alpha   = 0.05\n)\n\n\n\n9.6.8.4.1.2 2. Generate\nSecond, we set up the Generate() function:\n\nCodeGenerate &lt;- function(condition, fixed_objects = NULL) {\n  with(condition, {\n    # Means for each group\n    mean1 &lt;- 0\n    mean2 &lt;- f * sqrt((nGroup1 + nGroup2) / 2)\n    \n    # Generate data\n    group1 &lt;- rnorm(nGroup1, mean = mean1, sd = 1)\n    group2 &lt;- rnorm(nGroup2, mean = mean2, sd = 1)\n    \n    data.frame(\n      value = c(group1, group2),\n      group = factor(rep(c(\"Group1\", \"Group2\"), \n                         c(nGroup1, nGroup2)))\n    )\n  })\n}\n\n\n\n9.6.8.4.1.3 3. Analyse\nThird, we set up the Analyse() function:\n\nCodeAnalyse &lt;- function(condition, dat, fixed_objects = NULL) {\n  aov_result &lt;- aov(value ~ group, data = dat)\n  p_value &lt;- summary(aov_result)[[1]][[\"Pr(&gt;F)\"]][1]\n  \n  data.frame(\n    p_value  = p_value,\n    rejectH0 = as.numeric(p_value &lt; condition$alpha)\n  )\n}\n\n\n\n9.6.8.4.1.4 4. Summarise\nFourth, we set up the Summarise() function:\n\nCodeSummarise &lt;- function(condition, results, fixed_objects = NULL) {\n  # Estimated power = proportion of rejections\n  Power &lt;- mean(results$rejectH0)\n  \n  data.frame(Power = Power)\n}\n\n\n\n9.6.8.4.1.5 5. Run the Simulation\nFifth, we run the simulation using the SimDesign::runSimulation() function:\n\nCodemonteCarloSim_results &lt;- SimDesign::runSimulation(\n  design       = Design,\n  replications = 1000,\n  generate     = Generate,\n  analyse      = Analyse,\n  summarise    = Summarise,\n  seed = genSeeds(Design, iseed = 52242), # for reproducibility\n  parallel = TRUE # for faster (parallel) processing\n)\n\n\n\n9.6.8.4.1.6 Simulation Results\nHere are the simulation results:\n\nCodemonteCarloSim_results\n\n\n  \n\n\n\n\n9.6.8.5 Factorial ANOVA\nThe power analysis code below assumes the groups are of equal size (i.e., a balanced design). If the design is unbalanced (i.e., there are different numbers of participants in each group), it may be necessary to conduct a power analysis via a simulation. See Section 9.6.8.4 for an example power analysis simulation for one-way ANOVA.\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level; pwr::pwr.anova.test()):\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999238\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999962\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 1\n\nNOTE: n is number in each group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 32.05196\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 200\n              f = 0.1270373\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.117038\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 200\n              f = 0.09889082\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n9.6.8.6 Repeated Measures ANOVA\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level; WebPower::wp.rmanova()):\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.8484718\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.8536292\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.6756298\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    178.3971 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    175.7692 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    253.2369 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2358259  4  4     1  0.05   0.8\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2342726  4  4     1  0.05   0.8\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2817486  4  4     1  0.05   0.8\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\n\n9.6.8.7 Correlation\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level; pwr::pwr.r.test()):\n\nCodepwr::pwr.r.test(\n  n = N,\n  r = effectSize_r,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 200\n              r = 0.24\n      sig.level = 0.05\n          power = 0.9310138\n    alternative = two.sided\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.r.test(\n  power = power,\n  r = effectSize_r,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 133.1299\n              r = 0.24\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.r.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 200\n              r = 0.1965767\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n9.6.8.8 Multiple Regression\nSolving for statistical power achieved (given effect size, sample size, and alpha level; pwr::pwr.f2.test()):\n\nCodepwr::pwr.f2.test(\n  f2 = effectSize_fSquared,\n  sig.level = alpha,\n  u = numPredictors,\n  v = N - numPredictors - 1)\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 194\n             f2 = 0.06\n      sig.level = 0.05\n          power = 0.7548031\n\nCodepwrss::pwrss.t.reg(\n  n = N,\n  beta1 = effectSize_beta,\n  k = numPredictors,\n  alpha = alpha,\n  alternative = \"not equal\")\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.936 \n  n = 200 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 194 \n Non-centrality parameter = 3.496 \n Type I error rate = 0.05 \n Type II error rate = 0.064 \n\n\nSolving for sample size needed (given effect size, power, and alpha level)—\\(v = N - \\text{numberOfPredictors} - 1\\); thus, \\(N = v + \\text{numberOfPredictors} + 1\\):\n\nCodemultipleRegressionSampleSizeModel &lt;- pwr::pwr.f2.test(\n  power = power,\n  f2 = effectSize_fSquared,\n  sig.level = alpha,\n  u = numPredictors)\n\nmultipleRegressionSampleSizeModel\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 213.3947\n             f2 = 0.06\n      sig.level = 0.05\n          power = 0.8\n\nCodevNeeded &lt;- multipleRegressionSampleSizeModel$v\nsampleSizeNeeded &lt;- vNeeded + numPredictors + 1\nsampleSizeNeeded\n\n[1] 219.3947\n\nCodepwrss::pwrss.t.reg(\n  power = power,\n  beta1 = effectSize_beta,\n  k = numPredictors,\n  alpha = alpha,\n  alternative = \"not equal\")\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 131 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 124.427 \n Non-centrality parameter = 2.823 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.f2.test(\n  power = power,\n  sig.level = alpha,\n  u = numPredictors,\n  v = N - numPredictors - 1)\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 194\n             f2 = 0.06597765\n      sig.level = 0.05\n          power = 0.8\n\n\n\n9.6.8.9 Chi-Square Goodness-of-Fit Test\nSolving for statistical power achieved (given effect size, sample size, and alpha level; pwr::pwr.chisq.test()):\n\nCodepwr::pwr.chisq.test(\n  N = N,\n  w = effectSize_omega,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 200\n             df = 5\n      sig.level = 0.05\n          power = 0.9269225\n\nNOTE: N is the number of observations\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  w = effectSize_omega,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 142.529\n             df = 5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  N = N,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.2532543\n              N = 200\n             df = 5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\n\n9.6.8.10 Chi-Square Test for Association\nSolving for statistical power achieved (given effect size, sample size, and alpha level; pwr::pwr.chisq.test()):\n\nCodepwr::pwr.chisq.test(\n  N = N,\n  w = effectSize_omega,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 200\n             df = 4\n      sig.level = 0.05\n          power = 0.9431195\n\nNOTE: N is the number of observations\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  w = effectSize_omega,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 132.6143\n             df = 4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  N = N,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.2442875\n              N = 200\n             df = 4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\n\n9.6.8.11 Multilevel Modeling\nPower analysis for multilevel modeling approaches is more complicated than it is for other statistical analyses, such as correlation, multiple regression, t-tests, ANOVA, etc.\nThere are free web applications for calculating power in multilevel modeling:\n\n\nhttps://aguinis.shinyapps.io/ml_power (Mathieu et al., 2012)\n\n\nhttps://koumurayama.shinyapps.io/summary_statistics_based_power (Murayama et al., 2022)\n\n\nhttps://webpower.psychstat.org/wiki/models/index (Zhang & Yuan, 2018)\n\n\n9.6.8.12 Path Analysis, Factor Analysis, and Structural Equation Modeling\nPower analysis for latent variable modeling approaches like structural equation modeling (SEM) is more complicated than it is for other statistical analyses, such as correlation, multiple regression, t-tests, ANOVA, etc.\nI provide an example of power analysis in SEM using Monte Carlo simulation in R here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/sem.html#monteCarloPowerAnalysis (Petersen, 2025b).\nThere are also free web applications for calculating power in SEM:\n\n\nhttps://sjak.shinyapps.io/power4SEM (Jak et al., 2020)\n\n\nhttps://sempower.shinyapps.io/sempower (Moshagen & Bader, 2024)\n\n\nhttps://yilinandrewang.shinyapps.io/pwrSEM (Wang & Rhemtulla, 2021)\n\n\nhttps://webpower.psychstat.org/wiki/models/index (Zhang & Yuan, 2018)\n\n\n9.6.8.13 Mediation and Moderation\nThere are free tools for calculating power for tests of mediation and moderation:\n\n\nhttps://schoemanna.shinyapps.io/mc_power_med (Schoemann et al., 2017)\n\n\nhttps://www.causalevaluation.org/power-analysis.html (web application: https://powerupr.shinyapps.io/index/) (Ataneka et al., 2023), based on the PowerUpR package (Bulus et al., 2021)\n\n\nhttps://webpower.psychstat.org/wiki/models/index (Zhang & Yuan, 2018)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-ethicsReportingStatistics",
    "href": "basic-statistics.html#sec-ethicsReportingStatistics",
    "title": "9  Basic Statistics",
    "section": "\n9.7 Ethics of Reporting Statistics",
    "text": "9.7 Ethics of Reporting Statistics\nIt is important to report statistics ethically and responsibly. People can use statistics to lie (Huff, 2023). It is important to present statistics fairly and accurately even if they are not convenient to your preferred narrative. In addition, it is important to consider practical significance in addition to statistical significance.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsConclusion",
    "href": "basic-statistics.html#sec-basicStatsConclusion",
    "title": "9  Basic Statistics",
    "section": "\n9.8 Conclusion",
    "text": "9.8 Conclusion\nDescriptive statistics are used to describe the data, including the center, spread, or shape of data. Inferential statistics are used to draw inferences regarding differences between groups or associations between variables. Null hypothesis signficance testing is a framework for inferential statistics, in which there is a null hypothesis and alternative hypothesis. The null hypothesis is that there is no difference between groups or that there is no association between variables. Statistical significance is evaluated with a \\(p\\)-value, which represents the probability of obtaining a result at least as extreme as the result observed if the null hypothesis is true. Effects with p-values less than .05 are considered statistically significant. However, it is also important to consider practical significance and effect sizes. When designing a study, it is important to consider statistical power and the sample size needed to detect the hypothesized effect size. You can determine a study’s power based on the effect size, sample size, and alpha level.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsSessionInfo",
    "href": "basic-statistics.html#sec-basicStatsSessionInfo",
    "title": "9  Basic Statistics",
    "section": "\n9.9 Session Info",
    "text": "9.9 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] grid      parallel  stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n [9] ggplot2_3.5.2     tidyverse_2.0.0   SimDesign_2.20.0  psych_2.5.6      \n[13] WebPower_0.9.4    PearsonDS_1.3.2   lavaan_0.6-19     lme4_1.1-37      \n[17] Matrix_1.7-3      MASS_7.3-65       pwrss_0.3.1       pwr_1.3-0        \n[21] DescTools_0.99.60 petersenlab_1.2.0\n\nloaded via a namespace (and not attached):\n [1] Rdpack_2.6.4        DBI_1.2.3           mnormt_2.1.1       \n [4] pbapply_1.7-4       gridExtra_2.3       gld_2.6.7          \n [7] testthat_3.2.3      readxl_1.4.5        rlang_1.1.6        \n[10] magrittr_2.0.3      e1071_1.7-16        compiler_4.5.1     \n[13] mgcv_1.9-3          vctrs_0.6.5         reshape2_1.4.4     \n[16] quadprog_1.5-8      pkgconfig_2.0.3     fastmap_1.2.0      \n[19] backports_1.5.0     labeling_0.4.3      pbivnorm_0.6.0     \n[22] rmarkdown_2.29      sessioninfo_1.2.3   tzdb_0.5.0         \n[25] haven_2.5.5         nloptr_2.2.1        xfun_0.53          \n[28] jsonlite_2.0.0      cluster_2.1.8.1     R6_2.6.1           \n[31] stringi_1.8.7       RColorBrewer_1.1-3  parallelly_1.45.1  \n[34] boot_1.3-31         rpart_4.1.24        brio_1.1.5         \n[37] cellranger_1.1.0    Rcpp_1.1.0          knitr_1.50         \n[40] future.apply_1.20.0 audio_0.1-11        base64enc_0.1-3    \n[43] R.utils_2.13.0      timechange_0.3.0    splines_4.5.1      \n[46] nnet_7.3-20         tidyselect_1.2.1    rstudioapi_0.17.1  \n[49] yaml_2.3.10         codetools_0.2-20    listenv_0.9.1      \n[52] lattice_0.22-7      plyr_1.8.9          withr_3.0.2        \n[55] evaluate_1.0.4      foreign_0.8-90      future_1.67.0      \n[58] proxy_0.4-27        pillar_1.11.0       checkmate_2.3.3    \n[61] stats4_4.5.1        reformulas_0.4.1    generics_0.1.4     \n[64] mix_1.0-13          hms_1.1.3           scales_1.4.0       \n[67] rootSolve_1.8.2.4   minqa_1.2.8         globals_0.18.0     \n[70] xtable_1.8-4        class_7.3-23        glue_1.8.0         \n[73] Hmisc_5.2-3         lmom_3.2            tools_4.5.1        \n[76] data.table_1.17.8   beepr_2.0           Exact_3.3          \n[79] fs_1.6.6            mvtnorm_1.3-3       mitools_2.4        \n[82] rbibutils_2.3       colorspace_2.1-1    nlme_3.1-168       \n[85] htmlTable_2.4.3     Formula_1.2-5       cli_3.6.5          \n[88] expm_1.0-0          viridisLite_0.4.2   gtable_0.3.6       \n[91] R.methodsS3_1.8.2   digest_0.6.37       progressr_0.15.1   \n[94] htmlwidgets_1.6.4   farver_2.1.2        htmltools_0.5.8.1  \n[97] R.oo_1.27.1         lifecycle_1.0.4     httr_1.4.7         \n\n\n\n\n\n\nAkinshin, A. (2023). Weighted quantile estimators. arXiv. https://doi.org/10.48550/arXiv.2304.07265\n\n\nAtaneka, A., Kelcey, B., Dong, N., Bulus, M., & Bai, F. (2023). PowerUp R Shiny app (v. 0.9) manual. https://www.causalevaluation.org/uploads/7/3/3/6/73366257/r_shinnyapp_manual_0.9.pdf\n\n\nBecker, M., & Klößner, S. (2025). PearsonDS: Pearson distribution system. https://doi.org/10.32614/CRAN.package.PearsonDS\n\n\nBrauer, M., & Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://doi.org/10.1037/met0000159\n\n\nBulus, M. (2023). pwrss: Statistical power and sample size calculation tools. https://doi.org/10.32614/CRAN.package.pwrss\n\n\nBulus, M., Dong, N., Kelcey, B., & Spybrook, J. (2021). PowerUpR: Power analysis tools for multilevel randomized experiments. https://doi.org/10.32614/CRAN.package.PowerUpR\n\n\nChalmers, R. P. (2025). SimDesign: Structure for organizing Monte Carlo simulation designs. https://doi.org/10.32614/CRAN.package.SimDesign\n\n\nChalmers, R. P., & Adkins, M. C. (2020). Writing effective and reliable Monte Carlo simulations with the SimDesign package. The Quantitative Methods for Psychology, 16(4), 248–280. https://doi.org/10.20982/tqmp.16.4.p248\n\n\nChampely, S. (2020). pwr: Basic functions for power analysis. https://doi.org/10.32614/CRAN.package.pwr\n\n\nChang, W., Cheng, J., Allaire, J., Sievert, C., Schloerke, B., Xie, Y., Allen, J., McPherson, J., Dipert, A., & Borges, B. (2024). shiny: Web application framework for R. https://doi.org/10.32614/CRAN.package.shiny\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates, Publishers. https://doi.org/10.4324/9780203771587\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGoodman, S. (2008). A dirty dozen: Twelve p-value misconceptions. Seminars in Hematology, 45(3), 135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nHuff, D. (2023). How to lie with statistics. Penguin UK.\n\n\nJackson-Wood, M. (2017). statistical test flowchart. https://www.statsflowchart.co.uk\n\n\nJak, S., Jorgensen, T. D., Verdam, M. G. E., Oort, F. J., & Elffers, L. (2020). Analytical power calculations for structural equation modeling: A tutorial and shiny app. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01479-0\n\n\nMagnusson, K. (2013). Creating a typical textbook illustration of statistical power using either ggplot or base graphics. https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics\n\n\nMagnusson, K. (2014). Understanding statistical power and significance testing. https://rpsychologist.com/d3/nhst/\n\n\nMagnusson, K. (2015). Distribution of p-values when comparing two groups. https://rpsychologist.com/d3/pdist\n\n\nMagnusson, K. (2021). Understanding p-values through simulations. https://rpsychologist.com/pvalue\n\n\nMathieu, J. E., Aguinis, H., Culpepper, S. A., & Chen, G. (2012). Understanding and estimating the power to detect cross-level interaction effects in multilevel modeling. Journal of Applied Psychology, 97(5), 951–966. https://doi.org/10.1037/a0028380\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree: The case of r and d. Psychological Methods, 11(4), 386–401. https://doi.org/10.1037/1082-989X.11.4.386\n\n\nMoshagen, M., & Bader, M. (2024). semPower: General power analysis for structural equation models. Behavior Research Methods, 56(4), 2901–2922. https://doi.org/10.3758/s13428-023-02254-7\n\n\nMurayama, K., Usami, S., & Sakaki, M. (2022). Summary-statistics-based power analysis: A new and practical method to determine sample size for mixed-effects modeling. Psychological Methods, 27(6), 1014–1038. https://doi.org/10.1037/met0000330\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRevelle, W. (2025). psych: Procedures for psychological, psychometric, and personality research. https://doi.org/10.32614/CRAN.package.psych\n\n\nSchoemann, A. M., Boulton, A. J., & Short, S. D. (2017). Determining power and sample size for simple and complex mediation models. Social Psychological and Personality Science, 8(4), 379–386. https://doi.org/10.1177/1948550617715068\n\n\nSignorell, A. (2025). DescTools: Tools for descriptive statistics. https://doi.org/10.32614/CRAN.package.DescTools\n\n\nWang, Y. A., & Rhemtulla, M. (2021). Power analysis for parameter estimation in structural equation modeling: A discussion and tutorial. Advances in Methods and Practices in Psychological Science, 4(1), 1–17. https://doi.org/10.1177/2515245920918253\n\n\nZhang, Z., & Yuan, K.-H. (2018). Practical statistical power analysis using WebPower and R. ISDSA Press. https://doi.org/10.35566/power",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "10  Correlation Analysis",
    "section": "",
    "text": "10.1 Getting Started\nThis chapter provides an overview of correlation analysis.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationGettingStarted",
    "href": "correlation.html#sec-correlationGettingStarted",
    "title": "10  Correlation Analysis",
    "section": "",
    "text": "10.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"XICOR\")\nlibrary(\"psych\")\nlibrary(\"DescTools\")\nlibrary(\"psych\")\nlibrary(\"correlation\")\nlibrary(\"effectsize\")\nlibrary(\"GGally\")\nlibrary(\"ggridges\")\nlibrary(\"tidyverse\")\n\n\n\n10.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationOverview",
    "href": "correlation.html#sec-correlationOverview",
    "title": "10  Correlation Analysis",
    "section": "\n10.2 Overview of Correlation",
    "text": "10.2 Overview of Correlation\nCorrelation is an index of the association between variables. Covariance is the association between variables and in an unstandardized metric that differs for variables with different scales. By contrast, correlation is in a standardized metric that does not differ for variables with different scales. When examining the association between variables that are interval or ratio levels of measurement, Pearson correlation is used. When examining the association between variables that are ordinal in level of measurement, Spearman correlation is used. Pearson correlation is an index of the linear association between variables. If a nonlinear association is present, other indices like xi [\\(\\xi\\); Chatterjee (2021)] and distance correlation coefficients are better suited to detect the association.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-correlation-coefficient-r",
    "href": "correlation.html#the-correlation-coefficient-r",
    "title": "10  Correlation Analysis",
    "section": "\n10.3 The Correlation Coefficient (\\(r\\))",
    "text": "10.3 The Correlation Coefficient (\\(r\\))\nThe formula for the (Pearson) correlation coefficient is in Equation 9.22. As noted in Section 9.6.6, the correlation coefficient can be thought of as the ratio of shared variance (i.e., covariance) to total variance, as in Equation 9.24.\nThe correlation coefficient ranges from −1.0 to +1.0. The correlation coefficient (\\(r\\)) tells you two things: (1) the direction (sign) of the association (positive or negative) and (2) the magnitude of the association. If the correlation coefficient is positive, the association is positive. If the correlation coefficient is negative, the association is negative. If the association is positive, as X increases, Y increases (or conversely, as X decreases, Y decreases). If the association is negative, as X increases, Y decreases (or conversely, as X decreases, Y increases). The smaller the absolute value of the correlation coefficient (i.e., the closer the \\(r\\) value is to zero), the weaker the association and the flatter the slope of the best-fit line in a scatterplot. The larger the absolute value of the correlation coefficient (i.e., the closer the absolute value of the \\(r\\) value is to one), the stronger the association. See Figure 10.1 for a range of different correlation coefficients and what some example data may look like for each direction and strength of association.\n\nCodeset.seed(52242)\ncorrelations &lt;- data.frame(criterion = rnorm(1000))\n\ncorrelations$v1 &lt;- complement(correlations$criterion, -1)\ncorrelations$v2 &lt;- complement(correlations$criterion, -.9)\ncorrelations$v3 &lt;- complement(correlations$criterion, -.8)\ncorrelations$v4 &lt;- complement(correlations$criterion, -.7)\ncorrelations$v5 &lt;- complement(correlations$criterion, -.6)\ncorrelations$v6 &lt;- complement(correlations$criterion, -.5)\ncorrelations$v7 &lt;- complement(correlations$criterion, -.4)\ncorrelations$v8 &lt;- complement(correlations$criterion, -.3)\ncorrelations$v9 &lt;- complement(correlations$criterion, -.2)\ncorrelations$v10 &lt;-complement(correlations$criterion, -.1)\ncorrelations$v11 &lt;-complement(correlations$criterion, 0)\ncorrelations$v12 &lt;-complement(correlations$criterion, .1)\ncorrelations$v13 &lt;-complement(correlations$criterion, .2)\ncorrelations$v14 &lt;-complement(correlations$criterion, .3)\ncorrelations$v15 &lt;-complement(correlations$criterion, .4)\ncorrelations$v16 &lt;-complement(correlations$criterion, .5)\ncorrelations$v17 &lt;-complement(correlations$criterion, .6)\ncorrelations$v18 &lt;-complement(correlations$criterion, .7)\ncorrelations$v19 &lt;-complement(correlations$criterion, .8)\ncorrelations$v20 &lt;-complement(correlations$criterion, .9)\ncorrelations$v21 &lt;-complement(correlations$criterion, 1)\n\npar(mfrow = c(7,3), mar = c(1, 0, 1, 0))\n\n# -1.0\nplot(correlations$criterion, correlations$v1, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v1)$estimate, 2))))\nabline(lm(v1 ~ criterion, data = correlations), col = \"black\")\n\n# -.9\nplot(correlations$criterion, correlations$v2, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v2)$estimate, 2))))\nabline(lm(v2 ~ criterion, data = correlations), col = \"black\")\n\n# -.8\nplot(correlations$criterion, correlations$v3, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v3)$estimate, 2))))\nabline(lm(v3 ~ criterion, data = correlations), col = \"black\")\n\n# -.7\nplot(correlations$criterion, correlations$v4, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v4)$estimate, 2))))\nabline(lm(v4 ~ criterion, data = correlations), col = \"black\")\n\n# -.6\nplot(correlations$criterion, correlations$v5, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v5)$estimate, 2))))\nabline(lm(v5 ~ criterion, data = correlations), col = \"black\")\n\n# -.5\nplot(correlations$criterion, correlations$v6, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v6)$estimate, 2))))\nabline(lm(v6 ~ criterion, data = correlations), col = \"black\")\n\n# -.4\nplot(correlations$criterion, correlations$v7, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v7)$estimate, 2))))\nabline(lm(v7 ~ criterion, data = correlations), col = \"black\")\n\n# -.3\nplot(correlations$criterion, correlations$v8, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v8)$estimate, 2))))\nabline(lm(v8 ~ criterion, data = correlations), col = \"black\")\n\n# -.2\nplot(correlations$criterion, correlations$v9, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v9)$estimate, 2))))\nabline(lm(v9 ~ criterion, data = correlations), col = \"black\")\n\n# -.1\nplot(correlations$criterion, correlations$v10, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v10)$estimate, 2))))\nabline(lm(v10 ~ criterion, data = correlations), col = \"black\")\n\n# 0.0\nplot(correlations$criterion, correlations$v11, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v11)$estimate, 2))))\nabline(lm(v11 ~ criterion, data = correlations), col = \"black\")\n\n# 0.1\nplot(correlations$criterion, correlations$v12, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v12)$estimate, 2))))\nabline(lm(v12 ~ criterion, data = correlations), col = \"black\")\n\n# 0.2\nplot(correlations$criterion, correlations$v13, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v13)$estimate, 2))))\nabline(lm(v13 ~ criterion, data = correlations), col = \"black\")\n\n# 0.3\nplot(correlations$criterion, correlations$v14, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v14)$estimate, 2))))\nabline(lm(v14 ~ criterion, data = correlations), col = \"black\")\n\n# 0.4\nplot(correlations$criterion, correlations$v15, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v15)$estimate, 2))))\nabline(lm(v15 ~ criterion, data = correlations), col = \"black\")\n\n# 0.5\nplot(correlations$criterion, correlations$v16, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v16)$estimate, 2))))\nabline(lm(v16 ~ criterion, data = correlations), col = \"black\")\n\n# 0.6\nplot(correlations$criterion, correlations$v17, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v17)$estimate, 2))))\nabline(lm(v17 ~ criterion, data = correlations), col = \"black\")\n\n# 0.7\nplot(correlations$criterion, correlations$v18, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v18)$estimate, 2))))\nabline(lm(v18 ~ criterion, data = correlations), col = \"black\")\n\n# 0.8\nplot(correlations$criterion, correlations$v19, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v19)$estimate, 2))))\nabline(lm(v19 ~ criterion, data = correlations), col = \"black\")\n\n# 0.9\nplot(correlations$criterion, correlations$v20, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v20)$estimate, 2))))\nabline(lm(v20 ~ criterion, data = correlations), col = \"black\")\n\n# 1.0\nplot(correlations$criterion, correlations$v21, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v21)$estimate, 2))))\nabline(lm(v21 ~ criterion, data = correlations), col = \"black\")\n\ninvisible(dev.off()) #par(mfrow = c(1,1))\n\n\n\n\n\n\nFigure 10.1: Correlation Coefficients.\n\n\n\n\nSee Figure 10.2 for the interpretation of the magnitude and direction (sign) of various correlation coefficients.\n\nCodelibrary(\"patchwork\")\n\nset.seed(52242)\ncorrelations2 &lt;- data.frame(criterion = rnorm(15))\n\ncorrelations2$v1 &lt;- complement(correlations2$criterion, -1)\ncorrelations2$v2 &lt;- complement(correlations2$criterion, -.9)\ncorrelations2$v3 &lt;- complement(correlations2$criterion, -.8)\ncorrelations2$v4 &lt;- complement(correlations2$criterion, -.7)\ncorrelations2$v5 &lt;- complement(correlations2$criterion, -.6)\ncorrelations2$v6 &lt;- complement(correlations2$criterion, -.5)\ncorrelations2$v7 &lt;- complement(correlations2$criterion, -.4)\ncorrelations2$v8 &lt;- complement(correlations2$criterion, -.3)\ncorrelations2$v9 &lt;- complement(correlations2$criterion, -.2)\ncorrelations2$v10 &lt;-complement(correlations2$criterion, -.1)\ncorrelations2$v11 &lt;-complement(correlations2$criterion, 0)\ncorrelations2$v12 &lt;-complement(correlations2$criterion, .1)\ncorrelations2$v13 &lt;-complement(correlations2$criterion, .2)\ncorrelations2$v14 &lt;-complement(correlations2$criterion, .3)\ncorrelations2$v15 &lt;-complement(correlations2$criterion, .4)\ncorrelations2$v16 &lt;-complement(correlations2$criterion, .5)\ncorrelations2$v17 &lt;-complement(correlations2$criterion, .6)\ncorrelations2$v18 &lt;-complement(correlations2$criterion, .7)\ncorrelations2$v19 &lt;-complement(correlations2$criterion, .8)\ncorrelations2$v20 &lt;-complement(correlations2$criterion, .9)\ncorrelations2$v21 &lt;-complement(correlations2$criterion, 1)\n\n# -1.0\np1 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v1\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Perfect Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−1.0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.9\np2 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v2\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Strong Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.9\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.5\np3 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v6\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Moderate Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.5\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.2\np4 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v9\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Weak Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.2\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.0\np5 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v11\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"No Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.2\np6 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v13\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Weak Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".2\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.5\np7 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v16\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Moderate Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".5\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.9\np8 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v20\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Strong Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".9\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 1.0\np9 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v21\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Perfect Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"1.0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n  plot_layout(\n    ncol = 3,\n    heights = 1,\n    widths = 1)\n\n\n\n\n\n\nFigure 10.2: Interpretation of the Magnitude and Direction (Sign) of Correlation Coefficients.\n\n\n\n\nAn interactive visualization by Magnusson (2023) on interpreting correlations is at the following link: https://rpsychologist.com/correlation/ (archived at https://perma.cc/G8YR-VCM4)\nKeep in mind that the (Pearson) correlation examines the strength of the linear association between two variables. If the association between two variables is nonlinear, the (Pearson) correlation provides the strength of the linear trend and may not provide a meaningful index of the strength of the association between the variables. For instance, Anscombe’s quartet includes four sets of data that have nearly identical basic descriptive statistics (see Tables 10.1 and 10.2), including the same bivariate correlation, yet have very different distributions and whose association takes very different forms (see Figure 10.3).\n\n\nTable 10.1: Anscombe’s Quartet\n\n\n\nx1\ny1\nx2\ny2\nx3\ny3\nx4\ny4\n\n\n\n10\n8.04\n10\n9.14\n10\n7.46\n8\n6.58\n\n\n8\n6.95\n8\n8.14\n8\n6.77\n8\n5.76\n\n\n13\n7.58\n13\n8.74\n13\n12.74\n8\n7.71\n\n\n9\n8.81\n9\n8.77\n9\n7.11\n8\n8.84\n\n\n11\n8.33\n11\n9.26\n11\n7.81\n8\n8.47\n\n\n14\n9.96\n14\n8.10\n14\n8.84\n8\n7.04\n\n\n6\n7.24\n6\n6.13\n6\n6.08\n8\n5.25\n\n\n4\n4.26\n4\n3.10\n4\n5.39\n19\n12.50\n\n\n12\n10.84\n12\n9.13\n12\n8.15\n8\n5.56\n\n\n7\n4.82\n7\n7.26\n7\n6.42\n8\n7.91\n\n\n5\n5.68\n5\n4.74\n5\n5.73\n8\n6.89\n\n\n\n\n\n\n\n\nTable 10.2: Descriptive Statistics of Anscombe’s Quartet\n\n\n\nProperty\nValue\n\n\n\nSample size\n11\n\n\nMean of X\n9.0\n\n\nMean of Y\n~7.5\n\n\nVariance of X\n11.0\n\n\nVariance of Y\n~4.1\n\n\nEquation of regression line\nY = 3 + 0.5X\n\n\nStandard error of slope\n0.118\n\n\nOne-sample t-statistic\n4.24\n\n\nSum of squares of X\n110.0\n\n\nRegression sum of squares\n27.50\n\n\nResidual sum of squares of Y\n13.75\n\n\nCorrelation coefficient\n.816\n\n\nCoefficient of determination\n.67\n\n\n\n\n\n\n\nCodeanscombe\n\n\n\nCodepar(mfrow = c(2,2))\n\nplot(\n  anscombe$x1,\n  anscombe$y1,\n  xlab = \"x\",\n  ylab = \"y\",\n  xlim = c(0,20),\n  ylim = c(0,15),\n  main = \n    substitute(\n      paste(italic(r), \" = \", x, sep = \"\"),\n      list(x = round(cor.test(x = anscombe$x1, y = anscombe$y1)$estimate, 2))))\n\nabline(lm(\n  y1 ~ x1,\n  data = anscombe),\n  col = \"black\",\n  lty = 2)\n\nplot(\n  anscombe$x2,\n  anscombe$y2,\n  xlab = \"x\",\n  ylab = \"y\",\n  xlim = c(0,20),\n  ylim = c(0,15),\n  main = substitute(\n    paste(italic(r), \" = \", x, sep = \"\"),\n    list(x = round(cor.test(x = anscombe$x2, y = anscombe$y2)$estimate, 2))))\n\nabline(lm(\n  y2 ~ x2,\n  data = anscombe),\n  col = \"black\",\n  lty = 2)\n\nplot(\n  anscombe$x3,\n  anscombe$y3,\n  xlab = \"x\",\n  ylab = \"y\",\n  xlim = c(0,20),\n  ylim = c(0,15),\n  main = substitute(\n    paste(italic(r), \" = \", x, sep = \"\"),\n    list(x = round(cor.test(x = anscombe$x3, y = anscombe$y3)$estimate, 2))))\n\nabline(lm(\n  y3 ~ x3,\n  data = anscombe),\n  col = \"black\",\n  lty = 2)\n\nplot(\n  anscombe$x4,\n  anscombe$y4,\n  xlab = \"x\",\n  ylab = \"y\",\n  xlim = c(0,20),\n  ylim = c(0,15),\n  main = substitute(\n    paste(italic(r), \" = \", x, sep = \"\"),\n    list(x = round(cor.test(x = anscombe$x4, y = anscombe$y4)$estimate, 2))))\n\nabline(lm(\n  y4 ~ x4,\n  data = anscombe),\n  col = \"black\",\n  lty = 2)\n\n\n\n\n\n\nFigure 10.3: Anscombe’s Quartet.\n\n\n\n\nAlso note that, although the (Pearson) correlation reflects the strength of the (linear) association between two variables, it does not reflect the (unstandardized) slope of that association, as depicted in Figure 10.4.\n\n\n\n\n\nFigure 10.4: Several sets of (\\(x\\), \\(y\\)) points, with the (Pearson) correlation coefficient of \\(x\\) and \\(y\\) for each set. The (Pearson) correlation reflects the strength and direction of a linear association (top row), but not the slope of that association (middle), nor many aspects of nonlinear associations (bottom). N.B.: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of \\(y\\) is zero. From https://en.wikipedia.org/wiki/File:Correlation_examples2.svg.\n\n\nThe Pearson correlation coefficient can be greatly impacted by the extent of variability in the data, differences in the shapes of the two distributions, nonlinearity, outliers, characteristics of the sample, and measurement error (Goodwin & Leech, 2006). These factors tend to have greater impact when the sample size is smaller (compared to when the sample size is larger). The Pearson correlation coefficient tends to be smaller when:\n\nthere is low variability in one or both variables (i.e., restricted range)\nthe two distributions show dissimilar shapes (in terms of skewness and kurtosis)\nthe underlying association is nonlinear, such that Pearson correlation does not capture the association\n\noutliers weaken the apparent linear trend\nthe association differs across subgroups of the sample\nthe data contain random measurement error\n\nBy contrast, the association between two variables can be artificially inflated by factors such as:\n\n\noutliers that happen to fall along a linear trend\nsystematic measurement error (e.g., common method bias)\nthird variable confounding",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationHeightFantasyPoints",
    "href": "correlation.html#sec-correlationHeightFantasyPoints",
    "title": "10  Correlation Analysis",
    "section": "\n10.4 Example: Player Height and Fantasy Points",
    "text": "10.4 Example: Player Height and Fantasy Points\nIs there an association between a player’s height and the number of fantasy points they score? We can examine this possibility separately by position.\nFirst, let’s examine descriptive statistics of height and fantasy points (height is measured in inches):\n\nCodeplayer_stats_seasonal %&gt;% \n  dplyr::select(height, fantasyPoints) %&gt;% \n  dplyr::summarise(across(\n      everything(),\n      .fns = list(\n        n = ~ length(na.omit(.)),\n        missingness = ~ mean(is.na(.)) * 100,\n        M = ~ mean(., na.rm = TRUE),\n        SD = ~ sd(., na.rm = TRUE),\n        min = ~ min(., na.rm = TRUE),\n        max = ~ max(., na.rm = TRUE),\n        q10 = ~ quantile(., .10, na.rm = TRUE), # 10th quantile\n        q90 = ~ quantile(., .90, na.rm = TRUE), # 90th quantile\n        range = ~ max(., na.rm = TRUE) - min(., na.rm = TRUE),\n        IQR = ~ IQR(., na.rm = TRUE),\n        MAD = ~ mad(., na.rm = TRUE),\n        CV = ~ sd(., na.rm = TRUE) / mean(., na.rm = TRUE),\n        median = ~ median(., na.rm = TRUE),\n        pseudomedian = ~ DescTools::HodgesLehmann(., na.rm = TRUE),\n        mode = ~ petersenlab::Mode(., multipleModes = \"mean\"),\n        skewness = ~ psych::skew(., na.rm = TRUE),\n        kurtosis = ~ psych::kurtosi(., na.rm = TRUE)),\n      .names = \"{.col}.{.fn}\")) %&gt;%\n    tidyr::pivot_longer(\n      cols = everything(),\n      names_to = c(\"variable\",\"index\"),\n      names_sep = \"\\\\.\") %&gt;% \n    tidyr::pivot_wider(\n      names_from = index,\n      values_from = value)\n\n\n  \n\n\n\n\n10.4.1 Covariance\nThe covariance is an unstandardized index of the association between two variables. It represents the average product of two variables’ deviations from their respective means. The formula for (sample) covariance is in Equation 10.1 and is the numerator to the formula for correlation.\n\\[\n\\begin{aligned}\n  \\text{Cov}(x, y) &= \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\\\\n  &= r_{xy} \\cdot s_x \\cdot s_y\n\\end{aligned}\n\\tag{10.1}\\]\nwhere \\(n\\) is the sample size, \\(x\\) is the predictor variable, \\(y\\) is the outcome variable, \\(x_i\\) is the ith observation on the predictor variable, \\(y_i\\) is the ith observation of the outcome variable, \\(\\bar{x}\\) is the mean of the predictor variable, \\(\\bar{y}\\) is the mean of the outcome variable, \\(r_{xy}\\) is the correlation between the predictor and outcome variables, \\(s_x\\) is the standard deviation of the predictor variable, and \\(s_y\\) is the standard deviation of the outcome variable.\nFor instance, we can calculate the covariance between two variables manually (we subset to just those rows that have data on both \\(x\\) and \\(y\\) to ensure we compute the means using the same values that are included in the estimate of the covariance):\n\nCodecomplete_cases &lt;- player_stats_seasonal[complete.cases(player_stats_seasonal$height, player_stats_seasonal$fantasyPoints),]\n\nn &lt;- nrow(complete_cases)\nx &lt;- complete_cases$height\ny &lt;- complete_cases$fantasyPoints\nx_mean &lt;- mean(complete_cases$height)\ny_mean &lt;- mean(complete_cases$fantasyPoints)\n\ncov_manual &lt;- sum((x - x_mean) * (y - y_mean)) / (n - 1)\ncov_manual\n\n[1] -11.57919\n\n\nThe covariance of a variable with itself is the variable’s variance:\n\\[\n\\begin{aligned}\n  \\text{Cov}(x, x) &= \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})\\\\\n  &= \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\\\\n  &= \\frac{\\sum (x_i - \\bar{x})^2}{n-1}\\\\\n  &= s^2\n\\end{aligned}\n\\tag{10.2}\\]\nThe following syntax shows the variance-covariance matrix of a set of variables, where the covariance of a variable with itself is the variable’s variance. The variances are the values on the diagonal; the covariances are the values on the off-diagonal.\n\nCodecov(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")],\n  use = \"pairwise.complete.obs\")\n\n                  height     weight fantasyPoints\nheight          6.756082   82.81768     -11.57919\nweight         82.817677 2062.57351    -421.22559\nfantasyPoints -11.579191 -421.22559    3753.80257\n\n\nIf you just want the covariance between two variables, you can use the following syntax.\n\nCodecov(\n  player_stats_seasonal$height,\n  player_stats_seasonal$fantasyPoints,\n  use = \"pairwise.complete.obs\")\n\n[1] -11.57919\n\n\nThus, it appears there is a negative covariance between height and fantasy points in the whole population of National Football League (NFL) players.\nAs shown below, the negative covariance between height and fantasy points holds when examining just Quarterbacks, Running Backs, Wide Receivers, and Tight Ends.\n\nCodecov(\n  player_stats_seasonal %&gt;% filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% select(height,fantasyPoints),\n  use = \"pairwise.complete.obs\")\n\n                 height fantasyPoints\nheight         7.762595     -5.505416\nfantasyPoints -5.505416   7519.932024\n\n\nWe can also examine the association separately by position. Here is the association for Quarterbacks:\n\nCodecov(\n  player_stats_seasonal %&gt;% filter(position == \"QB\") %&gt;% select(height,fantasyPoints),\n  use = \"pairwise.complete.obs\")\n\n                 height fantasyPoints\nheight         2.803888      10.70975\nfantasyPoints 10.709751   12375.42941\n\n\nHere is the association for Running Backs:\n\nCodecov(\n  player_stats_seasonal %&gt;% filter(position == \"RB\") %&gt;% select(height,fantasyPoints),\n  use = \"pairwise.complete.obs\")\n\n                height fantasyPoints\nheight        3.149978      5.260645\nfantasyPoints 5.260645   7845.217336\n\n\nHere is the association for Wide Receivers:\n\nCodecov(\n  player_stats_seasonal %&gt;% filter(position == \"WR\") %&gt;% select(height,fantasyPoints),\n  use = \"pairwise.complete.obs\")\n\n                height fantasyPoints\nheight         5.51653      15.38999\nfantasyPoints 15.38999    7075.02075\n\n\nHere is the association for Tight Ends:\n\nCodecov(\n  player_stats_seasonal %&gt;% filter(position == \"TE\") %&gt;% select(height,fantasyPoints),\n  use = \"pairwise.complete.obs\")\n\n                height fantasyPoints\nheight        1.884198      2.760161\nfantasyPoints 2.760161   3357.695180\n\n\nInterestingly, there is a positive covariance between height and fantasy points when examining each position separately. This is an example of Simpson’s paradox, where the sign of an association differs at different levels of analysis, as described in Section 12.2.2. There is a negative covariance between height and fantasy points when examining Quarterbacks, Running Backs, Wide Receivers, and Tight Ends altogether, but there is a positive covariance between height and fantasy points when examining the association between each position separately. That is, if you were to examine all positions together, you might assume that being taller might be disadvantageous to scoring fantasy fantasy points; however, once we examine the association within a given position, it emerges that height appears to be somewhat advantageous, if anything, for scoring fantasy points.\nThe covariance is unstandardized—its metric depends on the metrics of the two variables examined in the association. However, it can be helpful to use a standardized index of the association between the variables. A standardized index means that the index is on a common metric and does not depend on the metrics of the two variables. This is valuable because it allows fairly comparing the strength of associations between sets of variables with different metrics, which is meaningful for estimating the effect size—i.e., the strength of the association. The Pearson correlation coefficient, \\(r\\), is an example of a standardized index of the covariance between two variables because it has a common metric (whose possible values range from −1 to +1, regardless of the variables examined in the association). We can convert a covariance to a correlation by standardizing it; i.e., by dividing by the multiplication of the standard deviation of \\(x\\) and the standard deviation of \\(y\\), as in Equation 9.22.\nFor instance, for the covariance between height and fantasy points, we can convert it to a correlation:\n\nCodecov_value &lt;- cov(\n  complete_cases$height,\n  complete_cases$fantasyPoints,\n  use = \"pairwise.complete.obs\")\n\ncor_value &lt;- cov_value / (sd(complete_cases$height, na.rm = TRUE) * sd(complete_cases$fantasyPoints, na.rm = TRUE))\n\ncov_value\n\n[1] -11.57919\n\nCodecor_value\n\n[1] -0.07763408\n\nCodecor(\n  complete_cases$height,\n  complete_cases$fantasyPoints,\n  use = \"pairwise.complete.obs\")\n\n[1] -0.07763408\n\n\nWe can also convert a correlation back to covariance using the formula in Equation 10.1.\n\nCodecor_value * sd(complete_cases$height, na.rm = TRUE) * sd(complete_cases$fantasyPoints, na.rm = TRUE)\n\n[1] -11.57919\n\n\n\n10.4.2 Pearson Correlation\nCorrelation is a standardized index of the association between two variables. Pearson correlation is the most common type of correlation. The formula for Pearson correlation is in Equation 9.22. Pearson correlation assumes that the variables are interval or ratio level of measurement. If the data are ordinal, Spearman correlation is more appropriate.\nThe following syntax shows the Pearson correlation matrix of a set of variables.\n\nCodecor(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")],\n  use = \"pairwise.complete.obs\")\n\n                   height     weight fantasyPoints\nheight         1.00000000  0.7019252   -0.07763408\nweight         0.70192518  1.0000000   -0.18195318\nfantasyPoints -0.07763408 -0.1819532    1.00000000\n\n\nNotice the diagonal values are all 1.0. That is because a variable is perfectly correlated with itself. Also notice that the values above the diagonal are a mirror image of (i.e., they are the same as) the respective values below the diagonal. The association between two variables is the same regardless of the order (i.e., which is the predictor variable and which is the outcome variable); that is, the association between variables A and B is the same as the association between variables B and A.\nYou can compute a correlation manually using the z scores, as in Equation 9.22:\n\nCodesum(scale(complete_cases$height) * scale(complete_cases$fantasyPoints)) / nrow(complete_cases)\n\n[1] -0.0776322\n\n\nIf you just want the Pearson correlation between two variables, you can use the following syntax.\n\nCodecor.test(\n  ~ height + fantasyPoints,\n  data = player_stats_seasonal\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  height and fantasyPoints\nt = -15.792, df = 41130, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.08723272 -0.06802104\nsample estimates:\n        cor \n-0.07763408 \n\n\nThe \\(r\\) value (-.08) is slightly negative. The effect size of the association is small—for what is considered a small, medium, and large effect size for \\(r\\) values, see Section 9.4.2.9. The number of degrees of freedom (\\(df\\)) for a statistical test of a correlation coefficient is in Equation 10.3. The \\(p\\)-value is less than .05, so it is a statistically significant correlation. We could report this finding as follows: There was a statistically significant, weak negative association between height and fantasy points (\\(r(41130) = -.08\\), \\(p &lt; .001\\)); the taller a player was, the fewer fantasy points they tended to score.\n\\[\ndf = n - 2\n\\tag{10.3}\\]\nNow let’s examine the association separately by position. Here is the association for Quarterbacks:\n\nCodecor.test(\n  ~ height + fantasyPoints,\n  data = player_stats_seasonal %&gt;% \n    filter(position == \"QB\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  height and fantasyPoints\nt = 2.5754, df = 2000, p-value = 0.01008\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.01371905 0.10104803\nsample estimates:\n       cor \n0.05749352 \n\n\nHere is the association for Running Backs:\n\nCodecor.test(\n  ~ height + fantasyPoints,\n  data = player_stats_seasonal %&gt;% \n    filter(position == \"RB\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  height and fantasyPoints\nt = 2.0515, df = 3754, p-value = 0.04029\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.001483591 0.065376776\nsample estimates:\n       cor \n0.03346437 \n\n\nHere is the association for Wide Receivers:\n\nCodecor.test(\n  ~ height + fantasyPoints,\n  data = player_stats_seasonal %&gt;% \n    filter(position == \"WR\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  height and fantasyPoints\nt = 5.735, df = 5387, p-value = 1.028e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.05130742 0.10438364\nsample estimates:\n       cor \n0.07790072 \n\n\nHere is the association for Tight Ends:\n\nCodecor.test(\n  ~ height + fantasyPoints,\n  data = player_stats_seasonal %&gt;% \n    filter(position == \"TE\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  height and fantasyPoints\nt = 1.9022, df = 3001, p-value = 0.05725\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.001068264  0.070382934\nsample estimates:\n       cor \n0.03470168 \n\n\nThe sign of the association was positive for each position, suggesting that, for a given position (among Quarterbacks, Running Backs, Wide Receivers, and Tight Ends), taller players tend to score more fantasy points. However, the effect size is small. Moreover, as described in Section 10.8, just because there is an association between variables does not mean that the association reflects a causal effect.\n\n10.4.3 Scatterplot with Best-Fit Line\nA scatterplot with best-fit line is in Figure 10.5.\n\nCodeplot_scatterplot &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  aes(\n    x = height,\n    y = fantasyPoints)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season), # add season for mouse over tooltip\n    alpha = 0.05) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\") +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Player Height (Inches)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Height\",\n    subtitle = \"(Among QBs, RBs, WRs, and TEs)\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(plot_scatterplot)\n\n\n\n\n\n\nFigure 10.5: Scatterplot of Fantasy Points (Season) by Player Height With Best-Fit Line. The linear best-fit line is in black. The nonlinear best-fit line is in blue.\n\n\n\nExamining the linear line of best fit, we would see that there is a slight negative slope, consistent with a weak negative association. However, the nonlinear best-fit line suggests that there may be an inverted-U-shaped association, which might suggest that there may be an “optimal range of height,” where being too tall or too short may be a disadvantage. We now examine the association between height and fantasy points separately by position in Figure 10.6.\n\nCodeplot_scatterplotByPosition &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  aes(\n    x = height,\n    y = fantasyPoints,\n    color = position,\n    fill = position)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season), # add season for mouse over tooltip\n    alpha = 0.7) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\") +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5) +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Player Height (Inches)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Height and Position\"\n  ) +\n  guides(fill = \"none\") +\n  theme_classic()\n\nplotly::ggplotly(plot_scatterplotByPosition)\n\n\n\n\n\n\nFigure 10.6: Scatterplot of Fantasy Points (Season) by Player Height and Position With Best-Fit Line. The linear best-fit line for each position is in black. The nonlinear best-fit line for each group is represented by the color line.\n\n\n\nExamining the association between height and fantasy points separately by position, it appears that there is an inverted-U-shaped associations for Wide Receivers, a relatively flat association for Running Backs and Tight Ends, and a nonlinear association among Quarterbacks. Interestingly, the best-fit lines suggest that the different positions have different height ranges, which is confirmed in Figure 10.7.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = height,\n    y = position,\n    group = position,\n    fill = position)\n) +\n  ggridges::geom_density_ridges(\n    bandwidth = 0.6\n  ) +\n  labs(\n    x = \"Height (inches)\",\n    y = \"Position\",\n    title = \"Ridgeline Plot of Player Height by Position\"\n  ) +\n  theme_classic() + \n  theme(\n    legend.position = \"none\",\n    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 10.7: Ridgeline Plot of Player Height by Position.\n\n\n\n\nAccording to the distributions of player height by position (among Quarterbacks, Running Backs, Wide Receivers, and Tight Ends), Running Backs tend to be the shortest, followed by Wide Receivers; Tight Ends tend to be the tallest, followed by Quarterbacks. Wide Receivers showed the greatest variability; some were quite short, whereas others were quite tall.\n\n10.4.4 Rank Correlation: Spearman’s Rho (\\(\\rho\\))\nSpearman’s rho (\\(\\rho\\)) is a rank correlation. Spearman’s rho does not assume that the data are interval or ratio level of measurement. Unlike Pearson correlation, Spearman’s rho allows estimating associations among variables that are ordinal level of measurement.\nFor instance, using Spearman’s rho, we could examine the association between a player’s draft round and their fantasy points, because draft round is an ordinal variable:\n\nCodecor(\n  player_stats_seasonal[,c(\"draftround\",\"fantasyPoints\")],\n  use = \"pairwise.complete.obs\",\n  method = \"spearman\")\n\n              draftround fantasyPoints\ndraftround     1.0000000    -0.3666174\nfantasyPoints -0.3666174     1.0000000\n\nCodecor.test(\n  ~ draftround + fantasyPoints,\n  data = player_stats_seasonal,\n  method = \"spearman\"\n)\n\n\n    Spearman's rank correlation rho\n\ndata:  draftround and fantasyPoints\nS = 6.5245e+11, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.3666174 \n\n\nUsing Spearman’s rho, there is a negative association between draft round and fantasy points (\\(r(14200) = -.37\\), \\(p &lt; .001\\)); unsurprisingly, the earlier the round a player is drafted, the more fantasy points they tend to score.\n\n10.4.5 Partial Correlation\nA partial correlation examines the association between variables after controlling for one or more variables. Below, we examine a partial Pearson correlation between height and fantasy points after controlling for age using the the psych::partial.r() function from the psych package (Revelle, 2025).\n\nCodepsych::partial.r(\n  data = player_stats_seasonal,\n  x = c(\"height\",\"fantasyPoints\"),\n  y = \"age\" # control variable(s)\n)\n\npartial correlations \n              height fantasyPoints\nheight          1.00         -0.08\nfantasyPoints  -0.08          1.00\n\n\nBelow, we examine a partial Spearman’s rho rank correlation between height and fantasy points after controlling for age.\n\nCodepsych::partial.r(\n  data = player_stats_seasonal,\n  x = c(\"draftround\",\"fantasyPoints\"),\n  y = \"age\",\n  method = \"spearman\"\n)\n\npartial correlations \n              draftround fantasyPoints\ndraftround          1.00         -0.36\nfantasyPoints      -0.36          1.00\n\n\n\n10.4.6 Nonlinear Correlation\nExamples of a nonlinear correlation include the correlation ratio [\\(\\eta\\); Goodwin & Leech (2006)], distance correlation, and the xi (\\(\\xi\\)) coefficient (Chatterjee, 2021). These estimates of nonlinear correlation only range from 0–1 (where 0 represents no association and 1 represents a strong association), so they are interpreted differently than a traditional correlation coefficient (which can be negative).\nHere is how to compute the correlation ratio (eta; \\(\\eta\\)) using the stats::aov() and effectsize::eta_squared() functions:\n\nCodemod &lt;- stats::aov(\n  fantasyPoints ~ factor(height), # convert the predictor variable to a categorical variable (i.e., factor); if there are too many unique values, can also consider binning the variable into ranges\n  data = player_stats_seasonal)\n\netaSquared &lt;- effectsize::eta_squared(mod)\nprint(etaSquared, digits = 9)\n\n# Effect Size for ANOVA\n\nParameter      |        Eta2 |                     95% CI\n---------------------------------------------------------\nfactor(height) | 0.009345024 | [0.007513579, 1.000000000]\n\n- One-sided CIs: upper bound fixed at [1.000000000].\n\nCodecorrelationRatio &lt;- etaSquared\n\ncols_to_sqrt &lt;- intersect(\n  colnames(correlationRatio),\n  c(\"Eta2\", \"CI_low\", \"CI_high\"))\n\ncorrelationRatio[cols_to_sqrt] &lt;- sqrt(correlationRatio[cols_to_sqrt]) # take the square root of eta-squared to get eta\n\ncorrelationRatio &lt;- correlationRatio %&gt;%\n  rename(Eta = Eta2)\n\nprint(correlationRatio, digits = 9)\n\n# Effect Size for ANOVA\n\nParameter      |         Eta |                     95% CI\n---------------------------------------------------------\nfactor(height) | 0.096669662 | [0.086680902, 1.000000000]\n\n- One-sided CIs: upper bound fixed at [1.000000000].\n\nCode# the correlation ratio (eta) is equivalent to square root of R-squared (when the predictor is converted to a factor)\nRsquared &lt;- summary(stats::lm(\n  fantasyPoints ~ factor(height),\n  data = player_stats_seasonal))$r.squared\n\nRsquared\n\n[1] 0.009345024\n\nCodeRvalue &lt;- sqrt(Rsquared)\nRvalue\n\n[1] 0.09666966\n\n\nWe can estimate a distance correlation using the correlation::correlation() function of the correlation package (Makowski et al., 2020, 2025).\n\nCodecorrelation::correlation(\n  data = player_stats_seasonal %&gt;% select(draftround, fantasyPoints),\n  method = \"distance\"\n)\n\n\n  \n\n\n\nA nonlinear correlation can be estimated using the xi (\\(\\xi\\)) coefficient from the XICOR::xicor() function of the XICOR package (Chatterjee, 2021; Holmes & Chatterjee, 2023):\n\nCodeset.seed(52242) # for reproducibility\n\nXICOR::xicor(\n  player_stats_seasonal$height,\n  player_stats_seasonal$fantasyPoints,\n  method = \"permutation\",\n  pvalue = TRUE\n)\n\n$xi\n[1] 0.01057212\n\n$sd\n[1] 0.0031279\n\n$pval\n[1] 0\n\n\n\n10.4.7 Correlation Matrix\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::cor.table() function that generates a correlation matrix of variables, including the \\(r\\) value, number of observations (\\(n\\)), asterisks denoting statistical significance, and \\(p\\)-value for each association. The asterisks follow the following traditional convention for statistical significance: \\(^\\dagger p &lt; .1\\); \\(^* p &lt; .05\\); \\(^{**} p &lt; .01\\); \\(^{***} p &lt; .001\\). Here is a Pearson correlation matrix:\n\nCodepetersenlab::cor.table(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")]\n)\n\n\n  \n\n\n\nHere is a Spearman correlation matrix:\n\nCodepetersenlab::cor.table(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")],\n  correlation = \"spearman\"\n)\n\n\n  \n\n\n\nHere is a correlation matrix of variables that includes just the \\(r\\) value and asterisks denoting the statistical significance of the association, for greater concision.\n\nCodepetersenlab::cor.table(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")],\n  type = \"manuscript\" # include r values and asterisks\n)\n\n\n  \n\n\n\nHere is a correlation matrix of variables that includes just the \\(r\\) value for even greater concision.\n\nCodepetersenlab::cor.table(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")],\n  type = \"manuscriptBig\" # include just r values\n)\n\n\n  \n\n\n\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::partialcor.table() function that generates a partial correlation matrix. Here is a partial correlation matrix, controlling for the player’s age:\n\nCodepetersenlab::partialcor.table(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")],\n  z = player_stats_seasonal$age # control for the player's age\n)\n\n\n  \n\n\n\n\n10.4.8 Correlogram\nWe can depict a correlation matrix plot (correlogram) using the GGally::ggcorr() function of the GGally (Schloerke et al., 2025) package, as shown in Figure 10.8.\n\nCodeGGally::ggcorr(\n  player_stats_seasonal[,c(\"height\",\"weight\",\"fantasyPoints\")],\n  label = TRUE,\n  label_round = 2)\n\n\n\n\n\n\nFigure 10.8: Correlation Matrix Plot (Correlogram).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationNonIndependence",
    "href": "correlation.html#sec-correlationNonIndependence",
    "title": "10  Correlation Analysis",
    "section": "\n10.5 Addressing Non-Independence of Observations",
    "text": "10.5 Addressing Non-Independence of Observations\nPlease note that the \\(p\\)-value for a correlation assumes that the observations are independent—in particular, that the residuals are not correlated. However, the observations are not independent in the player_stats_seasonal dataframe used above, because the same player has multiple rows—one row corresponding to each season they played. This non-independence violates the traditional assumptions of the significance test of a correlation. We could address this assumption by analyzing only one season from each player or by estimating the significance of the correlation coefficient using cluster-robust standard errors. For simplicity, we present results above from the whole dataframe. In Chapter 12, we discuss mixed model approaches that handle repeated measures and other data that violate assumptions of non-independence that are shared by correlation and multiple regression; assumptions of multiple regression are described in Section 11.5. In section Section 11.12, we demonstrate how to account for non-independence of observations using cluster-robust standard errors.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationOutliers",
    "href": "correlation.html#sec-correlationOutliers",
    "title": "10  Correlation Analysis",
    "section": "\n10.6 Impact of Outliers",
    "text": "10.6 Impact of Outliers\nThe correlation coefficient is strongly impacted by outliers—i.e., extreme (i.e., very large or very small) values relative to the other observations. Outliers can arise from errors that arise from data collection, measurement, or data entry, or they could represent valid (albeit atypical) values (Goodwin & Leech, 2006). Outliers can either artificially weaken the estimate of association—if they weaken the apparent linear trend—or artificially strengthen it—if they happen to fall along a linear trend. Consider the following example:\n\nCodeset.seed(52242)\n\nv1 &lt;- rnorm(10)\nv2 &lt;- rnorm(10)\n\ncor.test(\n  v1,\n  v2\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  v1 and v2\nt = 0.31786, df = 8, p-value = 0.7587\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.5571223  0.6926038\nsample estimates:\n      cor \n0.1116785 \n\n\nThe associated scatterplot and best-fit line is in Figure 10.9.\n\nCodeplot(\n  v1,\n  v2\n)\n\npetersenlab::addText(\n  v1,\n  v2,\n  ycoord = 0.7\n)\n\nabline(lm(\n  v2 ~ v1),\n  col = \"black\")\n\n\n\n\n\n\nFigure 10.9: Scatterplot Without Strong Outliers.\n\n\n\n\nNow, let’s add one outlier.\n\nCodev1[length(v1) + 1] &lt;- 4\nv2[length(v2) + 1] &lt;- 4\n\ncor.test(\n  v1,\n  v2\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  v1 and v2\nt = 2.7132, df = 9, p-value = 0.02387\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1186112 0.9060613\nsample estimates:\n      cor \n0.6707604 \n\n\nThe associated scatterplot and best-fit line for the updated data are in Figure 10.10.\n\nCodeplot(\n  v1,\n  v2\n)\n\npetersenlab::addText(\n  v1,\n  v2\n)\n\npoints(\n  v1[length(v1)],\n  v2[length(v2)],\n  pch = 19,\n  col = \"red\"\n)\n\ntext(\n  x = 3.8,\n  y = 3.8,\n  labels = \"outlier\")\n\nabline(lm(\n  v2 ~ v1),\n  col = \"black\")\n\n\n\n\n\n\nFigure 10.10: Scatterplot With Outlier (in Red).\n\n\n\n\nNote how the association was not close to being statistically significant without the outlier, but with the outlier added, the association is statistically significant. One way to combat this is to use methods for estimating the association between variables that are robust to (i.e., less impacted by) outliers. In the next section, we describe robust correlation methods.\n\n10.6.1 Examining Robust Correlation\nThere are various approaches to estimating correlation in the presence of outliers—so-called robust correlation methods.\nOne approach is the biweight midcorrelation, which is based on the median rather than the mean, and is thus less sensitive to outliers. Another approach is the percentage bend correlation, which gives less weight to observations that are farther away from the median. We can estimate the each using the correlation::correlation() function of the correlation package (Makowski et al., 2020, 2025).\n\nCodecorrelation::correlation(\n  data = data.frame(v1, v2),\n  method = \"biweight\"\n)\n\n\n  \n\n\nCodecorrelation::correlation(\n  data = data.frame(v1, v2),\n  method = \"percentage\"\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationRestrictedRange",
    "href": "correlation.html#sec-correlationRestrictedRange",
    "title": "10  Correlation Analysis",
    "section": "\n10.7 Impact of Restricted Range",
    "text": "10.7 Impact of Restricted Range\nIn addition to being impacted by outliers, correlations can also be greatly impacted by restricted variability or restricted range (Petersen, 2024, 2025b). Correlation depends on variability; if there is no or limited variability, it can be difficult to detect an association with another variable. Thus, if a variable has restricted range—such as owing to a floor effect or ceiling effect—that tends to artificially weaken associations. For instance, a floor effect is one in which many of the scores are the lowest possible score. By contrast, a ceiling effect is one in which many of the scores are the highest possible score.\nConsider the following association between passing attempts and expected points added (EPA) via passing. Expected points added from a given play is calculated as the difference between a team’s expected points before the play and the team’s expected points after the play. This can be summed across all passing plays to determine a player’s EPA from passing during a given season.\n\nCodecor.test(\n  player_stats_seasonal$attempts,\n  player_stats_seasonal$passing_epa\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  player_stats_seasonal$attempts and player_stats_seasonal$passing_epa\nt = 24.822, df = 2758, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3963356 0.4573453\nsample estimates:\n      cor \n0.4273268 \n\n\nThere is a statistically significant, moderate positive association between passing attempts and expected points added (EPA) via passing, as depicted in Figure 10.11.\n\nCodeplot_scatterplotWithoutRangeRestriction &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal,\n  aes(\n    x = attempts,\n    y = passing_epa)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season), # add season for mouse over tooltip\n    alpha = 0.3) +\n  geom_smooth(\n    method = \"lm\") +\n  #geom_smooth() +\n  coord_cartesian(\n    expand = FALSE) +\n  labs(\n    x = \"Player's Passing Attempts\",\n    y = \"Player's Expected Points Added (EPA) via Passing\",\n    title = \"Player's Passing Expected Points Added (EPA)\\nby Passing Attempts (Season)\",\n    #subtitle = \"\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(plot_scatterplotWithoutRangeRestriction)\n\n\n\n\n\n\nFigure 10.11: Scatterplot of Player Passing Expected Points Added (Season) by Passing Attempts, Without Range Restriction. The best-fit line is in blue.\n\n\n\nNow, consider the same association when we restrict the range to examine only those players who had fewer than 450 pass attempts in a season (thus resulting in less variability):\n\nCodecor.test(\n  player_stats_seasonal %&gt;% \n    filter(attempts &lt; 450) %&gt;% \n    select(attempts) %&gt;% \n    pull(),\n  player_stats_seasonal %&gt;% \n    filter(attempts &lt; 450) %&gt;% \n    select(passing_epa) %&gt;% \n    pull()\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  player_stats_seasonal %&gt;% filter(attempts &lt; 450) %&gt;% select(attempts) %&gt;% pull() and player_stats_seasonal %&gt;% filter(attempts &lt; 450) %&gt;% select(passing_epa) %&gt;% pull()\nt = -2.4303, df = 2302, p-value = 0.01516\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.091236097 -0.009771823\nsample estimates:\n        cor \n-0.05058811 \n\n\nThe association is no longer positive and is weak in terms of effect size, as depicted in Figure 10.12.\n\nCodeplot_scatterplotWithRangeRestriction &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% filter(attempts &lt; 450),\n  aes(\n    x = attempts,\n    y = passing_epa)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season), # add season for mouse over tooltip\n    alpha = 0.3) +\n  geom_smooth(\n    method = \"lm\") +\n  #geom_smooth() +\n  coord_cartesian(\n    expand = FALSE) +\n  labs(\n    x = \"Player's Passing Attempts\",\n    y = \"Player's Expected Points Added (EPA) via Passing\",\n    title = \"Player's Passing Expected Points Added (EPA)\\nby Passing Attempts (Season)\",\n    subtitle = \"(Among Players with &lt; 450 Passing Attempts)\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(plot_scatterplotWithRangeRestriction)\n\n\n\n\n\n\nFigure 10.12: Scatterplot of Player Passing Expected Points Added (Season) by Passing Attempts, With Range Restriction. The best-fit line is in blue.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlation-correlationAndCausation",
    "href": "correlation.html#sec-correlation-correlationAndCausation",
    "title": "10  Correlation Analysis",
    "section": "\n10.8 Correlation Does Not Imply Causation",
    "text": "10.8 Correlation Does Not Imply Causation\nAs described in Section 8.5.2.1, correlation does not imply causation. There are several reasons (described in Section 8.5.2.1) that, just because X is correlated with Y does not necessarily mean that X causes Y. However, correlation can still be useful. In order for two processes to be causally related, they must be associated. That is, association is necessary but insufficient for causality.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationConclusion",
    "href": "correlation.html#sec-correlationConclusion",
    "title": "10  Correlation Analysis",
    "section": "\n10.9 Conclusion",
    "text": "10.9 Conclusion\nCorrelation is a standardized index of the association between variables. The correlation coefficient (\\(r\\)) ranges from −1 to +1, and indicates the sign and magnitude of the association. Although correlation does not imply causation, identifying associations between variables can still be useful because association is a necessary (but insufficient) condition for causality. The correlation coefficient can be heavily impacted by outliers and restricted range.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationSessionInfo",
    "href": "correlation.html#sec-correlationSessionInfo",
    "title": "10  Correlation Analysis",
    "section": "\n10.10 Session Info",
    "text": "10.10 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] patchwork_1.3.2   lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    \n [5] dplyr_1.1.4       purrr_1.1.0       readr_2.1.5       tidyr_1.3.1      \n [9] tibble_3.3.0      tidyverse_2.0.0   ggridges_0.5.6    GGally_2.4.0     \n[13] ggplot2_3.5.2     effectsize_1.0.1  correlation_0.8.8 DescTools_0.99.60\n[17] psych_2.5.6       XICOR_0.4.1       petersenlab_1.2.0\n\nloaded via a namespace (and not attached):\n  [1] Rdpack_2.6.4        DBI_1.2.3           mnormt_2.1.1       \n  [4] gridExtra_2.3       gld_2.6.7           sandwich_3.1-1     \n  [7] readxl_1.4.5        rlang_1.1.6         magrittr_2.0.3     \n [10] multcomp_1.4-28     e1071_1.7-16        compiler_4.5.1     \n [13] mgcv_1.9-3          vctrs_0.6.5         reshape2_1.4.4     \n [16] quadprog_1.5-8      pkgconfig_2.0.3     fastmap_1.2.0      \n [19] backports_1.5.0     labeling_0.4.3      pbivnorm_0.6.0     \n [22] rmarkdown_2.29      psychTools_2.5.7.22 tzdb_0.5.0         \n [25] haven_2.5.5         nloptr_2.2.1        xfun_0.53          \n [28] jsonlite_2.0.0      parallel_4.5.1      lavaan_0.6-19      \n [31] cluster_2.1.8.1     R6_2.6.1            stringi_1.8.7      \n [34] RColorBrewer_1.1-3  boot_1.3-31         rpart_4.1.24       \n [37] estimability_1.5.1  cellranger_1.1.0    Rcpp_1.1.0         \n [40] knitr_1.50          zoo_1.8-14          parameters_0.28.0  \n [43] base64enc_0.1-3     timechange_0.3.0    Matrix_1.7-3       \n [46] splines_4.5.1       nnet_7.3-20         tidyselect_1.2.1   \n [49] rstudioapi_0.17.1   yaml_2.3.10         codetools_0.2-20   \n [52] lattice_0.22-7      plyr_1.8.9          S7_0.2.0           \n [55] withr_3.0.2         bayestestR_0.16.1   coda_0.19-4.1      \n [58] evaluate_1.0.4      foreign_0.8-90      survival_3.8-3     \n [61] ggstats_0.10.0      proxy_0.4-27        pillar_1.11.0      \n [64] checkmate_2.3.3     rtf_0.4-14.1        stats4_4.5.1       \n [67] reformulas_0.4.1    insight_1.4.0       plotly_4.11.0      \n [70] generics_0.1.4      mix_1.0-13          hms_1.1.3          \n [73] scales_1.4.0        rootSolve_1.8.2.4   minqa_1.2.8        \n [76] xtable_1.8-4        class_7.3-23        glue_1.8.0         \n [79] lazyeval_0.2.2      emmeans_1.11.2      Hmisc_5.2-3        \n [82] lmom_3.2            tools_4.5.1         data.table_1.17.8  \n [85] lme4_1.1-37         Exact_3.3           fs_1.6.6           \n [88] mvtnorm_1.3-3       grid_4.5.1          mitools_2.4        \n [91] crosstalk_1.2.1     rbibutils_2.3       datawizard_1.2.0   \n [94] colorspace_2.1-1    nlme_3.1-168        htmlTable_2.4.3    \n [97] Formula_1.2-5       cli_3.6.5           expm_1.0-0         \n[100] viridisLite_0.4.2   gtable_0.3.6        R.methodsS3_1.8.2  \n[103] digest_0.6.37       TH.data_1.1-3       htmlwidgets_1.6.4  \n[106] farver_2.1.2        htmltools_0.5.8.1   R.oo_1.27.1        \n[109] lifecycle_1.0.4     httr_1.4.7          MASS_7.3-65        \n\n\n\n\n\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009–2022. https://doi.org/10.1080/01621459.2020.1758115\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGoodwin, L. D., & Leech, N. L. (2006). Understanding correlation: Factors that affect the size of r. The Journal of Experimental Education, 74(3), 249–266. https://doi.org/10.3200/JEXE.74.3.249-266\n\n\nHolmes, S., & Chatterjee, S. (2023). XICOR: Association measurement through cross rank increments. https://doi.org/10.32614/CRAN.package.XICOR\n\n\nMagnusson, K. (2023). Interpreting correlations: An interactive visualization. https://rpsychologist.com/correlation\n\n\nMakowski, D., Ben-Shachar, M. S., Patil, I., & Lüdecke, D. (2020). Methods and algorithms for correlation analysis in R. Journal of Open Source Software, 5(51), 2306. https://doi.org/10.21105/joss.02306\n\n\nMakowski, D., Wiernik, B. M., Patil, I., Lüdecke, D., Ben-Shachar, M. S., & Thériault, R. (2025). correlation: Methods for correlation analysis. https://doi.org/10.32614/CRAN.package.correlation\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRevelle, W. (2025). psych: Procedures for psychological, psychometric, and personality research. https://doi.org/10.32614/CRAN.package.psych\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., & Crowley, J. (2025). GGally: Extension to ggplot2. https://doi.org/10.32614/CRAN.package.GGally",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html",
    "href": "multiple-regression.html",
    "title": "11  Multiple Regression",
    "section": "",
    "text": "11.1 Getting Started\nThis chapter provides an overview of multiple regression.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionGettingStarted",
    "href": "multiple-regression.html#sec-multipleRegressionGettingStarted",
    "title": "11  Multiple Regression",
    "section": "",
    "text": "11.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"rms\")\nlibrary(\"car\")\nlibrary(\"bestNormalize\")\nlibrary(\"lme4\")\nlibrary(\"performance\")\nlibrary(\"lavaan\")\nlibrary(\"mice\")\nlibrary(\"miceadds\")\nlibrary(\"interactions\")\nlibrary(\"brms\")\nlibrary(\"parallelly\")\nlibrary(\"robustbase\")\nlibrary(\"ordinal\")\nlibrary(\"MASS\")\nlibrary(\"broom\")\nlibrary(\"effectsize\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\nlibrary(\"knitr\")\n\n\n\n11.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionOverview",
    "href": "multiple-regression.html#sec-multipleRegressionOverview",
    "title": "11  Multiple Regression",
    "section": "\n11.2 Overview of Multiple Regression",
    "text": "11.2 Overview of Multiple Regression\nMultiple regression is an extension of correlation. Correlation examines the association between one predictor variables and one outcome variable. Multiple regression examines the association between multiple predictor variables and one outcome variable. It allows obtaining a more accurate estimate of the unique contribution of a given predictor variable, by controlling for other variables (covariates). By including multiple predictor variables in prediction of the outcome variable, it also allows improved prediction accuracy.\nAll statistical analyses follow the same basic structure, as in Equation 11.1:\n\\[\n\\text{DATA} = \\text{MODEL} + \\text{ERROR}\n\\tag{11.1}\\]\nRegression with one predictor variable takes the form of Equation 11.2:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\epsilon\n\\tag{11.2}\\]\nwhere \\(y\\) is the outcome variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, \\(x_1\\) is the predictor variable, and \\(\\epsilon\\) is the error term.\nA regression line is depicted in Figure 11.29.\n\n\n\n\n\nFigure 11.1: A Regression Best-Fit Line.\n\n\nRegression with multiple predictors—i.e., multiple regression—takes the form of Equation 11.3:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon\n\\tag{11.3}\\]\nwhere \\(p\\) is the number of predictor variables. Multiple regression is basically a weighted sum of the predictor variables, and adding an intercept. Under the hood, multiple regression seeks to identify the best weight for each predictor. The intercept is the expected value of the outcome variable when all of the predictor variables have a value of zero.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionComponents",
    "href": "multiple-regression.html#sec-multipleRegressionComponents",
    "title": "11  Multiple Regression",
    "section": "\n11.3 Components",
    "text": "11.3 Components\n\n\n\\(B\\) = unstandardized coefficient: direction and magnitude of the estimate (original scale)\n\n\\(\\beta\\) (beta) = standardized coefficient: direction and magnitude of the estimate (standard deviation scale)\n\n\\(SE\\) = standard error: uncertainty of unstandardized estimate\n\nThe unstandardized regression coefficient (\\(B\\)) is interpreted such that, for every unit change in the predictor variable, there is a __ unit change in the outcome variable. For instance, when examining the association between age and fantasy points, if the unstandardized regression coefficient is 2.3, players score on average 2.3 more points for each additional year of age. (In reality, we might expect a nonlinear, inverted-U-shaped association between age and fantasy points such that players tend to reach their peak in the middle of their careers.) Unstandardized regression coefficients are tied to the metric of the raw data. Thus, a large unstandardized regression coefficient for two variables may mean completely different things. Holding the strength of the association constant, you tend to see larger unstandardized regression coefficients for variables with smaller units and smaller unstandardized regression coefficients for variables with larger units.\nStandardized regression coefficients can be obtained by standardizing the variables to z scores so they all have a mean of zero and standard deviation of one. The standardized regression coefficient (\\(\\beta\\)) is interpreted such that, for every standard deviation change in the predictor variable, there is a __ standard deviation change in the outcome variable. For instance, when examining the association between age and fantasy points, if the standardized regression coefficient is 0.1, players score on average 0.1 standard deviation more points for each additional standard deviation of their year of age. Standardized regression coefficients—though not the case in all instances—tend to fall between [−1, 1]. Thus, standardized regression coefficients tend to be more comparable across variables and models compared to unstandardized regression coefficients. In this way, standardized regression coefficients provide a meaningful index of effect size and can be used to identify the predictors with the strongest predictive validity.\nThe standard error of a regression coefficient represents the imprecision or uncertainty of the parameter. If we have less uncertainty (i.e., more confidence) about the parameter, the standard error will be small, reflecting greater precision of the regression coefficient. If we have more uncertainty (i.e., less confidence) about the parameter, the standard error will be large, reflecting less precision of the regression coefficient. If we used the same sampling procedure repeatedly and calculated the regression coefficient each time, the true parameter in the population would fall 68% of the time within the interval of: \\([\\text{model parameter estimate for the regression coefficient} \\pm 1 \\text{ standard error}]\\). The standard error is related to the sample size—the larger the sample size, the smaller the standard error (the greater the precision of our estimate of the regression coefficient). Otherwise said, having more data gives more precise estimates and thus increases statistical power.\nA confidence interval represents a range of plausible values such that, with repeated sampling, the true value falls within a given interval with some confidence. Our parameter estimate for the regression coefficient, plus or minus 1 standard error, reflects the 68% confidence interval for the coefficient. The 95% confidence interval is computed as the parameter estimated plus or minus 1.96 standard errors (because in a standard normal distribution, the middle 95% of the distribution lies between −1.96 and +1.96). For instance, if the parameter estimate for the regression coefficient is 0.50, and the standard error is 0.10, the 95% confidence interval is [0.30, 0.70]: \\(0.5 - (1.96 \\times 0.10) = 0.3\\); \\(0.5 + (1.96 \\times 0.10) = 0.7\\). That is, if we used the same sampling procedure repeatedly, the true value of the regression coefficient would be expected to be 95% of the time somewhere between 0.30 to 0.70.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionTypes",
    "href": "multiple-regression.html#sec-multipleRegressionTypes",
    "title": "11  Multiple Regression",
    "section": "\n11.4 Types of Regression",
    "text": "11.4 Types of Regression\nWhen the outcome variable is continuous, linear regression is common. However, there are other types of regression depending on type and distribution of the outcome variable. For instance, if the outcome variable is binary, logistic regression would be used. If the outcome variable is an ordered categorical variable, ordinal regression would be used. If the outcome variable is a count, Poisson or negative binomial regression would be preferable. If the outcome variable is a proportion, beta regression would be used.\nHere are examples of each:\n\n11.4.1 Linear Regression\nWe fit a linear regression model using the stats::lm() function.\n\nCodelinearRegression &lt;- lm(\n  fantasyPoints ~ age + height + weight + target_share,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(linearRegression)\n\n\nCall:\nlm(formula = fantasyPoints ~ age + height + weight + target_share, \n    data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")), \n    na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-739.80  -18.68  -10.50   10.74  292.78 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -34.62684   23.85029  -1.452 0.146617    \nage            0.75193    0.22418   3.354 0.000803 ***\nheight         0.05816    0.42094   0.138 0.890107    \nweight         0.14716    0.06686   2.201 0.027794 *  \ntarget_share 743.14867    7.38684 100.604  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.71 on 4432 degrees of freedom\n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.7054,    Adjusted R-squared:  0.7052 \nF-statistic:  2654 on 4 and 4432 DF,  p-value: &lt; 2.2e-16\n\nCodeprint(effectsize::standardize_parameters(linearRegression, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter    | Std. Coef. |        95% CI\n-----------------------------------------\n(Intercept)  |  -8.91e-17 | [-0.02, 0.02]\nage          |       0.03 | [ 0.01, 0.04]\nheight       |   1.63e-03 | [-0.02, 0.02]\nweight       |       0.03 | [ 0.00, 0.05]\ntarget share |       0.83 | [ 0.82, 0.85]\n\n\n\n11.4.2 Logistic Regression\nWe fit a logistic regression model using the stats::glm() function and specifying family = binomial(). To calculate the model \\(R^2\\), we use the performance::r2() function of the performance package (Lüdecke et al., 2021; Lüdecke, Makowski, Ben-Shachar, Patil, Waggoner, et al., 2025).\n\nCodenewdata &lt;- player_stats_weekly %&gt;%\n  mutate( # create a binary variable to be used as the outcome variable\n    receiving_td = case_when(\n      is.na(receiving_tds) ~ NA_real_, # keep NA\n      receiving_tds &gt;= 1   ~ 1,\n      receiving_tds == 0   ~ 0\n    )\n  )\n\nlogisticRegression &lt;- glm(\n  receiving_td ~ age + height + weight + target_share,\n  data = newdata %&gt;% filter(position %in% c(\"WR\")),\n  family = binomial(),\n  na.action = \"na.exclude\"\n)\n\nsummary(logisticRegression)\n\n\nCall:\nglm(formula = receiving_td ~ age + height + weight + target_share, \n    family = binomial(), data = newdata %&gt;% filter(position %in% \n        c(\"WR\")), na.action = \"na.exclude\")\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -6.4762075  0.4747551 -13.641  &lt; 2e-16 ***\nage          -0.0043680  0.0041374  -1.056    0.291    \nheight        0.0495091  0.0083609   5.922 3.19e-09 ***\nweight        0.0008781  0.0012750   0.689    0.491    \ntarget_share  8.0628633  0.1242041  64.916  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 42304  on 44307  degrees of freedom\nResidual deviance: 37107  on 44303  degrees of freedom\n  (12044 observations deleted due to missingness)\nAIC: 37117\n\nNumber of Fisher Scoring iterations: 5\n\nCodeperformance::r2(logisticRegression)\n\n# R2 for Logistic Regression\n  Tjur's R2: 0.120\n\nCodeprint(effectsize::standardize_parameters(logisticRegression, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter    | Std. Coef. |         95% CI\n------------------------------------------\n(Intercept)  |      -1.72 | [-1.75, -1.69]\nage          |      -0.01 | [-0.04,  0.01]\nheight       |       0.12 | [ 0.08,  0.16]\nweight       |       0.01 | [-0.02,  0.05]\ntarget share |       0.88 | [ 0.85,  0.90]\n\n- Response is unstandardized.\n\nCodebroom::tidy(\n  logisticRegression,\n  exponentiate = TRUE,\n  conf.int = TRUE)\n\n\n  \n\n\n\n\n11.4.3 Ordinal Regression\nWe fit an ordinal regression model using the ordinal::clm() function of the ordinal package (Christensen, 2024).\n\nCodenewdata &lt;- player_stats_weekly\nnewdata$receiving_tdsFactor &lt;- factor(newdata$receiving_tds, ordered = TRUE)\ntable(newdata$receiving_tdsFactor)\n\n\n     0      1      2      3      4 \n420249  15050   1818    182     10 \n\nCodeordinalRegression &lt;- ordinal::clm(\n  receiving_tdsFactor ~ age + height + target_share,\n  data = newdata %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(ordinalRegression)\n\nformula: receiving_tdsFactor ~ age + height + target_share\ndata:    newdata %&gt;% filter(position %in% c(\"WR\"))\n\n link  threshold nobs  logLik    AIC      niter max.grad cond.H \n logit flexible  44308 -22202.80 44419.59 7(0)  5.34e-07 3.0e+07\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \nage          -0.004442   0.004092  -1.085    0.278    \nheight        0.055903   0.005731   9.754   &lt;2e-16 ***\ntarget_share  8.171193   0.121637  67.177   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n0|1   6.7753     0.4320   15.69\n1|2   9.0558     0.4334   20.89\n2|3  11.4826     0.4429   25.93\n3|4  14.8144     0.6612   22.41\n(12044 observations deleted due to missingness)\n\nCodeperformance::r2(ordinalRegression)\n\n  Nagelkerke's R2: 0.170\n\nCodeprint(effectsize::standardize_parameters(ordinalRegression, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nComponent |    Parameter | Std. Coef. |         95% CI\n------------------------------------------------------\nintercept |          0|1 |       1.72 | [ 1.69,  1.75]\nintercept |          1|2 |       4.00 | [ 3.94,  4.06]\nintercept |          2|3 |       6.43 | [ 6.24,  6.61]\nintercept |          3|4 |       9.76 | [ 8.78, 10.74]\nlocation  |          age |      -0.01 | [-0.04,  0.01]\nlocation  |       height |       0.13 | [ 0.11,  0.16]\nlocation  | target share |       0.89 | [ 0.86,  0.91]\n\n- Response is unstandardized.\n\nCodebroom::tidy(\n  ordinalRegression,\n  exponentiate = TRUE,\n  conf.int = TRUE)\n\n\n  \n\n\n\n\n11.4.4 Poisson Regression\nWe fit a Poisson regression model using the stats::glm() function and specifying family = poisson().\n\nCodepoissonRegression &lt;- glm(\n  receiving_tds ~ age + height + weight + target_share,\n  data = newdata %&gt;% filter(position %in% c(\"WR\")),\n  family = poisson(),\n  na.action = na.exclude\n)\n\nsummary(poissonRegression)\n\n\nCall:\nglm(formula = receiving_tds ~ age + height + weight + target_share, \n    family = poisson(), data = newdata %&gt;% filter(position %in% \n        c(\"WR\")), na.action = na.exclude)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -5.9753891  0.3733793 -16.004  &lt; 2e-16 ***\nage           0.0001296  0.0032293   0.040    0.968    \nheight        0.0459619  0.0065711   6.995 2.66e-12 ***\nweight        0.0009397  0.0010008   0.939    0.348    \ntarget_share  5.1190772  0.0634014  80.741  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 32902  on 44307  degrees of freedom\nResidual deviance: 27962  on 44303  degrees of freedom\n  (12044 observations deleted due to missingness)\nAIC: 45041\n\nNumber of Fisher Scoring iterations: 6\n\nCodeperformance::r2(poissonRegression)\n\n# R2 for Generalized Linear Regression\n  Nagelkerke's R2: 0.201\n\nCodeprint(effectsize::standardize_parameters(poissonRegression, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter    | Std. Coef. |         95% CI\n------------------------------------------\n(Intercept)  |      -1.75 | [-1.77, -1.73]\nage          |   4.06e-04 | [-0.02,  0.02]\nheight       |       0.11 | [ 0.08,  0.14]\nweight       |       0.01 | [-0.02,  0.04]\ntarget share |       0.56 | [ 0.54,  0.57]\n\n- Response is unstandardized.\n\nCodebroom::tidy(\n  poissonRegression,\n  exponentiate = TRUE,\n  conf.int = TRUE)\n\n\n  \n\n\n\n\n11.4.5 Negative Binomial Regression\nWe fit a negative binomial regression model using the MASS::glm.nb() function of the MASS package (Ripley & Venables, 2025).\n\nCodenegativeBinomialRegression &lt;- MASS::glm.nb(\n  receiving_tds ~ age + height + weight + target_share,\n  data = newdata %&gt;% filter(position %in% c(\"WR\")),\n  na.action = na.exclude,\n  control = glm.control(maxit = 10000)\n)\n\nsummary(negativeBinomialRegression)\n\n\nCall:\nMASS::glm.nb(formula = receiving_tds ~ age + height + weight + \n    target_share, data = newdata %&gt;% filter(position %in% c(\"WR\")), \n    na.action = na.exclude, control = glm.control(maxit = 10000), \n    init.theta = 4.750305306, link = log)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -5.7604913  0.3884147 -14.831  &lt; 2e-16 ***\nage          -0.0022711  0.0033461  -0.679    0.497    \nheight        0.0418343  0.0068311   6.124 9.12e-10 ***\nweight        0.0008346  0.0010331   0.808    0.419    \ntarget_share  6.0065807  0.0793939  75.655  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(4.7503) family taken to be 1)\n\n    Null deviance: 30971  on 44307  degrees of freedom\nResidual deviance: 25953  on 44303  degrees of freedom\n  (12044 observations deleted due to missingness)\nAIC: 44851\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  4.750 \n          Std. Err.:  0.424 \n\n 2 x log-likelihood:  -44839.055 \n\nCodeperformance::r2(negativeBinomialRegression)\n\n# R2 for Generalized Linear Regression\n  Nagelkerke's R2: 0.213\n\nCodeprint(effectsize::standardize_parameters(negativeBinomialRegression, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter    | Std. Coef. |         95% CI\n------------------------------------------\n(Intercept)  |      -1.80 | [-1.82, -1.77]\nage          |  -7.12e-03 | [-0.03,  0.01]\nheight       |       0.10 | [ 0.07,  0.13]\nweight       |       0.01 | [-0.02,  0.04]\ntarget share |       0.65 | [ 0.63,  0.67]\n\n- Response is unstandardized.\n\nCodebroom::tidy(\n  negativeBinomialRegression,\n  exponentiate = TRUE,\n  conf.int = TRUE)\n\n\n  \n\n\n\n\n11.4.6 Beta Regression\nWe fit a (Bayesian) zero-one-inflated beta regression model (to allow for zeros and ones) using the brms::brm() function of the brms package (Bürkner, 2024) and specifying family = zero_one_inflated_beta().\n\n\n\n\n\n\nNote 11.1: Bayesian beta regression\n\n\n\nNote: the following code that runs the model takes a while. If you just want to save time and load the model object instead of running the model, you can load the model object (which has already been fit) using this code:\n\nCodeload(url(\"https://osf.io/download/fe37j/\"))\n\n\n\n\n\nCodebetaRegression &lt;- brms::brm(\n  formula = target_share ~ age + height + weight,\n  data = newdata %&gt;% filter(position %in% c(\"WR\")),\n  family = zero_one_inflated_beta(),\n  cores = 4,\n  threads = threading(parallelly::availableCores()),\n  seed = 52242,\n  silent = 0\n)\n\n\n\nCodesummary(betaRegression)\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; phi = identity; zoi = identity; coi = identity \nFormula: target_share ~ age + height + weight \n   Data: newdata %&gt;% filter(position %in% c(\"WR\")) (Number of observations: 44308) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -2.76      0.13    -3.00    -2.51 1.00     4624     3011\nage           0.02      0.00     0.02     0.02 1.00     6163     2738\nheight       -0.01      0.00    -0.01    -0.00 1.00     4842     2812\nweight        0.01      0.00     0.00     0.01 1.00     4447     3271\n\nFurther Distributional Parameters:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi    12.24      0.09    12.07    12.40 1.00     2333     2348\nzoi     0.13      0.00     0.13     0.14 1.00     2540     2413\ncoi     0.00      0.00     0.00     0.00 1.00     2114     2134\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodeperformance::r2(betaRegression)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.011 (95% CI [0.009, 0.012])",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-assumptionsRegression",
    "href": "multiple-regression.html#sec-assumptionsRegression",
    "title": "11  Multiple Regression",
    "section": "\n11.5 Assumptions of Multiple Regression",
    "text": "11.5 Assumptions of Multiple Regression\nLinear regression models make the following assumptions:\n\nthere is a linear association between the predictor variable and the outcome variable\n\nthere is homoscedasticity of the residuals; the residuals do not differ as a function of the predictor variable or as a function of the outcome variable\n\nthe residuals are independent; they are uncorrelated with each other\nthe residuals are normally distributed\n\nHomoscedasticity of the residuals means that the variance of the residuals does not differ as a function of the outcome variable or as a function of the predictor variable (i.e., the residuals show constant variance as a function of outcome/predictors). If the residuals differ as a function of the outcome or predictor variable, this is called heterotoscedasticity.\nThose are some of the key assumptions of multiple regression. However, there are additional assumptions of multiple regression, including ones discussed in the chapter on Causal Inference. For instance, the variables included should reflect a causal process such that the predictor variables influence the outcome variable, and not the other way around. That is, the outcome variable should not influence the predictor variables (i.e., there should be no reverse causation). In addition, it is important control for any confound(s). If a confound is not controlled for, this is called omitted-variable bias, and it leads the researcher to incorrectly attribute the effects of the omitted variable to the included variables.\n\n11.5.1 Evaluating and Addressing Assumptions of Multiple Regression\n\n11.5.1.1 Linear Association\nTo evaluate the shape of the association between the predictor variables and the outcome variable, we can examine scatterplots (Figure 11.2), residual plots (Figure 11.21), marginal model plots (Figure 11.12), added-variable plots (Figure 11.13), and component-plus-residual plots (Figure 11.14). Residual plots depict the residuals (errors) on the y-axis as a function of the fitted values or a specific predictor on the x-axis. Marginal model plots are basically glorified scatterplots that depict the outcome variable (both in terms of observed values and model-fitted values) on the y-axis and the predictor variables on the x-axis. Added-variable plots depict the unique association of each predictor variables with the outcome variable when controlling for all the other predictor variables in the model. Component-plus-residual plots depict partial residuals on the y-axis as a function of each predictor variable on the x-axis, where a partial residual for a given predictor is the effect of a given predictor (thus controlling for all the other predictor variables in the model) plus the residual from the full model.\nExamples of linear and nonlinear associations are depicted with scatterplots in Figure 11.2.\nCodeset.seed(52242)\n\nsampleSize &lt;- 1000\nquadraticX &lt;- runif(\n  sampleSize,\n  min = -4,\n  max = 4)\nlinearY &lt;- quadraticX + rnorm(sampleSize, mean = 0, sd = 0.5)\nquadraticY &lt;- quadraticX ^ 2 + rnorm(sampleSize)\nquadraticData &lt;- cbind(quadraticX, quadraticY) %&gt;%\n  data.frame %&gt;%\n  arrange(quadraticX)\n\nquadraticModel &lt;- lm(\n  quadraticY ~ quadraticX + I(quadraticX ^ 2),\n  data = quadraticData)\n\nquadraticNewData &lt;- data.frame(\n  quadraticX = seq(\n    from = min(quadraticData$quadraticX),\n    to = max(quadraticData$quadraticY),\n    length.out = sampleSize))\n\nquadraticNewData$quadraticY &lt;- predict(\n  quadraticModel,\n  newdata = quadraticNewData)\n\nplot(\n  x = quadraticX,\n  y = linearY,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Linear Association\")\n\nabline(\n  lm(linearY ~ quadraticX),\n  lwd = 2,\n  col = \"blue\")\n\nplot(\n  x = quadraticData$quadraticX,\n  y = quadraticData$quadraticY,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Nonlinear Association\")\n\nlines(\n  quadraticNewData$quadraticY ~ quadraticNewData$quadraticX,\n  lwd = 2,\n  col = \"blue\")\n\n\n\n\n\n\n\n\n\n(a) Example of a Linear Association\n\n\n\n\n\n\n\n\n\n(b) Example of a Nonlinear Association\n\n\n\n\n\n\nFigure 11.2: Example Associations Depicted With Scatterplots.\n\n\nIf the shape of the association is nonlinear (as indicated by any of these plots), various approaches may be necessary such as including nonlinear terms (e.g., polynomial terms such as quadratic, cubic, quartic, or higher-degree terms), transforming the predictors (e.g., log, square root, inverse, exponential, Box-Cox, Yeo-Johnson transform), use of splines/piecewise regression, and generalized additive models.\n\n11.5.1.2 Homoscedasticity\nTo evaluate homoscedasticity, we can evaluate a residual plot (Figure 11.21) and spread-level plot (Figure 11.22). A residual plot depicts the residuals on the y-axis as a function of the model’s fitted values on the x-axis. Homoscedasticity in a residual plot is identified as a constant spread of residuals versus fitted values—the residuals do not show a fan, cone, or bow-tie shape; a fan, cone, or bow-tie shape indicates heteroscedasticity. In a residual plot, a fan or cone shape indicates increasing or decreasing variance in the residuals as a function of the fitted values; a bow-tie shape indicates that the residuals are smallest in the middle of the fitted values and greatest on the extremes of the fitted values. A spread-level plot depicts the log of the absolute value of studentized residuals on the y-axis as a function of the log of the model’s fitted values on the x-axis. Homoscedasticity in a spread-level plot is identified as a flat slope; a slope that differs from zero indicates heteroscedasticity.\n\nCodeset.seed(52242)\nsampleSize &lt;- 1000\n\n# 1. Homoscedasticity\nx1 &lt;- runif(sampleSize, 0, 10)\ny1 &lt;- 3 + 2 * x1 + rnorm(sampleSize, mean = 0, sd = 2)\nfit1 &lt;- lm(y1 ~ x1)\nres1 &lt;- resid(fit1)\nfitted1 &lt;- fitted(fit1)\n\n# 2. Fan-shaped heteroscedasticity\nx2 &lt;- runif(sampleSize, 0, 10)\ny2 &lt;- 3 + 2 * x2 + rnorm(sampleSize, mean = 0, sd = 0.5 * x2) # increasing variance\nfit2 &lt;- lm(y2 ~ x2)\nres2 &lt;- resid(fit2)\nfitted2 &lt;- fitted(fit2)\n\n# 3. Bow-tie-shaped heteroscedasticity\nx3 &lt;- runif(sampleSize, 0, 10)\nsd3 &lt;- abs(x3 - 5) + 0.5  # variance smallest in middle, higher on edges\ny3 &lt;- 3 + 2 * x3 + rnorm(sampleSize, mean = 0, sd = sd3)\nfit3 &lt;- lm(y3 ~ x3)\nres3 &lt;- resid(fit3)\nfitted3 &lt;- fitted(fit3)\n\n# 4. Diamond-shaped heteroscedasticity\nx4 &lt;- runif(sampleSize, 0, 10)\nsd4 &lt;- -abs(x4 - 5) + 5.5  # inverted bow-tie\ny4 &lt;- 3 + 2 * x4 + rnorm(sampleSize, mean = 0, sd = sd4)\nfit4 &lt;- lm(y4 ~ x4)\nres4 &lt;- resid(fit4)\nfitted4 &lt;- fitted(fit4)\n\n#5. Triangle-shaped heteroscedasticity\nheight &lt;- 10\n\nsample_triangle &lt;- function(n, v1, v2, v3) {\n  u &lt;- runif(n)\n  v &lt;- runif(n)\n  is_flip &lt;- u + v &gt; 1\n  u[is_flip] &lt;- 1 - u[is_flip]\n  v[is_flip] &lt;- 1 - v[is_flip]\n  \n  x &lt;- (1 - u - v) * v1[1] + u * v2[1] + v * v3[1]\n  y &lt;- (1 - u - v) * v1[2] + u * v2[2] + v * v3[2]\n  \n  cbind(x, y)\n}\n\n# Triangle vertices\nv1 &lt;- c(0, height)  # top left\nv2 &lt;- c(10, height) # top right\nv3 &lt;- c(5, 0)       # bottom center\n\n# Sample points inside triangle\ntri_points &lt;- sample_triangle(\n  sampleSize,\n  v1,\n  v2,\n  v3)\n\n# Extract x and residuals\nx5 &lt;- tri_points[, 1]\nresiduals5 &lt;- tri_points[, 2]\n\n# Generate outcome\ny5 &lt;- 3 + 2 * x5 + residuals5\n\n# Fit model\nfit5 &lt;- lm(y5 ~ x5)\nres5 &lt;- resid(fit5)\nfitted5 &lt;- fitted(fit5)\n\n# Plots\nplot(\n  fitted1,\n  res1,\n  main = \"Homoscedasticity\",\n  xlab = \"Fitted value\",\n  ylab = \"Residual\")\n\nabline(\n  h = 0,\n  lwd = 2,\n  col = \"blue\")\n\nplot(\n  fitted2,\n  res2,\n  main = \"Fan-Shaped Heteroscedasticity\",\n  xlab = \"Fitted value\",\n  ylab = \"Residual\")\n\nabline(\n  h = 0,\n  lwd = 2,\n  col = \"blue\")\n\nplot(\n  fitted3,\n  res3,\n  main = \"Bow-Tie-Shaped Heteroscedasticity\",\n  xlab = \"Fitted value\",\n  ylab = \"Residual\")\n\nabline(\n  h = 0,\n  lwd = 2,\n  col = \"blue\")\n\nplot(\n  fitted4,\n  res4,\n  main = \"Diamond-Shaped Heteroscedasticity\",\n  xlab = \"Fitted value\",\n  ylab = \"Residual\")\n\nabline(\n  h = 0,\n  lwd = 2,\n  col = \"blue\")\n\nplot(\n  fitted5,\n  res5,\n  main = \"Triangle-Shaped Heteroscedasticity\",\n  xlab = \"Fitted value\",\n  ylab = \"Residual\")\n\nabline(\n  h = 0,\n  lwd = 2,\n  col = \"blue\")\n\n\n\n\n\n\n\n\n\n(a) Homoscedasticity\n\n\n\n\n\n\n\n\n\n\n\n(b) Fan-Shaped Heteroscedasticity\n\n\n\n\n\n\n\n\n\n(c) Bow-Tie-Shaped Heteroscedasticity\n\n\n\n\n\n\n\n\n\n\n\n(d) Diamond-Shaped Heteroscedasticity\n\n\n\n\n\n\n\n\n\n(e) Triangle-Shaped Heteroscedasticity\n\n\n\n\n\n\nFigure 11.3: Example of Homoscedasticity and Heteroscedasticity in Residual Plots.\n\n\n\nIf there is heteroscedasticity, it may be necessary to transform the outcome variable to be more normally distributed. The spread-level plot provides a suggested power transformation to transform the outcome variable so that the spread of residuals becomes more uniform across the fitted values.\n\n11.5.1.3 Uncorrelated Residuals\nTo determine if residuals are correlated by a grouping level, we can examine the proportion of variance that is attributable to the grouping level using the intraclass correlation coefficient (ICC) from a mixed model. The greater the ICC value, the more variance is accounted for by the grouping level, and the more the residuals are intercorrelated. If the residuals are intercorrelated, it may be necessary to account for the grouping structure of the data using a mixed model.\n\n11.5.1.4 Normally Distributed Residuals\nTo examine whether residuals are normally distributed, we can examine quantile–quantile (QQ) plots and probability–probability (PP) plots. QQ plots depict quantiles of a sample distribution (y-axis) compared to the quantiles of a theoretical (in this case, normal) distribution (x-axis). PP plots depict cumulative probabilities of a sample distribution (y-axis) compared to those of a theoretical (in this case, normal) distribution (x-axis). QQ plots are particularly useful for identifying deviations from normality in the tails of the distribution; PP plots are particularly useful for identifying deviations from normality in the center of the distribution. Researchers tend to be more concerned about the tails of the distribution, because extreme values tend to have a greater impact on inferences, so researchers tend to use QQ plots more often than PP plots. Various examples of QQ plots and deviations from normality are depicted in Figure 11.4. If the residuals are normally distributed, they will stay close to the diagonal reference line of the QQ and PP plots.\nCodeset.seed(52242)\n\nsampleSize &lt;- 10000\n\ndistribution_normal &lt;- rnorm(\n  sampleSize,\n  mean = 0,\n  sd = 1)\n\ndistribution_bimodal &lt;- c(\n  rnorm(\n    sampleSize/2,\n    mean = -2,\n    sd = 1),\n  rnorm(\n    sampleSize/2,\n    mean = 2,\n    sd = 1))\n\ndistribution_negativeSkew &lt;- -rlnorm(\n  sampleSize,\n  meanlog = 0,\n  sdlog = 1)\n\ndistribution_positiveSkew &lt;- rlnorm(\n  sampleSize,\n  meanlog = 0,\n  sdlog = 1)\n\ndistribution_lightTailed &lt;- runif(\n  sampleSize,\n  min = -2,\n  max = 2)\n\ndistribution_heavyTailed &lt;- rt(\n  sampleSize,\n  df = 2)\n\nhist(\n  distribution_normal,\n  main = \"Normal Distribution\",\n  col = \"#0099F8\")\n\ncar::qqPlot(\n  distribution_normal,\n  main = \"QQ Plot\",\n  id = FALSE)\n\nhist(\n  distribution_bimodal,\n  main = \"Bimodal Distribution\",\n  col = \"#0099F8\")\n\ncar::qqPlot(\n  distribution_bimodal,\n  main = \"QQ Plot\",\n  id = FALSE)\n\nhist(\n  distribution_negativeSkew,\n  main = \"Negatively Skewed Distribution\",\n  col = \"#0099F8\")\n\ncar::qqPlot(\n  distribution_negativeSkew,\n  main = \"QQ Plot\",\n  id = FALSE)\n\nhist(\n  distribution_positiveSkew,\n  main = \"Positively Skewed Distribution\",\n  col = \"#0099F8\")\n\ncar::qqPlot(\n  distribution_positiveSkew,\n  main = \"QQ Plot\",\n  id = FALSE)\n\nhist(\n  distribution_lightTailed,\n  main = \"Platykurtic (Light Tailed) Distribution\",\n  col = \"#0099F8\")\n\ncar::qqPlot(\n  distribution_lightTailed,\n  main = \"QQ Plot\",\n  id = FALSE)\n\nhist(\n  distribution_heavyTailed,\n  main = \"Leptokurtic (Heavy Tailed) Distribution\",\n  col = \"#0099F8\")\n\ncar::qqPlot(\n  distribution_heavyTailed,\n  main = \"QQ Plot\",\n  id = FALSE)\n\n\n\n\n\n\n\n\n\n(a) Normal Distribution\n\n\n\n\n\n\n\n\n\n(b) Normal Distribution\n\n\n\n\n\n\n\n\n\n\n\n(c) Bimodal Distribution\n\n\n\n\n\n\n\n\n\n(d) Bimodal Distribution\n\n\n\n\n\n\n\n\n\n\n\n(e) Negatively Skewed Distribution\n\n\n\n\n\n\n\n\n\n(f) Negatively Skewed Distribution\n\n\n\n\n\n\n\n\n\n\n\n(g) Positively Skewed Distribution\n\n\n\n\n\n\n\n\n\n(h) Positively Skewed Distribution\n\n\n\n\n\n\n\n\n\n\n\n(i) Platykurtic (Light-Tailed) Distribution\n\n\n\n\n\n\n\n\n\n(j) Platykurtic (Light-Tailed) Distribution\n\n\n\n\n\n\n\n\n\n\n\n(k) Leptokurtic (Heavy-Tailed) Distribution\n\n\n\n\n\n\n\n\n\n(l) Leptokurtic (Heavy-Tailed) Distribution\n\n\n\n\n\n\nFigure 11.4: Quantile–Quantile (QQ) Plots of Various Distributions (Right Side) with the Histogram of the Associated Distribution (Left Side).\n\n\nIf the residuals are not normally distributed (i.e., they do not stay close to the diagonal reference line of the QQ and PP plots), it may be necessary to transform the outcome variable to be more normally distributed or to use a generalized linear model (GLM) that more closely matches the distribution of the outcome variable (e.g., Poisson, binomial, gamma).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionExplainedVariance",
    "href": "multiple-regression.html#sec-multipleRegressionExplainedVariance",
    "title": "11  Multiple Regression",
    "section": "\n11.6 How Much Variance the Model Explains",
    "text": "11.6 How Much Variance the Model Explains\nWhen estimating a multiple regression model, it can be useful to evaluate how much variance in the outcome variable that the predictor variable(s) explain (i.e., account for). If the variables collectively explain only a small amount of variance, it suggest that the predictor variables have a small effect size and that other predictor variables will be necessary to account for the majority of variability in the outcome variable. There are two primary indices of how much how much variance in the outcome variable is explained by the predictor variable(s): the coefficient of determination (\\(R^2\\)) and adjusted \\(R^2\\) (\\(R^2_{adj}\\)), described below.\n\n11.6.1 Coefficient of Determination (\\(R^2\\))\nAs noted in Section 9.6.6, The coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions (i.e., by the predictor variable(s)), as in Equation 9.25: \\(R^2 = \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\\). Various formulas for \\(R^2\\) are in Equation 9.19. Larger \\(R^2\\) values indicate greater accuracy. Multiple regression can be conceptualized with overlapping circles (similar to a venn diagram), where the non-overlapping portions of the circles reflect nonshared variance and the overlapping portions of the circles reflect shared variance, as in Figure 11.29.\n\n\n\n\n\nFigure 11.5: Conceptual Depiction of Proportion of Variance Explained (\\(R^2\\)) in an Outcome Variable (\\(Y\\)) by Multiple Predictors (\\(X1\\) and \\(X2\\)) in Multiple Regression. The size of each circle represents the variable’s variance. The proportion of variance in \\(Y\\) that is explained by the predictors is depicted by the areas in orange. The dark orange space (\\(G\\)) is where multiple predictors explain overlapping variance in the outcome. Overlapping variance that is explained in the outcome (\\(G\\)) will not be recovered in the regression coefficients when both predictors are included in the regression model. From Petersen (2024) and Petersen (2025).\n\n\nOne issue with \\(R^2\\) is that it increases as the number of predictors increases, which can lead to overfitting if using \\(R^2\\) as an index to compare models for purposes of selecting the “best-fitting” model. Consider the following example (adapted from Petersen (2025)) in which you have one predictor variable and one outcome variable, as shown in Table 11.1.\n\n\n\nTable 11.1: Example Data of Predictor (x1) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\n\n\n\n7\n1\n\n\n13\n2\n\n\n29\n7\n\n\n10\n2\n\n\n\n\n\n\n\n\nUsing the data, the best fitting regression model is: \\(y =\\) 3.98 \\(+\\) 3.59 \\(\\cdot x_1\\). In this example, the \\(R^2\\) is 0.98. The equation is not a perfect prediction, but with a single predictor variable, it captures the majority of the variance in the outcome.\nNow consider the following example where you add a second predictor variable to the data above, as shown in Table 11.2.\n\n\n\nTable 11.2: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\nx2\n\n\n\n7\n1\n3\n\n\n13\n2\n5\n\n\n29\n7\n1\n\n\n10\n2\n2\n\n\n\n\n\n\n\n\nWith the second predictor variable, the best fitting regression model is: \\(y =\\) 0.00 + 4.00 \\(\\cdot x_1 +\\) 1.00 \\(\\cdot x_2\\). In this example, the \\(R^2\\) is 1.00. The equation with the second predictor variable provides a perfect prediction of the outcome.\nProviding perfect prediction with the right set of predictor variables is the dream of multiple regression. So, using multiple regression, we often add predictor variables to incrementally improve prediction. Knowing how much variance would be accounted for by random chance follows Equation 11.4:\n\\[\nE(R^2) = \\frac{K}{n-1}\n\\tag{11.4}\\]\nwhere \\(E(R^2)\\) is the expected value of \\(R^2\\) (the proportion of variance explained), \\(K\\) is the number of predictor variables, and \\(n\\) is the sample size. The formula demonstrates that the more predictor variables in the regression model, the more variance will be accounted for by chance. With many predictor variables and a small sample, you can account for a large share of the variance merely by chance.\nAs an example, consider that we have 13 predictor variables to predict fantasy performance for 43 players. Assume that, with 13 predictor variables, we explain 38% of the variance (\\(R^2 = .38; r = .62\\)). We explained a lot of the variance in the outcome, but it is important to consider how much variance could have been explained by random chance: \\(E(R^2) = \\frac{K}{n-1} = \\frac{13}{43 - 1} = .31\\). We expect to explain 31% of the variance, by chance, in the outcome. So, 82% of the variance explained was likely spurious (i.e., \\(\\frac{.31}{.38} = .82\\)). As the sample size increases, the spuriousness decreases. To account for the number of predictor variables in the model, we can use a modified version of \\(R^2\\) called adjusted \\(R^2\\) (\\(R^2_{adj}\\)), described next.\n\n11.6.2 Adjusted \\(R^2\\) (\\(R^2_{adj}\\))\nAdjusted \\(R^2\\) (\\(R^2_{adj}\\)) accounts for the number of predictor variables in the model, based on how much would be expected to be accounted for by chance to penalize overfitting. Adjusted \\(R^2\\) (\\(R^2_{adj}\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictor variables in the model. The formula for adjusted \\(R^2\\) (\\(R^2_{adj}\\)) is in Equation 11.5:\n\\[\nR^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1}\n\\tag{11.5}\\]\nwhere \\(p\\) is the number of predictor variables in the model, and \\(n\\) is the sample size.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-overfitting",
    "href": "multiple-regression.html#sec-overfitting",
    "title": "11  Multiple Regression",
    "section": "\n11.7 Overfitting",
    "text": "11.7 Overfitting\nStatistical models applied to big data (e.g., data with many predictor variables) can overfit the data, which means that the statistical model accounts for error variance, which will not generalize to future samples. So, even though an overfitting statistical model appears to be accurate because it is accounting for more variance, it is not actually that accurate—it will predict new data less accurately than how accurately it accounts for the data with which the model was built. Overfitting is most likely to occur when the model is too complex relative to the sample size (e.g., many predictor variables or parameters) In the case of fantasy football analytics, this is especially relevant because there are hundreds if not thousands of variables we could consider for inclusion and many, many players when considering historical data.\nConsider an example where you develop an algorithm to predict players’ fantasy performance based on 2024 data using hundreds of predictor variables. To some extent, these predictor variables will likely account for true variance (i.e., signal) and error variance (i.e., noise). If we were to apply the same algorithm based on the 2024 prediction model to 2025 data, the prediction model would likely predict less accurately than with 2024 data. The regression coefficients (and resulting accuracy) tend to become weaker when applied to new data, a phenomenon called shrinkage, which is described in Section 15.7.1. For instance, shrinking is observed when applying multiple regression models to new data in Section 19.7 and when applying machine learning models to new data in Section 19.8.\nIn Figure 11.6, the blue line represents the true distribution of the data, and the red line is an overfitting model:\n\nCodeset.seed(52242)\n\nsampleSize &lt;- 200\nquadraticX &lt;- rnorm(sampleSize)\nquadraticY &lt;- quadraticX ^ 2 + rnorm(sampleSize)\nquadraticData &lt;- cbind(quadraticX, quadraticY) %&gt;%\n  data.frame %&gt;%\n  arrange(quadraticX)\n\nquadraticModel &lt;- lm(\n  quadraticY ~ quadraticX + I(quadraticX ^ 2),\n  data = quadraticData)\n\nquadraticNewData &lt;- data.frame(\n  quadraticX = seq(\n    from = min(quadraticData$quadraticX),\n    to = max(quadraticData$quadraticY),\n    length.out = sampleSize))\n\nquadraticNewData$quadraticY &lt;- predict(\n  quadraticModel,\n  newdata = quadraticNewData)\n\nloessFit &lt;- loess(\n  quadraticY ~ quadraticX,\n  data = quadraticData,\n  span = 0.01,\n  degree = 1)\n\nloessNewData &lt;- data.frame(\n  quadraticX = seq(\n    from = min(quadraticData$quadraticX),\n    to = max(quadraticData$quadraticY),\n    length.out = sampleSize))\n\nquadraticNewData$loessY &lt;- predict(\n  loessFit,\n  newdata = quadraticNewData)\n\nplot(\n  x = quadraticData$quadraticX,\n  y = quadraticData$quadraticY,\n  xlab = \"\",\n  ylab = \"\")\n\nlines(\n  quadraticNewData$quadraticY ~ quadraticNewData$quadraticX,\n  lwd = 2,\n  col = \"blue\")\n\nlines(\n  quadraticNewData$loessY ~ quadraticNewData$quadraticX,\n  lwd = 2,\n  col = \"red\")\n\n\n\n\n\n\nFigure 11.6: Over-fitting Model in Red Relative to the True Distribution of the Data in Blue. From Petersen (2024) and Petersen (2025).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-covariates",
    "href": "multiple-regression.html#sec-covariates",
    "title": "11  Multiple Regression",
    "section": "\n11.8 Covariates",
    "text": "11.8 Covariates\nCovariates are variables that you include in the statistical model to try to control for them so you can better isolate the unique contribution of the predictor variable(s) in relation to the outcome variable. Use of covariates examines the association between the predictor variable and the outcome variable when holding people’s level constant on the covariates. Inclusion of confounds as covariates allows potentially gaining a more accurate estimate of the causal effect of the predictor variable on the outcome variable. Ideally, you want to include any and all confounds as covariates. As described in Section 8.5.2.1, confounds are third variables that influence both the predictor variable and the outcome variable and explain their association. Covariates are potentially (but not necessarily) confounds. For instance, you might include the player’s age as a covariate in a model that examines whether a player’s 40-yard dash time at the NFL Combine predicts their fantasy points in their rookie year, but it may not be a confound.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionFantasyPointsWRs",
    "href": "multiple-regression.html#sec-multipleRegressionFantasyPointsWRs",
    "title": "11  Multiple Regression",
    "section": "\n11.9 Example: Predicting Wide Receivers’ Fantasy Points",
    "text": "11.9 Example: Predicting Wide Receivers’ Fantasy Points\nLet’s say we want to use a number of variables to predict a wide receiver’s fantasy performance. We want to consider several predictors, including the player’s age, height, weight, and target share. Target share is computed as the number of targets a player receives divided by the team’s total number of targets. We have only a few predictors and our sample size is large enough such that overfitting is not likely a concern.\n\n11.9.1 Examine Descriptive Statistics\nLet’s first examine descriptive statistics of the predictor and outcome variables.\n\nCodeplayer_stats_seasonal %&gt;% \n  dplyr::select(fantasyPoints, age, height, weight, target_share) %&gt;% \n  dplyr::summarise(across(\n      everything(),\n      .fns = list(\n        n = ~ length(na.omit(.)),\n        missingness = ~ mean(is.na(.)) * 100,\n        M = ~ mean(., na.rm = TRUE),\n        SD = ~ sd(., na.rm = TRUE),\n        min = ~ min(., na.rm = TRUE),\n        max = ~ max(., na.rm = TRUE),\n        range = ~ max(., na.rm = TRUE) - min(., na.rm = TRUE),\n        IQR = ~ IQR(., na.rm = TRUE),\n        MAD = ~ mad(., na.rm = TRUE),\n        median = ~ median(., na.rm = TRUE),\n        pseudomedian = ~ DescTools::HodgesLehmann(., na.rm = TRUE),\n        mode = ~ petersenlab::Mode(., multipleModes = \"mean\"),\n        skewness = ~ psych::skew(., na.rm = TRUE),\n        kurtosis = ~ psych::kurtosi(., na.rm = TRUE)),\n      .names = \"{.col}.{.fn}\")) %&gt;%\n    tidyr::pivot_longer(\n      cols = everything(),\n      names_to = c(\"variable\",\"index\"),\n      names_sep = \"\\\\.\") %&gt;% \n    tidyr::pivot_wider(\n      names_from = index,\n      values_from = value)\n\n\n  \n\n\n\nLet’s also examine the distributions of the variables using a density plot, as depicted in Figure 11.7.\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"WR\")),\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Fantasy Points with Histogram and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"WR\")),\n  mapping = aes(\n    x = age)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Age (years)\",\n    y = \"Density\",\n    title = \"Density Plot of Player Age with Histogram and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"WR\")),\n  mapping = aes(\n    x = height)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Height (inches)\",\n    y = \"Density\",\n    title = \"Density Plot of Player Height with Histogram and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"WR\")),\n  mapping = aes(\n    x = weight)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Weight (pounds)\",\n    y = \"Density\",\n    title = \"Density Plot of Player Weight with Histogram and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;%\n    filter(position_group %in% c(\"WR\")),\n  mapping = aes(\n    x = target_share)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  labs(\n    x = \"Target Share\",\n    y = \"Density\",\n    title = \"Density Plot of Target Share with Histogram and Rug Plot\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\n\n\n\n(a) Fantasy Points\n\n\n\n\n\n\n\n\n\n\n\n(b) Age\n\n\n\n\n\n\n\n\n\n(c) Height\n\n\n\n\n\n\n\n\n\n\n\n(d) Weight\n\n\n\n\n\n\n\n\n\n(e) Target Share\n\n\n\n\n\n\nFigure 11.7: Density Plot of Model Variables (i.e., Predictor and Outcome Variables).\n\n\n\n\n11.9.2 Examine Bivariate Associations\nThen, let’s examine the bivariate association of each using a scatterplot to evaluate for any potential nonlinearity, as depicted in Figure 11.8.\nCodeggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  aes(\n    x = age,\n    y = fantasyPoints)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\") +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Player Age (Years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age\",\n    subtitle = \"(Among Wide Receivers)\"\n  ) +\n  theme_classic()\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  aes(\n    x = height,\n    y = fantasyPoints)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\") +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Player Height (Inches)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Height\",\n    subtitle = \"(Among Wide Receivers)\"\n  ) +\n  theme_classic()\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  aes(\n    x = weight,\n    y = fantasyPoints)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\") +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Player Weight (Pounds)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Weight\",\n    subtitle = \"(Among Wide Receivers)\"\n  ) +\n  theme_classic()\n\nggplot2::ggplot(\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  aes(\n    x = target_share,\n    y = fantasyPoints)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\") +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Target Share\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Target Share\",\n    subtitle = \"(Among Wide Receivers)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n(a) Age\n\n\n\n\n\n\n\n\n\n(b) Height\n\n\n\n\n\n\n\n\n\n\n\n(c) Weight\n\n\n\n\n\n\n\n\n\n(d) Target Share\n\n\n\n\n\n\nFigure 11.8: Scatterplots With Fantasy Points (Season) Among Wide Receivers. The linear best-fit line is in black. The nonlinear best-fit line is in blue.\n\n\nThere are some suggestions of potential nonlinearity, such as an inverted-U-shaped association between height and fantasy points, suggesting that there may an optimal range for height among Wide Receivers—being too short or too tall could be a disadvantage. In addition, target share shows a weakening association as target share increases. Thus, after evaluating the linear association between the predictors and outcome, we will also examine the possibility for curvilinear associations.\n\n11.9.3 Estimate Multiple Regression Model\nNow that we have examined descriptive statistics and bivariate associations, let’s first estimate a multiple regression model with only linear terms:\n\nCodelinearRegressionModel &lt;- lm(\n  fantasyPoints ~ age + height + weight + target_share,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\n\nThe model formula is in Equation 11.6:\n\\[\n\\text{fantasy points} = \\beta_0 + \\beta_1 \\cdot \\text{age} + \\beta_2 \\cdot \\text{height} + \\beta_3 \\cdot \\text{weight} + \\beta_4 \\cdot \\text{target share} + \\epsilon\n\\tag{11.6}\\]\nHere are the model results:\n\nCodesummary(linearRegressionModel)\n\n\nCall:\nlm(formula = fantasyPoints ~ age + height + weight + target_share, \n    data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")), \n    na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-739.80  -18.68  -10.50   10.74  292.78 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -34.62684   23.85029  -1.452 0.146617    \nage            0.75193    0.22418   3.354 0.000803 ***\nheight         0.05816    0.42094   0.138 0.890107    \nweight         0.14716    0.06686   2.201 0.027794 *  \ntarget_share 743.14867    7.38684 100.604  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.71 on 4432 degrees of freedom\n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.7054,    Adjusted R-squared:  0.7052 \nF-statistic:  2654 on 4 and 4432 DF,  p-value: &lt; 2.2e-16\n\n\nThe only terms that were significantly associated with fantasy performance among Wide Receivers are weight and target share, both of which showed a positive association with fantasy points.\nWe can obtain the coefficient of determination (\\(R^2\\)) and adjusted \\(R^2\\) (\\(R^2_{adj}\\)) using the following code:\n\nCodesummary(linearRegressionModel)$r.squared\n\n[1] 0.7054381\n\nCodesummary(linearRegressionModel)$adj.r.squared\n\n[1] 0.7051723\n\n\nThe model explained 71% of the variability in fantasy points (i.e., \\(R^2 = .71\\)).\nThe model formula with substituted values is in Equation 11.7:\n\\[\n\\begin{aligned}\n  \\text{fantasy points} = &\\beta_0 + \\beta_1 \\cdot \\text{age} + \\beta_2 \\cdot \\text{height} + \\beta_3 \\cdot \\text{weight} + \\beta_4 \\cdot \\text{target share} + \\epsilon \\\\\n  = &-34.63 + 0.75 \\cdot \\text{age} + 0.06 \\cdot \\text{height} + 0.15 \\cdot \\text{weight} + 743.15 \\cdot \\text{target share} + \\epsilon\n\\end{aligned}\n\\tag{11.7}\\]\nIf we want to obtain standardized regression coefficients, we can use the effectsize::standardize_parameters() function of the effectsize package (Ben-Shachar et al., 2020, 2025).\n\nCodeprint(effectsize::standardize_parameters(linearRegressionModel, method = \"basic\"), digits = 2)\n\n# Standardization method: basic\n\nParameter    | Std. Coef. |        95% CI\n-----------------------------------------\n(Intercept)  |       0.00 | [ 0.00, 0.00]\nage          |       0.03 | [ 0.01, 0.04]\nheight       |   1.63e-03 | [-0.02, 0.02]\nweight       |       0.03 | [ 0.00, 0.05]\ntarget share |       0.83 | [ 0.82, 0.85]\n\n\nOr, we can standardize the outcome variable and each predictor variable using the base::scale() function:\n\nCodelinearRegressionModelStandardized &lt;- lm(\n  scale(fantasyPoints) ~ scale(age) + scale(height) + scale(weight) + scale(target_share),\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(linearRegressionModelStandardized)\n\n\nCall:\nlm(formula = scale(fantasyPoints) ~ scale(age) + scale(height) + \n    scale(weight) + scale(target_share), data = player_stats_seasonal %&gt;% \n    filter(position %in% c(\"WR\")), na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7953 -0.2221 -0.1249  0.1277  3.4808 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -0.0003851  0.0081630  -0.047 0.962380    \nscale(age)           0.0281808  0.0084020   3.354 0.000803 ***\nscale(height)        0.0016241  0.0117541   0.138 0.890107    \nscale(weight)        0.0259735  0.0118013   2.201 0.027794 *  \nscale(target_share)  0.8331762  0.0082817 100.604  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5435 on 4432 degrees of freedom\n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.7054,    Adjusted R-squared:  0.7052 \nF-statistic:  2654 on 4 and 4432 DF,  p-value: &lt; 2.2e-16\n\nCodeprint(effectsize::standardize_parameters(linearRegressionModel, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter    | Std. Coef. |        95% CI\n-----------------------------------------\n(Intercept)  |  -8.91e-17 | [-0.02, 0.02]\nage          |       0.03 | [ 0.01, 0.04]\nheight       |   1.63e-03 | [-0.02, 0.02]\nweight       |       0.03 | [ 0.00, 0.05]\ntarget share |       0.83 | [ 0.82, 0.85]\n\n\nTarget share has a large effect size. All of the other predictors have a small effect size.\nNow let’s consider whether any of the terms show curvilinear associations with fantasy points by adding quadratic terms:\n\nCodequadraticTermsRegressionModel &lt;- lm(\n  fantasyPoints ~ age + I(age^2) + height + I(height^2) + weight + I(weight^2) + target_share + I(target_share^2),\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\n\nThe model formula is in Equation 11.8:\n\\[\n\\begin{aligned}\n  \\text{fantasy points} \\; = \\; &\\beta_0 + \\beta_1 \\cdot \\text{age} + \\beta_2 \\cdot \\text{age}^2 + \\beta_3 \\cdot \\text{height} + \\beta_4 \\cdot \\text{height}^2 + \\\\\n  &\\beta_5 \\cdot \\text{weight} + \\beta_6 \\cdot \\text{weight}^2 + \\beta_7 \\cdot \\text{target share} + \\beta_8 \\cdot \\text{target share}^2 + \\epsilon\n\\end{aligned}\n\\tag{11.8}\\]\nHere are the model results:\n\nCodesummary(quadraticTermsRegressionModel)\n\n\nCall:\nlm(formula = fantasyPoints ~ age + I(age^2) + height + I(height^2) + \n    weight + I(weight^2) + target_share + I(target_share^2), \n    data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")), \n    na.action = \"na.exclude\")\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-276.575  -15.242   -3.012    6.187  310.190 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.379e+02  4.258e+02   0.324   0.7462    \nage                3.698e+00  2.232e+00   1.657   0.0976 .  \nI(age^2)          -5.963e-02  4.022e-02  -1.483   0.1382    \nheight            -3.861e+00  1.247e+01  -0.310   0.7568    \nI(height^2)        2.315e-02  8.601e-02   0.269   0.7878    \nweight            -5.090e-01  7.525e-01  -0.676   0.4988    \nI(weight^2)        1.694e-03  1.868e-03   0.907   0.3646    \ntarget_share       1.121e+03  9.067e+00 123.605   &lt;2e-16 ***\nI(target_share^2) -9.512e+02  1.764e+01 -53.936   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.52 on 4428 degrees of freedom\n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.8224,    Adjusted R-squared:  0.822 \nF-statistic:  2563 on 8 and 4428 DF,  p-value: &lt; 2.2e-16\n\n\nOnly target share (not height or weight) shows a significant association, including its linear and quadratic terms.\nHere are the standardized coefficients:\n\nCodeprint(effectsize::standardize_parameters(quadraticTermsRegressionModel, method = \"basic\"), digits = 2)\n\n# Standardization method: basic\n\nParameter      | Std. Coef. |         95% CI\n--------------------------------------------\n(Intercept)    |       0.00 | [ 0.00,  0.00]\nage            |       0.14 | [-0.02,  0.30]\nage^2          |      -0.12 | [-0.28,  0.04]\nheight         |      -0.11 | [-0.79,  0.58]\nheight^2       |       0.09 | [-0.59,  0.78]\nweight         |      -0.09 | [-0.35,  0.17]\nweight^2       |       0.12 | [-0.14,  0.38]\ntarget share   |       1.26 | [ 1.24,  1.28]\ntarget share^2 |      -0.54 | [-0.56, -0.52]\n\n\n\nCodequadraticTermsRegressionModelStandardized &lt;- lm(\n  scale(fantasyPoints) ~ scale(age) + scale(I(age^2)) + scale(height) + scale(I(height^2)) + scale(weight) + scale(I(weight^2)) + scale(target_share) + scale(I(target_share^2)),\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(quadraticTermsRegressionModelStandardized)\n\n\nCall:\nlm(formula = scale(fantasyPoints) ~ scale(age) + scale(I(age^2)) + \n    scale(height) + scale(I(height^2)) + scale(weight) + scale(I(weight^2)) + \n    scale(target_share) + scale(I(target_share^2)), data = player_stats_seasonal %&gt;% \n    filter(position %in% c(\"WR\")), na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2881 -0.1812 -0.0358  0.0736  3.6878 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -0.0009542  0.0063426  -0.150   0.8804    \nscale(age)                0.1386022  0.0836588   1.657   0.0976 .  \nscale(I(age^2))          -0.1246500  0.0840684  -1.483   0.1382    \nscale(height)            -0.1078183  0.3480878  -0.310   0.7568    \nscale(I(height^2))        0.0936578  0.3479758   0.269   0.7878    \nscale(weight)            -0.0898412  0.1328264  -0.676   0.4988    \nscale(I(weight^2))        0.1203022  0.1326650   0.907   0.3646    \nscale(target_share)       1.2565612  0.0101659 123.605   &lt;2e-16 ***\nscale(I(target_share^2)) -0.5428470  0.0100647 -53.936   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4222 on 4428 degrees of freedom\n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.8224,    Adjusted R-squared:  0.822 \nF-statistic:  2563 on 8 and 4428 DF,  p-value: &lt; 2.2e-16\n\nCodeprint(effectsize::standardize_parameters(quadraticTermsRegressionModel, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter      | Std. Coef. |         95% CI\n--------------------------------------------\n(Intercept)    |       0.10 | [ 0.08,  0.12]\nage            |       0.02 | [ 0.01,  0.04]\nage^2          |  -6.76e-03 | [-0.02,  0.00]\nheight         |      -0.01 | [-0.03,  0.00]\nheight^2       |   1.53e-03 | [-0.01,  0.01]\nweight         |       0.03 | [ 0.01,  0.05]\nweight^2       |   4.48e-03 | [-0.01,  0.01]\ntarget share   |       1.07 | [ 1.05,  1.08]\ntarget share^2 |      -0.10 | [-0.10, -0.10]\n\n\n\n11.9.4 Dominance Analysis\nWe can perform a dominance analysis to evaluate the relative importance of the predictors in the regression model. To perform the dominance analysis, we use the parameters::dominance_analysis() function of the parameters package (Lüdecke et al., 2020; Lüdecke, Makowski, Ben-Shachar, Patil, Højsgaard, et al., 2025), which leverages the domir package (Luchman, 2024).\n\nCodeparameters::dominance_analysis(linearRegressionModel)\n\n# Dominance Analysis Results\n\nModel R2 Value:  0.705 \n\nGeneral Dominance Statistics\n\nParameter    | General Dominance | Percent | Ranks |       Subset\n-----------------------------------------------------------------\n(Intercept)  |                   |         |       |     constant\nage          |             0.009 |   0.013 |     2 |          age\nheight       |             0.002 |   0.003 |     4 |       height\nweight       |             0.007 |   0.010 |     3 |       weight\ntarget_share |             0.687 |   0.974 |     1 | target_share\n\nConditional Dominance Statistics\n\nSubset       | IVs: 1 | IVs: 2 |    IVs: 3 |    IVs: 4\n------------------------------------------------------\nage          |  0.017 |  0.012 |     0.006 | 7.477e-04\nheight       |  0.006 |  0.002 | 1.967e-04 | 1.269e-06\nweight       |  0.016 |  0.009 |     0.003 | 3.219e-04\ntarget_share |  0.704 |  0.692 |     0.681 |     0.673\n\nComplete Dominance Designations\n\nSubset       | &lt; age | &lt; height | &lt; weight | &lt; target_share\n-----------------------------------------------------------\nage          |       |    FALSE |    FALSE |           TRUE\nheight       |  TRUE |          |     TRUE |           TRUE\nweight       |  TRUE |    FALSE |          |           TRUE\ntarget_share | FALSE |    FALSE |    FALSE |               \n\n\n\n11.9.5 Visualizing Regression Results\n\n11.9.5.1 Regression Coefficients\nTo visualize the regression coefficients, we can use the broom::tidy() function of the broom package (Robinson et al., 2025), as in Figures 11.9 and 11.10.\n\nCodequadraticTermsRegressionModel_tidy &lt;- broom::tidy(\n  quadraticTermsRegressionModel,\n  conf.int = TRUE)\n\nquadraticTermsRegressionModelStandardized_tidy &lt;- broom::tidy(\n  quadraticTermsRegressionModelStandardized,\n  conf.int = TRUE)\n\n\n\nCodeggplot2::ggplot(\n  data = quadraticTermsRegressionModel_tidy,\n  aes(\n    x = term,\n    y = estimate)) +\n  geom_point() +\n  geom_errorbar(\n    aes(\n      ymin = conf.low,\n      ymax = conf.high),\n    width = 0.2) +\n  coord_flip() + # flip axes for readability\n  labs(\n    title = \"Regression Coefficients with 95% CI\",\n    x = \"Predictor\",\n    y = \"Coefficient Estimate\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 11.9: Regression Coefficients with 95% Confidence Interval.\n\n\n\n\n\nCodeggplot2::ggplot(\n  data = quadraticTermsRegressionModelStandardized_tidy,\n  aes(\n    x = term,\n    y = estimate)) +\n  geom_point() +\n  geom_errorbar(\n    aes(\n      ymin = conf.low,\n      ymax = conf.high),\n    width = 0.2) +\n  coord_flip() + # flip axes for readability\n  labs(\n    title = \"Standardized Regression Coefficients with 95% CI\",\n    x = \"Predictor\",\n    y = \"Standardized Coefficient Estimate\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 11.10: Standardized Regression Coefficients with 95% Confidence Interval.\n\n\n\n\n\n11.9.5.2 Model-Implied Association\nIf we wanted to visualize the shape of the model-implied association between target share and fantasy points, we could generate the model-implied predictions using the data range that we want to visualize.\n\nCodenewdata &lt;- data.frame(\n  target_share = seq(\n    from = min(player_stats_seasonal$target_share[which(player_stats_seasonal$position == \"WR\")], na.rm = TRUE),\n    to = max(player_stats_seasonal$target_share[which(player_stats_seasonal$position == \"WR\")], na.rm = TRUE),\n    length.out = 1000\n  )\n)\n\nnewdata$age &lt;- mean(player_stats_seasonal$age[which(player_stats_seasonal$position == \"WR\")], na.rm = TRUE)\nnewdata$height &lt;- mean(player_stats_seasonal$height[which(player_stats_seasonal$position == \"WR\")], na.rm = TRUE)\nnewdata$weight &lt;- mean(player_stats_seasonal$weight[which(player_stats_seasonal$position == \"WR\")], na.rm = TRUE)\n\n\n\nCodenewdata$fantasyPoints &lt;- predict(\n  quadraticTermsRegressionModel,\n  newdata = newdata\n)\n\n\nWe can depict the model-implied predictions of fantasy points as a function of target share, as shown in Figure 11.11.\n\nCodeggplot2::ggplot(\n  data = newdata,\n  aes(\n    x = target_share,\n    y = fantasyPoints)) +\n  geom_smooth() +\n  coord_cartesian(\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Target Share\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Target Share\",\n    subtitle = \"(Among Wide Receivers)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 11.11: Model-Implied Predictions of A Wide Receiver’s Fantasy Points as a Function of Target Share. The model-implied predictions were estimated based on a multiple regression model.\n\n\n\n\nWe could also generate the model-implied prediction of fantasy points for any value of the predictor variables. For instance, here are the number of fantasy points expected for a Wide Receiver who is 23 years old, is 6’2” tall (72 inches), weighs 200 pounds, and has a target share of 50% (i.e., 0.5):\n\nCodepredict(\n  quadraticTermsRegressionModel,\n  newdata = data.frame(\n    age = 23,\n    height = 72,\n    weight = 200,\n    target_share = .5\n  )\n)\n\n       1 \n321.9066 \n\n\nThe player would be expected to score 322 fantasy points.\nWe can calculate this by substituting in the regression coefficients and predictor values. Here is the computation:\n\nCodeage &lt;- 23\nheight &lt;- 72\nweight &lt;- 200\ntarget_share &lt;- .5\n\nbeta0 &lt;- coef(quadraticTermsRegressionModel)[[\"(Intercept)\"]]\nbeta1 &lt;- coef(quadraticTermsRegressionModel)[[\"age\"]]\nbeta2 &lt;- coef(quadraticTermsRegressionModel)[[\"I(age^2)\"]]\nbeta3 &lt;- coef(quadraticTermsRegressionModel)[[\"height\"]]\nbeta4 &lt;- coef(quadraticTermsRegressionModel)[[\"I(height^2)\"]]\nbeta5 &lt;- coef(quadraticTermsRegressionModel)[[\"weight\"]]\nbeta6 &lt;- coef(quadraticTermsRegressionModel)[[\"I(weight^2)\"]]\nbeta7 &lt;- coef(quadraticTermsRegressionModel)[[\"target_share\"]]\nbeta8 &lt;- coef(quadraticTermsRegressionModel)[[\"I(target_share^2)\"]]\n\npredictedFantasyPoints &lt;- beta0 + beta1*age + beta2*(age^2) + beta3*height + beta4*(height^2) + beta5*weight + beta6*(weight^2) + beta7*target_share + beta8*(target_share^2)\npredictedFantasyPoints\n\n[1] 321.9066\n\n\nThe model formula with substituted values is in Equation 11.9:\n\\[\n\\begin{aligned}\n  \\text{fantasy points} \\; = \\; &\\beta_0 + \\beta_1 \\cdot \\text{age} + \\beta_2 \\cdot \\text{age}^2 + \\beta_3 \\cdot \\text{height} + \\beta_4 \\cdot \\text{height}^2 + \\\\\n  &\\beta_5 \\cdot \\text{weight} + \\beta_6 \\cdot \\text{weight}^2 + \\beta_7 \\cdot \\text{target share} + \\beta_8 \\cdot \\text{target share}^2 + \\epsilon \\\\\n  = &137.86 + 3.70 \\cdot 23 + -0.06 \\cdot 23^2 + -3.86 \\cdot 72 + 0.02 \\cdot 72^2 + \\\\\n  &-0.51 \\cdot 200 + 0.00 \\cdot 200^2 + 1120.79 \\cdot 0.5 + -951.24 \\cdot 0.5^2 + \\epsilon \\\\\n  = &321.91\n\\end{aligned}\n\\tag{11.9}\\]\n\n11.9.6 Evaluating and Addressing Assumptions\nThe assumptions for multiple regression are described in Section 11.5. I describe ways to evaluate assumptions in Section 11.5.1.\nAs a reminder, here are four assumptions:\n\nthere is a linear association between the predictor variables and the outcome variable\n\nthere is homoscedasticity of the residuals; the residuals do not differ as a function of the predictor variables or as a function of the outcome variable\n\nthe residuals are independent; they are uncorrelated with each other\nthe residuals are normally distributed\n\n\n11.9.6.1 Linear Association\nWe evaluated the shape of the association between the predictor variables and the outcome variables using scatterplots. We accounted for potential curvilinearity in the associations with a quadratic term. Other ways to account for nonlinearity, in addition to polynomials, include transforming predictors, use of splines/piecewise regression, and generalized additive models.\nTo evaluate for potential nonlinearity in the associations, we can also evaluate residual plots (Figure 11.21), marginal model plots (Figure 11.12), added-variable plots (Figure 11.13), and component-plus-residual plots (Figure 11.14) from the car package (Fox et al., 2024; Fox & Weisberg, 2019). For evaluating linearity, we would expect minimal bend/curvature in the lines.\n\nCodecar::marginalModelPlots(\n  quadraticTermsRegressionModel,\n  sd = TRUE,\n  id = TRUE)\n\n\n\n\n\n\nFigure 11.12: Marginal Model Plots.\n\n\n\n\n\nCodecar::avPlots(\n  quadraticTermsRegressionModel,\n  id = TRUE)\n\n\n\n\n\n\nFigure 11.13: Added-Variable Plots.\n\n\n\n\n\nCodecar::crPlots(\n  quadraticTermsRegressionModel,\n  sd = TRUE,\n  id = TRUE)\n\n\n\n\n\n\nFigure 11.14: Component-Plus-Residual Plots.\n\n\n\n\nThe marginal model plots (Figure 11.12), residual plots (Figure 11.21), and component-plus-residual plots (Figure 11.14) suggest that the nonlinearity of the association between target share and fantasy points may not be fully captured by the quadratic term. Thus, we may need to apply a different approach to handling the nonlinear association between target share and fantasy points.\nOne approach we can take is to transform the target_shares variable to be more normally distributed.\nThe histogram for the raw target_shares variable is in Figure 11.15.\n\nCodehist(\n  player_stats_seasonal$target_share[which(player_stats_seasonal$position == \"WR\")],\n  main = \"Histogram of Target Share\")\n\n\n\n\n\n\nFigure 11.15: Histogram of Target Share.\n\n\n\n\nThe variable shows a strong positive skew. To address a strong positive skew, we can use a log transformation. The histogram of the log-transformed variable is in Figure 11.16.\n\nCodehist(\n  log(player_stats_seasonal$target_share[which(player_stats_seasonal$position == \"WR\")] + 1),\n  main = \"Histogram of Target Share (Log Transformed)\")\n\n\n\n\n\n\nFigure 11.16: Histogram of Target Share, Transformed.\n\n\n\n\nNow we can re-fit the model with the log-transformed variable.\n\nCodelinearRegressionModel_logTargetShare &lt;- lm(\n  fantasyPoints ~ age + I(age^2) + height + I(height^2) + weight + I(weight^2) + I(log(target_share + 1)),\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(linearRegressionModel_logTargetShare)\n\n\nCall:\nlm(formula = fantasyPoints ~ age + I(age^2) + height + I(height^2) + \n    weight + I(weight^2) + I(log(target_share + 1)), data = player_stats_seasonal %&gt;% \n    filter(position %in% c(\"WR\")), na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-610.70  -15.60   -8.09    7.67  299.80 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -1.130e+02  4.995e+02  -0.226    0.821    \nage                       3.243e+00  2.619e+00   1.238    0.216    \nI(age^2)                 -4.782e-02  4.718e-02  -1.014    0.311    \nheight                    4.906e+00  1.462e+01   0.336    0.737    \nI(height^2)              -3.416e-02  1.009e-01  -0.339    0.735    \nweight                   -1.168e+00  8.826e-01  -1.324    0.186    \nI(weight^2)               3.247e-03  2.191e-03   1.482    0.138    \nI(log(target_share + 1))  8.974e+02  7.864e+00 114.108   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.66 on 4429 degrees of freedom\n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.7555,    Adjusted R-squared:  0.7551 \nF-statistic:  1955 on 7 and 4429 DF,  p-value: &lt; 2.2e-16\n\n\nTarget share shows a more linear association with fantasy points after log-transforming it (albeit still not perfect), as depicted in Figures 11.17, 11.18, 11.19, and 11.20.\n\nCodecar::marginalModelPlots(\n  linearRegressionModel_logTargetShare,\n  sd = TRUE,\n  id = TRUE)\n\n\n\n\n\n\nFigure 11.17: Marginal Model Plots After Log Transformation of Target Share.\n\n\n\n\n\nCodecar::avPlots(\n  linearRegressionModel_logTargetShare,\n  id = TRUE)\n\n\n\n\n\n\nFigure 11.18: Added-Variable Plots After Log Transformation of Target Share.\n\n\n\n\n\nCodecar::crPlots(\n  linearRegressionModel_logTargetShare,\n  id = TRUE)\n\n\n\n\n\n\nFigure 11.19: Component-Plus-Residual Plots After Log Transformation of Target Share.\n\n\n\n\nWhen creating a residual plot, the car package (Fox et al., 2024; Fox & Weisberg, 2019) also provides a test of the nonlinearity of each predictor.\n\nCodecar::residualPlots(\n  linearRegressionModel_logTargetShare,\n  id = TRUE)\n\n                         Test stat Pr(&gt;|Test stat|)    \nage                         1.2532           0.2102    \nI(age^2)                    1.0520           0.2928    \nheight                      0.2004           0.8412    \nI(height^2)                 1.3782           0.1682    \nweight                      0.7991           0.4243    \nI(weight^2)                -0.4440           0.6571    \nI(log(target_share + 1))  -34.8271           &lt;2e-16 ***\nTukey test                -34.7456           &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\nFigure 11.20: Residual Plots After Log Transformation of Target Share.\n\n\n\n\n\n11.9.6.2 Homoscedasticity\nTo evaluate homoscedasticity, we can evaluate a residual plot (Figure 11.21) and spread-level plot (Figure 11.22) from the car package (Fox et al., 2024; Fox & Weisberg, 2019). In a residual plot, you want a constant spread of residuals versus fitted values—you do not want the residuals to show a fan or cone shape.\n\nCodecar::residualPlots(\n  quadraticTermsRegressionModel,\n  id = TRUE)\n\n                  Test stat Pr(&gt;|Test stat|)    \nage                  1.0848          0.27806    \nI(age^2)             1.9964          0.04595 *  \nheight               0.5255          0.59927    \nI(height^2)          1.7659          0.07747 .  \nweight               0.8483          0.39634    \nI(weight^2)         -0.7810          0.43485    \ntarget_share         1.5656          0.11751    \nI(target_share^2)  -15.0992          &lt; 2e-16 ***\nTukey test          20.2150          &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\nFigure 11.21: Residual Plots.\n\n\n\n\nIn a spread-level plot, you want a flat (zero) slope—you do not want a positive or negative slope.\n\nCodecar::spreadLevelPlot(\n  quadraticTermsRegressionModel,\n  id = TRUE)\n\n\nSuggested power transformation:  0.4647539 \n\n\n\n\n\n\n\nFigure 11.22: Spread-Level Plot.\n\n\n\n\nIn this example, the residuals appear to increase as a function of the fitted values. To handle this, we may need to transform the outcome variable to be more normally distributed.\nThe histogram for raw fantasy points is in Figure 12.35.\n\nCodehist(\n  player_stats_seasonal$fantasyPoints[which(player_stats_seasonal$position == \"WR\")],\n  main = \"Histogram of Fantasy Points\")\n\n\n\n\n\n\nFigure 11.23: Histogram of Fantasy Points (Among Wide Receivers).\n\n\n\n\nWe can apply a transformation to the outcome variable to generate a more normally distributed variable using the bestNormalize::bestNormalize() function of the bestNormalize package (Peterson, 2021; Peterson, 2023; Peterson & Cavanaugh, 2020).\n\nCodebnTransformed &lt;- bestNormalize::bestNormalize(player_stats_seasonal$fantasyPoints)\n\n\nHere are the transformation results—in this case, the best-fitting transformation is an ordered quantile (ORQ) normalization transformation:\n\nCodebnTransformed\n\nBest Normalizing transformation with 41156 Observations\n Estimated Normality Statistics (Pearson P / df, lower =&gt; more normal):\n - arcsinh(x): 15.4849\n - Center+scale: 108.5775\n - Double Reversed Log_b(x+a): 144.3802\n - Exp(x): 77.6528\n - Log_b(x+a): 15.1748\n - orderNorm (ORQ): 4.2877\n - sqrt(x + a): 39.6506\n - Yeo-Johnson: 8.1302\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 41156 nonmissing obs and ties\n - 4951 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n -7.28   8.50  29.30  69.98 485.10 \n\n\nThen, we apply the best-fitting transformation to the variable, to create a transformed variable that is more normally distributed.\n\nCodeplayer_stats_seasonal$fantasyPoints_transformed &lt;- predict(bnTransformed)\n\n# Can back-transform transformed values back to the original metric later if needed:\noriginal &lt;- predict(\n  bnTransformed,\n  newdata = player_stats_seasonal$fantasyPoints_transformed,\n  inverse = TRUE)\n\n\nThe histogram of the transformed variable is in Figure 11.24.\n\nCodehist(\n  player_stats_seasonal$fantasyPoints_transformed[which(player_stats_seasonal$position == \"WR\")],\n  main = \"Histogram of Fantasy Points (Transformed)\")\n\n\n\n\n\n\nFigure 11.24: Histogram of Fantasy Points (Among Wide Receivers), Transformed.\n\n\n\n\nNow we can refit the model.\n\nCodelinearRegressionModel_outcomeTransformed &lt;- lm(\n  fantasyPoints_transformed ~ age + height + weight + I(log(target_share + 1)),\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(linearRegressionModel_outcomeTransformed)\n\n\nCall:\nlm(formula = fantasyPoints_transformed ~ age + height + weight + \n    I(log(target_share + 1)), data = player_stats_seasonal %&gt;% \n    filter(position %in% c(\"WR\")), na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6898 -0.2314  0.0787  0.3327  3.0328 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -1.0023793  0.3386383  -2.960  0.00309 ** \nage                       0.0137253  0.0031839   4.311 1.66e-05 ***\nheight                   -0.0029027  0.0059758  -0.486  0.62718    \nweight                    0.0015978  0.0009491   1.683  0.09235 .  \nI(log(target_share + 1)) 11.6558461  0.1222334  95.357  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.649 on 4432 degrees of freedom\n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.6831,    Adjusted R-squared:  0.6828 \nF-statistic:  2388 on 4 and 4432 DF,  p-value: &lt; 2.2e-16\n\n\nThe residual plot is in Figure 11.25.\n\nCodecar::residualPlots(\n  linearRegressionModel_outcomeTransformed,\n  id = TRUE)\n\n                         Test stat Pr(&gt;|Test stat|)    \nage                        -0.1390           0.8894    \nheight                     -0.4390           0.6607    \nweight                     -1.5104           0.1310    \nI(log(target_share + 1))  -46.7411           &lt;2e-16 ***\nTukey test                -47.2803           &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\nFigure 11.25: Residual Plots After Transformation of Fantasy Points.\n\n\n\n\nThe spread-level plot is in Figure 11.26.\n\nCodecar::spreadLevelPlot(\n  linearRegressionModel_outcomeTransformed,\n  id = TRUE)\n\n\nSuggested power transformation:  1.063421 \n\n\n\n\n\n\n\nFigure 11.26: Spread-Level Plot After Transformation of Fantasy Points.\n\n\n\n\nThe residuals show more constant variance after transforming the outcome variable.\n\n11.9.6.3 Uncorrelated Residuals\nTo determine if residuals are correlated given the nested structure of the data, we can examine the proportion of variance that is attributable to the particular player. To do this, we can estimate the intraclass correlation coefficient (ICC) from a mixed model using the performance package (Lüdecke et al., 2021; Lüdecke, Makowski, Ben-Shachar, Patil, Waggoner, et al., 2025).\n\nCodemixedModel &lt;- lmer(\n  fantasyPoints_transformed ~ 1 + (1 | player_id),\n  data = player_stats_seasonal)\n\nperformance::icc(mixedModel)\n\n\n  \n\n\n\nThe ICC indicates that over half of the variance is attribute to between-player variance, so it would be important to account for the player-specific variance using a mixed model. For simplicity, we focus on multiple regression models in this chapter; mixed models are described in Chapter 12.\n\n11.9.6.4 Normally Distributed Residuals\nWe can examine whether residuals are normally distributed using quantile–quantile (QQ) plots and probability–probability (PP) plots, as in Figures 11.27 and 11.28. If the residuals are normally distributed, they should stay close to the diagonal reference line.\n\nCodecar::qqPlot(\n  linearRegressionModel_outcomeTransformed,\n  main = \"QQ Plot\",\n  id = TRUE)\n\n[1] 1338 1843\n\n\n\n\n\n\n\nFigure 11.27: Residual Plots After Transformation of Fantasy Points.\n\n\n\n\n\nCodepetersenlab::ppPlot(\n  linearRegressionModel_outcomeTransformed)\n\n\n\n\n\n\nFigure 11.28: Residual Plots After Transformation of Fantasy Points.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionMulticollinearity",
    "href": "multiple-regression.html#sec-multipleRegressionMulticollinearity",
    "title": "11  Multiple Regression",
    "section": "\n11.10 Multicollinearity",
    "text": "11.10 Multicollinearity\nMulticollinearity occurs when two or more predictor variables in a regression model are highly correlated. The problem of having multiple predictor variables that are highly correlated is that it makes it challenging to estimate the regression coefficients accurately.\nMulticollinearity in multiple regression is depicted conceptually in Figure 11.29.\n\n\n\n\n\nFigure 11.29: Conceptual Depiction of Multicollinearity in Multiple Regression. From Petersen (2024) and Petersen (2025).\n\n\nConsider the following example adapted from Petersen (2025) where you have two predictor variables and one outcome variable, as shown in Table 11.3.\n\n\n\nTable 11.3: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\nx2\n\n\n\n9\n2.0\n4\n\n\n11\n3.0\n6\n\n\n17\n4.0\n8\n\n\n3\n1.0\n2\n\n\n21\n5.0\n10\n\n\n13\n3.5\n7\n\n\n\n\n\n\n\n\nThe second predictor variable is not very good—it is exactly twice the value of the first predictor variable; thus, the two predictor variables are perfectly correlated (i.e., \\(r = 1.0\\)). This means that there are different prediction equation possibilities that are equally good—see Equations in Equation 11.10:\n\\[\n\\begin{aligned}\n  2x_2 &= y \\\\\n  0x_1 + 2x_2 &= y \\\\\n  4x_1 &= y \\\\\n  4x_1 + 0x_2 &= y \\\\\n  2x_1 + 1x_2 &= y \\\\\n  5x_1 - 0.5x_2 &= y \\\\\n  ...\n&= y\n\\end{aligned}\n\\tag{11.10}\\]\nThen, what are the regression coefficients? We do not know what are the correct regression coefficients because each of the possibilities fits the data equally well. Thus, when estimating the regression model, we could obtain arbitrary estimates of the regression coefficients with an enormous standard error around each estimate. In general, multicollinearity increases the uncertainty (i.e., standard errors and confidence intervals) around the parameter estimates. Any predictor variables that have a correlation above ~ \\(r = .30\\) with each other could have an impact on the confidence interval of the regression coefficient. As the correlations among the predictor variables increase, the chance of getting an arbitrary answer increases, sometimes called “bouncing betas.” So, it is important to examine a correlation matrix of the predictor variables before putting them in the same regression model. You can also examine indices such as variance inflation factor (VIF), where a value greater than 5 or 10 indicates multicollinearity.\nHere are the VIFs from our earlier model:\n\nCodecar::vif(linearRegressionModel_outcomeTransformed)\n\n                     age                   height                   weight \n                1.019535                 2.095026                 2.111745 \nI(log(target_share + 1)) \n                1.030896 \n\n\nTo address multicollinearity, you can drop a redundant predictor or you can also use principal component analysis or factor analysis of the predictors to reduce the predictors down to a smaller number of meaningful predictors. For a meaningful answer regarding predictors in a regression framework that is precise and confident, you need a low level of intercorrelation among predictors, unless you have a very large sample size. However, if you are merely interested in prediction—and are not interested in interpreting the regression coefficients of individual predictors—multicollinearity poses less of a problem. For instance, machine learning cares more about achieving the greatest predictive accuracy possible and cares less about explaining which predictors are causally related to the outcome. So, multicollinearity is less of a concern for machine learning approaches.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionMissingness",
    "href": "multiple-regression.html#sec-multipleRegressionMissingness",
    "title": "11  Multiple Regression",
    "section": "\n11.11 Handling of Missing Data",
    "text": "11.11 Handling of Missing Data\nAn important consideration in multiple regression is how missing data are handled. Multiple regression in R using the stats::lm() function applies listwise deletion. Listwise deletion (also called complete case analysis) removes any row (in the data file) from analysis that has a missing value on the outcome variable or any of the predictor variables. Removing all rows from analysis that have any missingness in the model variables can be a problem because missingness is often not completely at random—missingness often occurs systematically (i.e., for a reason). For instance, participants may be less likely to have data for all variables if they are from a lower socioeconomic status background and do not have the time to participate in all study procedures. Thus, applying listwise deletion, we might systematically exclude participants from lower socioeconomic status backgrounds (or other groups), which could lead to less generalizable inferences.\nIt is thus important to consider approaches to handle missingness. Various approaches to handle missingness include pairwise deletion (aka available-case analysis), multiple imputation, and full information maximum likelihood (FIML).\n\n11.11.1 Pairwise Deletion\nPairwise deletion (also called available case analysis) uses all available data for each pair of variables when estimating the correlations/covariances, and the resulting correlation/covariance matrix can be used to estimate a multiple regression model. We can estimate a regression model that uses pairwise deletion using the lavaan package (Rosseel, 2012; Rosseel et al., 2024).\n\nCodeplayer_stats_seasonal$target_share_log &lt;- log(player_stats_seasonal$target_share + 1)\n\nmodelData &lt;- player_stats_seasonal %&gt;% \n  filter(position %in% c(\"WR\")) %&gt;% \n  select(fantasyPoints_transformed, age, height, weight, target_share_log)\n\nnumObs &lt;- sum(complete.cases(modelData))\nvarMeans &lt;- colMeans(modelData, na.rm = TRUE)\nvarCovariances &lt;- cov(modelData, use = \"pairwise.complete.obs\")\n\npairwiseRegression_syntax &lt;- '\n  fantasyPoints_transformed ~ age + height + weight + target_share_log\n  fantasyPoints_transformed ~~ fantasyPoints_transformed\n  fantasyPoints_transformed ~ 1\n'\n\npairwiseRegression_fit &lt;- lavaan::lavaan(\n  pairwiseRegression_syntax,\n  sample.mean = varMeans,\n  sample.cov = varCovariances,\n  sample.nobs = numObs\n)\n\nsummary(\n  pairwiseRegression_fit,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         6\n\n  Number of observations                          4437\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                              Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  fantasyPoints_transformed ~                                             \n    age                          0.023    0.003    7.282    0.000    0.023\n    height                       0.004    0.006    0.732    0.464    0.004\n    weight                       0.002    0.001    1.772    0.076    0.002\n    target_shar_lg              11.596    0.121   95.503    0.000   11.596\n  Std.all\n         \n    0.062\n    0.009\n    0.021\n    0.815\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .fntsyPnts_trns   -1.767    0.337   -5.243    0.000   -1.767   -1.535\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .fntsyPnts_trns    0.416    0.009   47.101    0.000    0.416    0.313\n\nR-Square:\n                   Estimate\n    fntsyPnts_trns    0.687\n\n\n\n11.11.2 Multiple Imputation\nMultiple imputation takes a data set with missingness and it uses available information on other variables to estimate what likely values could have been for the missing value. It repeats the imputation multiple times, so there are multiple imputed data sets, which can give a sense of the degree of uncertainty of each imputed value. The model is then fit to each of the multiply imputed data sets separately, and the results are combined. We can multiply impute data using the mice package (van Buuren & Groothuis-Oudshoorn, 2011, 2024).\n\nCodenumImputations &lt;- 5\n\ndataToImpute &lt;- player_stats_seasonal %&gt;% \n  filter(position %in% c(\"WR\")) %&gt;% \n  select(player_id, position, where(is.numeric)) %&gt;% \n  select(\n    player_id:games, carries:wopr, fantasy_points, fantasy_points_ppr,\n    rush_40_yds, rec_40_yds, fumbles, two_pts, return_yds,\n    rush_100_yds:draftround, height:target_share_log) %&gt;% \n  select(-c(fantasy_points_ppr, ageCentered20, target_share_log)) # drop collinear variables\n\npredictors &lt;- c(\"targets\",\"receiving_yards\",\"receiving_air_yards\",\"receiving_yards_after_catch\",\"receiving_first_downs\",\"racr\")\n\ndataToImpute$player_id_integer &lt;- as.integer(as.factor(dataToImpute$player_id))\n\nvarsToImpute &lt;- c(\"age\",\"height\",\"weight\",\"target_share\")\nY &lt;- varsToImpute\n\n\nNow, let’s specify the imputation method—we use the two-level predictive mean matching (2l.pmm) method from the miceadds package (Robitzsch et al., 2024) to account for the nonindependent data (owing to multiple seasons per player):\n\nCodemeth &lt;- mice::make.method(dataToImpute)\nmeth[1:length(meth)] &lt;- \"\"\nmeth[Y] &lt;- \"2l.pmm\" # specify the imputation method here; this can differ by outcome variable\n\n\nNow, let’s specify the prediction matrix. A predictor matrix is a matrix of values, where:\n\nrows with non-zero values are the target variables to be imputed\ncolumns with non-zero values are predictors of the variable specified in the given row\nthe diagonal of the predictor matrix should be zero because a variable cannot predict itself\n\nThe values are:\n\nNOT a predictor of the outcome: 0\n\n\ncluster variable: -2\n\n\nfixed effect of predictor: 1\n\n\nfixed effect and random effect of predictor: 2\n\ninclude cluster mean of predictor in addition to fixed effect of predictor: 3\n\ninclude cluster mean of predictor in addition to fixed effect and random effect of predictor: 4\n\n\n\nCodepred &lt;- mice::make.predictorMatrix(dataToImpute)\npred[1:nrow(pred), 1:ncol(pred)] &lt;- 0\npred[Y, \"player_id_integer\"] &lt;- (-2) # cluster variable\npred[Y, predictors] &lt;- 1 # fixed effect predictors\npred[Y, \"age\"] &lt;- 2 # random effect predictor\npred[Y, Y] &lt;- 1 # fixed effect predictor\n\ndiag(pred) &lt;- 0\n\n\nNow, let’s run the imputation:\n\nCodeimp &lt;- mice::mice(\n  as.data.frame(dataToImpute),\n  method = meth,\n  predictorMatrix = pred,\n  m = numImputations,\n  maxit = 5, # generally use 100 maximum iterations; this example uses 5 for speed\n  seed = 52242)\n\n\nBelow are some imputation diagnostics. Trace plots are in Figure 11.30.\n\nCodeplot(imp, c(\"target_share\"))\n\n\n\n\n\n\nFigure 11.30: Trace plots from multiple imputation.\n\n\n\n\nA density plot is in Figure 11.31.\n\nCodedensityplot(imp, ~ target_share)\n\n\n\n\n\n\nFigure 11.31: Density plot from multiple imputation.\n\n\n\n\nThe imputated data does not match well the distribution of the observed data. Thus, it may be necessary to select a different imputation method for more accurate imputation.\nNow, let’s do some post-processing:\n\nCodeimp_long &lt;- mice::complete(\n  imp,\n  action = \"long\",\n  include = TRUE)\n\nimp_long$target_share_log &lt;- log(imp_long$target_share + 1)\n\nimp_long$fantasyPoints_transformed &lt;- predict(\n  bnTransformed,\n  newdata = imp_long[[\"fantasyPoints\"]])\n\nimp_mids &lt;- mice::as.mids(imp_long)\n\n\nNow let’s estimate multiple regression with the multiply imputed data:\n\nCodeimp_regression &lt;- with(\n  imp_mids,\n  lm(\n    fantasyPoints_transformed ~ age + height + weight + target_share_log)\n  )\n\nmice::pool(imp_regression)\n\nError in (function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, : arguments imply differing number of rows: 1, 0\n\nCodesummary(mice::pool(imp_regression))\n\n\n  \n\n\n\n\n11.11.3 Full Information Maximum Likelihood\nFull information maximum likelihood (FIML) estimates model parameters using all available data for each case, without imputing missing values. FIML is commonly used for handling missingness in approaches to latent variable modeling, such as factor analysis and structural equation modeling. We can estimate a regression model that uses full information maximum likelihood using the lavaan package (Rosseel, 2012; Rosseel et al., 2024).\n\nCodefimlRegression_syntax &lt;- '\n  fantasyPoints_transformed ~ age + height + weight + target_share_log\n  fantasyPoints_transformed ~~ fantasyPoints_transformed\n  fantasyPoints_transformed ~ 1\n'\n\nfimlRegression_fit &lt;- lavaan::sem(\n  fimlRegression_syntax,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  missing = \"ML\",\n  fixed.x = FALSE\n)\n\nsummary(\n  fimlRegression_fit,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 48 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        20\n\n  Number of observations                          5389\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Observed\n  Observed information based on                Hessian\n\nRegressions:\n                              Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  fantasyPoints_transformed ~                                             \n    age                          0.016    0.003    5.375    0.000    0.016\n    height                      -0.000    0.006   -0.065    0.948   -0.000\n    weight                       0.002    0.001    1.853    0.064    0.002\n    target_shar_lg              11.614    0.117   98.894    0.000   11.614\n  Std.all\n         \n    0.045\n   -0.001\n    0.022\n    0.816\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  age ~~                                                                \n    height           -0.297    0.101   -2.946    0.003   -0.297   -0.040\n    weight           -0.155    0.637   -0.243    0.808   -0.155   -0.003\n    target_shar_lg    0.037    0.004   10.115    0.000    0.037    0.145\n  height ~~                                                             \n    weight           24.979    0.584   42.756    0.000   24.979    0.716\n    target_shar_lg    0.016    0.003    5.827    0.000    0.016    0.082\n  weight ~~                                                             \n    target_shar_lg    0.152    0.017    8.957    0.000    0.152    0.127\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .fntsyPnts_trns   -1.277    0.328   -3.891    0.000   -1.277   -1.109\n    age              26.254    0.043  611.422    0.000   26.254    8.329\n    height           72.606    0.032 2269.513    0.000   72.606   30.916\n    weight          200.480    0.202  991.398    0.000  200.480   13.505\n    target_shar_lg    0.081    0.001   70.829    0.000    0.081    0.997\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .fntsyPnts_trns    0.419    0.009   47.679    0.000    0.419    0.316\n    age               9.936    0.191   51.908    0.000    9.936    1.000\n    height            5.515    0.106   51.909    0.000    5.515    1.000\n    weight          220.370    4.245   51.909    0.000  220.370    1.000\n    target_shar_lg    0.007    0.000   49.154    0.000    0.007    1.000\n\nR-Square:\n                   Estimate\n    fntsyPnts_trns    0.684",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionNonIndependence",
    "href": "multiple-regression.html#sec-multipleRegressionNonIndependence",
    "title": "11  Multiple Regression",
    "section": "\n11.12 Addressing Non-Independence of Observations",
    "text": "11.12 Addressing Non-Independence of Observations\nPlease note that the \\(p\\)-value for regression coefficients assumes that the observations are independent—in particular, that the residuals are not correlated. However, the observations are not independent in the player_stats_seasonal dataframe used above, because the same player has multiple rows—one row corresponding to each season they played. This non-independence violates the traditional assumptions of the significance of regression coefficients. We could address this assumption by analyzing only one season from each player or by estimating the significance of the regression coefficients using cluster-robust standard errors. For simplicity in the models above, we present results above from the whole dataframe. In Chapter 12, we discuss mixed model approaches that handle repeated measures and other data that violate assumptions of non-independence. Below, we demonstrate how to account for non-independence of observations using cluster-robust standard errors with the rms package (Harrell, Jr., 2025).\n\nCodeplayer_stats_seasonal_subset &lt;- player_stats_seasonal %&gt;% \n  filter(!is.na(player_id)) %&gt;% \n  filter(position %in% c(\"WR\"))\n\nregressionWithClusterVariable &lt;- rms::robcov(rms::ols(\n  fantasyPoints_transformed ~ age + height + weight + I(log(target_share + 1)),\n  data = player_stats_seasonal_subset,\n  x = TRUE,\n  y = TRUE),\n  cluster = player_stats_seasonal_subset$player_id) #account for nested data within player\n\nregressionWithClusterVariable\n\nFrequencies of Missing Values Due to Each Variable\nfantasyPoints_transformed                       age                    height \n                        0                         0                         0 \n                   weight              target_share \n                        0                       952 \n\nLinear Regression Model\n\nrms::ols(formula = fantasyPoints_transformed ~ age + height + \n    weight + I(log(target_share + 1)), data = player_stats_seasonal_subset, \n    x = TRUE, y = TRUE)\n\n                                                      Model Likelihood    Discrimination    \n                                                            Ratio Test           Indexes    \n Obs                                        4437    LR chi2    5098.25    R2       0.683    \n sigma                                    0.6490    d.f.             4    R2 adj   0.683    \n d.f.                                       4432    Pr(&gt; chi2)  0.0000    g        1.016    \nCluster onplayer_stats_seasonal_subset$player_id                                            \n Clusters                                   1292                                            \n\nResiduals\n\n     Min       1Q   Median       3Q      Max \n-7.68982 -0.23138  0.07868  0.33270  3.03276 \n\n             Coef    S.E.   t     Pr(&gt;|t|)\nIntercept    -1.0024 0.4302 -2.33 0.0198  \nage           0.0137 0.0036  3.76 0.0002  \nheight       -0.0029 0.0075 -0.39 0.6998  \nweight        0.0016 0.0011  1.43 0.1531  \ntarget_share 11.6558 0.4596 25.36 &lt;0.0001 \n\nCodeperformance::r2(regressionWithClusterVariable)\n\n# R2 for Linear Regression\n  R2: 0.683",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionOutliers",
    "href": "multiple-regression.html#sec-multipleRegressionOutliers",
    "title": "11  Multiple Regression",
    "section": "\n11.13 Impact of Outliers and Restricted Range",
    "text": "11.13 Impact of Outliers and Restricted Range\nAs with correlation, multiple regression can be strongly impacted by outliers and restricted range.\n\n11.13.1 Robust Regression\nTo address outliers, there are various approaches to robust regression. One approach is to use an MM-type estimator, such as is used in the robustbase::lmrob() and robustbase::glmrob() functions of the robustbase package (Maechler et al., 2024; Todorov & Filzmoser, 2009).\n\nCoderobustRegression &lt;- robustbase::lmrob(\n  fantasyPoints_transformed ~ age + height + weight + I(log(target_share + 1)),\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\"))\n)\n\nsummary(robustRegression)\n\n\nCall:\nrobustbase::lmrob(formula = fantasyPoints_transformed ~ age + height + weight + \n    I(log(target_share + 1)), data = player_stats_seasonal %&gt;% filter(position %in% \n    c(\"WR\")))\n \\--&gt; method = \"MM\"\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5424 -0.2885  0.0131  0.2655  3.0997 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -0.2041773  0.2341573  -0.872    0.383    \nage                       0.0001486  0.0020360   0.073    0.942    \nheight                   -0.0056493  0.0041741  -1.353    0.176    \nweight                    0.0001317  0.0006516   0.202    0.840    \nI(log(target_share + 1)) 12.9291586  0.1110105 116.468   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 0.4013 \n  (952 observations deleted due to missingness)\nMultiple R-squared:  0.8469,    Adjusted R-squared:  0.8468 \nConvergence in 13 IRWLS iterations\n\nRobustness weights: \n 68 observations c(134,164,175,221,260,277,343,368,374,480,481,705,706,1059,1094,1114,1130,1131,1186,1234,1314,1315,1350,1425,1437,1486,1513,1549,1699,1786,1960,2143,2172,2188,2203,2222,2228,2264,2413,2437,2469,2577,2636,2671,2745,2779,2787,2918,2946,3120,3251,3260,3428,3508,3550,3551,3668,3669,3697,3788,3905,3920,3921,4042,4061,4158,4242,4372)\n     are outliers with |weight| = 0 ( &lt; 2.3e-05); \n 378 weights are ~= 1. The remaining 3991 ones are summarized as\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n0.0002297 0.8587000 0.9494000 0.8689000 0.9864000 0.9990000 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol          zero.tol \n        1.000e-07         1.000e-10         1.000e-07         1.000e-10 \n      eps.outlier             eps.x warn.limit.reject warn.limit.meanrw \n        2.254e-05         4.820e-10         5.000e-01         5.000e-01 \n     nResample         max.it         groups        n.group       best.r.s \n           500             50              5            400              2 \n      k.fast.s          k.max    maxit.scale      trace.lev            mts \n             1            200            200              0           1000 \n    compute.rd fast.s.large.n \n             0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\nAnother approach to handling outliers is to use boostrapping, which involves fitting models to various bootstrap resamples of the data. Bootstrap samples are datasets generated by sampling repeatedly with replacement.\nWe set up the bootstrap folds using the rsample::bootstraps() function of the rsample package (Frick et al., 2025).\n\nCodeset.seed(52242)\nbootstrapSamples &lt;- 2000\n\n# Create bootstrap resamples (with apparent = TRUE)\nboots &lt;- rsample::bootstraps(\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  times = bootstrapSamples,\n  apparent = TRUE\n)\n\n# Define recipe\nrec &lt;- recipes::recipe(\n  fantasyPoints_transformed ~ age + height + weight + target_share,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\"))\n) %&gt;%\n  recipes::step_log(target_share, offset = 1) # replace I(log(target_share + 1))\n\n# Define model spec and workflow\nlm_spec &lt;- parsnip::linear_reg() %&gt;%\n  parsnip::set_engine(\"lm\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\nlm_workflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(lm_spec)\n\n# Function for fitting the models on the bootstrap samples\nfit_lm &lt;- function(split, ...) {\n  analysis_data &lt;- rsample::analysis(split)\n  fit &lt;- workflows::fit(lm_workflow, data = analysis_data)\n  broom::tidy(fit)\n}\n\n# Fit bootstrap models and save results\nboot_models &lt;- boots %&gt;%\n  mutate(coef_info = purrr::map(splits, fit_lm))\n\n# Extract bootstrapped coefficient estimates\nboot_coefs &lt;- boot_models %&gt;%\n  tidyr::unnest(coef_info)\n\n# Percentile confidence intervals\npercentile_intervals &lt;- rsample::int_pctl(\n  .data = boot_models,\n  statistics = coef_info)\n\n# Bias-corrected and accelerated confidence intervals\nbca_intervals &lt;- rsample::int_bca(\n  .data = boot_models,\n  statistics = coef_info,\n  .fn = fit_lm\n)\n\n# View confidence intervals\npercentile_intervals\n\n\n  \n\n\nCodebca_intervals\n\n\n  \n\n\n\nThe distributions of the regression coefficients across boostraps are depicted in Figure 11.32.\n\nCodeggplot(\n  data = boot_coefs,\n  aes(x = estimate)) +\n  geom_histogram(\n    fill = \"gray80\",\n    color = \"black\") +\n  facet_wrap(\n    ~ term,\n    scales = \"free\") +\n  geom_vline(\n    data = percentile_intervals,\n    aes(\n      xintercept = .lower),\n    col = \"blue\",\n    linetype = \"dashed\") +\n  geom_vline(\n    data = percentile_intervals,\n    aes(\n      xintercept = .upper),\n    col = \"blue\",\n    linetype = \"dashed\") +\n  labs(\n    title = \"Bootstrap Distributions of Coefficients\",\n    x = \"Coefficient Estimate\",\n    y = \"Count\") +\n  theme_classic()\n\n\n\n\n\n\nFigure 11.32: Histogram of Parameter Estimates Across Bootstraps.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-moderatedMultipleRegression",
    "href": "multiple-regression.html#sec-moderatedMultipleRegression",
    "title": "11  Multiple Regression",
    "section": "\n11.14 Moderated Multiple Regression",
    "text": "11.14 Moderated Multiple Regression\nWhen examining moderation in multiple regression, several steps are important:\n\nWhen computing the interaction term, first mean-center the predictor variables. Calculate the interaction term as the multiplication of the mean-centered predictor variables. Mean-centering the predictor variables when computing the interaction term is important for addressing issues regarding multicollinearity (Iacobucci et al., 2016).\nWhen including an interaction term in the model, make sure also to include the main effects.\n\nFirst, we mean-center the predictors. In this case, we center the predictors around the mean of height and weight for Wide Receivers:\n\nCodeplayer_stats_seasonal$height_centered &lt;- player_stats_seasonal$height - mean(player_stats_seasonal$height[which(player_stats_seasonal$position == \"WR\")], na.rm = TRUE)\nplayer_stats_seasonal$weight_centered &lt;- player_stats_seasonal$weight - mean(player_stats_seasonal$weight[which(player_stats_seasonal$position == \"WR\")], na.rm = TRUE)\n\n\nThen, we compute the interaction term as the multiplication of the two centered predictors:\n\nCodeplayer_stats_seasonal$heightXweight &lt;- player_stats_seasonal$height_centered * player_stats_seasonal$weight_centered\n\n\nThen, we fit the moderated multiple regression model:\n\nCodemoderationModel &lt;- lm(\n  fantasyPoints_transformed ~ height_centered + weight_centered + height_centered:weight_centered,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(moderationModel)\n\n\nCall:\nlm(formula = fantasyPoints_transformed ~ height_centered + weight_centered + \n    height_centered:weight_centered, data = player_stats_seasonal %&gt;% \n    filter(position %in% c(\"WR\")), na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9342 -0.8338  0.0573  0.9657  3.0803 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      0.4251074  0.0185309  22.940  &lt; 2e-16 ***\nheight_centered                 -0.0111228  0.0095578  -1.164   0.2446    \nweight_centered                  0.0109193  0.0015068   7.247 4.88e-13 ***\nheight_centered:weight_centered -0.0007124  0.0004026  -1.769   0.0769 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.143 on 5385 degrees of freedom\nMultiple R-squared:  0.01631,   Adjusted R-squared:  0.01576 \nF-statistic: 29.77 on 3 and 5385 DF,  p-value: &lt; 2.2e-16\n\n\nThis model is equivalent to the model that includes the interaction term explicitly:\n\nCodemoderationModel &lt;- lm(\n  fantasyPoints_transformed ~ height_centered + weight_centered + heightXweight,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  na.action = \"na.exclude\"\n)\n\nsummary(moderationModel)\n\n\nNow, we can visualize the interaction to understand it. We create an interaction plot (Figure 11.33) and Johnson-Neyman plot (Figure 11.34) using the interactions package (Long, 2024).\n\nCodeinteractions::interact_plot(\n  moderationModel,\n  pred = height_centered,\n  modx = weight_centered)\n\n\n\n\n\n\nFigure 11.33: Interaction Plot from Moderated Multiple Regression.\n\n\n\n\n\nCodeinteractions::johnson_neyman(\n  moderationModel,\n  pred = height_centered,\n  modx = weight_centered,\n  alpha = .05)\n\nJOHNSON-NEYMAN INTERVAL\n\nWhen weight_centered is INSIDE the interval [22.81, 86.32], the slope of\nheight_centered is p &lt; .05.\n\nNote: The range of observed values of weight_centered is [-47.48, 64.52]\n\n\n\n\n\n\n\nFigure 11.34: Johnson-Neyman Plot from Moderated Multiple Regression.\n\n\n\n\nHere is a simple slopes analysis:\n\nCodeinteractions::sim_slopes(\n  moderationModel,\n  pred = height_centered,\n  modx = weight_centered,\n  johnson_neyman = FALSE)\n\nSIMPLE SLOPES ANALYSIS\n\nSlope of height_centered when weight_centered = -1.484626e+01 (- 1 SD): \n\n   Est.   S.E.   t val.      p\n------- ------ -------- ------\n  -0.00   0.01    -0.05   0.96\n\nSlope of height_centered when weight_centered = -1.002064e-16 (Mean): \n\n   Est.   S.E.   t val.      p\n------- ------ -------- ------\n  -0.01   0.01    -1.16   0.24\n\nSlope of height_centered when weight_centered =  1.484626e+01 (+ 1 SD): \n\n   Est.   S.E.   t val.      p\n------- ------ -------- ------\n  -0.02   0.01    -1.84   0.07",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionMediation",
    "href": "multiple-regression.html#sec-multipleRegressionMediation",
    "title": "11  Multiple Regression",
    "section": "\n11.15 Mediation",
    "text": "11.15 Mediation\nA mediation model takes the following general form in the lavaan package (Rosseel, 2012; Rosseel et al., 2024).\n\nCodemediationModel &lt;- '\n  # direct effect (cPrime)\n  Y ~ direct*X\n  \n  # mediator\n  M ~ a*X\n  Y ~ b*M\n  \n  # indirect effect = a*b\n  indirect := a*b\n  \n  # total effect (c)\n  total := abs(direct) + abs(indirect)\n  \n  # proportion mediated\n  Pm := abs(indirect) / total\n'\n\n\nLet’s substitute in our predictor, outcome, and hypothesized mediator. In this case, we predict that receiving touchdowns partially accounts for the association between Wide Receiver’s target share and their fantasy points. This is a silly example because fantasy points are derived, in part, from touchdowns, so of course touchdowns will partially account for almost any effect on Wide Receivers’ fantasy points. This example is merely for demonstrating the process of developing and examining a mediation model.\n\nCodemediationModel &lt;- '\n  # direct effect (cPrime)\n  fantasyPoints_transformed ~ direct*target_share\n  \n  # mediator\n  receiving_tds ~ a*target_share\n  fantasyPoints_transformed ~ b*receiving_tds\n  \n  # indirect effect = a*b\n  indirect := a*b\n  \n  # total effect (c)\n  total := abs(direct) + abs(indirect)\n  \n  # proportion mediated\n  Pm := abs(indirect) / total\n'\n\n\nTo get a robust estimate of the indirect effect, we obtain bootstrapped estimates from 1,000 bootstrap draws. Typically, we would obtain bootstrapped estimates from 10,000 bootstrap draws, but this example uses only 1,000 bootstrap draws for a shorter runtime.\n\nCodemediationFit &lt;- lavaan::sem(\n  mediationModel,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  se = \"bootstrap\",\n  bootstrap = 1000, # generally use 10,000 bootstrap draws; this example uses 1,000 for speed\n  parallel = \"multicore\", # parallelization for speed: use \"multicore\" for Mac/Linux; \"snow\" for PC\n  iseed = 52242, # for reproducibility\n  missing = \"ML\",\n  estimator = \"ML\",\n  fixed.x = FALSE)\n\n\nHere are the model results:\n\nCodesummary(\n  mediationFit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n\n  Number of observations                          5389\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nModel Test Baseline Model:\n\n  Test statistic                              9587.535\n  Degrees of freedom                                 3\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n                                                      \n  Robust Comparative Fit Index (CFI)             1.000\n  Robust Tucker-Lewis Index (TLI)                1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -12892.830\n  Loglikelihood unrestricted model (H1)     -12892.830\n                                                      \n  Akaike (AIC)                               25803.660\n  Bayesian (BIC)                             25862.989\n  Sample-size adjusted Bayesian (SABIC)      25834.390\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: RMSEA &lt;= 0.050                       NA\n  P-value H_0: RMSEA &gt;= 0.080                       NA\n                                                      \n  Robust RMSEA                                   0.000\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.000\n  P-value H_0: Robust RMSEA &lt;= 0.050                NA\n  P-value H_0: Robust RMSEA &gt;= 0.080                NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws             997\n\nRegressions:\n                              Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv\n  fantasyPoints_transformed ~                                             \n    trgt_sh (drct)               5.795    0.628    9.221    0.000    5.795\n  receiving_tds ~                                                         \n    trgt_sh    (a)              21.859    1.307   16.724    0.000   21.859\n  fantasyPoints_transformed ~                                             \n    rcvng_t    (b)               0.175    0.014   12.229    0.000    0.175\n  Std.all\n         \n    0.475\n         \n    0.701\n         \n    0.447\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .fntsyPnts_trns   -0.495    0.024  -20.546    0.000   -0.495   -0.430\n   .receiving_tds     0.334    0.101    3.297    0.001    0.334    0.114\n    target_share      0.088    0.001   64.775    0.000    0.088    0.930\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .fntsyPnts_trns    0.367    0.018   20.584    0.000    0.367    0.277\n   .receiving_tds     4.398    0.286   15.354    0.000    4.398    0.508\n    target_share      0.009    0.001   15.902    0.000    0.009    1.000\n\nR-Square:\n                   Estimate\n    fntsyPnts_trns    0.723\n    receiving_tds     0.492\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    indirect          3.827    0.124   30.812    0.000    3.827    0.314\n    total             9.622    0.543   17.704    0.000    9.622    0.788\n    Pm                0.398    0.031   12.646    0.000    0.398    0.398\n\n\nWe can also estimate a model with multiple hypothesized mediators:\n\nCodemultipleMediatorModel &lt;- '\n  # direct effect (cPrime)\n  fantasyPoints_transformed ~ direct*target_share\n  \n  # mediator\n  receiving_tds ~ a1*target_share\n  receiving_yards ~ a2*target_share\n  \n  fantasyPoints_transformed ~ b1*receiving_tds + b2*receiving_yards\n  \n  # indirect effect = a*b\n  indirect1 := a1*b1\n  indirect2 := a2*b2\n  indirectTotal := indirect1 + indirect2\n  \n  # total effect (c)\n  total := abs(direct) + abs(indirectTotal)\n  \n  # proportion mediated\n  Pm1 := abs(indirect1) / total\n  Pm2 := abs(indirect2) / total\n  PmTotal := abs(indirectTotal) / total\n'\n\n\n\nCodemultipleMediatorFit &lt;- lavaan::sem(\n  multipleMediatorModel,\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  se = \"bootstrap\",\n  bootstrap = 1000, # generally use 10,000 bootstrap draws; this example uses 1,000 for speed\n  parallel = \"multicore\", # parallelization for speed: use \"multicore\" for Mac/Linux; \"snow\" for PC\n  iseed = 52242, # for reproducibility\n  missing = \"ML\",\n  estimator = \"ML\",\n  fixed.x = FALSE)\n\n\nHere are the model results:\n\nCodesummary(\n  multipleMediatorFit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                          5389\n  Number of missing patterns                         2\n\nModel Test User Model:\n                                                      \n  Test statistic                              3188.635\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             21288.443\n  Degrees of freedom                                 6\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.850\n  Tucker-Lewis Index (TLI)                       0.101\n                                                      \n  Robust Comparative Fit Index (CFI)             0.845\n  Robust Tucker-Lewis Index (TLI)                0.070\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -48446.990\n  Loglikelihood unrestricted model (H1)     -46852.672\n                                                      \n  Akaike (AIC)                               96919.980\n  Bayesian (BIC)                             97005.677\n  Sample-size adjusted Bayesian (SABIC)      96964.367\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.769\n  90 Percent confidence interval - lower         0.747\n  90 Percent confidence interval - upper         0.792\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000\n                                                      \n  Robust RMSEA                                   0.804\n  90 Percent confidence interval - lower         0.778\n  90 Percent confidence interval - upper         0.830\n  P-value H_0: Robust RMSEA &lt;= 0.050             0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080             1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.080\n\nParameter Estimates:\n\n  Standard errors                            Bootstrap\n  Number of requested bootstrap draws             1000\n  Number of successful bootstrap draws             998\n\nRegressions:\n                              Estimate   Std.Err  z-value  P(&gt;|z|)   Std.lv \n  fantasyPoints_transformed ~                                               \n    trgt_sh (drct)                1.550    0.303    5.109    0.000     1.550\n  receiving_tds ~                                                           \n    trgt_sh   (a1)               22.442    1.282   17.502    0.000    22.442\n  receiving_yards ~                                                         \n    trgt_sh   (a2)             3472.359  196.760   17.648    0.000  3472.359\n  fantasyPoints_transformed ~                                               \n    rcvng_t   (b1)                0.023    0.004    5.884    0.000     0.023\n    rcvng_y   (b2)                0.002    0.000   30.525    0.000     0.002\n  Std.all\n         \n    0.130\n         \n    0.728\n         \n    0.848\n         \n    0.059\n    0.742\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)   Std.lv   Std.all\n   .fntsyPnts_trns    -0.592    0.012  -48.449    0.000    -0.592   -0.520\n   .receiving_tds      0.281    0.099    2.850    0.004     0.281    0.096\n   .receiving_yrds    70.411   15.370    4.581    0.000    70.411    0.180\n    target_share       0.088    0.001   65.111    0.000     0.088    0.920\n\nVariances:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)   Std.lv   Std.all\n   .fntsyPnts_trns     0.260    0.007   37.490    0.000     0.260    0.200\n   .receiving_tds      4.061    0.262   15.524    0.000     4.061    0.469\n   .receiving_yrds 42860.728 5571.178    7.693    0.000 42860.728    0.281\n    target_share       0.009    0.001   15.139    0.000     0.009    1.000\n\nR-Square:\n                   Estimate \n    fntsyPnts_trns     0.800\n    receiving_tds      0.531\n    receiving_yrds     0.719\n\nDefined Parameters:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)   Std.lv   Std.all\n    indirect1          0.516    0.090    5.746    0.000     0.516    0.043\n    indirect2          7.510    0.256   29.382    0.000     7.510    0.629\n    indirectTotal      8.027    0.266   30.169    0.000     8.027    0.672\n    total              9.576    0.538   17.795    0.000     9.576    0.802\n    Pm1                0.054    0.009    6.146    0.000     0.054    0.054\n    Pm2                0.784    0.024   32.067    0.000     0.784    0.784\n    PmTotal            0.838    0.022   37.485    0.000     0.838    0.838",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionBayesian",
    "href": "multiple-regression.html#sec-multipleRegressionBayesian",
    "title": "11  Multiple Regression",
    "section": "\n11.16 Bayesian Multiple Regression",
    "text": "11.16 Bayesian Multiple Regression\n\nCodebayesianMultipleRegressionModel &lt;- brm(\n  formula = fantasyPoints_transformed ~ age + height + weight + I(log(target_share + 1)),\n  data = player_stats_seasonal %&gt;% filter(position %in% c(\"WR\")),\n  family = gaussian()\n)\n\n\n\nCodesummary(bayesianMultipleRegressionModel)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: fantasyPoints_transformed ~ age + height + weight + I(log(target_share + 1)) \n   Data: player_stats_seasonal %&gt;% filter(position %in% c(\" (Number of observations: 4437) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             -1.01      0.33    -1.67    -0.34 1.00     3814     3231\nage                    0.01      0.00     0.01     0.02 1.00     6040     2932\nheight                -0.00      0.01    -0.01     0.01 1.00     3635     2882\nweight                 0.00      0.00    -0.00     0.00 1.00     3790     3337\nIlogtarget_shareP1    11.66      0.12    11.42    11.90 1.00     2976     2667\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.65      0.01     0.64     0.66 1.00     3472     2808\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodeperformance::r2(bayesianMultipleRegressionModel)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.683 (95% CI [0.673, 0.691])",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionConclusion",
    "href": "multiple-regression.html#sec-multipleRegressionConclusion",
    "title": "11  Multiple Regression",
    "section": "\n11.17 Conclusion",
    "text": "11.17 Conclusion\nMultiple regression allows examining the association between multiple predictor variables and one outcome variable. Inclusion of multiple predictors in the model allows for potentially greater predictive accuracy and identification of the extent to which each variable uniquely contributes to the outcome variable. As with correlation, an association does not imply causation. However, identifying associations is important because associations are a necessary (but insufficient) condition for causality. When developing a multiple regression model, there are various assumptions that are important to evaluate. In addition, it is important to pay attention for potential multicollinearity—it may become difficult to detect a given predictor variable as statistically significant due to the greater uncertainty around the parameter estimates.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionSessionInfo",
    "href": "multiple-regression.html#sec-multipleRegressionSessionInfo",
    "title": "11  Multiple Regression",
    "section": "\n11.18 Session Info",
    "text": "11.18 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] future_1.67.0       knitr_1.50          lubridate_1.9.4    \n [4] forcats_1.0.0       stringr_1.5.1       readr_2.1.5        \n [7] tidyverse_2.0.0     yardstick_1.3.2     workflowsets_1.1.1 \n[10] workflows_1.2.0     tune_1.3.0          tidyr_1.3.1        \n[13] tibble_3.3.0        rsample_1.3.1       recipes_1.3.1      \n[16] purrr_1.1.0         parsnip_1.3.2       modeldata_1.5.1    \n[19] infer_1.0.9         ggplot2_3.5.2       dplyr_1.1.4        \n[22] dials_1.4.1         scales_1.4.0        tidymodels_1.3.0   \n[25] effectsize_1.0.1    broom_1.0.9         MASS_7.3-65        \n[28] ordinal_2023.12-4.1 robustbase_0.99-4-1 parallelly_1.45.1  \n[31] brms_2.22.0         Rcpp_1.1.0          interactions_1.2.0 \n[34] miceadds_3.17-44    mice_3.18.0         lavaan_0.6-19      \n[37] performance_0.15.0  lme4_1.1-37         Matrix_1.7-3       \n[40] bestNormalize_1.9.1 car_3.1-3           carData_3.0-5      \n[43] rms_8.0-0           Hmisc_5.2-3         petersenlab_1.2.0  \n\nloaded via a namespace (and not attached):\n  [1] fs_1.6.6             matrixStats_1.5.0    sparsevctrs_0.3.4   \n  [4] DiceDesign_1.10      httr_1.4.7           RColorBrewer_1.1-3  \n  [7] insight_1.4.0        doParallel_1.0.17    numDeriv_2016.8-1.1 \n [10] tools_4.5.1          doRNG_1.8.6.2        backports_1.5.0     \n [13] R6_2.6.1             nortest_1.0-4        mgcv_1.9-3          \n [16] jomo_2.7-6           withr_3.0.2          Brobdingnag_1.2-9   \n [19] gridExtra_2.3        quantreg_6.1         cli_3.6.5           \n [22] domir_1.2.0          mix_1.0-13           sandwich_3.1-1      \n [25] labeling_0.4.3       mvtnorm_1.3-3        polspline_1.1.25    \n [28] proxy_0.4-27         QuickJSR_1.8.0       pbivnorm_0.6.0      \n [31] StanHeaders_2.32.10  foreign_0.8-90       readxl_1.4.5        \n [34] rstudioapi_0.17.1    generics_0.1.4       shape_1.4.6.1       \n [37] distributional_0.5.0 inline_0.3.21        loo_2.8.0           \n [40] DescTools_0.99.60    abind_1.4-8          lifecycle_1.0.4     \n [43] multcomp_1.4-28      yaml_2.3.10          grid_4.5.1          \n [46] mitml_0.4-5          butcher_0.3.6        lattice_0.22-7      \n [49] haven_2.5.5          jtools_2.3.0         pillar_1.11.0       \n [52] boot_1.3-31          gld_2.6.7            estimability_1.5.1  \n [55] future.apply_1.20.0  codetools_0.2-20     pan_1.9             \n [58] glue_1.8.0           V8_6.0.6             data.table_1.17.8   \n [61] vctrs_0.6.5          Rdpack_2.6.4         cellranger_1.1.0    \n [64] gtable_0.3.6         datawizard_1.2.0     gower_1.0.2         \n [67] xfun_0.53            rbibutils_2.3        prodlim_2025.04.28  \n [70] coda_0.19-4.1        reformulas_0.4.1     survival_3.8-3      \n [73] timeDate_4041.110    iterators_1.0.14     hardhat_1.4.2       \n [76] lava_1.8.1           TH.data_1.1-3        ipred_0.9-15        \n [79] nlme_3.1-168         rstan_2.32.7         tensorA_0.36.2.1    \n [82] rpart_4.1.24         colorspace_2.1-1     DBI_1.2.3           \n [85] nnet_7.3-20          processx_3.8.6       Exact_3.3           \n [88] mnormt_2.1.1         tidyselect_1.2.1     emmeans_1.11.2      \n [91] compiler_4.5.1       curl_7.0.0           glmnet_4.1-10       \n [94] htmlTable_2.4.3      SparseM_1.84-2       expm_1.0-0          \n [97] bayestestR_0.16.1    posterior_1.6.1      checkmate_2.3.3     \n[100] DEoptimR_1.1-4       psych_2.5.6          quadprog_1.5-8      \n[103] callr_3.7.6          digest_0.6.37        minqa_1.2.8         \n[106] rmarkdown_2.29       htmltools_0.5.8.1    pkgconfig_2.0.3     \n[109] base64enc_0.1-3      lhs_1.2.0            fastmap_1.2.0       \n[112] rlang_1.1.6          htmlwidgets_1.6.4    farver_2.1.2        \n[115] zoo_1.8-14           jsonlite_2.0.0       broom.mixed_0.2.9.6 \n[118] magrittr_2.0.3       Formula_1.2-5        bayesplot_1.13.0    \n[121] parameters_0.28.0    GPfit_1.0-9          ucminf_1.2.2        \n[124] furrr_0.3.1          stringi_1.8.7        rootSolve_1.8.2.4   \n[127] plyr_1.8.9           pkgbuild_1.4.8       parallel_4.5.1      \n[130] listenv_0.9.1        lmom_3.2             splines_4.5.1       \n[133] pander_0.6.6         hms_1.1.3            ps_1.9.1            \n[136] rngtools_1.5.2       reshape2_1.4.4       stats4_4.5.1        \n[139] rstantools_2.4.0     evaluate_1.0.4       mitools_2.4         \n[142] RcppParallel_5.1.10  nloptr_2.2.1         tzdb_0.5.0          \n[145] foreach_1.5.2        MatrixModels_0.5-4   xtable_1.8-4        \n[148] e1071_1.7-16         viridisLite_0.4.2    class_7.3-23        \n[151] cluster_2.1.8.1      timechange_0.3.0     globals_0.18.0      \n[154] bridgesampling_1.1-2\n\n\n\n\n\n\nBen-Shachar, M. S., Lüdecke, D., & Makowski, D. (2020). effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software, 5(56), 2815. https://doi.org/10.21105/joss.02815\n\n\nBen-Shachar, M. S., Makowski, D., Lüdecke, D., Patil, I., Wiernik, B. M., Thériault, R., & Waggoner, P. (2025). effectsize: Indices of effect size. https://doi.org/10.32614/CRAN.package.effectsize\n\n\nBürkner, P.-C. (2024). brms: Bayesian regression models using Stan. https://doi.org/10.32614/CRAN.package.brms\n\n\nChristensen, R. H. B. (2024). ordinal: Regression models for ordinal data. https://doi.org/10.32614/CRAN.package.ordinal\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFox, J., & Weisberg, S. (2019). An R companion to applied regression (Third). Sage. https://www.john-fox.ca/Companion\n\n\nFox, J., Weisberg, S., & Price, B. (2024). car: Companion to applied regression. https://doi.org/10.32614/CRAN.package.car\n\n\nFrick, H., Chow, F., Kuhn, M., Mahoney, M., Silge, J., & Wickham, H. (2025). rsample: General resampling infrastructure. https://doi.org/10.32614/CRAN.package.rsample\n\n\nHarrell, Jr., F. E. (2025). rms: Regression modeling strategies. https://doi.org/10.32614/CRAN.package.rms\n\n\nIacobucci, D., Schneider, M. J., Popovich, D. L., & Bakamitsos, G. A. (2016). Mean centering helps alleviate “micro” but not “macro” multicollinearity. Behavior Research Methods, 48(4), 1308–1317. https://doi.org/10.3758/s13428-015-0624-x\n\n\nLong, J. A. (2024). interactions: Comprehensive, user-friendly toolkit for probing interactions. https://doi.org/10.32614/CRAN.package.interactions\n\n\nLuchman, J. (2024). domir: Tools to support relative importance analysis. https://doi.org/10.32614/CRAN.package.domir\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., & Makowski, D. (2020). Extracting, computing and exploring the parameters of statistical models using R. Journal of Open Source Software, 5(53), 2445. https://doi.org/10.21105/joss.02445\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Makowski, D., Ben-Shachar, M. S., Patil, I., Højsgaard, S., & Wiernik, B. M. (2025). parameters: Processing of model parameters. https://doi.org/10.32614/CRAN.package.parameters\n\n\nLüdecke, D., Makowski, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., Wiernik, B. M., & Thériault, R. (2025). performance: Assessment of regression models performance. https://doi.org/10.32614/CRAN.package.performance\n\n\nMaechler, M., Todorov, V., Ruckstuhl, A., Salibian-Barrera, M., Koller, M., & Conceicao, E. L. T. (2024). robustbase: Basic robust statistics. https://doi.org/10.32614/CRAN.package.robustbase\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nPeterson, R. A. (2021). Finding optimal normalizing transformations via bestNormalize. The R Journal, 13(1), 310–329. https://doi.org/10.32614/RJ-2021-041\n\n\nPeterson, R. A. (2023). bestNormalize: Normalizing transformation functions. https://doi.org/10.32614/CRAN.package.bestNormalize\n\n\nPeterson, R. A., & Cavanaugh, J. E. (2020). Ordered quantile normalization: A semiparametric transformation built for the cross-validation era. Journal of Applied Statistics, 47(13-15), 2312–2327. https://doi.org/10.1080/02664763.2019.1630372\n\n\nRipley, B., & Venables, B. (2025). MASS: Support functions and datasets for Venables and Ripley’s MASS. https://doi.org/10.32614/CRAN.package.MASS\n\n\nRobinson, D., Hayes, A., & Couch, S. (2025). broom: Convert statistical objects into tidy tibbles. https://doi.org/10.32614/CRAN.package.broom\n\n\nRobitzsch, A., Grund, S., & Henke, T. (2024). miceadds: Some additional multiple imputation functions, especially for mice. https://doi.org/10.32614/CRAN.package.miceadds\n\n\nRosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02\n\n\nRosseel, Y., Jorgensen, T. D., & De Wilde, L. (2024). lavaan: Latent variable analysis. https://doi.org/10.32614/CRAN.package.lavaan\n\n\nTodorov, V., & Filzmoser, P. (2009). An object-oriented framework for robust multivariate analysis. Journal of Statistical Software, 32(3), 1–47. https://doi.org/10.18637/jss.v032.i03\n\n\nvan Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations in R. Journal of Statistical Software, 45(3), 1–67. https://doi.org/10.18637/jss.v045.i03\n\n\nvan Buuren, S., & Groothuis-Oudshoorn, K. (2024). mice: Multivariate imputation by chained equations. https://doi.org/10.32614/CRAN.package.mice",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "mixed-models.html",
    "href": "mixed-models.html",
    "title": "12  Mixed Models",
    "section": "",
    "text": "12.1 Getting Started\nThis chapter provides an overview of mixed models.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsGettingStarted",
    "href": "mixed-models.html#sec-mixedModelsGettingStarted",
    "title": "12  Mixed Models",
    "section": "",
    "text": "12.1.1 Load Packages\n\nCodelibrary(\"lme4\")\nlibrary(\"lmerTest\")\nlibrary(\"effectsize\")\nlibrary(\"MuMIn\")\nlibrary(\"emmeans\")\nlibrary(\"sjstats\")\nlibrary(\"mgcv\")\nlibrary(\"AICcmodavg\")\nlibrary(\"bbmle\")\nlibrary(\"rstan\")\nlibrary(\"brms\")\nlibrary(\"cmdstanr\") # todo: install.packages(\"cmdstanr\", repos = c(\"https://stan-dev.r-universe.dev\", getOption(\"repos\"))); cmdstanr::check_cmdstan_toolchain(); cmdstanr::install_cmdstan()\nlibrary(\"fitdistrplus\")\nlibrary(\"performance\")\nlibrary(\"parallelly\")\nlibrary(\"broom.mixed\")\nlibrary(\"tidybayes\")\nlibrary(\"plotly\")\nlibrary(\"viridis\")\nlibrary(\"tidyverse\")\n\n\n\n12.1.2 Specify Package Options\n\nCodeemm_options(lmerTest.limit = 100000)\nemm_options(pbkrtest.limit = 100000)\n\n\n\n12.1.3 Load Data\n\nCodeload(file = \"./data/nfl_depthCharts.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsOverview",
    "href": "mixed-models.html#sec-mixedModelsOverview",
    "title": "12  Mixed Models",
    "section": "\n12.2 Overview of Mixed Models",
    "text": "12.2 Overview of Mixed Models\nMixed models are simply regression models that account for the nested or hierarchical structure of the data. Mixed models can be be used to address the assumption in multiple regression that the residuals are independent (i.e., uncorrelated). When data come from the same group, that can lead to residuals being correlated, so accounting for the grouping-level structure can address the assumption.\nThe modeling framework goes by many terms, including mixed models, mixed-effects models, multilevel models, hierarchical linear models. They are sometimes called multilevel models and hierarchical linear models, whose name emphasizes the hierarchical structure of the data because the data are nonindependent. When observations (i.e., data points) are collected from multiple lower-level units (e.g., people) in an upper-level unit (e.g., married couple, family, classroom, school, neighborhood, team), the data from the lower-level units are considered “nested” within the upper-level unit. In this context, nested data refers to multiple observations from the same upper-level unit. For instance, longitudinal data are nested within the same participant. Students are nested within classrooms, which are nested within schools. Players are nested within teams.\nWhen data are nested, the data from the lower-level unit are likely to be correlated, to some degree, because they come from the same upper-level unit. For example, multiple players may come from the same team, and the players’ performance on that team is likely interrelated because they share common experiences and influence one another. Thus, data from multiple players on a given team are considered nested within that team. Longitudinal data can also be considered nested data, in which time points are nested within the person (i.e., the same player provides an observation across multiple time points). As we will discuss, it is important to account for levels of nesting when the observations are nonindependent. Mixed models provide a framework for accounting for levels of nesting, in order to account for why data from the same upper-level unit are likely to be intercorrelated.\nThese models are also sometimes called mixed models or mixed-effects models because the models can include a mix of fixed and random effects. Fixed effects are effects that are constant across individuals (i.e., upper-level units). Random effects are effects that vary across individuals (i.e., upper-level units). For instance, consider a longitudinal study of fantasy performance as a function of age. If we have longitudinal data for multiple players, the time points are nested within players. Examining the association between age as a fixed effect in relation to fantasy performance would examine the association between a player’s age and their fantasy performance while holding the association between age and fantasy performance constant across all players. That is, it would assume that all players show the same trajectory such as increase from ages 20 to 24 and then decrease. Examining the association between age as a random effect in relation to fantasy performance would examine the association between a player’s age and their fantasy performance while allowing the association between age and fantasy performance to vary across players. That is, it would allow the possibility that some players improve with age, whereas other players decline in performance with age.\nWhen including random effects of a variable (e.g., age) in a mixed model, it is also important to include fixed effects of that variable in the model. This is because random effects have a mean of zero. Fixed effects allow the mean to differ from zero. Thus, inclusion of random effects without the corresponding fixed effect can lead to bias in estimation of the association between the predictor variables and the outcome variable.\nA visualization of mixed models, including the distinction between fixed and random intercepts and slopes, is provided by Freeman (2017): http://mfviz.com/hierarchical-models (archived at https://perma.cc/H2GZ-P5RW).\nMixed modeling is a more flexible modeling framework than analysis of variance (ANOVA). Unlike ANOVA, mixed modeling can accommodate both continuous and categorical predictor variables that vary within units (Brauer & Curtin, 2018). ANOVA, cannot handle continuous predictor variables that vary within units (Brauer & Curtin, 2018). In addition, ANOVA yields biased inferences (i.e., a higher Type I error rate) when the same participants are exposed to multiple items or stimuli [i.e., responses by the same respondent tend to be correlated across items, and responses to the same item tend to be correlated across respondents; Brauer & Curtin (2018)]. Additionally, mixed models better handle missing data compared to ANOVA (Brauer & Curtin, 2018).\n\n12.2.1 Ecological Fallacy\nAs described in Section 14.5.9, the ecological fallacy is the error of drawing inferences about an individual from group-level data. A type of ecological fallacy is Simpson’s paradox.\n\n12.2.2 Simpson’s Paradox\nSimpson’s paradox occurs when the association between between the predictor variable and outcome variable for the subgroups differ from the association when the subgroups are combined. Examples of Simpson’s paradox are depicted in Figures 13.5 and 12.1. In the example below, there is a positive association between the predictor variable and outcome variable for each group. However, when the groups are combined, there is a negative association between predictor variable and outcome variable. That is, the sign of an association can differ at different levels of analysis (e.g., group level versus person level).\n\n\n\n\n\nFigure 12.1: An Example of Simpson’s Paradox. In this example, there is a positive association between x and y within each group (i.e., red or blue), but there is a negative association between x and y when the groups are combined. (Figure retrieved from https://en.wikipedia.org/wiki/File:Simpson%27s_paradox_continuous.svg)\n\n\nConsider that we observe a between-person association between a predictor, for example, how much sports drink a player drinks and their performance, such that players who drink more sports drink before games tend to perform better during games. However, based on this association, if we draw the inference that sports drink consumption leads to better performance, this could be a faulty inference. It is possible that, at the within-person level, there is no association or even a negative association between sports drink consumption and performance. For helping to approximate causality, a much stronger test than relying on the between-person association would be to examine the association within the individual. That is, examining the association within the individual would examine: when a player consumes more sports drink, whether they perform better than when the same player consumes less sports drink. We describe within-person analyses to approximate causal inferences in Section 13.5.2.2.\nIn short, we often need to account for the groups or “levels” within which people are nested. When multiple observations are nested within the same upper-level unit, they are often correlated. That is, some variance is attributable to the lower-level unit (e.g., the player) and some variance is attributable to the upper-level unit (e.g., the team). It is important to evaluate how much variance is attributable to the upper-level unit. A way of evaluating how much variance is attributable to an upper-level unit is with the intraclass correlation coefficient (ICC). If substantial variance is attributable to the upper-level unit, it is important to account for the nested structure of the data. Mixed models are a useful approach to account for data with a nested structure so you can avoid committing the ecological fallacy.\n\n12.2.3 Data Structure\nFor fitting mixed models, the data should be in long form—each player (or unit of interest) should have multiple rows based on the grouping structure. The cluster variable is the variable or variables that represent the upper-level unit(s), within which the data are nested. When estimating mixed models, it is important to know the grouping structure of the data because the data structure influences the model syntax. Examples of data from nonindependent units include nested data, multiple membership data, and cross-classified data, as depicted in Figure 12.2.\n\n\n\n\n\nFigure 12.2: Various Data Structures Involving Data from Nonindependent Units, Including Nested data, Multiple Membership Data, and Cross-Classified Data.\n\n\nOne example of nonindependent data is if your data are nested. When data are collected from multiple lower-level units (e.g., people) in an upper-level unit (e.g., married couple, family, classroom, school, neighborhood), and each lower-level unit belongs to one and only one higher-level unit, the data from the lower-level units are considered nested within the upper-level unit. For example, multiple participants in your sample may come from the same classroom. Data from multiple people can be nested within the same family, classroom, school, etc. Longitudinal data can also be considered nested data, in which time points are nested within the participant (i.e., the same participant provides an observation across multiple time points). And if you have multiple informants of a child’s behavior (e.g., parent-, teacher-, friend-, and self-report), the ratings could also be nested within the participant (i.e., there are ratings from multiple informants of the same child).\nAnother form of nonindependent data is when data involve multiple membership. Data involve multiple membership when the lower-level units belong to more than one upper-level unit. As an example of multiple membership, children may have more than one teacher, and therefore, in a modeling sense, children “belong” to more than one teacher cluster.\nAnother form of nonindependent data is when data are cross-classified (also called crossed). Data are cross-classified when the lower-level units are classified by two or more upper-level units, but the upper-level units are not hierarchical or nested within one another. For instance, children may be nested within the crossing of schools and neighborhoods. That is, children are nested within schools, and children are also nested within neighborhoods; however, children attending the same school may not necessarily be from the same neighborhood, and children from the same neighborhood may not necessarily attend the same school. The data would still have a two-level hierarchy, but the data would be nested in specific school-neighborhood combinations. That is, children are nested within the cross-classification of schools and neighborhoods.\nApplied to longitudinal fantasy football data, Most simply, players’ longitudinal performance data would be modeled as nested data with a two-level model: timepoints nested within players, with the player’s ID as the cluster variable. However, you could get more complicated. For instance, you could estimate a three-level model of timepoints nested within players nested within teams. This would help account for players’ productivity benefitting or taking a hit from being on a given team, and it would account for players changing teams. Alternatively, consistent with Usami & Murayama (2018), you could estimate a cross-classified model of players’ individual observations/measurement occasions belonging to two upper-level units: players and time points (e.g., week 1, week 2, 2025 season, etc.). This would help account for systematic effects that are specific to a given timepoint (e.g., week or season).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelMissingness",
    "href": "mixed-models.html#sec-mixedModelMissingness",
    "title": "12  Mixed Models",
    "section": "\n12.3 Handling of Missing Data",
    "text": "12.3 Handling of Missing Data\nMixed models handle missing data better than ANOVA and multiple regression (Brauer & Curtin, 2018). ANOVA and multiple regression use listwise deletion by default and, thus, exclude participants who have missing values on any of the variables in the model (i.e., independent/dependent or predictor/outcome). By contrast, with the data in long form, mixed models make use of all rows that have values on all of the predictor variables. That is, mixed models exclude only those rows that have a missing value on any of the predictor variables. Unlike ANOVA and multiple regression, mixed models retain rows with missing values on the outcome variables.\n\n12.3.1 Comparing Model Fit\nWhen trying to determine what predictors to include and how complex of a model to fit, it can be helpful to compare a variety a models to determine how well they fit the data (i.e., how much variance they account for). To compare non-nested models, you can use fit criteria such as Akaike information criterion (AIC), which penalizes model complexity. If two models are nested, you can compare their model fit using a likelihood ratio test. Two models are considered nested if one model includes all of the terms of the original model along with additional terms. The simpler model is considered “nested within” the more complex model. In the likelihood ratio test, a significant difference indicates that the more complex model is significantly better fitting than the simpler model. If the likelihood ratio test is non-significant, it indicates that the more complex model is not significantly better fitting, and thus you should prefer the more parsimonious model.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-fantasyPointsByAgeExperience",
    "href": "mixed-models.html#sec-fantasyPointsByAgeExperience",
    "title": "12  Mixed Models",
    "section": "\n12.4 Fantasy Points Per Season by Position, Age, and Experience",
    "text": "12.4 Fantasy Points Per Season by Position, Age, and Experience\nIn this chapter, we use mixed models to demonstrate how to model trajectories (growth curves) of players’ fantasy points as a function of age. When estimating growth curve models, time points are nested within individuals (i.e., players). Mixed models provide a flexible framework for estimating growth curve models because they allow unequal time intervals between measurement occasions, time intervals that differ between individuals, and different numbers of observations for each individual (Brauer & Curtin, 2018).\nOne of the challenges when modeling players’ fantasy points as a function of age is that fantasy points are a function of both ability and opportunity. With age, the player’s ability and opportunity may both decline, but it is difficult to disentangle the extent to which a player’s decline is related to declines in ability versus opportunity. As players get older, they may be supplanted by younger, more talented players. With age, despite still having latent (unobserved) ability, players will get fewer and, eventually, no opportunity. And we do not know the counterfactual—how many fantasy points they would have scored had they been given the full opportunity each season. Thus, players may go from scoring 100+ points in a season to all of a sudden scoring way fewer points, with little in the way of intermediate steps. Thus, this should inform how you interpret the resulting trajectories. For instance, the steep declines can make it look like the model-implied fantasy points go well below zero at later ages, even though it is obviously not likely for players to obtain such a magnitude of negative fantasy points. The negative model-implied values of fantasy points at later ages are likely an artifact of the decreasing opportunity that players tend to get as they age. The crossover point, where the positive values becomes negative might indicate, for instance, that players at that age tend to have retired and thus have no opportunity. Regardless, the form/shape/steepness of the decline is still potentially meaningful even if the precise negative number at a given age is not.\n\nCodeplayer_stats_seasonal_offense_subset &lt;- player_stats_seasonal %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\") | position %in% c(\"K\"))\n\nplayer_stats_seasonal_offense_subset$position[which(player_stats_seasonal_offense_subset$position == \"HB\")] &lt;- \"RB\"\n\nplayer_stats_seasonal_offense_subset$player_idFactor &lt;- factor(player_stats_seasonal_offense_subset$player_id)\nplayer_stats_seasonal_offense_subset$positionFactor &lt;- factor(player_stats_seasonal_offense_subset$position)\n\n\n\nCodeseasons17week &lt;- 2001:2020\nseasons18week &lt;- 2021:max(nfl_depthCharts$season, na.rm = TRUE)\n\nendOfSeasonDepthCharts &lt;- nfl_depthCharts %&gt;% \n  filter((season %in% seasons17week & week == 18) | (season %in% seasons18week & week == 19)) # get end-of-season depth charts\n\nqb1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"QB\", depth_team == 1)\n\nfb1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"FB\", depth_team == 1)\n\nk1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"K\", depth_team == 1)\n\nrb1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"RB\", depth_team == 1)\n\nwr1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"WR\", depth_team == 1)\n\nte1s &lt;- endOfSeasonDepthCharts %&gt;% \n  filter(position == \"TE\", depth_team == 1)\n\nplayer_stats_seasonal_offense_subsetDepth &lt;- player_stats_seasonal_offense_subset %&gt;% \n  filter(player_id %in% c(\n    qb1s$gsis_id,\n    fb1s$gsis_id,\n    k1s$gsis_id,\n    rb1s$gsis_id,\n    wr1s$gsis_id,\n    te1s$gsis_id\n    ))\n\n\nCreate a newdata object for generating the plots of model-implied fantasy points by age and position:\n\nCodepointsPerSeason_positionAge_newData &lt;- expand.grid(\n  positionFactor = factor(c(\"FB\",\"QB\",\"RB\",\"TE\",\"WR\")), #,\"K\"\n  age = seq(from = 20, to = 40, length.out = 10000)\n)\n\npointsPerSeason_positionAge_newData$ageCentered20 &lt;- pointsPerSeason_positionAge_newData$age - 20\npointsPerSeason_positionAge_newData$ageCentered20Quadratic &lt;- pointsPerSeason_positionAge_newData$ageCentered20 ^ 2\npointsPerSeason_positionAge_newData$years_of_experience &lt;- floor(pointsPerSeason_positionAge_newData$age - 22) # assuming that most players start at age 22 (i.e., rookie year) and thus have 1 year of experience at age 23\npointsPerSeason_positionAge_newData$years_of_experience[which(pointsPerSeason_positionAge_newData$years_of_experience &lt; 0)] &lt;- 0\n\n\nCreate an object with complete cases for generating the plots of individuals’ model-implied fantasy points by age and position:\n\nCodeplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subset %&gt;%\n  filter(\n    !is.na(player_idFactor),\n    !is.na(fantasyPoints),\n    !is.na(positionFactor),\n    !is.na(ageCentered20),\n    !is.na(ageCentered20Quadratic),\n    !is.na(years_of_experience))\n\n\n\n12.4.1 Scatterplots of Fantasy Points by Age and Position\nScatterplots are a helpful tool for quickly examining the association between two variables. However, scatterplots—as well as correlation and multiple regression—can hide meaningful associations that differ across units of analysis.\n\n12.4.1.1 Quarterbacks\nA scatterplot of Quarterbacks’ fantasy points by age is in Figure 12.3.\n\nCodeplot_scatterplotFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"QB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_scatterplotFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.3: Scatterplot of Fantasy Points by Age for Quarterbacks.\n\n\n\nBased on the scatterplot (and the bivariate association below), Quarterbacks’ fantasy points appear to (slightly) increase with age.\n\nCodecor.test(\n  formula = ~ age + fantasyPoints,\n  data = player_stats_seasonal_offense_subset %&gt;% filter(position == \"QB\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  age and fantasyPoints\nt = 2.2771, df = 2000, p-value = 0.02288\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.007058576 0.094450494\nsample estimates:\n       cor \n0.05085188 \n\n\n\n12.4.1.2 Fullbacks\nA scatterplot of Fullbacks’ fantasy points by age is in Figure 12.4.\n\nCodeplot_scatterplotFantasyPointsByAgeFB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"FB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Fullbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_scatterplotFantasyPointsByAgeFB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.4: Scatterplot of Fantasy Points by Age for Fullbacks.\n\n\n\nBased on the scatterplot, Fullbacks’ fantasy points appear to be relatively stable across ages.\n\n12.4.1.3 Running Backs\nA scatterplot of Running Backs’ fantasy points by age is in Figure 12.5.\n\nCodeplot_scatterplotFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"RB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_scatterplotFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.5: Scatterplot of Fantasy Points by Age for Running Backs.\n\n\n\nBased on the scatterplot, Running Backs’ fantasy points appear to be relatively stable across ages.\n\n12.4.1.4 Wide Receivers\nA scatterplot of Wide Receivers’ fantasy points by age is in Figure 12.6.\n\nCodeplot_scatterplotFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"WR\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_scatterplotFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.6: Scatterplot of Fantasy Points by Age for Wide Receivers.\n\n\n\nBased on the scatterplot, Wide Receivers’ fantasy points appear to be relatively stable across ages.\n\n12.4.1.5 Tight Ends\nA scatterplot of Tight Ends’ fantasy points by age is in Figure 12.7.\n\nCodeplot_scatterplotFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"TE\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_point(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  geom_smooth(\n    mapping = aes(\n    x = age,\n    y = fantasyPoints),\n    inherit.aes = FALSE\n  ) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_scatterplotFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.7: Scatterplot of Fantasy Points by Age for Tight Ends.\n\n\n\nBased on the scatterplot (and the bivariate association below), Tight Ends’ fantasy points appear to increase with age.\n\nCodecor.test(\n  formula = ~ age + fantasyPoints,\n  data = player_stats_seasonal_offense_subset %&gt;% filter(position == \"TE\")\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  age and fantasyPoints\nt = 6.1558, df = 3001, p-value = 8.463e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.07620327 0.14684959\nsample estimates:\n      cor \n0.1116675 \n\n\n\n12.4.2 Plots of Raw Trajectories of Fantasy Points By Age and Player\nScatterplots can be helpful for quickly visualizing the association between two variables. However, as mentioned earlier, scatterplots can hide the association between variables at different units of analysis. For instance, consider that we are trying to predict how a player will perform based on their age. We are interested not only in what the association is between age and fantasy points between players (i.e., a between-person association). We are also interested in what the association is between age and fantasy points within a given player (and within each player; i.e., a within-individual association). Arguably, the within-individual association between age and fantasy points is more relevant to the prediction of performance than the association between age and fantasy points between players. Assuming that the between-player association between age and fantasy points is the same as the within-player association when it is not is an example of the ecological fallacy.\nBelow, we depict players’ raw trajectories of fantasy points as a function of age. These are known as spaghetti plots. By examining the trajectory for each player, we can get a better understanding of how performance changes (within an individual) as a function of age.\n\n12.4.2.1 Quarterbacks\nA plot of Quarterbacks’ raw fantasy points data by age is in Figure 12.8.\n\nCodeplot_rawFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"QB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_rawFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.8: Plot of Raw Trajectories of Fantasy Points by Age for Quarterbacks.\n\n\n\n\n12.4.2.2 Fullbacks\nA plot of Fullbacks’ raw fantasy points data by age is in Figure 12.9.\n\nCodeplot_rawFantasyPointsByAgeFB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"FB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Fullbacks\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_rawFantasyPointsByAgeFB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.9: Plot of Raw Trajectories of Fantasy Points by Age for Fullbacks.\n\n\n\n\n12.4.2.3 Running Backs\nA plot of Running Backs’ raw fantasy points data by age is in Figure 12.10.\n\nCodeplot_rawFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"RB\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_rawFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.10: Plot of Raw Trajectories of Fantasy Points by Age for Running Backs.\n\n\n\n\n12.4.2.4 Wide Receivers\nA plot of Wide Receivers’ raw fantasy points data by age is in Figure 12.11.\n\nCodeplot_rawFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"WR\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_rawFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.11: Plot of Raw Trajectories of Fantasy Points by Age for Wide Receivers.\n\n\n\n\n12.4.2.5 Tight Ends\nA plot of Tight Ends’ raw fantasy points data by age is in Figure 12.12.\n\nCodeplot_rawFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subset %&gt;% \n    filter(position == \"TE\") %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints = round(fantasyPoints, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints,\n    color = player_id)) +\n  geom_line(\n    aes(\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n  )) +\n  scale_color_viridis(discrete = TRUE) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nplotly::ggplotly(\n  plot_rawFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints\",\"text\",\"label\"))\n\n\n\n\n\n\nFigure 12.12: Plot of Raw Trajectories of Fantasy Points by Age for Tight Ends.\n\n\n\n\n12.4.3 Linear Regression Models\n\n12.4.3.1 Null Model\nWe can estimate model fit using the MuMIn package (Bartoń, 2024):\n\nCodepointsPerSeason_nullModel &lt;- lm(\n  fantasyPoints ~ 1,\n  data = player_stats_seasonal_offense_subset,\n  na.action = \"na.exclude\"\n)\n\nsummary(pointsPerSeason_nullModel)\n\n\nCall:\nlm(formula = fantasyPoints ~ 1, data = player_stats_seasonal_offense_subset, \n    na.action = \"na.exclude\")\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-85.25 -66.27 -30.67  40.23 407.13 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.968      0.661     118   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 83.39 on 15916 degrees of freedom\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_nullModel, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter   | Std. Coef. |        95% CI\n----------------------------------------\n(Intercept) |  -1.78e-17 | [-0.02, 0.02]\n\nCodesummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodeAIC(pointsPerSeason_nullModel)\n\n[1] 185993.1\n\nCodeMuMIn::AICc(pointsPerSeason_nullModel)\n\n[1] 185993.2\n\n\nA plot of the model-implied trajectories of fantasy points by age from the null model is in Figure 12.13.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_nullModel &lt;- predict(\n  object = pointsPerSeason_nullModel,\n  newdata = pointsPerSeason_positionAge_newData\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_nullModel\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Null Model\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.13: Plot of Model-Implied Trajectories of Fantasy Points by Age in Null Model.\n\n\n\n\n\n12.4.3.2 Linear Model\n\nCodepointsPerSeason_linearRegression &lt;- lm(\n  fantasyPoints ~ positionFactor + ageCentered20 + positionFactor:ageCentered20,\n  data = player_stats_seasonal_offense_subset,\n  na.action = \"na.exclude\"\n)\n\nsummary(pointsPerSeason_linearRegression)\n\n\nCall:\nlm(formula = fantasyPoints ~ positionFactor + ageCentered20 + \n    positionFactor:ageCentered20, data = player_stats_seasonal_offense_subset, \n    na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-128.66  -61.42  -21.57   39.66  400.85 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     14.4390     7.8388   1.842  0.06549 .  \npositionFactorK                 57.3489     9.4468   6.071 1.30e-09 ***\npositionFactorQB                81.5235     8.7709   9.295  &lt; 2e-16 ***\npositionFactorRB                62.3698     8.4255   7.402 1.40e-13 ***\npositionFactorTE                23.3212     8.5817   2.718  0.00658 ** \npositionFactorWR                43.5919     8.2109   5.309 1.12e-07 ***\nageCentered20                    1.9687     1.0034   1.962  0.04977 *  \npositionFactorK:ageCentered20   -0.8097     1.1140  -0.727  0.46736    \npositionFactorQB:ageCentered20  -0.6984     1.0822  -0.645  0.51870    \npositionFactorRB:ageCentered20  -0.9362     1.1093  -0.844  0.39869    \npositionFactorTE:ageCentered20   0.1178     1.1102   0.106  0.91552    \npositionFactorWR:ageCentered20   2.1724     1.0623   2.045  0.04088 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 80.75 on 15905 degrees of freedom\nMultiple R-squared:  0.06314,   Adjusted R-squared:  0.06249 \nF-statistic: 97.44 on 11 and 15905 DF,  p-value: &lt; 2.2e-16\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_linearRegression, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter                           | Std. Coef. |         95% CI\n-----------------------------------------------------------------\n(Intercept)                         |      -0.60 | [-0.67, -0.53]\npositionFactor [K]                  |       0.62 | [ 0.52,  0.72]\npositionFactor [QB]                 |       0.92 | [ 0.84,  1.01]\npositionFactor [RB]                 |       0.67 | [ 0.59,  0.75]\npositionFactor [TE]                 |       0.29 | [ 0.21,  0.37]\npositionFactor [WR]                 |       0.70 | [ 0.62,  0.78]\nageCentered20                       |       0.08 | [ 0.00,  0.17]\npositionFactor [K] × ageCentered20  |      -0.03 | [-0.13,  0.06]\npositionFactor [QB] × ageCentered20 |      -0.03 | [-0.12,  0.06]\npositionFactor [RB] × ageCentered20 |      -0.04 | [-0.13,  0.05]\npositionFactor [TE] × ageCentered20 |   5.07e-03 | [-0.09,  0.10]\npositionFactor [WR] × ageCentered20 |       0.09 | [ 0.00,  0.18]\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.06313637\n\nCodeAIC(pointsPerSeason_linearRegression)\n\n[1] 184977.1\n\nCodeMuMIn::AICc(pointsPerSeason_linearRegression)\n\n[1] 184977.1\n\n\nA plot of the model-implied trajectories of fantasy points by age from the linear regression model is in Figure 12.14.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_linearRegression &lt;- predict(\n  object = pointsPerSeason_linearRegression,\n  newdata = pointsPerSeason_positionAge_newData\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_linearRegression,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Linear Regression Model\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.14: Plot of Model-Implied Trajectories of Fantasy Points by Age in Linear Regression Model.\n\n\n\n\n\n12.4.3.3 Quadratic Model\n\nCodepointsPerSeason_quadraticRegression &lt;- lm(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic,\n  data = player_stats_seasonal_offense_subset,\n  na.action = \"na.exclude\"\n)\n\nsummary(pointsPerSeason_quadraticRegression)\n\n\nCall:\nlm(formula = fantasyPoints ~ positionFactor + ageCentered20 + \n    ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic, \n    data = player_stats_seasonal_offense_subset, na.action = \"na.exclude\")\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-134.73  -61.09  -21.45   39.41  399.24 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              -0.03710   16.87594  -0.002 0.998246\npositionFactorK                          61.80221   19.47753   3.173 0.001512\npositionFactorQB                        102.54481   18.45548   5.556  2.8e-08\npositionFactorRB                         69.59997   17.91610   3.885 0.000103\npositionFactorTE                         32.36758   18.20660   1.778 0.075457\npositionFactorWR                         54.48618   17.49610   3.114 0.001848\nageCentered20                             6.04181    4.32312   1.398 0.162266\nageCentered20Quadratic                   -0.24347    0.25136  -0.969 0.332750\npositionFactorK:ageCentered20            -2.68392    4.70511  -0.570 0.568396\npositionFactorQB:ageCentered20           -6.38987    4.61726  -1.384 0.166405\npositionFactorRB:ageCentered20           -2.49377    4.70371  -0.530 0.596000\npositionFactorTE:ageCentered20           -2.31045    4.69894  -0.492 0.622942\npositionFactorWR:ageCentered20           -0.74960    4.51620  -0.166 0.868175\npositionFactorK:ageCentered20Quadratic    0.14985    0.26269   0.570 0.568379\npositionFactorQB:ageCentered20Quadratic   0.32216    0.26270   1.226 0.220083\npositionFactorRB:ageCentered20Quadratic   0.06494    0.28171   0.231 0.817689\npositionFactorTE:ageCentered20Quadratic   0.14107    0.27468   0.514 0.607538\npositionFactorWR:ageCentered20Quadratic   0.16972    0.26398   0.643 0.520288\n                                           \n(Intercept)                                \npositionFactorK                         ** \npositionFactorQB                        ***\npositionFactorRB                        ***\npositionFactorTE                        .  \npositionFactorWR                        ** \nageCentered20                              \nageCentered20Quadratic                     \npositionFactorK:ageCentered20              \npositionFactorQB:ageCentered20             \npositionFactorRB:ageCentered20             \npositionFactorTE:ageCentered20             \npositionFactorWR:ageCentered20             \npositionFactorK:ageCentered20Quadratic     \npositionFactorQB:ageCentered20Quadratic    \npositionFactorRB:ageCentered20Quadratic    \npositionFactorTE:ageCentered20Quadratic    \npositionFactorWR:ageCentered20Quadratic    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 80.74 on 15899 degrees of freedom\nMultiple R-squared:  0.06356,   Adjusted R-squared:  0.06256 \nF-statistic: 63.48 on 17 and 15899 DF,  p-value: &lt; 2.2e-16\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_linearRegression, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter                           | Std. Coef. |         95% CI\n-----------------------------------------------------------------\n(Intercept)                         |      -0.60 | [-0.67, -0.53]\npositionFactor [K]                  |       0.62 | [ 0.52,  0.72]\npositionFactor [QB]                 |       0.92 | [ 0.84,  1.01]\npositionFactor [RB]                 |       0.67 | [ 0.59,  0.75]\npositionFactor [TE]                 |       0.29 | [ 0.21,  0.37]\npositionFactor [WR]                 |       0.70 | [ 0.62,  0.78]\nageCentered20                       |       0.08 | [ 0.00,  0.17]\npositionFactor [K] × ageCentered20  |      -0.03 | [-0.13,  0.06]\npositionFactor [QB] × ageCentered20 |      -0.03 | [-0.12,  0.06]\npositionFactor [RB] × ageCentered20 |      -0.04 | [-0.13,  0.05]\npositionFactor [TE] × ageCentered20 |   5.07e-03 | [-0.09,  0.10]\npositionFactor [WR] × ageCentered20 |       0.09 | [ 0.00,  0.18]\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.06355845\n\nCodeAIC(pointsPerSeason_quadraticRegression)\n\n[1] 184981.9\n\nCodeMuMIn::AICc(pointsPerSeason_quadraticRegression)\n\n[1] 184982\n\n\nA plot of the model-implied trajectories of fantasy points by age from the regression model with a quadratic term for age is in Figure 12.15.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_quadraticRegression &lt;- predict(\n  object = pointsPerSeason_quadraticRegression,\n  newdata = pointsPerSeason_positionAge_newData\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_quadraticRegression,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Quadratic Regression Model\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.15: Plot of Model-Implied Trajectories of Fantasy Points by Age in Quadratic Regression Model.\n\n\n\n\n\n12.4.3.4 Compare Models\nWe compare nested models using an \\(F\\)-test with the stats::anova() function. We compare non-nested models using the bbmle (Bolker & R Development Core Team, 2023) (bbmle::AICtab()) and MuMIn (Bartoń, 2024) (MuMIn::AICc()) packages.\n\nCodeanova(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression\n)\n\n\n  \n\n\nCodeAIC(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression\n  )\n\n\n  \n\n\nCodelmModels &lt;- list(\n  \"nullModel\" = pointsPerSeason_nullModel,\n  \"linearRegression\" = pointsPerSeason_linearRegression,\n  \"quadraticRegression\" = pointsPerSeason_quadraticRegression\n)\n\nbbmle::AICtab(lmModels)\n\n                    dAIC   df\nlinearRegression       0.0 13\nquadraticRegression    4.8 19\nnullModel           1016.1 2 \n\nCodeMuMIn::AICc(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression\n)\n\n\n  \n\n\nCodesummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.06313637\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.06355845\n\nCodedeviance(pointsPerSeason_nullModel)\n\n[1] 110685501\n\nCodedeviance(pointsPerSeason_linearRegression)\n\n[1] 103697220\n\nCodedeviance(pointsPerSeason_quadraticRegression)\n\n[1] 103650502\n\nCodelogLik(pointsPerSeason_nullModel)\n\n'log Lik.' -92994.57 (df=2)\n\nCodelogLik(pointsPerSeason_linearRegression)\n\n'log Lik.' -92475.54 (df=13)\n\nCodelogLik(pointsPerSeason_quadraticRegression)\n\n'log Lik.' -92471.95 (df=19)\n\n\n\n12.4.4 Mixed Models\nBy accounting for which player each observation comes from using mixed models, we can examine the association between age and fantasy points in a more meaningful way, without violating the assumption in multiple regression that the observations are independent (i.e., that the residuals are uncorrelated).\n\n12.4.4.1 Random Intercepts Model\nWe estimate the multilevel models using the lmerTest::lmer() function of the lmerTest package (Kuznetsova et al., 2020), which is an extension of the lme4 package (Bates et al., 2015, 2025):\n\nCodepointsPerSeason_randomIntercepts &lt;- lmerTest::lmer(\n  fantasyPoints ~ 1 + (1 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_randomIntercepts)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ 1 + (1 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 178390.6  178413.6  -89192.3  178384.6     15914 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.9404 -0.4666 -0.1535  0.4197  4.8106 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n player_idFactor (Intercept) 3037     55.11   \n Residual                    3037     55.11   \nNumber of obs: 15917, groups:  player_idFactor, 3833\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)   58.172      1.046 4254.560   55.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_randomIntercepts, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter   | Std. Coef. |         95% CI\n-----------------------------------------\n(Intercept) |      -0.24 | [-0.26, -0.21]\n\nCodeperformance::r2(pointsPerSeason_randomIntercepts)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.500\n     Marginal R2: 0.000\n\nCodeperformance::icc(pointsPerSeason_randomIntercepts)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_randomIntercepts)\n\n[1] 178390.6\n\nCodeAICc(pointsPerSeason_randomIntercepts)\n\nnumeric(0)\n\n\nA plot of the model-implied trajectories of fantasy points by age from the mixed model with random intercepts is in Figure 12.16.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomIntercepts &lt;- predict(\n  object = pointsPerSeason_randomIntercepts,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomIntercepts\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age\",\n    subtitle = \"Random Intercepts Model\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.16: Plot of Model-Implied Trajectories of Fantasy Points by Age in Random Intercepts Mixed Model.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age from the mixed model with random intercepts is in Figure 12.17.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomIntercepts &lt;- predict(\n  object = pointsPerSeason_randomIntercepts,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomIntercepts &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomIntercepts = round(fantasyPoints_randomIntercepts, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomIntercepts,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomIntercepts,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomIntercepts\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints = round(fantasyPoints_randomIntercepts, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    color = \"#3366FF\",\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position: Random Intercepts Model\",\n    #color = \"Position\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsRandomIntercepts,\n  tooltip = c(\"age\",\"fantasyPoints_randomIntercepts\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.17: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Mixed Model with Random Intercepts. Overlaid with the Model-Implied Trajectory.\n\n\n\n\n12.4.4.2 Random Intercepts Model with Position as Fixed-Effect Predictor\nTo obtain standardized regression coefficients, we use the effectsize::standardize_parameters() function of the effectsize package (Ben-Shachar et al., 2020, 2025). We use the emmeans::emmeans() function of the emmeans package (Lenth, 2025) to determine the model-implied number of points per season by position.\n\nCodepointsPerSeason_position &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + (1 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_position)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ positionFactor + (1 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 178234.1  178295.5  -89109.1  178218.1     15909 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.9526 -0.4753 -0.1408  0.4211  4.8094 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n player_idFactor (Intercept) 2850     53.38   \n Residual                    3039     55.12   \nNumber of obs: 15917, groups:  player_idFactor, 3833\n\nFixed effects:\n                 Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)        24.690      4.993 3921.298   4.945 7.93e-07 ***\npositionFactorK    42.764      6.749 3955.969   6.336 2.62e-10 ***\npositionFactorQB   54.117      5.891 3935.767   9.187  &lt; 2e-16 ***\npositionFactorRB   39.282      5.389 3971.313   7.289 3.74e-13 ***\npositionFactorTE   14.297      5.516 3964.911   2.592  0.00958 ** \npositionFactorWR   36.738      5.276 3961.540   6.963 3.89e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstnFK pstFQB pstFRB pstFTE\npositnFctrK -0.740                            \npostnFctrQB -0.848  0.627                     \npostnFctrRB -0.926  0.685  0.785              \npostnFctrTE -0.905  0.670  0.767  0.839       \npostnFctrWR -0.946  0.700  0.802  0.877  0.857\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_position, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter           | Std. Coef. |         95% CI\n-------------------------------------------------\n(Intercept)         |      -0.64 | [-0.76, -0.52]\npositionFactor [K]  |       0.51 | [ 0.35,  0.67]\npositionFactor [QB] |       0.65 | [ 0.51,  0.79]\npositionFactor [RB] |       0.47 | [ 0.34,  0.60]\npositionFactor [TE] |       0.17 | [ 0.04,  0.30]\npositionFactor [WR] |       0.44 | [ 0.32,  0.56]\n\nCodeperformance::r2(pointsPerSeason_position)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.500\n     Marginal R2: 0.031\n\nCodeemmeans::emmeans(pointsPerSeason_position, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               24.7 5.00 3402     14.9     34.5\n K                67.5 4.54 3472     58.5     76.4\n QB               78.8 3.13 3449     72.7     84.9\n RB               64.0 2.03 3739     60.0     68.0\n TE               39.0 2.35 3628     34.4     43.6\n WR               61.4 1.71 3772     58.1     64.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_position)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_position)\n\n[1] 178234.1\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and a fixed effect of position is in Figure 12.18.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_position &lt;- predict(\n  object = pointsPerSeason_position,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_position,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Random Intercepts Model With Position as Fixed-Effect Predictor\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.18: Plot of Model-Implied Trajectories of Fantasy Points by Age in Random Intercepts Mixed Model With Position as a Fixed-Effect Predictor.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and a fixed effect of position is in Figure 12.19.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_position &lt;- predict(\n  object = pointsPerSeason_position,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsPosition &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_position = round(fantasyPoints_position, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_position,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_position,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_position,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_position = round(fantasyPoints_position, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nRandom Intercepts Model With Position As Predictor\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsPosition,\n  tooltip = c(\"age\",\"fantasyPoints_position\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.19: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Mixed Model With Random Intercepts and a Fixed-Effect of Position. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.4.4.3 Identify the Best-Fitting Functional Form of Age\n\n12.4.4.3.1 Linear Models\n\n12.4.4.3.1.1 Random Intercepts, Fixed Linear Slopes\n\nCodepointsPerSeason_positionAgeFixedLinearSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + (1 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeFixedLinearSlopes)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ positionFactor + ageCentered20 + (1 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 178100.7  178169.8  -89041.3  178082.7     15908 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1817 -0.4750 -0.1381  0.4152  4.7640 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n player_idFactor (Intercept) 3021     54.97   \n Residual                    2971     54.51   \nNumber of obs: 15917, groups:  player_idFactor, 3833\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)         38.0476     5.2185  4125.8425   7.291 3.67e-13 ***\npositionFactorK     45.6316     6.8913  3850.5354   6.622 4.04e-11 ***\npositionFactorQB    55.8127     6.0138  3820.6310   9.281  &lt; 2e-16 ***\npositionFactorRB    36.4699     5.5026  3856.7143   6.628 3.88e-11 ***\npositionFactorTE    12.9138     5.6291  3843.0080   2.294   0.0218 *  \npositionFactorWR    34.2476     5.3869  3844.7062   6.358 2.29e-10 ***\nageCentered20       -2.0245     0.1697 15609.9893 -11.928  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstnFK pstFQB pstFRB pstFTE pstFWR\npositnFctrK -0.714                                   \npostnFctrQB -0.821  0.628                            \npostnFctrRB -0.912  0.683  0.783                     \npostnFctrTE -0.888  0.669  0.766  0.839              \npostnFctrWR -0.931  0.698  0.800  0.877  0.857       \nageCentrd20 -0.216 -0.038 -0.029  0.039  0.019  0.034\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_positionAgeFixedLinearSlopes, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter           | Std. Coef. |         95% CI\n-------------------------------------------------\n(Intercept)         |      -0.64 | [-0.76, -0.52]\npositionFactor [K]  |       0.55 | [ 0.39,  0.71]\npositionFactor [QB] |       0.67 | [ 0.53,  0.81]\npositionFactor [RB] |       0.44 | [ 0.31,  0.57]\npositionFactor [TE] |       0.15 | [ 0.02,  0.29]\npositionFactor [WR] |       0.41 | [ 0.28,  0.54]\nageCentered20       |      -0.09 | [-0.10, -0.07]\n\nCodeperformance::r2(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.523\n     Marginal R2: 0.037\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeFixedLinearSlopes, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               24.3 5.10 3426     14.3     34.3\n K                69.9 4.64 3531     60.8     79.0\n QB               80.1 3.20 3495     73.8     86.3\n RB               60.7 2.08 3816     56.6     64.8\n TE               37.2 2.40 3654     32.5     41.9\n WR               58.5 1.75 3843     55.1     61.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeFixedLinearSlopes, \"ageCentered20\")\n\n ageCentered20 emmean  SE   df lower.CL upper.CL\n          6.81   55.1 1.4 3504     52.4     57.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n[1] 178100.7\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and fixed linear slopes is in Figure 12.20.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_fixedLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeFixedLinearSlopes,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_fixedLinearSlopes,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts and Fixed Linear Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.20: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts and Fixed Linear Slopes.\n\n\n\n\nA plot of individuals model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and fixed linear slopes is in Figure 12.21.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_fixedLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeFixedLinearSlopes,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsFixedLinearSlopes &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_fixedLinearSlopes = round(fantasyPoints_fixedLinearSlopes, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_fixedLinearSlopes,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_fixedLinearSlopes,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_fixedLinearSlopes,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData,\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts and Fixed Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsFixedLinearSlopes,\n  tooltip = c(\"age\",\"fantasyPoints_fixedLinearSlopes\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.21: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts and Fixed Slopes. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.4.4.3.1.2 Random Intercepts, Random Linear Slopes\n\nCodepointsPerSeason_positionAgeRandomLinearSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearSlopes)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: fantasyPoints ~ positionFactor + ageCentered20 + (1 + ageCentered20 |  \n    player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 177755.0  177839.4  -88866.5  177733.0     15906 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.4960 -0.4641 -0.1367  0.4032  4.6098 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   4979.38  70.565        \n                 ageCentered20   26.98   5.195   -0.66\n Residual                      2736.22  52.309        \nNumber of obs: 15917, groups:  player_idFactor, 3833\n\nFixed effects:\n                  Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)        35.0278     5.2707 4108.4675   6.646 3.41e-11 ***\npositionFactorK    43.4050     6.8528 3576.7275   6.334 2.69e-10 ***\npositionFactorQB   55.1275     5.9738 3602.4245   9.228  &lt; 2e-16 ***\npositionFactorRB   36.1906     5.4730 3711.2263   6.613 4.32e-11 ***\npositionFactorTE   13.3428     5.5918 3677.4319   2.386   0.0171 *  \npositionFactorWR   34.6893     5.3535 3683.1302   6.480 1.04e-10 ***\nageCentered20      -1.4582     0.2177 1798.0900  -6.698 2.81e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstnFK pstFQB pstFRB pstFTE pstFWR\npositnFctrK -0.699                                   \npostnFctrQB -0.804  0.624                            \npostnFctrRB -0.898  0.680  0.780                     \npostnFctrTE -0.872  0.666  0.764  0.835              \npostnFctrWR -0.916  0.695  0.797  0.873  0.854       \nageCentrd20 -0.284 -0.027 -0.024  0.045  0.022  0.040\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_positionAgeRandomLinearSlopes, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter           | Std. Coef. |         95% CI\n-------------------------------------------------\n(Intercept)         |      -0.63 | [-0.75, -0.52]\npositionFactor [K]  |       0.52 | [ 0.36,  0.68]\npositionFactor [QB] |       0.66 | [ 0.52,  0.80]\npositionFactor [RB] |       0.43 | [ 0.31,  0.56]\npositionFactor [TE] |       0.16 | [ 0.03,  0.29]\npositionFactor [WR] |       0.42 | [ 0.29,  0.54]\nageCentered20       |      -0.06 | [-0.08, -0.04]\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.560\n     Marginal R2: 0.033\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearSlopes, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               25.1 5.06 3154     15.2     35.0\n K                68.5 4.64 3027     59.4     77.6\n QB               80.2 3.19 3122     74.0     86.5\n RB               61.3 2.10 3793     57.2     65.4\n TE               38.4 2.40 3523     33.7     43.1\n WR               59.8 1.77 3791     56.3     63.3\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearSlopes, \"ageCentered20\")\n\n ageCentered20 emmean  SE   df lower.CL upper.CL\n          6.81   55.6 1.4 3191     52.8     58.3\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n[1] 177755\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and random linear slopes is in Figure 12.22.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearSlopes,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearSlopes,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts and Random Linear Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.22: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts and Random Linear Slopes.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts and random linear slopes is in Figure 12.22.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomLinearSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearSlopes,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomLinearSlopes &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomLinearSlopes = round(fantasyPoints_randomLinearSlopes, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearSlopes,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomLinearSlopes,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomLinearSlopes,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_randomLinearSlopes = round(fantasyPoints_randomLinearSlopes, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts and Random Linear Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsRandomLinearSlopes,\n  tooltip = c(\"age\",\"fantasyPoints_randomLinearSlopes\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.23: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts and Random Linear Slopes. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.4.4.3.2 Quadratic Models\n\n12.4.4.3.2.1 Random Intercepts, Random Linear Slopes, Fixed Quadratic Slopes\n\nCodepointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 177256.6  177348.7  -88616.3  177232.6     15905 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.4027 -0.4587 -0.1283  0.3989  4.6419 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   5491.21  74.103        \n                 ageCentered20   53.52   7.316   -0.67\n Residual                      2521.87  50.218        \nNumber of obs: 15917, groups:  player_idFactor, 3833\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              -8.38224    5.67361 5016.20474  -1.477  0.13963    \npositionFactorK          43.16307    7.00112 3690.89381   6.165 7.80e-10 ***\npositionFactorQB         59.40729    6.08751 3655.01735   9.759  &lt; 2e-16 ***\npositionFactorRB         39.39356    5.54889 3707.18643   7.099 1.50e-12 ***\npositionFactorTE         14.65959    5.67212 3679.65622   2.584  0.00979 ** \npositionFactorWR         37.42611    5.43222 3688.01149   6.890 6.55e-12 ***\nageCentered20            11.92098    0.60819 8164.08780  19.601  &lt; 2e-16 ***\nageCentered20Quadratic   -0.93429    0.03751 5634.91329 -24.909  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) pstnFK pstFQB pstFRB pstFTE pstFWR agCn20\npositnFctrK -0.661                                          \npostnFctrQB -0.764  0.618                                   \npostnFctrRB -0.854  0.677  0.779                            \npostnFctrTE -0.828  0.663  0.763  0.838                     \npostnFctrWR -0.872  0.692  0.796  0.876  0.856              \nageCentrd20 -0.408 -0.002  0.009  0.035  0.019  0.036       \nagCntrd20Qd  0.320 -0.004 -0.015 -0.020 -0.011 -0.021 -0.911\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter              | Std. Coef. |         95% CI\n----------------------------------------------------\n(Intercept)            |      -0.73 | [-0.85, -0.61]\npositionFactor [K]     |       0.52 | [ 0.35,  0.68]\npositionFactor [QB]    |       0.71 | [ 0.57,  0.86]\npositionFactor [RB]    |       0.47 | [ 0.34,  0.60]\npositionFactor [TE]    |       0.18 | [ 0.04,  0.31]\npositionFactor [WR]    |       0.45 | [ 0.32,  0.58]\nageCentered20          |       0.51 | [ 0.46,  0.56]\nageCentered20Quadratic |      -0.76 | [-0.81, -0.70]\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.640\n     Marginal R2: 0.108\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               17.4 5.15 3253     7.33     27.5\n K                60.6 4.77 3400    51.22     69.9\n QB               76.8 3.28 3374    70.39     83.3\n RB               56.8 2.13 3883    52.63     61.0\n TE               32.1 2.44 3629    27.30     36.9\n WR               54.8 1.80 3935    51.31     58.4\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.81   49.8 1.45 3323     46.9     52.6\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   59.3   49.8 1.45 3323     46.9     52.6\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n[1] 177256.6\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes is in Figure 12.24.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomLinearFixedQuadraticSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.24: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes is in Figure 12.25.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomLinearFixedQuadraticSlopes &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomLinearFixedQuadracticSlopes &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomLinearFixedQuadraticSlopes = round(fantasyPoints_randomLinearFixedQuadraticSlopes, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopes,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_randomLinearFixedQuadraticSlopes = round(fantasyPoints_randomLinearFixedQuadraticSlopes, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts, Random Linear Slopes, and\\nFixed Quadratic Slopes\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsRandomLinearFixedQuadracticSlopes,\n  tooltip = c(\"age\",\"fantasyPoints_randomLinearFixedQuadraticSlopes\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.25: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.4.4.3.2.2 Random Intercepts, Random Linear Slopes, Random Quadratic Slopes\n\nCodepointsPerSeason_positionAgeRandomLinearRandomQuadraticSlopes &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + (1 + ageCentered20 + ageCentered20Quadratic | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\n\n\n12.4.4.3.2.3 Random Intercepts, Random Linear Slopes, Fixed Quadratic Slopes in Interaction With Position\n\nCodepointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic +  \n    (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 177010.2  177179.1  -88483.1  176966.2     15895 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6379 -0.4627 -0.1206  0.4067  4.6342 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   5176.12  71.945        \n                 ageCentered20   56.22   7.498   -0.62\n Residual                      2442.21  49.419        \nNumber of obs: 15917, groups:  player_idFactor, 3833\n\nFixed effects:\n                                         Estimate Std. Error        df t value\n(Intercept)                               -2.7905    14.9404 6450.4124  -0.187\npositionFactorK                           58.4394    18.0253 6160.2266   3.242\npositionFactorQB                          38.6244    16.6181 6448.9251   2.324\npositionFactorRB                          30.0956    15.8861 6566.7441   1.894\npositionFactorTE                           0.1800    16.1911 6646.5101   0.011\npositionFactorWR                           9.4418    15.5547 6487.4473   0.607\nageCentered20                              8.8617     3.7612 7000.6836   2.356\nageCentered20Quadratic                    -0.6352     0.2367 5438.4310  -2.684\npositionFactorK:ageCentered20             -5.6151     4.2024 7167.8193  -1.336\npositionFactorQB:ageCentered20             6.6292     4.0433 7306.0781   1.640\npositionFactorRB:ageCentered20             7.9690     4.0856 7388.3371   1.951\npositionFactorTE:ageCentered20             4.9607     4.1004 7443.9241   1.210\npositionFactorWR:ageCentered20            11.8352     3.9449 7162.9965   3.000\npositionFactorK:ageCentered20Quadratic     0.4198     0.2512 5693.3329   1.671\npositionFactorQB:ageCentered20Quadratic   -0.4724     0.2481 5700.3143  -1.904\npositionFactorRB:ageCentered20Quadratic   -0.9759     0.2642 5811.0044  -3.694\npositionFactorTE:ageCentered20Quadratic   -0.3654     0.2596 5823.0681  -1.407\npositionFactorWR:ageCentered20Quadratic   -1.0379     0.2501 5533.4824  -4.149\n                                        Pr(&gt;|t|)    \n(Intercept)                             0.851845    \npositionFactorK                         0.001193 ** \npositionFactorQB                        0.020144 *  \npositionFactorRB                        0.058209 .  \npositionFactorTE                        0.991130    \npositionFactorWR                        0.543870    \nageCentered20                           0.018497 *  \nageCentered20Quadratic                  0.007291 ** \npositionFactorK:ageCentered20           0.181543    \npositionFactorQB:ageCentered20          0.101141    \npositionFactorRB:ageCentered20          0.051154 .  \npositionFactorTE:ageCentered20          0.226386    \npositionFactorWR:ageCentered20          0.002708 ** \npositionFactorK:ageCentered20Quadratic  0.094700 .  \npositionFactorQB:ageCentered20Quadratic 0.056943 .  \npositionFactorRB:ageCentered20Quadratic 0.000223 ***\npositionFactorTE:ageCentered20Quadratic 0.159367    \npositionFactorWR:ageCentered20Quadratic 3.38e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter                                    | Std. Coef. |         95% CI\n--------------------------------------------------------------------------\n(Intercept)                                  |      -0.70 | [-0.83, -0.57]\npositionFactor [K]                           |       0.54 | [ 0.37,  0.71]\npositionFactor [QB]                          |       0.67 | [ 0.52,  0.82]\npositionFactor [RB]                          |       0.32 | [ 0.18,  0.46]\npositionFactor [TE]                          |       0.15 | [ 0.00,  0.29]\npositionFactor [WR]                          |       0.34 | [ 0.20,  0.48]\nageCentered20                                |       0.38 | [ 0.06,  0.70]\nageCentered20Quadratic                       |      -0.51 | [-0.89, -0.14]\npositionFactor [K] × ageCentered20           |      -0.24 | [-0.60,  0.11]\npositionFactor [QB] × ageCentered20          |       0.29 | [-0.06,  0.63]\npositionFactor [RB] × ageCentered20          |       0.34 | [ 0.00,  0.69]\npositionFactor [TE] × ageCentered20          |       0.21 | [-0.13,  0.56]\npositionFactor [WR] × ageCentered20          |       0.51 | [ 0.18,  0.84]\npositionFactor [K] × ageCentered20Quadratic  |       0.34 | [-0.06,  0.74]\npositionFactor [QB] × ageCentered20Quadratic |      -0.38 | [-0.78,  0.01]\npositionFactor [RB] × ageCentered20Quadratic |      -0.79 | [-1.21, -0.37]\npositionFactor [TE] × ageCentered20Quadratic |      -0.30 | [-0.71,  0.12]\npositionFactor [WR] × ageCentered20Quadratic |      -0.84 | [-1.24, -0.44]\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.661\n     Marginal R2: 0.112\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               19.9 5.55 3697     9.02     30.8\n K                65.0 4.90 3466    55.38     74.6\n QB               75.7 3.37 3387    69.07     82.3\n RB               46.4 2.42 3952    41.66     51.2\n TE               32.2 2.62 3643    27.07     37.3\n WR               48.4 1.96 3665    44.57     52.3\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.81   47.9 1.52 3632       45     50.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   59.3   49.8 1.45 3323     46.9     52.6\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n[1] 177010.2\n\n\nA plot of the model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes in interaction with position is in Figure 12.26.\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_randomLinearFixedQuadraticSlopesInteraction &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\nggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n    color = positionFactor\n  )\n) + \n  geom_line(linewidth = 2) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Mixed Model with Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes\\nin Interaction With Position\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.26: Plot of Model-Implied Trajectories of Fantasy Points by Age and Position in Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes in Interaction With Position.\n\n\n\n\nA plot of individuals’ model-implied trajectories of fantasy points by age and position from the mixed model with random intercepts, random linear slopes, and fixed quadratic slopes in interaction with position is in Figure 12.27.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_randomLinearFixedQuadraticSlopesInteraction &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplot_individualFantasyPointsRandomLinearFixedQuadraticSlopesInteraction &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% \n    mutate(\n      age = round(age, 2),\n      fantasyPoints_randomLinearFixedQuadraticSlopesInteraction = round(fantasyPoints_randomLinearFixedQuadraticSlopesInteraction, 2)\n    ),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n    color = positionFactor,\n    group = player_id)) +\n  geom_line(\n    aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    linewidth = 0.5) +\n  geom_line(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_randomLinearFixedQuadraticSlopesInteraction,\n      color = positionFactor\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% \n      mutate(\n        age = round(age, 2),\n        fantasyPoints_randomLinearFixedQuadraticSlopesInteraction = round(fantasyPoints_randomLinearFixedQuadraticSlopesInteraction, 2)\n        ),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position:\\nModel With Random Intercepts, Random Linear Slopes, and\\nFixed Quadratic Slopes in Interaction With Position\",\n    color = \"Position\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsRandomLinearFixedQuadraticSlopesInteraction,\n  tooltip = c(\"age\",\"fantasyPoints_randomLinearFixedQuadraticSlopesInteraction\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.27: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age and Position, from a Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes in Interaction With Position. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n12.4.4.3.2.4 Adding Fixed-Effect Predictor of Experience\n\nCodepointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  REML = FALSE,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\n      AIC       BIC    logLik -2*log(L)  df.resid \n 176573.0  176749.5  -88263.5  176527.0     15873 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6036 -0.4555 -0.1143  0.4082  4.6052 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   5065.75  71.17         \n                 ageCentered20   57.15   7.56    -0.65\n Residual                      2446.55  49.46         \nNumber of obs: 15896, groups:  player_idFactor, 3815\n\nFixed effects:\n                                         Estimate Std. Error        df t value\n(Intercept)                               14.7404    15.0529 6567.9894   0.979\npositionFactorK                           64.6170    18.1126 6243.6378   3.568\npositionFactorQB                          28.4833    16.6755 6546.2833   1.708\npositionFactorRB                          24.0904    15.9505 6649.4271   1.510\npositionFactorTE                          -5.3012    16.2495 6727.1096  -0.326\npositionFactorWR                           2.9393    15.6189 6573.4830   0.188\nageCentered20                              0.6250     3.8187 7316.7925   0.164\nageCentered20Quadratic                    -0.6912     0.2376 5516.9393  -2.909\nyears_of_experience                        9.8259     0.6788 5290.9154  14.476\npositionFactorK:ageCentered20             -5.6008     4.2181 7268.0432  -1.328\npositionFactorQB:ageCentered20             6.9414     4.0564 7407.2377   1.711\npositionFactorRB:ageCentered20             8.5981     4.0994 7475.2677   2.097\npositionFactorTE:ageCentered20             5.5093     4.1141 7534.2191   1.339\npositionFactorWR:ageCentered20            12.3709     3.9589 7256.3976   3.125\npositionFactorK:ageCentered20Quadratic     0.4423     0.2522 5782.5490   1.754\npositionFactorQB:ageCentered20Quadratic   -0.4450     0.2490 5778.2560  -1.787\npositionFactorRB:ageCentered20Quadratic   -1.0042     0.2651 5876.3862  -3.789\npositionFactorTE:ageCentered20Quadratic   -0.3848     0.2606 5892.2513  -1.477\npositionFactorWR:ageCentered20Quadratic   -1.0486     0.2510 5604.4558  -4.177\n                                        Pr(&gt;|t|)    \n(Intercept)                             0.327498    \npositionFactorK                         0.000363 ***\npositionFactorQB                        0.087665 .  \npositionFactorRB                        0.131009    \npositionFactorTE                        0.744254    \npositionFactorWR                        0.850736    \nageCentered20                           0.870000    \nageCentered20Quadratic                  0.003644 ** \nyears_of_experience                      &lt; 2e-16 ***\npositionFactorK:ageCentered20           0.184287    \npositionFactorQB:ageCentered20          0.087082 .  \npositionFactorRB:ageCentered20          0.035992 *  \npositionFactorTE:ageCentered20          0.180571    \npositionFactorWR:ageCentered20          0.001786 ** \npositionFactorK:ageCentered20Quadratic  0.079484 .  \npositionFactorQB:ageCentered20Quadratic 0.073998 .  \npositionFactorRB:ageCentered20Quadratic 0.000153 ***\npositionFactorTE:ageCentered20Quadratic 0.139746    \npositionFactorWR:ageCentered20Quadratic    3e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeprint(effectsize::standardize_parameters(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, method = \"refit\"), digits = 2)\n\n# Standardization method: refit\n\nParameter                                    | Std. Coef. |         95% CI\n--------------------------------------------------------------------------\n(Intercept)                                  |      -0.63 | [-0.76, -0.50]\npositionFactor [K]                           |       0.63 | [ 0.46,  0.80]\npositionFactor [QB]                          |       0.59 | [ 0.44,  0.74]\npositionFactor [RB]                          |       0.28 | [ 0.14,  0.42]\npositionFactor [TE]                          |       0.11 | [-0.03,  0.25]\npositionFactor [WR]                          |       0.30 | [ 0.17,  0.43]\nageCentered20                                |       0.03 | [-0.30,  0.35]\nageCentered20Quadratic                       |      -0.56 | [-0.94, -0.18]\nyears of experience                          |       0.41 | [ 0.35,  0.47]\npositionFactor [K] × ageCentered20           |      -0.24 | [-0.60,  0.11]\npositionFactor [QB] × ageCentered20          |       0.30 | [-0.04,  0.64]\npositionFactor [RB] × ageCentered20          |       0.37 | [ 0.02,  0.72]\npositionFactor [TE] × ageCentered20          |       0.24 | [-0.11,  0.58]\npositionFactor [WR] × ageCentered20          |       0.53 | [ 0.20,  0.87]\npositionFactor [K] × ageCentered20Quadratic  |       0.36 | [-0.04,  0.76]\npositionFactor [QB] × ageCentered20Quadratic |      -0.36 | [-0.75,  0.03]\npositionFactor [RB] × ageCentered20Quadratic |      -0.81 | [-1.23, -0.39]\npositionFactor [TE] × ageCentered20Quadratic |      -0.31 | [-0.72,  0.10]\npositionFactor [WR] × ageCentered20Quadratic |      -0.85 | [-1.25, -0.45]\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.655\n     Marginal R2: 0.135\n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               25.4 5.42 3698     14.8     36.0\n K                78.1 4.85 3411     68.6     87.6\n QB               74.8 3.26 3366     68.4     81.2\n RB               48.5 2.36 3904     43.9     53.1\n TE               34.8 2.55 3614     29.8     39.8\n WR               50.4 1.91 3614     46.7     54.2\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.82     52 1.49 3583     49.1     54.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   59.4     52 1.49 3583     49.1     54.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"years_of_experience\")\n\n years_of_experience emmean   SE   df lower.CL upper.CL\n                4.83     52 1.49 3583     49.1     54.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n\n  \n\n\nCodeAIC(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n[1] 176573\n\n\n\n12.4.4.3.3 Compare Models\nAfter fitting several models, we now must compare their fit to determine which model fits “best” while also considering parsimony. Parsimonious models are more likely to be true and more likely to generalize to other samples, because more complex models are more likely to overfit the data. Thus, more complex models will almost always fit better than simpler models. Thus, we are not just interested in whether a more complex model fits better than the simpler model; we also care about whether the more complex model fits significantly better than the simpler model given its additional complexity. For evaluating and comparing models, we examine the likelihood ratio test, the Akaike Information Criterion (AIC), the corrected AIC (AICc), the Bayesian Information Criterion (BIC), \\(R^2\\), deviance, and log likelihood.\nThe BIC penalizes model complexity more than the AIC does. The BIC is preferable when there is a “true” model, and one intends to identify the true model. The AIC is preferable when we are concerned more about predictive accuracy and when overfitting is less of a concern. Because we are more concerned about predictive accuracy and we do not believe one of these models is the “true” model per se of age-related changes in fantasy performance, we will give more weight to AIC than BIC.\nBelow, we specify various groups of models for the model fit comparisons:\n\nCodelmVsMixedModel &lt;- list(\n  \"nullModel\" = pointsPerSeason_nullModel,\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts\n)\n\nlmAndMixedModels &lt;- list(\n  \"nullModel\" = pointsPerSeason_nullModel,\n  \"linearRegression\" = pointsPerSeason_linearRegression,\n  \"quadraticRegression\" = pointsPerSeason_quadraticRegression,\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts,\n  \"position\" = pointsPerSeason_position,\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\nmixedModels &lt;- list(\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts,\n  \"position\" = pointsPerSeason_position,\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\nmixedModels1 &lt;- list(\n  \"randomIntercepts\" = pointsPerSeason_randomIntercepts,\n  \"position\" = pointsPerSeason_position\n)\n\nmixedModels2 &lt;- list(\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\n\n\n12.4.4.3.3.1 Likelihood Ratio Test\n\nCodeanova(\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_position,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\n\n  \n\n\n\n\n12.4.4.3.3.2 Akaike Information Criterion (AIC)\n\nCodeAIC(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n  )\n\n\n  \n\n\nCodebbmle::AICtab(lmAndMixedModels)\n\n                                      dAIC   df\nrandomLinearFixedQuadraticInteraction    0.0 22\nrandomLinearFixedQuadratic             246.4 12\nrandomLinear                           744.8 11\nfixedLinear                           1090.5 9 \nposition                              1223.9 8 \nrandomIntercepts                      1380.4 3 \nlinearRegression                      7966.9 13\nquadraticRegression                   7971.7 19\nnullModel                             8982.9 2 \n\n\n\n12.4.4.3.3.3 Corrected Akaike Information Criterion (AICc)\nWe compute model fit using the using the AICcmodavg (Mazerolle, 2025), bbmle (Bolker & R Development Core Team, 2023), and MuMIn (Bartoń, 2024) packages\n\nCode#AICcmodavg::aictab(lmVsMixedModel) # throws error (can't mix lm with lmer)\nbbmle::AICctab(lmVsMixedModel)\n\n                 dAICc  df\nrandomIntercepts    0.0 3 \nnullModel        7602.6 2 \n\nCodeAICcmodavg::aictab(mixedModels) # throws error (can't mix lm with lmer)\n\n\n  \n\n\nCode#bbmle::AICctab(mixedModels) # throws error (different numbers of observations)\n\nAICcmodavg::aictab(mixedModels1)\n\n\n  \n\n\nCodebbmle::AICctab(mixedModels1)\n\n                 dAICc df\nposition           0.0 8 \nrandomIntercepts 156.5 3 \n\nCodeAICcmodavg::aictab(mixedModels2)\n\n\n  \n\n\nCodebbmle::AICctab(mixedModels2)\n\n                                      dAICc  df\nrandomLinearFixedQuadraticInteraction    0.0 22\nrandomLinearFixedQuadratic             246.3 12\nrandomLinear                           744.7 11\nfixedLinear                           1090.4 9 \n\nCodeMuMIn::AICc(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n)\n\n\n  \n\n\n\n\n12.4.4.3.3.4 Bayesian Information Criterion (BIC)\n\nCodeBIC(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction\n  )\n\n\n  \n\n\nCode#AICcmodavg::bictab(lmAndMixedModels) # throws error (can't mix lm with lmer)\nbbmle::BICtab(lmVsMixedModel)\n\n                 dBIC   df\nrandomIntercepts    0.0 3 \nnullModel        7594.9 2 \n\nCodeAICcmodavg::bictab(mixedModels)\n\n\n  \n\n\nCode#bbmle::AICctab(mixedModels) # throws error (different numbers of observations)\n\nAICcmodavg::bictab(mixedModels1)\n\n\n  \n\n\nCodebbmle::BICtab(mixedModels1)\n\n                 dBIC  df\nposition           0.0 8 \nrandomIntercepts 118.1 3 \n\nCodeAICcmodavg::bictab(mixedModels2)\n\n\n  \n\n\nCodebbmle::BICtab(mixedModels2)\n\n                                      dBIC  df\nrandomLinearFixedQuadraticInteraction   0.0 22\nrandomLinearFixedQuadratic            169.6 12\nrandomLinear                          660.4 11\nfixedLinear                           990.7 9 \n\n\n\n12.4.4.3.3.5 \\(R^2\\)\n\n\nCodesummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.06313637\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.06355845\n\nCodeperformance::r2(pointsPerSeason_randomIntercepts)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.500\n     Marginal R2: 0.000\n\nCodeperformance::r2(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.523\n     Marginal R2: 0.037\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.560\n     Marginal R2: 0.033\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.640\n     Marginal R2: 0.108\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.661\n     Marginal R2: 0.112\n\n\n\n12.4.4.3.3.6 Deviance\n\nCodedeviance(pointsPerSeason_nullModel)\n\n[1] 110685501\n\nCodedeviance(pointsPerSeason_linearRegression)\n\n[1] 103697220\n\nCodedeviance(pointsPerSeason_quadraticRegression)\n\n[1] 103650502\n\nCodedeviance(pointsPerSeason_randomIntercepts)\n\n[1] 178384.6\n\nCodedeviance(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n[1] 178082.7\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n[1] 177733\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n[1] 177232.6\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n[1] 176966.2\n\n\n\n12.4.4.3.3.7 Log Likelihood\n\nCodelogLik(pointsPerSeason_nullModel)\n\n'log Lik.' -92994.57 (df=2)\n\nCodelogLik(pointsPerSeason_linearRegression)\n\n'log Lik.' -92475.54 (df=13)\n\nCodelogLik(pointsPerSeason_quadraticRegression)\n\n'log Lik.' -92471.95 (df=19)\n\nCodelogLik(pointsPerSeason_randomIntercepts)\n\n'log Lik.' -89192.29 (df=3)\n\nCodelogLik(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n'log Lik.' -89041.35 (df=9)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n'log Lik.' -88866.51 (df=11)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n'log Lik.' -88616.29 (df=12)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n'log Lik.' -88483.11 (df=22)\n\n\n\n12.4.4.4 Multicollinearity\nAs with multiple regression, multicollinearity is also a potential concern for mixed models. Here are the variance inflation factors (VIFs) from an earlier model (VIF is described in Section 11.10):\n\nCodecar::vif(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n                                              GVIF Df GVIF^(1/(2*Df))\npositionFactor                        9.726123e+03  5        2.504921\nageCentered20                         2.276950e+02  1       15.089565\nageCentered20Quadratic                2.295488e+02  1       15.150868\nyears_of_experience                   6.854568e+00  1        2.618123\npositionFactor:ageCentered20          2.176193e+07  5        5.417136\npositionFactor:ageCentered20Quadratic 1.026228e+06  5        3.991392\n\n\n\n12.4.4.5 Diagnostic Plots\n\n12.4.4.5.1 Regression Coefficients\nTo visualize the regression coefficients, we can use the broom.mixed::tidy() function of the broom.mixed package (Bolker & Robinson, 2024), as in Figure 12.28.\n\nCodepointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience_tidy &lt;- broom.mixed::tidy(\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  effects = \"fixed\",\n  conf.int = TRUE)\n\nggplot(\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience_tidy,\n  aes(\n    x = term,\n    y = estimate)) +\n  geom_point() +\n  geom_errorbar(\n    aes(\n      ymin = conf.low,\n      ymax = conf.high),\n    width = 0.2) +\n  coord_flip() + # flip axes for readability\n  labs(\n    title = \"Regression Coefficients with 95% CI\",\n    y = \"Coefficient Estimate\",\n    x = \"\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 12.28: Regression Coefficients with 95% Confidence Interval from a Mixed Model With Random Intercepts, Random Linear Slopes, and Fixed Quadratic Slopes in Interaction With Position. Overlaid with the Model-Implied Trajectory by Position.\n\n\n\n\n\n12.4.4.5.2 Plot of Residuals\n\nCodeplot(\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  type = c(\"p\",\"smooth\"),\n  col.line = 1)\n\n\n\n\n\n\nFigure 12.29: Plot of Residuals.\n\n\n\n\n\n12.4.4.5.3 Scale-Location Plot\n\nCodeplot(\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  sqrt(abs(resid(.))) ~ fitted(.),\n  type = c(\"p\",\"smooth\"),\n  col.line = 1)\n\n\n\n\n\n\nFigure 12.30: Scale-Location Plot.\n\n\n\n\n\n12.4.4.5.4 Quantile–Quantile (QQ) Plot\n\nCodeqqnorm(resid(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience))\nqqline(resid(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience))\n\n\n\n\n\n\nFigure 12.31: Quantile–Quantile (QQ) Plot.\n\n\n\n\n\n12.4.4.5.5 Probability–Probability (PP) Plot\n\nCodepetersenlab::ppPlot(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n\n\n\n\n\nFigure 12.32: Probability–Probability (PP) Plot.\n\n\n\n\n\n12.4.4.5.6 Residuals Versus Leverage Plot\n\nCodeplot(\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  rstudent(.) ~ hatvalues(.))\n\n\n\n\n\n\nFigure 12.33: Residuals Versus Leverage Plot.\n\n\n\n\n\n12.4.4.5.7 Other Diagnostic Plots\n\nCodeperformance::check_model(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n\n\n\n\n\nFigure 12.34: Other Diagnostic Plots.\n\n\n\n\n\n12.4.4.6 Generalized Additive Model\n\nCodenum_cores &lt;- parallelly::availableCores() - 1\nnum_true_cores &lt;- parallelly::availableCores(logical = FALSE) - 1\n\n\n\nCodepointsPerSeason_gam &lt;- bam( # using bam() instead of gam() for faster estimation due to large size of data\n  fantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\"),\n  data = player_stats_seasonal_offense_subset,\n  nthreads = num_cores\n)\n\n\n\nCodepointsPerSeason_gamSummary &lt;- summary(pointsPerSeason_gam)\n\npointsPerSeason_gamSummary\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nfantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + \n    years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\")\n\nParametric coefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -17.6654     6.0662  -2.912 0.003596 ** \npositionFactorK      53.7045     7.4059   7.252 4.34e-13 ***\npositionFactorQB     55.1597     6.4597   8.539  &lt; 2e-16 ***\npositionFactorRB     21.1206     6.0488   3.492 0.000482 ***\npositionFactorTE      8.7050     6.1021   1.427 0.153728    \npositionFactorWR     21.7265     5.8575   3.709 0.000209 ***\nyears_of_experience   9.2372     0.6402  14.428  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                       edf   Ref.df       F p-value    \ns(ageCentered20):positionFactorFB    3.016    3.805  14.774  &lt;2e-16 ***\ns(ageCentered20):positionFactorK     2.772    3.537  30.352  &lt;2e-16 ***\ns(ageCentered20):positionFactorQB    5.637    6.819  82.926  &lt;2e-16 ***\ns(ageCentered20):positionFactorRB    5.795    6.652 110.406  &lt;2e-16 ***\ns(ageCentered20):positionFactorTE    5.862    6.882  48.890  &lt;2e-16 ***\ns(ageCentered20):positionFactorWR    6.852    7.722 136.472  &lt;2e-16 ***\ns(player_idFactor,ageCentered20)  2375.178 3809.000   3.779  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.53   Deviance explained = 60.1%\nfREML =  89425  Scale est. = 3272.1    n = 15896\n\nCodepointsPerSeason_gamSummary$r.sq\n\n[1] 0.5296079\n\nCodeperformance::r2(pointsPerSeason_gam)\n\n$R2\nAdjusted R2 \n  0.5296079 \n\nCodeAIC(pointsPerSeason_gam)\n\n[1] 175974.9\n\n\n\n12.4.4.6.1 Compare Models\n\nCodelinearMixedModelsVsGAM &lt;- list(\n  \"fixedLinear\" = pointsPerSeason_positionAgeFixedLinearSlopes,\n  \"randomLinear\" = pointsPerSeason_positionAgeRandomLinearSlopes,\n  \"randomLinearFixedQuadratic\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  \"randomLinearFixedQuadraticInteraction\" = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  \"gam\" = pointsPerSeason_gam\n)\n\nAIC(\n  #pointsPerSeason_nullModel,\n  #pointsPerSeason_linearRegression,\n  #pointsPerSeason_quadraticRegression,\n  #pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  pointsPerSeason_gam\n)\n\n\n  \n\n\nCode#AICcmodavg::aictab(linearMixedModelsVsGAM) # throws error (can't mix bam with lmer)\n#bbmle::AICctab(linearMixedModelsVsGAM) # different numbers of observations\n\nMuMIn::AICc(\n  pointsPerSeason_nullModel,\n  pointsPerSeason_linearRegression,\n  pointsPerSeason_quadraticRegression,\n  pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  pointsPerSeason_gam\n)\n\n\n  \n\n\nCodeBIC(\n  #pointsPerSeason_nullModel,\n  #pointsPerSeason_linearRegression,\n  #pointsPerSeason_quadraticRegression,\n  #pointsPerSeason_randomIntercepts,\n  pointsPerSeason_positionAgeFixedLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes,\n  pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction,\n  pointsPerSeason_gam\n)\n\n\n  \n\n\nCode#AICcmodavg::bictab(linearMixedModelsVsGAM) # throws error (can't mix bam with lmer)\n#bbmle::AICctab(linearMixedModelsVsGAM) # different numbers of observations\n\nsummary(pointsPerSeason_nullModel)$r.squared\n\n[1] 0\n\nCodesummary(pointsPerSeason_linearRegression)$r.squared\n\n[1] 0.06313637\n\nCodesummary(pointsPerSeason_quadraticRegression)$r.squared\n\n[1] 0.06355845\n\nCodeperformance::r2(pointsPerSeason_randomIntercepts)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.500\n     Marginal R2: 0.000\n\nCodeperformance::r2(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.523\n     Marginal R2: 0.037\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.560\n     Marginal R2: 0.033\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.640\n     Marginal R2: 0.108\n\nCodeperformance::r2(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.661\n     Marginal R2: 0.112\n\nCodeperformance::r2(pointsPerSeason_gam)\n\n$R2\nAdjusted R2 \n  0.5296079 \n\nCodedeviance(pointsPerSeason_nullModel)\n\n[1] 110685501\n\nCodedeviance(pointsPerSeason_linearRegression)\n\n[1] 103697220\n\nCodedeviance(pointsPerSeason_quadraticRegression)\n\n[1] 103650502\n\nCodedeviance(pointsPerSeason_randomIntercepts)\n\n[1] 178384.6\n\nCodedeviance(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n[1] 178082.7\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n[1] 177733\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n[1] 177232.6\n\nCodedeviance(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n[1] 176966.2\n\nCodedeviance(pointsPerSeason_gam)\n\n[1] 44120381\n\nCodelogLik(pointsPerSeason_nullModel)\n\n'log Lik.' -92994.57 (df=2)\n\nCodelogLik(pointsPerSeason_linearRegression)\n\n'log Lik.' -92475.54 (df=13)\n\nCodelogLik(pointsPerSeason_quadraticRegression)\n\n'log Lik.' -92471.95 (df=19)\n\nCodelogLik(pointsPerSeason_randomIntercepts)\n\n'log Lik.' -89192.29 (df=3)\n\nCodelogLik(pointsPerSeason_positionAgeFixedLinearSlopes)\n\n'log Lik.' -89041.35 (df=9)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearSlopes)\n\n'log Lik.' -88866.51 (df=11)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopes)\n\n'log Lik.' -88616.29 (df=12)\n\nCodelogLik(pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteraction)\n\n'log Lik.' -88483.11 (df=22)\n\nCodelogLik(pointsPerSeason_gam)\n\n'log Lik.' -85572.04 (df=2415.427)\n\n\n\n12.4.4.6.2 Players Who Were (at Least Once) at the Top of the End-of-Season Depth Chart\n\nCodepointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience &lt;- lmerTest::lmer(\n  fantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic + positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_idFactor),\n  data = player_stats_seasonal_offense_subset,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ positionFactor + ageCentered20 + ageCentered20Quadratic +  \n    positionFactor:ageCentered20 + positionFactor:ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_idFactor)\n   Data: player_stats_seasonal_offense_subset\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 176511.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.6050 -0.4551 -0.1141  0.4080  4.6031 \n\nRandom effects:\n Groups          Name          Variance Std.Dev. Corr \n player_idFactor (Intercept)   5089.71  71.34         \n                 ageCentered20   57.76   7.60    -0.65\n Residual                      2447.12  49.47         \nNumber of obs: 15896, groups:  player_idFactor, 3815\n\nFixed effects:\n                                         Estimate Std. Error        df t value\n(Intercept)                               14.6785    15.0714 6560.4655   0.974\npositionFactorK                           64.6704    18.1352 6235.4082   3.566\npositionFactorQB                          28.4447    16.6958 6537.5155   1.704\npositionFactorRB                          24.0325    15.9699 6640.8230   1.505\npositionFactorTE                          -5.2999    16.2692 6718.6200  -0.326\npositionFactorWR                           2.8553    15.6380 6565.6614   0.183\nageCentered20                              0.6447     3.8235 7325.4234   0.169\nageCentered20Quadratic                    -0.6929     0.2380 5532.7830  -2.911\nyears_of_experience                        9.8268     0.6796 5281.1321  14.460\npositionFactorK:ageCentered20             -5.6171     4.2235 7275.9076  -1.330\npositionFactorQB:ageCentered20             6.9539     4.0614 7415.0660   1.712\npositionFactorRB:ageCentered20             8.6240     4.1045 7482.7073   2.101\npositionFactorTE:ageCentered20             5.5113     4.1192 7542.8978   1.338\npositionFactorWR:ageCentered20            12.4063     3.9638 7265.2096   3.130\npositionFactorK:ageCentered20Quadratic     0.4434     0.2525 5800.4847   1.756\npositionFactorQB:ageCentered20Quadratic   -0.4460     0.2494 5795.0834  -1.789\npositionFactorRB:ageCentered20Quadratic   -1.0069     0.2654 5891.6936  -3.794\npositionFactorTE:ageCentered20Quadratic   -0.3852     0.2609 5909.5001  -1.476\npositionFactorWR:ageCentered20Quadratic   -1.0520     0.2514 5620.9052  -4.185\n                                        Pr(&gt;|t|)    \n(Intercept)                             0.330127    \npositionFactorK                         0.000365 ***\npositionFactorQB                        0.088484 .  \npositionFactorRB                        0.132408    \npositionFactorTE                        0.744614    \npositionFactorWR                        0.855129    \nageCentered20                           0.866110    \nageCentered20Quadratic                  0.003612 ** \nyears_of_experience                      &lt; 2e-16 ***\npositionFactorK:ageCentered20           0.183568    \npositionFactorQB:ageCentered20          0.086908 .  \npositionFactorRB:ageCentered20          0.035663 *  \npositionFactorTE:ageCentered20          0.180950    \npositionFactorWR:ageCentered20          0.001756 ** \npositionFactorK:ageCentered20Quadratic  0.079139 .  \npositionFactorQB:ageCentered20Quadratic 0.073717 .  \npositionFactorRB:ageCentered20Quadratic 0.000150 ***\npositionFactorTE:ageCentered20Quadratic 0.139899    \npositionFactorWR:ageCentered20Quadratic  2.9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodepetersenlab::lm.beta.lmer(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n                        positionFactorK                        positionFactorQB \n                             0.19285201                              0.11315935 \n                       positionFactorRB                        positionFactorTE \n                             0.12234184                             -0.02486561 \n                       positionFactorWR                           ageCentered20 \n                             0.01620410                              0.02776605 \n                 ageCentered20Quadratic                     years_of_experience \n                            -0.56038290                              0.41038565 \n          positionFactorK:ageCentered20          positionFactorQB:ageCentered20 \n                            -0.18417477                              0.27258480 \n         positionFactorRB:ageCentered20          positionFactorTE:ageCentered20 \n                             0.29481164                              0.19400944 \n         positionFactorWR:ageCentered20  positionFactorK:ageCentered20Quadratic \n                             0.51813934                              0.23233539 \npositionFactorQB:ageCentered20Quadratic positionFactorRB:ageCentered20Quadratic \n                            -0.24530402                             -0.32338758 \npositionFactorTE:ageCentered20Quadratic positionFactorWR:ageCentered20Quadratic \n                            -0.14231967                             -0.47579691 \n\nCodeperformance::r2(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.656\n     Marginal R2: 0.135\n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"positionFactor\")\n\n positionFactor emmean   SE   df lower.CL upper.CL\n FB               25.4 5.42 3672     14.8     36.0\n K                78.1 4.85 3387     68.6     87.6\n QB               74.7 3.26 3342     68.4     81.1\n RB               48.4 2.36 3872     43.8     53.0\n TE               34.8 2.55 3587     29.8     39.8\n WR               50.3 1.91 3584     46.6     54.1\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20\")\n\n ageCentered20 emmean   SE   df lower.CL upper.CL\n          6.82     52 1.49 3557       49     54.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"ageCentered20Quadratic\")\n\n ageCentered20Quadratic emmean   SE   df lower.CL upper.CL\n                   59.4     52 1.49 3557       49     54.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeemmeans::emmeans(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience, \"years_of_experience\")\n\n years_of_experience emmean   SE   df lower.CL upper.CL\n                4.83     52 1.49 3557       49     54.9\n\nResults are averaged over the levels of: positionFactor \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodeperformance::icc(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n\n  \n\n\nCodeAIC(pointsPerSeasonDepth_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience)\n\n[1] 176557.2\n\n\n\nCodepointsPerSeasonDepth_gam &lt;- bam( # using bam() instead of gam() for faster estimation due to large size of data\n  fantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\"),\n  data = player_stats_seasonal_offense_subsetDepth,\n  nthreads = num_cores\n)\n\n\n\nCodepointsPerSeasonDepth_gamSummary &lt;- summary(pointsPerSeasonDepth_gam)\n\npointsPerSeasonDepth_gamSummary\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nfantasyPoints ~ positionFactor + s(ageCentered20, by = positionFactor) + \n    years_of_experience + s(player_idFactor, ageCentered20, bs = \"re\")\n\nParametric coefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -5.3816     8.6178  -0.624 0.532330    \npositionFactorK      62.5609     9.9820   6.267 3.83e-10 ***\npositionFactorQB     89.8028     8.9647  10.017  &lt; 2e-16 ***\npositionFactorRB     42.7145     8.4582   5.050 4.50e-07 ***\npositionFactorTE     31.7255     8.6365   3.673 0.000241 ***\npositionFactorWR     48.0938     8.0874   5.947 2.83e-09 ***\nyears_of_experience   5.5938     0.9289   6.022 1.79e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                       edf   Ref.df      F  p-value    \ns(ageCentered20):positionFactorFB    2.615    3.324  6.454 0.000151 ***\ns(ageCentered20):positionFactorK     2.013    2.586 11.002 4.41e-06 ***\ns(ageCentered20):positionFactorQB    4.992    6.147 36.692  &lt; 2e-16 ***\ns(ageCentered20):positionFactorRB    5.143    6.021 54.271  &lt; 2e-16 ***\ns(ageCentered20):positionFactorTE    4.825    5.877 22.363  &lt; 2e-16 ***\ns(ageCentered20):positionFactorWR    6.210    7.203 73.072  &lt; 2e-16 ***\ns(player_idFactor,ageCentered20)  1356.944 1795.000  5.364  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.51   Deviance explained =   57%\nfREML =  63947  Scale est. = 3813.7    n = 11242\n\nCodepointsPerSeasonDepth_gamSummary$r.sq\n\n[1] 0.509519\n\nCodeperformance::r2(pointsPerSeasonDepth_gam)\n\n$R2\nAdjusted R2 \n   0.509519 \n\nCodeAIC(pointsPerSeasonDepth_gam)\n\n[1] 125911.6\n\n\n\n12.4.5 Bayesian Mixed Models\n\n12.4.5.1 Determine Response Distribution\n\nCodeplayer_stats_seasonal_offense_subset$fantasyPoints_posOnlyNoZeros &lt;- player_stats_seasonal_offense_subset$fantasyPoints_posOnly &lt;- player_stats_seasonal_offense_subset$fantasyPoints\nplayer_stats_seasonal_offense_subset$fantasyPoints_posOnly[player_stats_seasonal_offense_subset$fantasyPoints &lt; 0] &lt;- 0\nplayer_stats_seasonal_offense_subset$fantasyPoints_posOnlyNoZeros[player_stats_seasonal_offense_subset$fantasyPoints &lt;= 0] &lt;- 0.01\n\nfantasyPointsVector &lt;- player_stats_seasonal_offense_subset$fantasyPoints %&gt;% \n    na.omit() %&gt;% \n    as.vector()\n\nfantasyPointsVector_posOnlyNoZeros &lt;- fantasyPointsVector_posOnly &lt;- fantasyPointsVector\nfantasyPointsVector_posOnly[fantasyPointsVector &lt; 0] &lt;- 0\nfantasyPointsVector_posOnlyNoZeros[fantasyPointsVector &lt;= 0] &lt;- 0.01\n\n\n\nCodeggplot2::ggplot(\n  data = player_stats_seasonal_offense_subset,\n  mapping = aes(\n    x = fantasyPoints)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  theme_classic()\n\n\n\n\n\n\nFigure 12.35: Histogram of Fantasy Points with Overlaid Density Plot and Rug Plot.\n\n\n\n\nWe can use the fitdistrplus package (Delignette-Muller & Dutang, 2015; Delignette-Muller et al., 2025):\n\nCodefitdistrplus::descdist(\n  fantasyPointsVector,\n  boot = 1000)\n\nsummary statistics\n------\nmin:  -7.28   max:  485.1 \nmedian:  47.3 \nmean:  77.96829 \nestimated sd:  83.39277 \nestimated skewness:  1.336272 \nestimated kurtosis:  4.317106 \n\n\n\n\n\n\n\nFigure 12.36: Cullen and Frey Graph.\n\n\n\n\n\nCode# all values\nfit.norm &lt;- fitdistrplus::fitdist(fantasyPointsVector, \"norm\")\n\n# positive-only\nfit.exp &lt;- fitdist(fantasyPointsVector_posOnly, \"exp\")\n\n# positive and no zeros\nfit.gamma &lt;- fitdistrplus::fitdist(fantasyPointsVector_posOnlyNoZeros, \"gamma\")\nfit.lognormal &lt;- fitdistrplus::fitdist(fantasyPointsVector_posOnlyNoZeros, \"lnorm\")\nfit.weibull &lt;- fitdistrplus::fitdist(fantasyPointsVector_posOnlyNoZeros, \"weibull\")\n\n# Model fit\nAIC(fit.norm)\n\n[1] 185993.1\n\nCodeAIC(fit.exp)\n\n[1] 170517.1\n\nCodeAIC(fit.gamma) # fits best\n\n[1] 166925\n\nCodeAIC(fit.lognormal)\n\n[1] 175044.3\n\nCodeAIC(fit.weibull)\n\n[1] 167767.3\n\n\n\nCodeplot(fit.norm)\n\n\n\n\n\n\nFigure 12.37: Fit of normal distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.lognormal)\n\n\n\n\n\n\nFigure 12.38: Fit of log normal distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.gamma)\n\n\n\n\n\n\nFigure 12.39: Fit of gamma distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.exp)\n\n\n\n\n\n\nFigure 12.40: Fit of exponential distribution to fantasy points.\n\n\n\n\n\nCodeplot(fit.weibull)\n\n\n\n\n\n\nFigure 12.41: Fit of Weibull distribution to fantasy points.\n\n\n\n\n\nCodemodel_normal &lt;- glm(\n  formula = fantasyPoints ~ 1,\n  family = gaussian(),\n  data = player_stats_seasonal_offense_subset,\n  maxit = 100000)\n\nmodel_gamma &lt;- glm(\n  formula = fantasyPoints_posOnlyNoZeros ~ 1,\n  family = Gamma(),\n  data = player_stats_seasonal_offense_subset,\n  maxit = 100000)\n\nsum(resid(model_normal)^2)\n\n[1] 110685501\n\nCodesum(resid(model_gamma)^2)\n\n[1] 33233.46\n\nCodeAIC(model_normal)\n\n[1] 185993.1\n\nCodeAIC(model_gamma)\n\n[1] 167412.1\n\n\nBecause the response distribution (i.e., fantasy points) is positively skewed, we will use a gamma response distribution.\n\n12.4.5.2 Specify Model Formula\nInformation about smooth terms in the mgcv package (Wood, 2017, 2025) is provided at the following link: https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/smooth.terms.html. We estimate the model using the brms package (Bürkner, 2017, 2018, 2024), which provides an interface to the rstan package (Guo et al., 2025).\nSpecify model formula:\n\nCodebayesianMixedModelFormula &lt;- brms::bf(\n  fantasyPoints_posOnly ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(ageCentered20, player_idFactor, bs = \"re\") + (1 | player_idFactor)\n)\n\n\n\n12.4.5.3 Run Model\nNow, we can run the model.\n\n\n\n\n\n\nNote 12.1: Bayesian mixed model\n\n\n\nNote: the following code that runs the model takes a while. If you just want to save time and load the model object instead of running the model, you can load the model object (which has already been fit) using this code:\n\nCodeload(url(\"https://osf.io/download/q6rjf/\"))\n\n\n\n\nWe use the cmdstanr backend (Gabry et al., 2024) and threading using the parellely package (Bengtsson, 2025) for parallel (faster) processing:\n\nCodebayesianMixedModelFit &lt;- brms::brm(\n  formula = bayesianMixedModelFormula,\n  data = player_stats_seasonal_offense_subset,\n  family = hurdle_gamma(),\n  cores = 4,\n  save_pars = save_pars(latent = FALSE, all = FALSE),\n  threads = threading(parallelly::availableCores()),\n  backend = \"cmdstanr\",\n  seed = 52242,\n  silent = 0\n)\n\n\n\n12.4.5.4 Model Summary\n\nCodesummary(bayesianMixedModelFit)\n\n Family: hurdle_gamma \n  Links: mu = log; shape = identity; hu = identity \nFormula: fantasyPoints_posOnly ~ positionFactor + s(ageCentered20, by = positionFactor) + years_of_experience + s(ageCentered20, player_idFactor, bs = \"re\") + (1 | player_idFactor) \n   Data: player_stats_seasonal_offense_subset (Number of observations: 15896) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n                                      Estimate Est.Error l-95% CI u-95% CI Rhat\nsds(sageCentered20positionFactorFB_1)     2.84      1.27     1.05     5.89 1.00\nsds(sageCentered20positionFactorK_1)      0.54      0.47     0.02     1.76 1.00\nsds(sageCentered20positionFactorQB_1)     1.26      0.61     0.47     2.82 1.00\nsds(sageCentered20positionFactorRB_1)     2.18      0.94     0.95     4.49 1.00\nsds(sageCentered20positionFactorTE_1)     2.87      1.18     1.31     5.83 1.00\nsds(sageCentered20positionFactorWR_1)     2.19      0.83     1.05     4.14 1.00\nsds(sageCentered20player_idFactor_1)      1.46      0.14     1.18     1.73 1.03\n                                      Bulk_ESS Tail_ESS\nsds(sageCentered20positionFactorFB_1)     1940     2549\nsds(sageCentered20positionFactorK_1)      2095     1774\nsds(sageCentered20positionFactorQB_1)     2320     2600\nsds(sageCentered20positionFactorRB_1)     2302     2992\nsds(sageCentered20positionFactorTE_1)     1989     2487\nsds(sageCentered20positionFactorWR_1)     1997     2630\nsds(sageCentered20player_idFactor_1)       183      377\n\nMultilevel Hyperparameters:\n~player_idFactor (Number of levels: 3815) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.98      0.02     0.94     1.02 1.00      831     1715\n\nRegression Coefficients:\n                                  Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                             1.89      0.11     1.66     2.11 1.00\npositionFactorK                       1.41      0.13     1.15     1.67 1.00\npositionFactorQB                      0.94      0.12     0.71     1.18 1.00\npositionFactorRB                      0.60      0.11     0.39     0.81 1.00\npositionFactorTE                      0.25      0.11     0.04     0.45 1.00\npositionFactorWR                      0.66      0.10     0.46     0.87 1.00\nyears_of_experience                   0.22      0.01     0.19     0.24 1.00\nsageCentered20:positionFactorFB_1   -10.74      6.98   -24.89     3.89 1.00\nsageCentered20:positionFactorK_1     -9.50      1.32   -12.42    -6.97 1.00\nsageCentered20:positionFactorQB_1   -11.35      2.49   -15.82    -5.77 1.00\nsageCentered20:positionFactorRB_1   -12.27      5.15   -21.91    -1.50 1.00\nsageCentered20:positionFactorTE_1   -15.17      6.63   -30.63    -3.93 1.00\nsageCentered20:positionFactorWR_1   -12.98      4.80   -22.98    -4.08 1.00\n                                  Bulk_ESS Tail_ESS\nIntercept                              950     2001\npositionFactorK                       1129     2133\npositionFactorQB                       817     1795\npositionFactorRB                       810     1183\npositionFactorTE                       825     1764\npositionFactorWR                       786     1621\nyears_of_experience                   1501     2639\nsageCentered20:positionFactorFB_1     2097     2013\nsageCentered20:positionFactorK_1      2367     2247\nsageCentered20:positionFactorQB_1     2363     2342\nsageCentered20:positionFactorRB_1     2515     2227\nsageCentered20:positionFactorTE_1     2170     1895\nsageCentered20:positionFactorWR_1     2510     2014\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     1.47      0.02     1.43     1.51 1.01     1285     2113\nhu        0.04      0.00     0.03     0.04 1.00     8995     2671\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nCodebrms::prior_summary(bayesianMixedModelFit)\n\n\n  \n\n\n\n\n\n\n\n\n\nNote 12.2: Bayesian mixed model R-squared\n\n\n\nNote: the following code that estimates the \\(R^2\\) for the Bayesian mixed model takes a while to run.\n\n\n\nCodeperformance::r2(bayesianMixedModelFit)\n\n\n\n12.4.5.5 Trace Plots\n\nCodeplot(bayesianMixedModelFit, ask = FALSE)\n\n\n\n\n\n\nFigure 12.42: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n\n\n\n\nFigure 12.43: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n\n\n\n\nFigure 12.44: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n\n\n\n\nFigure 12.45: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n\n\n\n\nFigure 12.46: Trace Plots from Bayesian Mixed Model.\n\n\n\n\n\n12.4.5.6 Posterior Predictive Check\n\nCodepp_check(bayesianMixedModelFit) + \n  ggplot2::xlim(0, 600)\n\n\n\n\n\n\nFigure 12.47: Posterior Predictive Check from Bayesian Mixed Model.\n\n\n\n\n\n12.4.5.7 Plot of Regression Coefficients\nTo visualize the regression coefficients, we can use the broom.mixed::tidy() function of the broom.mixed package (Bolker & Robinson, 2024), as in Figure 12.48.\n\nCodebayesianMixedModelFit_tidy &lt;- broom.mixed::tidy(\n  bayesianMixedModelFit,\n  effects = \"fixed\",\n  conf.int = TRUE)\n\nggplot(\n  bayesianMixedModelFit_tidy,\n  aes(\n    x = term,\n    y = estimate)) +\n  geom_point() +\n  geom_errorbar(\n    aes(\n      ymin = conf.low,\n      ymax = conf.high),\n    width = 0.2) +\n  coord_flip() + # flip axes for readability\n  labs(\n    title = \"Posterior Distributions and 95% Credible Intervals\",\n    x = \"Predictor\",\n    y = \"Posterior Estimate\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 12.48: Regression Coefficients with 95% Credible Interval from Bayesian Mixed Model.\n\n\n\n\nTo visualize the distribution of regression coefficients, we can use the tidybayes package (Kay, 2024), as in Figure 12.49.\n\nCodebayesianMixedModelFit %&gt;%\n  tidybayes::tidy_draws() %&gt;%\n  select(starts_with(\"b_\")) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\") %&gt;%\n  mutate(term = sub(\"^b_\", \"\", term)) %&gt;%\n  ggplot(\n    aes(\n      x = estimate,\n      y = term)) +\n  stat_halfeye() + # by default, .width shows the 66% (thick line) and 95% (thin line) credible intervals\n  labs(\n    title = \"Posterior Distributions of Regression Coefficients with 66% and 95% Credible Intervals\",\n    x = \"Predictor\",\n    y = \"Posterior Estimate\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 12.49: Distributions of Regression Coefficients with 66% (Thin Line) and 95% (Thin Line) Credible Intervals from Bayesian Mixed Model.\n\n\n\n\n\n12.4.6 Plots of Model-Implied Fantasy Points by Position and Age\n\nCode# From Quadratic Model: All Players\npointsPerSeason_positionAge_newData$fantasyPoints_quadratic &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\n# From Quadratic Model: Players at Top of End-of-Season Depth Chart\npointsPerSeason_positionAge_newData$fantasyPoints_depthQuadratic &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  newdata = pointsPerSeason_positionAge_newData,\n  re.form = NA\n)\n\n# From GAM Model: All Players\npointsPerSeason_positionAge_newData$fantasyPoints_gam &lt;- predict(\n  object = pointsPerSeason_gam,\n  newdata = pointsPerSeason_positionAge_newData,\n  newdata.guaranteed = TRUE,\n  exclude = \"s(player_idFactor,ageCentered20)\"\n)\n\n# From GAM Model: Players at Top of End-of-Season Depth Chart\npointsPerSeason_positionAge_newData$fantasyPoints_depthGAM &lt;- predict(\n  object = pointsPerSeasonDepth_gam,\n  newdata = pointsPerSeason_positionAge_newData,\n  newdata.guaranteed = TRUE,\n  exclude = \"s(player_idFactor,ageCentered20)\"\n)\n\n\n\nCode# From Bayesian Model: All Players\nmodel_levels &lt;- levels(bayesianMixedModelFit$data$player_idFactor)\n\npointsPerSeason_positionAge_newData$player_idFactor &lt;- factor(\n  rep(model_levels[1], nrow(pointsPerSeason_positionAge_newData)), \n  levels = model_levels\n)\n\n\n\nCodemodelPredictions_bayesian &lt;- predict(\n  object = bayesianMixedModelFit,\n  newdata = pointsPerSeason_positionAge_newData,\n  re_formula = NA,\n  cores = num_cores\n)\n\n\n\nCodepointsPerSeason_positionAge_newData$fantasyPoints_bayesian &lt;- modelPredictions_bayesian[,\"Estimate\"]\n\n\nPlots of model-implied fantasy points by position and age are in Figures 12.50–12.54.\n\n12.4.6.1 Quadratic Model\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_quadratic,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Quadratic Model with All Players\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.50: Plot of Model-Implied Quadratic Trajectories of Fantasy Points by Age.\n\n\n\n\n\n12.4.6.2 Quadratic Model: Top of Depth Chart\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_depthQuadratic,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Quadratic Model with Players Who Were Once at Top of End-of-Season Depth Chart\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.51: Plot of Model-Implied Quadratic Trajectories of Fantasy Points by Age For Players Who Were Once at the Top of the End-of-Season Depth Chart.\n\n\n\n\n\n12.4.6.3 Generalized Additive Model\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Generalized Additive Model with All Players\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.52: Plot of Implied Trajectories of Fantasy Points by Age from a Generalized Additive Model.\n\n\n\n\n\n12.4.6.4 Generalized Additive Model: Top of Depth Chart\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_depthGAM,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Generalized Additive Model with Players Who Were Once at Top of End-of-Season Depth Chart\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.53: Plot of Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, For Players Who Were Once at the Top of the End-of-Season Depth Chart.\n\n\n\n\n\n12.4.6.5 Bayesian Mixed Model\n\nCodeggplot2::ggplot(\n  data = pointsPerSeason_positionAge_newData,\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_bayesian,\n    color = positionFactor\n  )\n) + \n  geom_smooth() +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age and Position\",\n    subtitle = \"Bayesian Mixed Model with All Players\",\n    color = \"Position\"\n  ) +\n  theme_classic() +\n  guides(color = guide_legend(override.aes = list(fill = NA))) # transparent legend background\n\n\n\n\n\n\nFigure 12.54: Plot of Implied Trajectories of Fantasy Points by Age from a Bayesian Mixed Model.\n\n\n\n\n\n12.4.7 Plots of Individuals’ Model-Implied Fantasy Points by Age\nWe provide plots of individuals’ model-implied fantasy points by age from the Bayesian mixed model in Section 25.4.4.\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_quadratic &lt;- predict(\n  object = pointsPerSeason_positionAgeRandomLinearFixedQuadraticSlopesInteractionExperience,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\nplayer_stats_seasonal_offense_subsetCC$fantasyPoints_gam &lt;- predict(\n  object = pointsPerSeason_gam,\n  newdata = player_stats_seasonal_offense_subsetCC\n)\n\n\n\nCodezeroAge &lt;- pointsPerSeason_positionAge_newData %&gt;% \n  group_by(positionFactor) %&gt;% \n  filter(fantasyPoints_gam &lt; 0) %&gt;% \n  slice(which.min(age))\n\npeakAge &lt;- pointsPerSeason_positionAge_newData %&gt;% \n  group_by(positionFactor) %&gt;% \n  slice(which.max(fantasyPoints_gam))\n\npeakAge2 &lt;- pointsPerSeason_positionAge_newData %&gt;% \n  filter(age &gt; 22) %&gt;% \n  group_by(positionFactor) %&gt;% \n  slice(which.max(fantasyPoints_gam))\n\nqbPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"QB\")], 0)\nfbPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"FB\")], 0)\nrbPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"RB\")], 0)\nwrPeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"WR\")], 0)\nwrPeakAge2 &lt;- round(peakAge2$age[which(peakAge$positionFactor == \"WR\")], 0)\ntePeakAge &lt;- round(peakAge$age[which(peakAge$positionFactor == \"TE\")], 0)\n\nqbZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"QB\")], 0)\nfbZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"FB\")], 0)\nrbZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"RB\")], 0)\nwrZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"WR\")], 0)\nteZeroAge &lt;- round(zeroAge$age[which(zeroAge$positionFactor == \"TE\")], 0)\n\n\n\n12.4.7.1 Quarterbacks\nA plot of Quarterbacks’ model-implied fantasy points by age is in Figure 12.55. The model-implied peak of Quarterbacks’ fantasy points is at age 20. The model-predicted value of zero fantasy points for Quarterbacks is at age 35.\n\nCodeplot_individualFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"QB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"QB\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.55: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Quarterbacks. Overlaid with the Model-Implied Trajectory for Quarterbacks in Blue.\n\n\n\n\n12.4.7.2 Fullbacks\nA plot of Fullbacks’ model-implied fantasy points by age is in Figure 12.56. The model-implied peak of Fullbacks’ fantasy points is at age 27. The model-predicted value of zero fantasy points for Fullbacks is at age 33.\n\nCodeplot_individualFantasyPointsByAgeFB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"FB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"FB\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Fullbacks\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeFB,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.56: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Fullbacks. Overlaid with the Model-Implied Trajectory for Fullbacks in Blue.\n\n\n\n\n12.4.7.3 Running Backs\nA plot of Running Backs’ model-implied fantasy points by age is in Figure 12.57. The model-implied peak of Running Backs’ fantasy points is at age 20. The model-predicted value of zero fantasy points for Running Backs is at age 30.\n\nCodeplot_individualFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"RB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"RB\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.57: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Running Backs. Overlaid with the Model-Implied Trajectory for Running Backs in Blue.\n\n\n\n\n12.4.7.4 Wide Receivers\nA plot of Wide Receivers’ model-implied fantasy points by age is in Figure 12.58. The model-implied peaks of Wide Receivers’ fantasy points are at ages 20 and 26. The model-predicted value of zero fantasy points for Wide Receivers is at age 31.\n\nCodeplot_individualFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"WR\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"WR\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.58: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Wide Receivers. Overlaid with the Model-Implied Trajectory for Wide Receivers in Blue.\n\n\n\n\n12.4.7.5 Tight Ends\nA plot of Tight Ends’ model-implied fantasy points by age is in Figure 12.59. The model-implied peak of Tight Ends’ fantasy points is at age 20. The model-predicted value of zero fantasy points for Tight Ends is at age 32.\n\nCodeplot_individualFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"TE\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_gam,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_smooth(\n    mapping = aes(\n      x = age,\n      y = fantasyPoints_gam\n    ),\n    data = pointsPerSeason_positionAge_newData %&gt;% filter(positionFactor == \"TE\"),\n    inherit.aes = FALSE,\n    se = TRUE,\n    linewidth = 2\n  ) +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_gam,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints_gam\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 12.59: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Generalized Additive Model, for Tight Ends. Overlaid with the Model-Implied Trajectory for Wide Tight Ends in Blue.\n\n\n\n\n12.4.8 Summary of Findings\nWe applied mixed models with random intercepts and random slopes to allow our model estimates to account for the fact that different players have different starting points (intercepts) and different changes over time (slopes) in fantasy points. A quadratic, inverted-U-shaped form as a function of age fit better than a linear form as a function of age in predicting players’ fantasy points. A generalized additive model that allowed further nonlinearity fit even better than the quadratic model.\nBased on the bivariate scatterplots between age and fantasy points, we might conclude that players tend to stay stable or even increase in fantasy points with age. However, this conclusion would be wrong. When we account for the longitudinal data (i.e., multiple observations over time for the same player) using mixed models, we observe that fantasy points tend to decrease with age, with the timing and rate of decline differing for each position. In other words, the association between age and fantasy points differs at the person level versus the group level. This is an example of Simpson’s paradox.\nThe discrepancy between the positive or null association between age and players’ fantasy points at the group level, and the negative association at the person level may be due, in part, to the selective attrition of players with age. The players who play the longest will tend to be the highest performing players, whereas the poorest performing players will retire or get dropped from the team at younger ages. Thus, the selective attrition of weaker players may make it seem that there is no association between age and performance (or even a positive one!), when in fact, players’ performance tends to decrease with age after age 26 or so (with the timing differing from position to position), until the player eventually retires or is dropped from the team. Selective attrition is common in longitudinal studies (such as this one) and intervention studies. For instance, attrition may be more likely for individuals from lower socioeconomic status backgrounds because they may face more challenges in continuing in longitudinal studies such as fewer financial resources, greater life stressors, etc. In addition, people who experience side effects or lack of improvement may be more likely to drop out of intervention studies. Examining only those who completed treatment (an example of selection bias) would make the intervention look more effective than it actually was because the people who stay in the study are those who experience the greatest improvement. Thus, it is important to use approaches such as mixed models or other approaches that account for the multiple observations from the same person, that use all available information, and that do not exclude people who do not complete all portions of the study.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsConclusion",
    "href": "mixed-models.html#sec-mixedModelsConclusion",
    "title": "12  Mixed Models",
    "section": "\n12.5 Conclusion",
    "text": "12.5 Conclusion\nMixed models allow accounting for multiple levels or units of analysis and to include both fixed and random effects. Inclusion of random effects allows the association between the predictor variables (the intercept and age) and the outcome variable (fantasy points) to differ for each individual in the grouping level (in this case, each player). This allows for more accurately predicting phenomena. Based on the bivariate scatterplots between age and fantasy points, we might conclude that players tend to stay stable or even increase in fantasy points with age. However, this conclusion would be wrong. When we account for the longitudinal data using mixed models, we observe that players’ fantasy points tend to decrease with age, with the timing and rate of decline differing for each position. In other words, the association between age and fantasy points differs at the person level versus the group level, which is an example of Simpson’s paradox. In sum, mixed models are valuable for examining associations between variables when there are multiple levels of data (i.e., multiple observations within the same unit, known as clustering or nesting). It is important not to confuse the association at one level (e.g., group level) with the association at another level (e.g., person level), which is an example of the ecological fallacy.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "mixed-models.html#sec-mixedModelsSessionInfo",
    "href": "mixed-models.html#sec-mixedModelsSessionInfo",
    "title": "12  Mixed Models",
    "section": "\n12.6 Session Info",
    "text": "12.6 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n [4] dplyr_1.1.4         purrr_1.1.0         readr_2.1.5        \n [7] tidyr_1.3.1         tibble_3.3.0        tidyverse_2.0.0    \n[10] viridis_0.6.5       viridisLite_0.4.2   plotly_4.11.0      \n[13] ggplot2_3.5.2       tidybayes_3.0.7     broom.mixed_0.2.9.6\n[16] parallelly_1.45.1   performance_0.15.0  fitdistrplus_1.2-4 \n[19] survival_3.8-3      MASS_7.3-65         cmdstanr_0.9.0.9000\n[22] brms_2.22.0         Rcpp_1.1.0          rstan_2.32.7       \n[25] StanHeaders_2.32.10 bbmle_1.0.25.1      AICcmodavg_2.3-4   \n[28] mgcv_1.9-3          nlme_3.1-168        sjstats_0.19.1     \n[31] emmeans_1.11.2      MuMIn_1.48.11       effectsize_1.0.1   \n[34] lmerTest_3.1-3      lme4_1.1-37         Matrix_1.7-3       \n\nloaded via a namespace (and not attached):\n  [1] svUnit_1.0.8         splines_4.5.1        datawizard_1.2.0    \n  [4] rpart_4.1.24         lifecycle_1.0.4      Rdpack_2.6.4        \n  [7] globals_0.18.0       processx_3.8.6       lattice_0.22-7      \n [10] insight_1.4.0        crosstalk_1.2.1      ggdist_3.3.3        \n [13] backports_1.5.0      magrittr_2.0.3       Hmisc_5.2-3         \n [16] rmarkdown_2.29       yaml_2.3.10          pkgbuild_1.4.8      \n [19] DBI_1.2.3            minqa_1.2.8          RColorBrewer_1.1-3  \n [22] multcomp_1.4-28      abind_1.4-8          quadprog_1.5-8      \n [25] nnet_7.3-20          TH.data_1.1-3        tensorA_0.36.2.1    \n [28] sandwich_3.1-1       ggrepel_0.9.6        inline_0.3.21       \n [31] pbkrtest_0.5.5       listenv_0.9.1        bridgesampling_1.1-2\n [34] codetools_0.2-20     tidyselect_1.2.1     bayesplot_1.13.0    \n [37] farver_2.1.2         matrixStats_1.5.0    base64enc_0.1-3     \n [40] jsonlite_2.0.0       Formula_1.2-5        tools_4.5.1         \n [43] glue_1.8.0           mnormt_2.1.1         gridExtra_2.3       \n [46] xfun_0.53            distributional_0.5.0 loo_2.8.0           \n [49] withr_3.0.2          numDeriv_2016.8-1.1  fastmap_1.2.0       \n [52] mitools_2.4          boot_1.3-31          digest_0.6.37       \n [55] timechange_0.3.0     R6_2.6.1             estimability_1.5.1  \n [58] colorspace_2.1-1     mix_1.0-13           see_0.11.0          \n [61] generics_0.1.4       data.table_1.17.8    httr_1.4.7          \n [64] htmlwidgets_1.6.4    parameters_0.28.0    pkgconfig_2.0.3     \n [67] gtable_0.3.6         furrr_0.3.1          htmltools_0.5.8.1   \n [70] lavaan_0.6-19        carData_3.0-5        scales_1.4.0        \n [73] posterior_1.6.1      reformulas_0.4.1     knitr_1.50          \n [76] rstudioapi_0.17.1    tzdb_0.5.0           reshape2_1.4.4      \n [79] coda_0.19-4.1        checkmate_2.3.3      curl_7.0.0          \n [82] nloptr_2.2.1         bdsmatrix_1.3-7      zoo_1.8-14          \n [85] parallel_4.5.1       foreign_0.8-90       pillar_1.11.0       \n [88] grid_4.5.1           vctrs_0.6.5          VGAM_1.1-13         \n [91] car_3.1-3            arrayhelpers_1.1-0   xtable_1.8-4        \n [94] cluster_2.1.8.1      htmlTable_2.4.3      evaluate_1.0.4      \n [97] pbivnorm_0.6.0       mvtnorm_1.3-3        cli_3.6.5           \n[100] compiler_4.5.1       rlang_1.1.6          rstantools_2.4.0    \n[103] labeling_0.4.3       ps_1.9.1             plyr_1.8.9          \n[106] stringi_1.8.7        psych_2.5.6          QuickJSR_1.8.0      \n[109] lazyeval_0.2.2       Brobdingnag_1.2-9    bayestestR_0.16.1   \n[112] V8_6.0.6             petersenlab_1.2.0    hms_1.1.3           \n[115] patchwork_1.3.2      unmarked_1.5.0       future_1.67.0       \n[118] rbibutils_2.3        broom_1.0.9          RcppParallel_5.1.10 \n\n\n\n\n\n\nBartoń, K. (2024). MuMIn: Multi-model inference. https://doi.org/10.32614/CRAN.package.MuMIn\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2025). lme4: Linear mixed-effects models using Eigen and S4. https://doi.org/10.32614/CRAN.package.lme4\n\n\nBengtsson, H. (2025). parallelly: Enhancing the parallel package. https://doi.org/10.32614/CRAN.package.parallelly\n\n\nBen-Shachar, M. S., Lüdecke, D., & Makowski, D. (2020). effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software, 5(56), 2815. https://doi.org/10.21105/joss.02815\n\n\nBen-Shachar, M. S., Makowski, D., Lüdecke, D., Patil, I., Wiernik, B. M., Thériault, R., & Waggoner, P. (2025). effectsize: Indices of effect size. https://doi.org/10.32614/CRAN.package.effectsize\n\n\nBolker, B., & R Development Core Team. (2023). bbmle: Tools for general maximum likelihood estimation. https://doi.org/10.32614/CRAN.package.bbmle\n\n\nBolker, B., & Robinson, D. (2024). broom.mixed: Tidying methods for mixed models. https://doi.org/10.32614/CRAN.package.broom.mixed\n\n\nBrauer, M., & Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://doi.org/10.1037/met0000159\n\n\nBürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2024). brms: Bayesian regression models using Stan. https://doi.org/10.32614/CRAN.package.brms\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDelignette-Muller, M. L., & Dutang, C. (2015). fitdistrplus: An R package for fitting distributions. Journal of Statistical Software, 64(4), 1–34. https://doi.org/10.18637/jss.v064.i04\n\n\nDelignette-Muller, M.-L., Dutang, C., & Siberchicot, A. (2025). fitdistrplus: Help to fit of a parametric distribution to non-censored or censored data. https://doi.org/10.32614/CRAN.package.fitdistrplus\n\n\nFreeman, M. K. (2017). An introduction to hierarchical modeling. http://mfviz.com/hierarchical-models/\n\n\nGabry, J., Češnovar, R., & Johnson, A. (2024). cmdstanr: R interface to CmdStan. https://mc-stan.org/cmdstanr\n\n\nGuo, J., Gabry, J., Goodrich, B., Johnson, A., Weber, S., & Badr, H. S. (2025). rstan: R interface to Stan. https://doi.org/10.32614/CRAN.package.rstan\n\n\nKay, M. (2024). tidybayes: Tidy data and geoms for Bayesian models. https://doi.org/10.32614/CRAN.package.tidybayes\n\n\nKuznetsova, A., Bruun Brockhoff, P., & Haubo Bojesen Christensen, R. (2020). lmerTest: Tests in linear mixed effects models. https://doi.org/10.32614/CRAN.package.lmerTest\n\n\nLenth, R. V. (2025). emmeans: Estimated marginal means, aka least-squares means. https://doi.org/10.32614/CRAN.package.emmeans\n\n\nMazerolle, M. J. (2025). AICcmodavg: Model selection and multimodel inference based on (Q)AIC(c). https://doi.org/10.32614/CRAN.package.AICcmodavg\n\n\nUsami, S., & Murayama, K. (2018). Time-specific errors in growth curve modeling: Type-1 error inflation and a possible solution with mixed-effects models. Multivariate Behavioral Research, 53(6), 876–897. https://doi.org/10.1080/00273171.2018.1504273\n\n\nWood, S. N. (2017). Generalized additive models: An introduction with R (2nd ed.). CRC press. https://doi.org/10.1201/9781315370279\n\n\nWood, S. N. (2025). mgcv: Mixed GAM computation vehicle with automatic smoothness estimation. https://doi.org/10.32614/CRAN.package.mgcv",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Mixed Models</span>"
    ]
  },
  {
    "objectID": "causal-inference.html",
    "href": "causal-inference.html",
    "title": "13  Causal Inference",
    "section": "",
    "text": "13.1 Getting Started\nThis chapter provides an overview of principles of causal inference and causal diagrams.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceGettingStarted",
    "href": "causal-inference.html#sec-causalInferenceGettingStarted",
    "title": "13  Causal Inference",
    "section": "",
    "text": "13.1.1 Load Packages\n\nCodelibrary(\"dagitty\")\nlibrary(\"ggdag\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causation",
    "href": "causal-inference.html#sec-causation",
    "title": "13  Causal Inference",
    "section": "\n13.2 Causation",
    "text": "13.2 Causation\nA causal effect is the difference in an outcome that is directly attributable to different levels or values of an input variable. It reflects the difference in the outcome that would be observed for the same unit under two or more different levels of the input variable, holding all other relevant factors constant. For instance, for a given player, the extent to which consumption of sports drink influences their performance reflects the difference between how well the player would perform if they consume sports drink compared to how well they would perform if they do not consume sports drink.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-correlationCausality",
    "href": "causal-inference.html#sec-correlationCausality",
    "title": "13  Causal Inference",
    "section": "\n13.3 Correlation Does Not Imply Causation",
    "text": "13.3 Correlation Does Not Imply Causation\nAs described in Section 8.5.2.1, there are several reasons why two variables, X and Y, might be correlated:\n\n\nX causes Y\n\n\nY causes X\n\n\nX and Y are bidirectional: X causes Y and Y causes X\n\na third variable (i.e., confound), Z, influences both X and Y\n\nthe association between X and Y is spurious (noncausal and due to random chance)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-conditionsForCausality",
    "href": "causal-inference.html#sec-conditionsForCausality",
    "title": "13  Causal Inference",
    "section": "\n13.4 Criteria for Causality",
    "text": "13.4 Criteria for Causality\nHow do we know whether two processes are causally related? There are three criteria for establishing causality (Shadish et al., 2002):\n\nThe cause (e.g., the independent or predictor variable) temporally precedes the effect (i.e., the dependent or outcome variable).\nThe cause is related to (i.e., associated with) the effect.\nThere are no other alternative explanations for the effect apart from the cause.\n\nThe first criterion for establishing causality involves temporal precedence. In order for a cause to influence an effect, the cause must occur before the effect. For instance, if sports drink consumption influences player performance, the sports drink consumption (that is presumed to influence performance) must occur prior to the performance improvement. Establishing the first criterion eliminates the possibility that the association between the purported cause and effect reflects reverse causation. Reverse causation occurs when the purported effect is actually the cause of the purported cause, rather than the other way around. For instance, if sports drink consumption occurs only once, and it occurs only before and not after performance, then we have ruled out the possibility of reverse causation (i.e., that better performance causes players to consume sports drink). To help establish the first criterion, experiments and longitudinal designs are useful.\nThe second criterion involves association. The purported cause must be associated with the purported effect. Nevertheless, as the maxim goes, “correlation does not imply causation.” Just because two variables are correlated does not necessarily mean that they are causally related. However, correlation is useful because causality requires that the two processes be correlated. That is, correlation is a necessary but insufficient condition for causality. For instance, if sports drink consumption influences player performance, sports drink consumption must be associated with performance improvement.\nThe third criterion involves ruling out alternative reasons why the purported cause and effect may be related. As noted in Section 13.3, there are five reasons why X may be correlated with Y. If we meet the first criterion of causality, we have removed the possibility that Y causes X (i.e., reverse causality). To meet the third criterion of causality, we need to remove the possibility that the association reflects a third variable (confound) that influences both the cause and effect, and we need to remove the possibility that the association is spurious—the possibility that the association between the purported cause and effect is due to random chance.\nThere are multiple approaches to meeting the third criterion of causality, such as by use of experiments, longitudinal designs, control variables, within-subject designs, and genetically informed designs, as described in Section 13.5.\nIn general, to meet the third criterion of causality, one must consider the counterfactual—what would have happened “counter to the fact”. A counterfactual is what would have happened in the hypothetical scenario that the cause did not occur [i.e., what would have happened in the absence of the cause; Shadish et al. (2002)]. When engaging in causal inference, it is important to consider what would have happened if the hypothetical cause had actually not occurred. It is like an alternate universe where the cause did not happen. For instance, consider that we conduct an experiment to randomly assign some players to consume a sports drink before a game and other players to drink only water. In this case, our treatment/intervention group is the group of players that consumed a sports drink. The control group is the group players that drank only water. Now, consider that the players in the treatment group outperform the players in the control group in their football game. In such a study, we observe what did happen when players received a treatment. The counterfactual is knowledge of what would have happened to those same players if they simultaneously had not received treatment (Shadish et al., 2002). The true causal effect, then, is the difference between what did happen and what would have happened. However, we cannot observe a counterfactual. This is the fundamental problem of causal inference—we only observe one potential world/outcome/universe. That is, we do not know for sure what would have happened to the players who received treatment if those same players had actually not received treatment. We have a control group, but the control group does not have the same players as the intervention group, and it is impossible for a person to simultaneously receive and not receive treatment. Thus, an individual causal effect cannot typically be identified (D’Onofrio et al., 2020).\nSo, our goal in working toward causal inference as scientists is to create reasonable approximations to this impossible counterfactual (Shadish et al., 2002). For instance, if using a between-subject design, we want the two groups to be equivalent in every possible way except whether or not they receive the treatment, so we might stratify each group to be equivalent in terms of age, weight, position, experience, skill, etc. Or, we might test the same people using an A-B-A-B within-subject design. In an A-B-A-B within-subject design, players receive no treatment at baseline (timepoint 1: game 1), receive the treatment at timepoint 2 (game 2), receive no treatment at timepoint 3 (game 3), and receive the treatment at timepoint 4 (game 4). Neither of these approximations is a true counterfactual. In the between-subject design, the players differ between the two groups, so we cannot know how the individuals who received the treatment would have performed if they had actually not received the treatment. In the A-B-A-B within-subject design, all players receive the same sequence of alternating conditions—no treatment, treatment, no treatment, treatment—across four timepoints. Although the A-B-A-B within-subject design keeps participants constant, the specific occasion of measurement may differ from participant to participant (thus introducing time or context of measurement as a potential confound), and there can be carryover effects from one condition to the next. For instance, consuming sports drinks before game 2 might also help the player be better hydrated in general, including for subsequent games. Thus, we cannot know how a player would have performed in game 1 with treatment or in game 2 without treatment, and so on. Nevertheless, it is important to be aware of the counterfactual and to engage in counterfactual reasoning to consider what would have happened if the supposed cause had not occurred. Considering the counterfactual is important for designing closer approximations to the counterfactual in studies for stronger research designs and stronger causal inference.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-approachesCausalInference",
    "href": "causal-inference.html#sec-approachesCausalInference",
    "title": "13  Causal Inference",
    "section": "\n13.5 Approaches for Causal Inference",
    "text": "13.5 Approaches for Causal Inference\nBroadly, approaches for causal inference encompass 1) analysis approaches that account for measured factors and 2) designs that account for unmeasured factors (D’Onofrio et al., 2020).\n\n13.5.1 Experimental Designs\nAs described in Section 8.5.1, experimental designs are designs in which participants are randomly assigned to one or more levels of the independent variable to observe its effects on the dependent variable. Experimental designs provide the strongest tests of causality because they can rule out reverse causation and third variables. For instance, by manipulating sports drink consumption before the player performs, they can eliminate the possibility that reverse causation explains the effect of the independent variable on the dependent variable. Second, through randomly assigning players to consume or not consume sports drink, this holds everything else constant (so long as the groups are evenly distributed according to other factors, such as their age, weight, etc.) and thus removes the possibility that third variable confounds explain the effect of the independent variable on the dependent variable. However, not everything is able to, or ethical to, be manipulated. In addition, as described in Section 8.6.3, experimental designs tend to have lower capacity for external validity than correlational designs—that is, compared to findings from correlational designs, the findings from experimental designs tend to have lower capacity to be consistent with how things play out in the real world.\n\n13.5.2 Quasi-Experimental Designs\nAlthough experimental designs provide the strongest tests of causality, many times they are impossible, unethical, or impractical to conduct. For instance, it would likely not be practical to randomly assign National Football League (NFL) players to either consume or not consume sports drink before their games. Players have their pregame rituals and routines and many would likely not agree to participate in such a study. Moreover, as described in Section 8.6.3, experimental designs tend to be lower in external validity and thus their findings may not always generalize to how things occur in the real world. In addition, experimental designs commonly have sample sizes that are too small to study rare-but-serious outcomes (D’Onofrio et al., 2020). Thus, we often rely on (and need!) quasi-experimental designs such as natural experiments and observational/correlational designs.\nWe cannot directly test or establish causality from a non-experimental research design. Nevertheless, we can leverage various analysis approaches and design features that, in combination with other studies using different research methods, collectively strengthen our ability to make causal inferences. For instance, there are are no experiments in humans showing that smoking causes cancer—randomly assigning people to smoke or not smoke would not be ethical. The causal inference that smoking causes cancer was derived from a combination of experimental studies in rodents and observational studies in humans. To the extent that findings across designs that have different threats to research design validity show similar results in terms of the association between the predictor variable and the outcome variable, the case for causality is strengthened (D’Onofrio et al., 2020).\n\n13.5.2.1 Longitudinal Designs\nResearch designs can be compared in terms of their internal validity—the extent to which we can be confident about causal inferences. A cross-sectional association is depicted in Figure 13.1:\n\n\n\n\n\nFigure 13.1: Cross-Sectional Association. T1 = Timepoint 1. From Petersen (2024) and Petersen (2025).\n\n\nFor instance, we might observe that sports drink consumptions is concurrently associated with better player performance. Among observational/correlational research designs, cross-sectional designs tend to have the weakest internal validity. For the reasons described in Section 13.3, if we observe a cross-sectional association between X (e.g., sports drink consumption) and Y (e.g., player performance), we have little confidence that X causes Y. As a result, longitudinal designs can be valuable for more closely approximating causality if an experimental designs is not possible. Consider a lagged association that might be observed in a longitudinal design, as in Figure 13.2, which is a slightly better approach than relying on cross-sectional associations:\n\n\n\n\n\nFigure 13.2: Lagged Association. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024) and Petersen (2025).\n\n\nFor instance, we might observe that sports drink performance before the game is associated with better player performance during the game. A lagged association has somewhat better internal validity than a cross-sectional association because we have greater evidence of temporal precedence—that the influence of the predictor precedes the outcome because the predictor was assessed before the outcome and it shows a predictive association. However, part of the association between the predictor with later levels of the outcome could be due to prior levels of the outcome that are stable across time. That is, it could be that better player performance leads players to consume more sports drink and that player performance is relatively stable across time. In such a case, it may be observed that sports drink consumption predicts later player performance even though player performance influences sports drink consumption, rather than the other way around Thus, consider an even stronger alternative—a lagged association that controls for prior levels of the outcome, as in Figure 13.3:\n\n\n\n\n\nFigure 13.3: Lagged Association, Controlling for Prior Levels of the Outcome. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024) and Petersen (2025).\n\n\nFor instance, we might observe that sports drink performance before the game is associated with better player performance during the game, while controlling for prior player performance. A lagged association controlling for prior levels of the outcome has better internal validity than a lagged association that does not control for prior levels of the outcome. A lagged association that controls for prior levels further reduces the likelihood that the association owes to the reverse direction of effect, because earlier levels of the outcome are controlled. However, consider an even stronger alternative—lagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect, as depicted in Figure 13.4:\n\n\n\n\n\nFigure 13.4: Lagged Association, Controlling for Prior Levels of the Outcome, Simultaneously Testing Both Directions Of Effect. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024) and Petersen (2025).\n\n\nLagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect provide the strongest internal validity among observational/correlational designs. Such a design can help better clarify which among the variables is the chicken and the egg—which variable is more likely to be the cause and which is more likely to be the effect. If there are bidirectional effects, such a design can also help clarify the magnitude of each direction of effect. For instance, we can simultaneously evaluate the extent to which sports drink predicts later player performance (while controlling for prior performance) and the reverse—player performance predicting later sports drink consumption (while contorlling for prior sports drink consumption).\n\n13.5.2.2 Within-Subject Analyses\nAnother design feature of longitudinal designs that can lead to greater internal validity is the use of within-subject analyses, including interrupted time series designs (D’Onofrio et al., 2020). Between-subject analyses, might examine, for instance, whether players who consume more sports drink perform better on average compared to players who consume less sports drink. However, there are other between-person differences that could explain any observed between-subject associations between sports drink consumption and players performance. Another approach could be to apply within-subject analyses. For instance, you could examine whether, within the same individual, if a player consumes a sports drink, do they perform better compared to games in which they did not consume a sports drink. When we control for prior levels of the outcome in the prediction, we are evaluating whether the predictor is associated with within-person change in the outcome. Predicting within-person change provides stronger evidence consistent with causality because it uses the individual as their own control and controls for many time-invariant confounds (i.e., confounds that do not change across time). However, predicting within-person change does not, by itself, control for time-varying confounds. So, it can also be useful to control for time-varying confounds, such as by use of control variables.\n\n13.5.2.3 Control Variables\nOne of the plausible alternatives to the inference that X causes Y is that there are third variable confounds that influence both X and Y, thus explaining why X and Y are associated, as depicted in Figures 8.3 and 13.10. Thus, another approach that can help increase internal validity is to include plausible confounds as control variables. For instance, if a third variable such as education level might be a confound that influences both sports drink consumption and player performance, you could control for a player’s education level.\nThere are several ways to control for a variable:\n\nrandomization (when possible)\nrestriction\nmatching\ncovariate\nstratification\n\nOne way to control for a variable—and what is typically considered the most rigorous control—when possible, is to randomize (which would make it an experiment). For example, you could randomly assign players to consume sports drink during a game versus water. By randomly assigning players to each group, it is expected that, if the groups are large enough, the differences between the groups on all variables besides the independent and dependent variables will be approximately equal across both groups; thus, such an approach controls for both observed and unobserved confounds, and any difference in the dependent variable is thought to be attributable to the manipulation of the independent variable. However, sometimes a variable is not able to be manipulated for practical or ethical reasons. For instance, if you wanted to control for education level when examining the association between sports drink consumption and performance, you cannot randomly assign players to have lower educational levels (though you could possibly provide an intervention to increase the education levels of a subset of players). Thus, observational studies are also necessary.\nHowever, in observational studies, the groups may differ on the confounding factors; for instance, due to other factors (e.g., personality, beliefs, and attitudes), people may choose (or self-select into) the level of the variable (e.g., sport drink consumption) they receive. Thus, we need to attempt to control for the confounds; however, in an observational study, we can only potentially adjust for the observed confounds, unlike in an experiment in which we can control for both observed and unobserved confounds.\nA second way to control for a variable is restriction—e.g., you can select your sample so that it only includes portions/subgroups of a variable. For example, you could recruit—as participants—only those players who are college graduates.\nMatching means that you match people based on the control variable. If some people consume sports drink whereas others do not, matching would mean finding—for every player included in the study who consumes sports drink—an equivalent player in terms of education who does not consume sports drink.\nAnother way to control for a variable is to include the variable as a covariate in the model. Inclusion of a covariate attempts to control for the variable by examining the association between the predictor variable and the outcome variable while holding the covariate variables constant. For example, if you want to control for a player’s education level in examining the association between sports drink consumption and performance, you could statistically control for player education by including the player’s education level as a covariate in predicting performance. Such a model would examine whether, when accounting for education level (by holding education level constant—i.e., partialing out the association between education level and player performance), there is an association between sports drink consumption and player performance.\nAnother way to control for a variable is to conduct a stratified analysis based on the variable—e.g., you can examine the association between the variables of interest within subgroups of the control variable (D’Onofrio et al., 2020). For instance, taking the example of controlling for a player’s education in examining the association between sports drink consumption and performance, you could examine the association between sports drink consumption and performance separately for various education levels: a) high school or lower, b) some college, c) college degree, d) graduate degree.\nIn each of these examples, you are controlling for player education for purposes of examining the effect of sports drink consumption on performance. Failure to control for important third variables can lead to erroneous conclusions, as evidenced by the association depicted in Figure 13.5, which is an example of Simpson’s paradox described in Section 12.2.2. In the example, if we did not control for gender, we would infer that there is a positive association between dosage and recovery probability. However, when we examine each men and women separately, we learn that the association between dosage and recovery probability is actually negative within each gender group. Thus, in this case, failure to control for gender would lead to false inferences about the association between dosage and recovery probability.\n\n\n\n\n\nFigure 13.5: Example Where Failing to Control for a Variable (In This Case, Gender) Would Lead to False Inferences. In this example, the association between dosage and recovery probability is positive at the population level, but the association is negative among men and women separately. (Figure reprinted from Kievit et al. (2013), Figure 1, p. 2. Kievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513)\n\n\nHowever, it can be problematic to control for variables indiscriminantly (Spector & Brannick, 2010; Wysocki et al., 2022). The use of causal diagrams can inform which variables are important to be included as control variables, and—just as important—which variables not to include as control variables, as described in Section 13.6. Another approach that is sometimes used to control for variables—either by using matching, or as a sample weight, or with stratification—is the use of propensity scores.\n\n13.5.2.4 Genetically Informed Designs\nAnother approach to control for variables is to use genetically informed designs. Genetically informed designs allow controlling for potential genetic effects in order to more closely approximate the contributions of various environmental effects. Genetically informed designs exploit differing degrees of genetic relatedness among participants to capture the extent to which genetic factors may contribute to an outcome. The average percent of DNA shared between people of varying relationships is provided by International Society of Genetic Genealogy (2022) in Table 13.1 (archived at https://perma.cc/MK3D-DST8):\n\n\nTable 13.1: Average Percent of Autosomal DNA Shared by Pairs of Relatives by Relationship Type.\n\n\n\n\n\n\n\nRelationship\nAverage Percent of Autosomal DNA Shared by Pairs of Relatives\n\n\n\nMonozygotic (“identical”) twins\n100%\n\n\nDizygotic (“fraternal”) twins\n50%\n\n\nParent/child\n50%\n\n\nFull siblings\n50%\n\n\nGrandparent/grandchild\n25%\n\n\nAunt-or-uncle/niece-or-nephew\n25%\n\n\nHalf-siblings\n25%\n\n\nFirst cousin\n12.5%\n\n\nGreat-grandparent/great-grandchild\n12.5%\n\n\n\n\n\n\nGenetically informed designs evaluate the extent to which individuals who share greater genetic relatedness are more similar in a characteristic compared to individuals who share less genetic relatedness. For instance, researchers may compare monozygotic twins versus dizygotic twins in some outcome—a so-called “twin study”. It is assumed that the trait/outcome is attributable to genetic factors to the extent that the monozygotic twins (who share 100% of their DNA) are more similar in the trait or outcome compared to the dizygotic twins (who share on average 50% of their DNA). Alternatively, researchers could compare full siblings versus half-siblings, or they could compare full siblings versus first cousins.\nGenetically informed designs are not as relevant for fantasy football analytics, but they are useful to present as one of various design features that researchers can draw upon to strengthen their ability to make causal inferences. One of the limitations of genetically informed designs is that they cannot, without inclusion of measured factors, rule out the influence of factors that are not shared by family members (D’Onofrio et al., 2020).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalDiagrams",
    "href": "causal-inference.html#sec-causalDiagrams",
    "title": "13  Causal Inference",
    "section": "\n13.6 Causal Diagrams",
    "text": "13.6 Causal Diagrams\n\n13.6.1 Overview\nA key tool when describing a research question or hypothesis is to create a conceptual depiction of the hypothesized causal processes. A causal diagram depicts the hypothesized causal processes that link two or more variables. A common form of causal diagrams is the directed acyclic graph (DAG). DAGs provide a helpful tool to communicate about causal questions and help identify how to avoid bias (i.e., overestimation) in associations between variables due to confounding (i.e., common causes) (Digitale et al., 2022). For instance, from a DAG, it is possible to determine what variables it is important to control for (and how)—and just as importantly, what not to control for—in order to get unbiased estimates of the effect of one variable on another variable or of the association between two variables of interest. Thus, when designing and conducting studies, it is good to get in the habit of drawing causal diagrams to make your hypotheses, knowledge, and assumptions explicit (D’Onofrio et al., 2020). Using a causal diagram (DAG), it is also valuable to specify any potential alternative hypotheses, so that the researcher can design studies to evaluate and rule out potential alternative hypotheses.\nTo create DAGs, you can use the R package dagitty (Textor et al., 2016, 2023) or the associated browser-based extension, DAGitty: https://dagitty.net (archived at https://perma.cc/U9BY-VZE2). Examples of various causal diagrams that could explain why X is associated with Y are in Figures 13.6, 13.8 and 13.10.\n\nCodeXCausesY &lt;- dagitty::dagitty(\"dag{\n  X -&gt; Y\n}\")\n\nplot(dagitty::graphLayout(XCausesY))\n\ndagitty::impliedConditionalIndependencies(XCausesY)\n\n\n\n\n\n\nFigure 13.6: Causal Diagram (Directed Acyclic Graph) Depicting X Causing Y.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax) using the ggdag package (Barrett, 2024):\n\nCodeXCausesY_alt &lt;- ggdag::dagify(\n  Y ~ X\n)\n\n#plot(XCausesY_alt) # this creates the same plot as above\nggdag::ggdag(XCausesY_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.7: Causal Diagram (Directed Acyclic Graph) Depicting X Causing Y.\n\n\n\n\n\nCodeYCausesX &lt;- dagitty::dagitty(\"dag{\n  Y -&gt; X\n}\")\n\nplot(dagitty::graphLayout(YCausesX))\n\ndagitty::impliedConditionalIndependencies(YCausesX)\n\n\n\n\n\n\nFigure 13.8: Causal Diagram (Directed Acyclic Graph) Depicting Reverse Causation: Y Causing X.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodeYCausesX_alt &lt;- ggdag::dagify(\n  X ~ Y\n)\n\n#plot(YCausesX_alt) # this creates the same plot as above\nggdag::ggdag(YCausesX_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.9: Causal Diagram (Directed Acyclic Graph) Depicting Reverse Causation: Y Causing X.\n\n\n\n\n\nCodeZCausesXandY &lt;- dagitty::dagitty(\"dag{\n  Z -&gt; Y\n  Z -&gt; X\n  X &lt;-&gt; Y\n}\")\n\nplot(dagitty::graphLayout(ZCausesXandY))\n\n\n\n\n\n\nFigure 13.10: Causal Diagram (Directed Acyclic Graph) Depicting a Third Variable Confound, Z, Causing X and Y, Thus Explaining Why X and Y are associated.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodeZCausesXandY_alt &lt;- ggdag::dagify(\n  X ~ Z,\n  Y ~ Z,\n  X ~~ Y\n)\n\n#plot(ZCausesXandY_alt) # this creates the same plot as above\nggdag::ggdag(ZCausesXandY_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.11: Causal Diagram (Directed Acyclic Graph) Depicting a Third Variable Confound, Z, Causing X and Y, Thus Explaining Why X and Y are associated.\n\n\n\n\nConsider another example in Figure 13.12:\n\nCodemediationDag &lt;- dagitty::dagitty(\"dag{\n  X -&gt; M1\n  X -&gt; M2\n  M1 -&gt; Y\n  M2 -&gt; Y\n  M1 &lt;-&gt; M2\n}\")\n\nplot(dagitty::graphLayout(mediationDag))\n\n\n\n\n\n\nFigure 13.12: Causal Diagram (Directed Acyclic Graph).\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(mediationDag)\n\nX _||_ Y | M1, M2\n\nCodedagitty::adjustmentSets(\n  mediationDag,\n  exposure = \"M1\",\n  outcome = \"Y\",\n  effect = \"total\")\n\n{ M2 }\n\n\nIn this example, X influences Y via M1 and M2 (i.e., multiple mediators), and M1 is also associated with M2. The dagitty::impliedConditionalIndependencies() function identifies variables in the causal diagram that are conditionally independent (i.e., uncorrelated) after controlling for other variables in the model. For this causal diagram, X is conditionally independent with Y because X is independent with Y when controlling for M1 and M2.\nThe dagitty::adjustmentSets() function identifies variables that would be necessary to control for in order to identify an unbiased estimate of the association (whether the total effect, i.e., effect = \"total\"; or the direct effect, i.e., effect = \"direct\") between two variables (exposure and outcome). In this case, to identify the unbiased association between M1 and Y, it is important to control for M2.\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax) using the ggdag::dagify() and ggdag::ggdag() functions:\n\nCodemediationDag_alt &lt;- ggdag::dagify(\n  M1 ~ X,\n  M2 ~ X,\n  Y ~ M1,\n  Y ~ M2,\n  M1 ~~ M2\n)\n\n#plot(mediationDag_alt) # this creates the same plot as above\nggdag::ggdag(mediationDag_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.13: Causal Diagram (Directed Acyclic Graph).\n\n\n\n\n\n13.6.2 Confounding\nConfounding occurs when two variables—that are both caused by another variable(s)—have a spurious or noncausal association (D’Onofrio et al., 2020). That is, two variables share a common cause, and the common cause leads the variables to be associated even though they are not causally related. The common cause—i.e., the variable that influences the two variables—is known as a confound (or confounder). Nevertheless, causal effects and confounding—though competing hypotheses—are not mutually exclusive (D’Onofrio et al., 2020). A predictor can have an association with an outcome that is partly driven by confounding and partly driven by a causal effect. A key goal, then is determining the extent to which the observed association reflects confounding versus a causal effect.\nConfounding is among the most common sources of bias in observational studies in terms of accurately estimating the causal effect of one variable on another, because among all the processes that influence one’s exposure on a risk factor (i.e., predictor variable), some of the processes also likely influence the outcome variable [i.e., and are thus confounds; D’Onofrio et al. (2020)]. Two common sources of confounding that are often neglected are previous behavior and genetics (D’Onofrio et al., 2020). Part of the challenge in observational studies is that there can be known and unknown sources of confounding (D’Onofrio et al., 2020). In addition, the sources of counfounding can be either measured or unmeasured. Because some sources of confounding are likely to be unknown and/or unmeasured, it is unlikely that any single observational study can account for all sources of confounding (D’Onofrio et al., 2020). The causal diagram is thus useful for determining what sources of confounding a given study can and cannot address (D’Onofrio et al., 2020).\nAn example of confounding is depicted in Figure 13.14:\n\nCodeconfounding &lt;- ggdag::confounder_triangle(\n  x = \"Player Endurance\",\n  y = \"Field Goals Made\",\n  z = \"Stadium Altitude\")\n\nconfounding %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.14: Causal Diagram (Directed Acyclic Graph) Example of Confounding.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(confounding)\n\nx _||_ y | z\n\n\nThe output indicates that player endurance (X) and field goals made (Y) are conditionally independent when accounting for stadium altitude (Z). Conditional independence refers to two variables being unassociated when controlling for other variables.\n\nCodedagitty::adjustmentSets(\n  confounding,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n{ z }\n\n\nThe output indicates that, to obtain an unbiased estimate of the causal association between two variables, it is necessary to control for any confounding (Lederer et al., 2019). That is, to obtain an unbiased estimate of the causal association between player endurance (X) and field goals made (Y), it is necessary to control for stadium altitude (Z).\nAnother challenge with confounding is that all constructs are measured with error. Thus, including a measure of a confound as a covariate in a model will only partially account for the confound. The remaining effect of the confound on the outcome (after controlling for the measured confound via a covariate) is called residual confounding (D’Onofrio et al., 2020).\n\n13.6.2.1 Systematic Measurement Error\nAnother common source of bias, in terms of accurately estimating the causal effect of one variable on another, that functions similarly to confounding is systematic measurement error (as opposed to random measurement error). Systematic measurement error is measurement error that influences scores consistently for a person or across the sample. Systematic measurement error is described in Petersen (2025). Let’s consider that a researcher wants to study the effects of player stress on their happiness. The researcher selects a questionnaire that assesses player stress and a questionnaire that assesses happiness and assesses players on both. To some degree, the measurements will both be influenced by nonconstruct factors, because both measurements come from the same measurement method (questionnaire; i.e., common method bias) and from the same rater (i.e., common informant bias). People have response styles that lead them to have a tendency to rate themselves—regardless of the construct of interest—as better or worse than they actually are. For instance, Person A may rate themselves as better off in terms of stress and happiness (i.e., lower stress, higher happiness) than they actually are, whereas Person B may rate themselves as worse off in terms of stress and happiness (i.e., higher stress, lower happiness) than they actually are. Because these response styles influence both measurements and are not the construct of interest (i.e., stress or happiness), they function like confounding in that the influence both variables and can explain, to some degree, why the variables are correlated for reasons other than a causal effect of player stress on their happiness. Unlike confounding, however, systematic measurement error does not just influence observational studies—it influences experimental designs as well.\n\n13.6.3 Mediation\nMediation can be divided into two types: full and partial. In full mediation, the mediator(s) fully account for the effect of the predictor variable on the outcome variable. In partial mediation, the mediator(s) partially but do not fully account for the effect of the predictor variable on the outcome variable.\n\n13.6.3.1 Full Mediation\nAn example of full mediation is depicted in Figure 13.15:\n\nCodefull_mediation &lt;- ggdag::mediation_triangle(\n  x = \"Coaching Quality\",\n  y = \"Fantasy Points\",\n  m = \"Player Preparation\")\n\nfull_mediation %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.15: Causal Diagram (Directed Acyclic Graph) Example of Full Mediation.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(full_mediation)\n\nx _||_ y | m\n\n\nIn full mediation, X and Y are conditionally independent when accounting for the mediator (Z). In this case, coaching quality (X) and fantasy points (Y) are conditionally independent when accounting for player preparation (M). In other words, in this example, player preparation is the mechanism that fully (i.e., 100%) accounts for the effect of coaching quality on players’ fantasy points.\n\nCodedagitty::adjustmentSets(\n  full_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"direct\")\n\n{ m }\n\n\nThe output indicates that, to obtain an unbiased estimate of the direct causal association between coaching quality (X) and fantasy points (Y) (i.e., the effect that is not mediated through intermediate processes), it is necessary to control for player preparation (M).\n\nCodedagitty::adjustmentSets(\n  full_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nHowever, to obtain an unbiased estimate of the total causal association between coaching quality (X) and fantasy points (Y) (i.e., including the portion of the effect that is mediated through intermediate processes), it is important not to control for player preparation (M). When the goal is to understand the (total) causal effect of coaching quality (X) and fantasy points (Y), controlling for the mediator (player preparation; M) would be inappropriate because doing so would remove the causal effect, thus artificially reducing the estimate of the association between coaching quality (X) and fantasy points (Y) (Lederer et al., 2019).\n\n13.6.3.2 Partial Mediation\nAn example of partial mediation is depicted in Figure 13.16:\n\nCodepartial_mediation &lt;- ggdag::mediation_triangle(\n  x = \"Coaching Quality\",\n  y = \"Fantasy Points\",\n  m = \"Player Preparation\",\n  x_y_associated = TRUE)\n\npartial_mediation %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.16: Causal Diagram (Directed Acyclic Graph) Example of Partial Mediation.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(partial_mediation)\n\n\nIn partial mediation, X and Y are not conditionally independent when accounting for the mediator (Z). In this case, coaching quality (X) and fantasy points (Y) are still associated when accounting for player preparation (M). In other words, in this example, player preparation is a mechanism that partially but does not fully account for the effect of coaching quality on players’ fantasy points. That is, there are likely other mechanisms, in addition to player preparation, that collectively account for the effect of coaching quality on fantasy points. For instance, coaching quality could also influence player fantasy points through better play-calling.\n\nCodedagitty::adjustmentSets(\n  partial_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"direct\")\n\n{ m }\n\n\nAs with full mediation, the output indicates that, to obtain an unbiased estimate of the direct causal association between coaching quality (X) and fantasy points (Y) (i.e., the effect that is not mediated through intermediate processes), it is necessary to control for player preparation (M).\n\nCodedagitty::adjustmentSets(\n  partial_mediation,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nHowever, as with full mediation, to obtain an unbiased estimate of the total causal association between coaching quality (X) and fantasy points (Y) (i.e., including the portion of the effect that is mediated through intermediate processes), it is important not to control for player preparation (M). When the goal is to understand the (total) causal effect of coaching quality (X) and fantasy points (Y), controlling for a mediator (player preparation; M) would be inappropriate because doing so would remove the causal effect, thus artificially reducing the estimate of the association between coaching quality (X) and fantasy points (Y) (Lederer et al., 2019).\n\n13.6.4 Ancestors and Descendants\nIn a causal model, an ancestor is a variable that influences another variable, either directly or indirectly via another variable (Rohrer, 2018). A descendant is a variable that is influenced by another variable (Rohrer, 2018). In general, one should not control for descendants of the outcome variable, because doing so could eliminate the apparent effect of a legitimate cause on the outcome variable (Digitale et al., 2022). Moreover, as described above, when trying to understand the total causal effect of a predictor variable on an outcome variable, one should not control for descendants of the predictor variable that are also ancestors of the outcome variable (i.e., mediators of the effect of the predictor variable on the outcome variable) (Digitale et al., 2022).\nConsider the example in Figure 13.17:\n\nCodedescendentDag &lt;- dagitty::dagitty(\"dag{\n  X -&gt; M\n  M -&gt; Y\n  X -&gt; Y\n  Y -&gt; Z\n}\")\n\n#plot(dagitty::graphLayout(descendentDag))\nggdag::ggdag(descendentDag) + theme_dag_blank()\n\n\n\n\n\n\nFigure 13.17: Causal Diagram (Directed Acyclic Graph) Example of Ancestor (Mediation) and Descendant of Y.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(descendentDag)\n\nM _||_ Z | Y\nX _||_ Z | Y\n\n\nIn this example, X and M are conditionally independent with Z when accounting for the mediator (Y).\n\nCodedagitty::adjustmentSets(\n  descendentDag,\n  exposure = \"X\",\n  outcome = \"Y\",\n  effect = \"direct\")\n\n{ M }\n\nCodedagitty::adjustmentSets(\n  descendentDag,\n  exposure = \"X\",\n  outcome = \"Y\",\n  effect = \"total\")\n\n {}\n\n\nAs indicated above, one should not control for the descendant of the outcome variable; thus, one should not control for Z when examining the association between X or M and Y.\n\n13.6.5 Collider Bias\nCollision occurs when two variables influence a third variable, the collider (D’Onofrio et al., 2020). That is, a collider is a variable that is caused by two other variables (i.e., a common effect). An example collision is depicted in Figures 13.18 and 13.19:\n\nCodecolliderBias1 &lt;- ggdag::collider_triangle(\n  x = \"Diet\",\n  y = \"Coaching Strategy\",\n  m = \"Injury Status\")\n\ncolliderBias1 %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.18: Causal Diagram (Directed Acyclic Graph) Example of a Collision with a Collider (Injury Status).\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(colliderBias1)\n\nx _||_ y\n\n\nIn this example collision, diet (X) and coaching strategy (Y) are independent.\n\nCodedagitty::adjustmentSets(\n  colliderBias1,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nAs the output indicates, we should not control for the collider when examining the association between the two causes of the collider. That is, we should not control for injury status (M) when examining the association between diet (X) and coaching strategy. Controlling for the collider leads to confounding and can artificially induce an association between the two causes of the collider despite no causal association between them (Lederer et al., 2019). Obtaining a distorted or artificial association between two variables due to inappropriately controlling for a collider is known as collider bias.\nConsider another example:\n\nCodecolliderBias2 &lt;- ggdag::collider_triangle(\n  x = \"Coaching Quality\",\n  y = \"Player Preparation\",\n  m = \"Fantasy Points\",\n  x_y_associated = TRUE)\n\ncolliderBias2 %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.19: Causal Diagram (Directed Acyclic Graph) Example of Collider Bias.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(colliderBias2)\n\n\nIn this example of collider bias, there are no conditional independencies.\n\nCodedagitty::adjustmentSets(\n  colliderBias2,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nAgain, it would be important not to control for the collider, fantasy points (M), when examining the association between coaching quality (X) and player preparation (Y). In this case, controlling for the collider would remove some of the causal effect of coaching quality on player preparation and could lead to an artificially smaller estimate of the causal effect between coaching quality and player preparation.\n\n13.6.5.1 M-Bias\nCollider bias may also occur when neither variable of interest is a direct cause of the collider (Lederer et al., 2019). M-bias is a form of collider bias that occurs when two variables that are not causally related, A and B, both influence a collider, M, and each (A and B) also influences a separate variable—e.g., A influences X and B influences Y. M-bias is so-named from the M-shape of the DAG. An example of M-bias is depicted in Figure 13.20:\n\nCodemBias &lt;- ggdag::m_bias(\n  x = \"Number of Media Articles About the Team\",\n  y = \"Fantasy Points\",\n  a = \"Team Record\",\n  b = \"Coaching Quality\",\n  m = \"Fan Attendance at Game\")\n\nmBias %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.20: Causal Diagram (Directed Acyclic Graph) Example of M-Bias.\n\n\n\n\nIn this example, fan attendance is the collider that is influenced separately by the team record and the coaching quality. This is a fictitious example for purposes of demonstration; in reality, coaching quality influences the team’s record.\n\nCodedagitty::impliedConditionalIndependencies(mBias)\n\na _||_ b\na _||_ y\nb _||_ x\nm _||_ x | a\nm _||_ y | b\nx _||_ y\n\n\nAs the output indicates, there are several conditional independencies.\n\nCodedagitty::adjustmentSets(\n  mBias,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nIt is important not to control for the collider (fan attendance). If you control for the collider, you can induce an artificial association between team record and coaching quality. Moreover, because doing so induces an artificial association between team record and coaching quality, it can also induce an artificial association between the effects of team record and coaching quality: number of media articles about the team and fantasy points, respectively. That is, controlling for the collider can lead to an artificial association between X and Y that does not reflect a causal process.\n\n13.6.5.2 Butterfly Bias\nButterfly bias occurs when both confounding and M-bias are present. Butterfly bias (aka bow-tie bias) is so-named from the butterfly shape of the DAG. In butterfly bias, the following criteria are met:\n\nTwo variables (A and B) influence a collider (M).\nThe collider influences two variables, X and Y.\n\nA also influences X.\n\nB also influences Y.\n\nA and B are not causally related.\n\nX and Y are not causally related.\n\nOr, more succinctly:\n\n\nA influences M and X.\n\nB influences M and Y.\n\nM influences X and Y.\n\nIn butterfly bias, the collider (M) is also a confound. That is, a variable is both influenced by two variables and influences two variables. An example of butterfly bias is depicted in Figure 13.21:\n\nCodebutterflyBias &lt;- ggdag::butterfly_bias(\n  x = \"Off-field Behavior\",\n  y = \"Fantasy Points\",\n  a = \"Diet\",\n  b = \"Coaching Quality\",\n  m = \"Mental Health\")\n\nbutterflyBias %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 13.21: Causal Diagram (Directed Acyclic Graph) Example of Butterfly Bias.\n\n\n\n\nIn this case, players’ mental health is a collider of their diet and the quality of the coaching they receive. In addition, players’ mental health is a confound of their off-field behavior and fantasy points.\n\nCodedagitty::impliedConditionalIndependencies(butterflyBias)\n\na _||_ b\na _||_ y | b, m\nb _||_ x | a, m\nx _||_ y | b, m\nx _||_ y | a, m\n\n\nAs the output indicates, there are several conditional independencies.\n\nCodedagitty::adjustmentSets(\n  butterflyBias,\n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n{ b, m }\n{ a, m }\n\n\nWhen dealing with a collider that is also a confound, controlling for either set, B and M or A and M, will provide an unbiased estimate of the association between X and Y. In this case, controlling for either a) coaching quality and mental health or b) diet and mental health—but not both sets—will yield an unbiased estimate of the association between off-field behavior and fantasy points.\n\n13.6.6 Selection Bias\nSelection bias occurs when the selection of participants or their inclusion in analyses depends on the variables being studied (or on their colliders). For instance, if you are conducting a study on the extent to which sports drink consumption influences fantasy points, there would be selection bias if players are less likely to participate in the study if they score fewer fantasy points.\nNow, consider a study in which you conduct a randomized controlled trial (RCT; i.e., an experiment) to evaluate the effect of a new medication on player performance. You randomly assign some players to take the medication and other players to take a placebo. Assume the new medication has side effects and leads many of the players who take it to drop out of the study. This is an example of attrition bias (i.e., systematic attrition). If you were to exclude these individuals from your analysis, it may make it appear that the medication led to better performance, because the players who experienced the side effect (and worse performance) dropped out of the study. Hence, conducting an analysis that excludes these players from the analysis would involve selection bias.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceConclusion",
    "href": "causal-inference.html#sec-causalInferenceConclusion",
    "title": "13  Causal Inference",
    "section": "\n13.7 Conclusion",
    "text": "13.7 Conclusion\nThere are three criteria for establishing causality: 1) the cause precedes the effect. 2) The cause is related to the effect. 3) There are no other alternative explanations for the effect apart from the cause. In general, it is important to be aware of the counterfactual and to consider what would have happened if the supposed cause had not occurred. Various experimental and quasi-experimental designs and approaches can be leveraged to more closely approximate causal inferences. Longitudinal designs, within-subject analyses, inclusion of control variables, and genetically informed designs are all quasi-experimental designs that afford the researcher greater control over some possible third variable confounds. Causal diagrams can be a useful tool for identifying the proper variables to control for (and those not to control for). In general, the only variables you should control for are confound(s). When confounding exists, it is important to control for the confound(s). It is important not to control for mediators when interested in the total effect of the predictor variable on the outcome variable. In addition, it is important not to control for descendants of the outcome variable. When there is a collision, it is important not to control for the collider (unless the collider is also a confound).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceSessionInfo",
    "href": "causal-inference.html#sec-causalInferenceSessionInfo",
    "title": "13  Causal Inference",
    "section": "\n13.8 Session Info",
    "text": "13.8 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggdag_0.2.13  dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] viridis_0.6.5      generics_0.1.4     tidyr_1.3.1        stringi_1.8.7     \n [5] digest_0.6.37      magrittr_2.0.3     evaluate_1.0.4     grid_4.5.1        \n [9] RColorBrewer_1.1-3 fastmap_1.2.0      jsonlite_2.0.0     ggrepel_0.9.6     \n[13] gridExtra_2.3      purrr_1.1.0        viridisLite_0.4.2  scales_1.4.0      \n[17] tweenr_2.0.3       cli_3.6.5          rlang_1.1.6        graphlayouts_1.2.2\n[21] polyclip_1.10-7    tidygraph_1.3.1    withr_3.0.2        cachem_1.1.0      \n[25] yaml_2.3.10        tools_4.5.1        memoise_2.0.1      dplyr_1.1.4       \n[29] ggplot2_3.5.2      boot_1.3-31        curl_7.0.0         vctrs_0.6.5       \n[33] R6_2.6.1           lifecycle_1.0.4    stringr_1.5.1      V8_6.0.6          \n[37] htmlwidgets_1.6.4  MASS_7.3-65        ggraph_2.2.2       pkgconfig_2.0.3   \n[41] pillar_1.11.0      gtable_0.3.6       glue_1.8.0         Rcpp_1.1.0        \n[45] ggforce_0.5.0      xfun_0.53          tibble_3.3.0       tidyselect_1.2.1  \n[49] knitr_1.50         farver_2.1.2       htmltools_0.5.8.1  igraph_2.1.4      \n[53] rmarkdown_2.29     labeling_0.4.3     compiler_4.5.1    \n\n\n\n\n\n\nBarrett, M. (2024). ggdag: Analyze and create elegant directed acyclic graphs. https://doi.org/10.32614/CRAN.package.ggdag\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nD’Onofrio, B. M., Sjölander, A., Lahey, B. B., Lichtenstein, P., & Öberg, A. S. (2020). Accounting for confounding in observational studies. Annual Review of Clinical Psychology, 16(1), 25–48. https://doi.org/10.1146/annurev-clinpsy-032816-045030\n\n\nDigitale, J. C., Martin, J. N., & Glymour, M. M. (2022). Tutorial on directed acyclic graphs. Journal of Clinical Epidemiology, 142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001\n\n\nInternational Society of Genetic Genealogy. (2022). Autosomal DNA statistics. https://isogg.org/wiki/Autosomal_DNA_statistics\n\n\nKievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513\n\n\nLederer, D. J., Bell, S. C., Branson, R. D., Chalmers, J. D., Marshall, R., Maslove, D. M., Ost, D. E., Punjabi, N. M., Schatz, M., Smyth, A. R., Stewart, P. W., Suissa, S., Adjei, A. A., Akdis, C. A., Azoulay, É., Bakker, J., Ballas, Z. K., Bardin, P. G., Barreiro, E., … Vincent, J.-L. (2019). Control of confounding and reporting of results in causal inference studies. Guidance for authors from editors of respiratory, sleep, and critical care journals. Annals of the American Thoracic Society, 16(1), 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science, 1(1), 27–42. https://doi.org/10.1177/2515245917745629\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin.\n\n\nSpector, P. E., & Brannick, M. T. (2010). Methodological urban legends: The misuse of statistical control variables. Organizational Research Methods, 14(2), 287–305. https://doi.org/10.1177/1094428110369842\n\n\nTextor, J., van der Zander, B., & Ankan, A. (2023). dagitty: Graphical analysis of structural causal models. https://doi.org/10.32614/CRAN.package.dagitty\n\n\nTextor, J., van der Zander, B., Gilthorpe, M. S., Liśkiewicz, M., & Ellison, G. T. (2016). Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical control requires causal justification. Advances in Methods and Practices in Psychological Science, 5(2), 25152459221095823. https://doi.org/10.1177/25152459221095823",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html",
    "href": "cognitive-bias.html",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "",
    "text": "14.1 Getting Started\nThis chapter provides an overview of many heuristics, cognitive biases, and fallacies that people engage in when making judgments and predictions in fantasy football.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasGettingStarted",
    "href": "cognitive-bias.html#sec-cognitiveBiasGettingStarted",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "",
    "text": "14.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasOverview",
    "href": "cognitive-bias.html#sec-cognitiveBiasOverview",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.2 Overview",
    "text": "14.2 Overview\nWhen considering judgment and prediction, it is important to consider psychological concepts, including heuristics and cognitive biases. In the modern world of big data, research and society need people who know how to make sense of the information around us. Given humans’ cognitive biases, it is valuable to leverage more objective approaches than relying on our “gut” and intutition. Statistical approaches can be a more objective way to identify systematic patterns.\nStatistical analysis—and science more generally—is a process to the pursuit of knowledge. An epistemology is an approach to knowledge. Science is perhaps the best approach (epistemology) that society has to approximate truth. Unlike other approaches to knowledge, science relies on empirical evidence and does not give undue weight to anecdotal evidence, intuition, tradition, or authority.\nPer Petersen (2025), here are the characteristics of science that distinguish it from pseudoscience:\n\n\nRisky hypotheses are posed that are falsifiable. The hypotheses can be shown to be wrong.\nFindings can be replicated independently by different research groups and different methods. Evidence converges across studies and methods.\nPotential alternative explanations for findings are specified and examined empirically (with data).\nSteps are taken to guard against the undue influence of personal beliefs and biases.\nThe strength of claims reflects the strength of evidence. Findings and the ability to make judgments or predictions are not overstated. For instance, it is important to present the degree of uncertainty from assessments with error bars or confidence intervals.\nScientifically supported measurement strategies are used based on their psychometrics, including reliability and validity.\n\n\nBy contrast, according to Lilienfeld et al. (2015), some of the frequent features of pseudoscience include:\n\n\nAn overuse of ad hoc hypotheses designed to immunize claims from falsification\nAbsence of self-correction\nEvasion of peer review\nEmphasis on confirmation rather than refutation\nReversed burden of proof\nAbsence of connectivity\nOverreliance on testimonial and anecdotal evidence\nUse of obscurantist language\nAbsence of boundary conditions\nThe mantra of holism\n\n\nNevertheless, statistical analysis is not purely objective and is not a panacea. Science is a human enterprise—it is performed by humans each of whom has their own biases. For instance, cognitive biases can influence how people interpret statistics. As a result, the findings from any given study may be incorrect. Thus, it would be imprudent to make decisions based solely on the results of one study. That is why we wait for findings to be independently replicated by different groups of researchers using different methods.\nIf a research team publishes flashy new and exciting findings, other researchers have an incentive to disprove the prior findings. Thus, we have more confidence if findings stand up to scrutiny from independent groups of researchers. We also draw upon meta-analyses—studies of many studies, to summarize the results of many studies and not just the findings from any single study that may not replicate. In this way, we can identify which findings are robust and most likely true versus the findings that fail to replicate. Thus, despite its flaws like any other human enterprise, science is a self-correcting process in which the long arc bends toward truth.\nIn our everyday lives, humans are presented with overwhelming amounts of information. Because human minds cannot parse every piece of information equally, we tend to take mental shortcuts, called heuristics. These mental shortcuts can be helpful. They reduce our mental load and can help us make quick judgments to stay alive or to make complex decisions in the face of uncertainty. For instance, snap judgments can help us rapidly differentiate between a friend and a foe, for survival. However, these mental shortcuts can also lead us astray and to make systematic errors in our judgments and predictions. For instance, heuristics can lead us to identify patterns out of randomness. Cognitive biases are systematic errors in thinking. Cognitive biases can result from heuristics. Fallacies are forms of flawed reasoning (invoked as an argument). Fallacies are a characteristic of an argument, whereas cognitive biases are characteristics of the reasoning agent. Thus, fallacies can result from heuristics and cognitive biases.\nOne might wonder why fallacies and cognitive biases exist, given that errors in thinking and reasoning would seem to be disadvantageous. However, fallacies and cognitive biases reflect the tradeoffs between speed and accuracy in thinking and, in some cases, may also serve to help people feel better about themselves.\nBelow, we provide examples of heuristics, cognitive biases, and fallacies. These are not exhaustive lists of all the heuristics, cognitive biases, and fallacies. They are some examples of key ones that are relevant fantasy football.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-heuristics",
    "href": "cognitive-bias.html#sec-heuristics",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.3 Examples of Heuristics",
    "text": "14.3 Examples of Heuristics\nAs described above, heuristics are mental shortcuts that people use to handle the overwhelming amount of information to process. Important heuristics used in judgment and prediction in the face of uncertainty include (Critcher & Rosenzweig, 2014; Kahneman, 2011; Tversky & Kahneman, 1974):\n\navailability heuristic\nrepresentativeness heuristic\nanchoring and adjustment heuristic\naffect heuristic\nperformance heuristic\n“What You See Is All There Is” (WYSIATI) heuristic\n\nUse of heuristics has been observed among fantasy sports managers (B. Smith et al., 2006).\n\n14.3.1 Availability Heuristic\nThe availability heuristic refers to the tendency for a person to make judgments or predictions about the frequency or probability of something based on how readily instances can be brought to mind. For instance, when making fantasy predictions about a player, more recent big performance games (or games that receive more news coverage) may more easily come to mind compared to lower-scoring games and games that occurred longer ago. Thus, a manager may be more inclined to pick players to start who had more recent, stronger performances rather than players who have higher long-term averages.\n\n14.3.2 Representativeness Heuristic\nThe representativess heuristic refers to the tendency for a person’s judgments or predictions about individuals to be made based on how similar the individual is to (i.e., how closely the individual resembles) the person’s existing mental prototypes. In this way, the representativeness heuristic involves stereotyping, which is not a statistically optimal approach to predictions (Kahneman, 2011). Stereotypes are statements about groups or categories that are at least tentatively accepted as facts about every member of that group or category (Kahneman, 2011). That is, a (perhaps common) characteristic of a group is attributed to each member of that group. In addition, the representativness heuristic does not adequately consider the quality of the evidence (Kahneman, 2011). It is thus important to question the predictive accuracy of the evidence for the particular prediction question.\nConsider the following example of the representativeness heuristic. When coming out of college, Tight End Kyle Pitts drew comparisons to the “LeBron James” of Tight Ends [Nivison (2021); archived at https://perma.cc/JQB5-XPVL]. The idea that his athletic profile leads him to be similar to the prototype of LeBron James may have led him to be too highly drafted by fantasy managers in his first seasons.\nFigure 14.1 is a video from Get Up ESPN (2021) (archived at https://perma.cc/JW8E-KV2C) of Kyle Pitts drawing comparisons to the LeBron James of Tight Ends:\n\n\n\n\n\nFigure 14.1\n\n\nThe representativeness heuristic has been observed in gambling markets for predicting team wins in the National Football League (NFL) (Woodland & Woodland, 2015) and in decision making in fantasy soccer (Kotrba, 2020).\n\n14.3.3 Anchoring and Adjustment Heuristic\nThe anchoring and adjustment heuristic refers to the tendency for a person’s judgments or predictions to be made with a reference point—an anchor—as a starting point from which they adjust their estimates upward or downward. The anchor provides a suggestion to the person—even without their paying attention, leading to a priming effect which evokes information that is compatible with the suggestion (i.e., a suggestive influence). We are more suggestible than we would like to think we are. The anchor is often inaccurate and given too much weight in the person’s calculation, and too little adjustment is made away from the anchor. As an example, insufficient adjustment from an anchor may lead someone to drive too fast after exiting a highway (Kahneman, 2011). The anchoring effect occurs even when the anchor (i.e., suggestion) is a random number and is completely uninformative of the correct answer (Kahneman, 2011)!\nApplied to fantasy football, a manager is trying to predict how many fantasy points a top Running Back may score. The player scored 300 fantasy points last season, but the team added a stronger backup Running Back and changed the Offensive Coordinator to be a more pass-heavy offense. The manager may use 300 fantasy points as an anchor (based on the player’s performance last season), and may adjust downward 15 points to account for the offseason changes. However, it is possible that this downward adjustment is insufficient to account not only for the offseasons changes but also for potential regression effects. Regression effects are discussed further in Section 14.5.2. As another example, the first number or players named in a negotiation can serve as an anchor.\nTo counter the effect of the anchoring heuristic, Kahneman (2011) suggests deliberately thinking the opposite of the anchor. And, if a person you are negotiating with makes an outrageous initial offer, instead of making an equally outrageous counter-offer, Kahneman (2011) suggests storming out or threatening to leave, and indicating that you are unwilling to negotiate with that number on the table. This technique could be helpful during trade negotiations for players in fantasy football. Other techniques include, instead of focusing on the anchor, focusing on the minimal offer that the opponent would accept, or on the costs to the opponent of failing to reach an agreement (Kahneman, 2011).\n\n14.3.4 Affect Heuristic\nThe affect heuristic refers to the tendency for a person to make judgments or predictions based on their emotions and feelings—such as whether they like or dislike something—with little deliberation or reasoning. That is, the affect heuristic involves letting one’s likes and dislikes determine one’s beliefs about the world (Kahneman, 2011). For instance, if a manager’s favorite team is the Dallas Cowboys, they may hold a distaste for players on teams that are rivals of the Cowboys, such as the Philadelphia Eagles. The affect heuristic may lead them to believe that Eagles players are overrated and to avoid drafting Eagles players even though some Eagles players may perform well.\n\n14.3.5 Performance Heuristic\nThe performance heuristic refers to the tendency for people to predict their improvement in a task based on their previous performance (Critcher & Rosenzweig, 2014). Although past performance tends to be correlated with future performance, people tend to show regression to the mean, as described in Section 14.5.2. Thus, people who show strong performance in their first few games may in many cases show worse performance in subsequent games. Moreover, initial performance does not necessarily indicate the extent of one’s future improvement. For instance, if a manager has a successful first season, the performance heuristic may lead them to overestimate the extent to which they will improve in their skill or performance in the future.\n\n14.3.6 What You See Is All There Is (WYSIATI) Heuristic\nThe “What You See Is All There Is” (WYSIATI) heuristic refers to the tendency for a person to make a decision or judgment based solely on the information that is immediately available to them (i.e., “right in front of them”), without considering the possibility that there may be additional important information that they do not have (Enke, 2020). The WYSIATI heuristic involves jumping to conclusions on the basis of limited information (Kahneman, 2011). For instance, a manager might start a player based on the fact that the player scored lots of points in their most recent game. However, the manager may not consider other important factors that they did not know about, such as the fact that the player filled in for an injured player, and will get fewer touches in the upcoming game now that the original starter is back to full health.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiases",
    "href": "cognitive-bias.html#sec-cognitiveBiases",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.4 Examples of Cognitive Biases",
    "text": "14.4 Examples of Cognitive Biases\nAs described above, cognitive biases are systematic errors in thinking. They lead us to be less accurate in our judgments and predictions. Inaccuracy may not always be bad; several cognitive biases may serve to help people feel better about themselves (R. M. Miller, 2013), including confirmation bias, overconfidence bias, optimism bias, and self-serving bias; or to prevent feelings of regret (e.g., omission bias)—or to prevent having to admit failure (e.g., commitment bias). In fantasy football, however, cognitive biases and inaccurate judgments can lead to worse performance. Cognitive biases are often due to the use of heuristics.\nMiller (2013) describes cognitive biases in fantasy sports. Examples of cognitive biases relevant to fantasy football that result from one or more heuristics include:\n\noverconfidence bias\noptimism bias\nconfirmation bias\nin-group bias\nhindsight bias\noutcome bias\nself-serving bias\nomission bias\nloss aversion bias\nrisk aversion bias\nprimacy effect bias\nrecency effect bias\nframing effect bias\nendowment effect bias\nbandwagon effect bias\nDunning–Kruger effect bias\n\nIn general, when faced with a situation in which bias is likely, Kahneman (2011) advises slowing down, being thoughtful and deliberate, questioning your intuitions, and imposing orderly procedures, such as checklists, reference-class forecasting, and a “premortem”. A premortem is when you consider, in advance of the decision, what likely went wrong if the decision were to work out terribly. Reference-class forecasting involves examining a reference class of similar projects to identify the most likely range of outcomes for the project.\n\n14.4.1 Overconfidence Bias\nIn general, people tend to be overconfident in their judgments and predictions. Overconfidence bias is the tendency for a person to have greater confidence in their abilities (including judgments and predictions) than is objectively warranted. There are three general ways that overconfidence has been identified (Moore & Healy, 2008):\n\n\noverestimation of one’s actual performance\n\noverplacement of one’s performance relative to others\n\noverprecision in one’s beliefs/judgments/predictions\n\nOverestimation involves believing that one will perform better than one actually performs. Overestimation can be identified with a calibration plot of the predicted performance versus actual performance, where the person’s predicted performance is systematically higher (in at least some cases) than their actual performance. Overestimation corresponds to the “overprediction” form of miscalibration.\nOverplacement involves believing that one is better than others or will perform better than others, even when they do not. For instance, it is a common finding that more than half of people believe they are “above average” (i.e., above the median), even though that is statistically impossible. This calls to mind the fictitious Lake Wobegon in the radio show A Praririe Home Companion, “where all the women are strong, all the men are good-looking, and all the children are above average.” As an example, 90% of drivers believe they are better than average (Kahneman, 2011).\nOverprecision involves expressing excessive certainty regarding the accuracy of one’s beliefs/judgments/predictions. For instance, if when a given meteorologist says it will rain 80% of the time, it actually rains 30% of the time, the meteorologist’s predictions are overprecise. Likewise, if the weather forecast says it will rain 10% of the time and it actually rains 30% of the time, the predictions are also overprecise because the forecaster is expressing stronger confidence than is warranted that it will not rain. Overprecision can be identified with a calibration plot of the predicted probabilities versus the actual probabilities. Overprecision corresponds to the “overextremity” form of miscalibration.\nWith respect to fantasy football, projections of players’ fantasy points tend to be overprecise, as described in Section 17.12. However, predictions about how well oneself will do in a fantasy football league may involve overestimation and overplacement.\nOverestimation and overprecision are studied in various ways. Typically, people are asked about (a) whether an event will occur or (b) the likelihood that the event will occur, across many events. For the former (approach “a”), people may be asked to make a dichotomous judgment or prediction, by responding to the question: e.g., “Will it rain tomorrow? [YES/NO]”. They will then rate their confidence (as a percentage) in their answer (0–100%). They would make each of these two ratings for each of many events. Then, we can evaluate, for a given respondent, the degree to which the probabilistic estimate of an event reflects the true underlying probability of the event. For instance, for a given respondent (and for respondents in general), for the events when the respondent says they are 80% confident an event (e.g., rain) will occur, does the event actually occur around 80% of the time? For the latter (approach “b”), people may indicate the likelihood that the event will occur, by responding to the question: “How likely is it that it will rain tomorrow? (0–100%)”. Then, we can evaluate, for instance, for the events when the respondent says that an event (e.g., rain) is 80% likely to occur, does the event actually occur around 80% of the time?\nThe confidence that people tend to have in their predictions depends less and the quantity and quality of the evidence for their predictions and more on the quality of the narrative they can tell about the (little) information they have immediate access to (Kahneman, 2011). For instance, people tend not to consider other relevant, important factors that they do not have immediate access to, consistent with the WYSIATI heuristic. Instead, people tend to focus on the coherence and plausibility of the story, which differs from its probability (Kahneman, 2011). That is, one’s confidence is not necessarily related to the likelihood that the judgment is correct—it often reflects the “illusion of validity” (Kahneman, 2011). The illusion of validity is the unwarranted confidence that arises from a perceived good fit between the predicted outcome and the input information (Tversky & Kahneman, 1974). As one gains more knowledge in a domain, the person also tends to become unrealistically overconfident in their ability to make accurate judgments and predictions (Kahneman, 2011). In general, people tend to focus on what they know and to neglect what they do not know leading to overconfidence in one’s beliefs (Kahneman, 2011).\nA fantasy manager may be even more likely to exhibit overconfidence if they previously performed well or won their league, for which luck and random chance plays an important role. Indeed, it is estimated that nearly half (~45%) of the variability in fantasy football performance is estimated to be luck [and around 55% due to skill; Getty et al. (2018)]. A manager who won their league in the prior season may believe they will perform better than they actually will (overestimation), will perform better than average (overplacement), and may hold excessive confidence regarding the accuracy of their predictions about which players will perform well or poorly (overprecision). These various types of overconfidence may lead them to draft high-risk players based on gut feeling, neglecting statistical analysis and expert consensus.\nPeople tend to focus on the role of skill and to neglect the role of luck when explaining the past and predicting the future, giving people an illusion of control (Kahneman, 2011). Players’ performance in fantasy football, and human behavior more generally, is complex and multiply determined (i.e., is influenced by many factors). Despite the bluster of so-called experts who pretend to know more than they can know, no one can consistently and accurately predict how all players will perform. Remain humble in your predictions; do not be more confident than is warranted. If you approach the task of prediction with humility, you may be more able to be flexible and more willing to consider other players who you can draft for good value.\n\n14.4.2 Optimism Bias\nOptimism bias is the tendency for people to overestimate one’s likelihood of experiencing positive events and underestimate one’s likelihood of experiencing negative events. For instance a manager might overestimate the likelihood that their team will win the championship and may underestimate the likelihood that they may lose a player to injury. In terms of its key consequences for decisions, optimism bias may be considered the most important cognitive bias (Kahneman, 2011). In general, people show an optimism bias. However, some people (“optimists”) are even more optimistic than others (Kahneman, 2011). The optimism bias—in particular a delusional optimism—can lead to overconfidence (Kahneman, 2011). Although optimism is associated with many advantages in terms of well-being, it can also lead to excessive risk-taking and to costly persistence (Kahneman, 2011).\n\n14.4.3 Confirmation Bias\nConfirmation bias is the tendency for people to search for, interpret, and remember information that confirms one’s beliefs, as opposed to information that might disconfirm one’s beliefs. The search for confirming evidence is called “positive test strategy”; it runs counter to advice by philosophers of science who advise testing hypotheses by trying to refute them [i.e., by searching for disconfirming evidence; Kahneman (2011)]. The result of confirmation bias is that people are unlikely to change their minds about something that they have a pre-existing belief about, because they tend to look only for information that supports their pre-existing beliefs. For instance, if you believe that a particular player is a strong breakout candidate to be a sleeper, you may be more likely to pay attention to evidence that supports that the player will breakout and may be less likely to pay attention to evidence that indicates the player may struggle. That is, people tend to look for confirmation that the players they want to draft (or start) are preferred by the experts (R. M. Miller, 2013). Confirmation bias may lead to “going with your gut” (i.e., making a decision based on your intuition).\nAs a budding empiricist, you should actively seek out information that challenges or disconfirms your beliefs—particularly from trustworthy, reputable sources—and work to incorporate the information into your beliefs. Do your best to go into observation, data analysis, and data interpretation with an open mind.\n\n14.4.4 In-Group Bias\nIn-group bias is the tendency for people to prefer others who are members of their same (in-)group. For instance, a manager whose favorite team is the Dallas Cowboys might try to draft mostly Cowboys players, regardless of their historical performance.\n\n14.4.5 Hindsight Bias\n\n“Hindsight is 20/20.” – Idiom\n\nHindsight bias is the tendency to perceive that past events were more predictable than they were (i.e., the “I-knew-it-all-along” effect). Everything seems to make sense in hindsight (Kahneman, 2011). People tend to remember the successes of their predictions and forget the failures of their predictions. For instance, if a third-string Quarterback has a breakout game, a fantasy manager may claim that they “knew it all along” that the player was going to breakout, despite not having picked up the player. That same manager may forget the many other predictions they had that did not come true.\n\n14.4.6 Outcome Bias\nOutcome bias is the tendency to evaluate the quality of a decision based on the eventual outcome of that decision, rather than based on the quality of the decision based on the information that was known at the time of the decision. For instance, consider that a manager starts a lower-ranked player over a higher-ranked player because the lower-ranked player has a more favorable matchup. Now, consider that the lower-ranked player scores fewer points than the higher-ranked player because the lower-ranked player got injured. If the manager concludes that their decision-making process for who to start was flawed and thus not pay attention to matchups in subsequent decisions, this would reflect outcome bias. Likewise, if the manager makes a poor-quality decision but gets lucky, and they attribute their success to a sound decision-making process, this would also reflect outcome bias. In general, people tend to blame decision makers for good decisions that turned out badly, and to give them too little credit for good decisions that turned out well [because the decisions appear obvious but only after the fact; Kahneman (2011)]. Hindsight bias and outcome bias tend to lead to risk aversion (Kahneman, 2011).\nFocus on the decision-making process when evaluating the quality of decisions, rather than the outcome (Tanney, 2021). Yes, also evaluate the outcomes of decisions, but do not give yourself too much credit for lucky decisions or too much blame for unlucky decisions (like injuries). Luck is going to happen. If you are making optimal decisions that maximize the expected value of returns (e.g., projected fantasy points), you are following a strong process. That said, one thing you can count on is that players get injured. So, it is to your benefit to draft and compose your team to have some bench depth so you can handle injuries when they inevitably occur.\n\n14.4.7 Self-Serving Bias\nSelf-serving bias is the tendency to perceive oneself in an overly positive manner. Doing so may function to bolster people’s self-esteem. One way that self-serving bias can commonly manifest is the tendency people to attribute successes to internal factors—such as one’s character, ability, effort, or actions—but to attribute failures to external factors beyond one’s control—such as others, unfairness, bad luck, etc. That is, self-serving bias involves claiming more responsibility for one’s success than for one’s failure (R. M. Miller, 2013). For instance, a manager who wins the league may attribute their success to their fantasy football skill and smart decisions (rather than luck). However, if they get last place, they may attribute their failure not to themselves but instead to factors outside of their control, such as bad luck, player injuries, and poor performance by players due to circumstances out of their control (e.g., bad weather conditions, coaching decisions).\nAlthough the self-serving bias may help us feel better about ourselves, it can lead us to overestimate our abilities when things go well and to fail to learn from mistakes when things go poorly. Remember that nearly half of the variability in fantasy football performance is estimated to be luck, and the other half likely due to skill (Getty et al., 2018). Thus, successes in fantasy football likely involve a good amount of luck; likewise, failures in fantasy football can also reflect mistakes and poor decision making that arise from lower skill.\n\n14.4.8 Omission Bias\nOmission bias is the tendency to prefer inaction over action, or to prefer the default option over changes from the default option (Kahneman, 2011). For instance, consider a manager who is on the fence to drop an underperforming player and to pick up someone else to start. If the manager does not drop the player and the player scores few points, the manager is likely to be less critical and regretful of their decision than if they had picked up another player who then scored the same (low) number of points. Thus, a person’s decisions may be influenced by the anticipation or fear of regret so as to keep the status quo, avoid losses, and minimize the expectation of regret—such as choosing the same order each time at a restaurant to avoid regret (Kahneman, 2011). However, people may anticipate more regret than they actually experience, because people tend to underestimate the protective impact of the psychological defenses they will deploy against regret (Kahneman, 2011). In general, Kahneman (2011) recommends not to put too much weight on potential future regret when making decisions: “even if you have some [regret], it will hurt less than you now think.” (p. 352).\nConsider another example adapted from Miller (2013): two players scored the same number of points in a given week. A Wide Receiver scored 1.6 points from 16 receiving yards on 1 target. A Running Back scored 1.6 points from 56 receiving yards and two dropped fumbles. The manager may harbor more blame for the player who performed a harmful action (losing two fumbles) than the player who received only one target (harmful inaction).\nPay attention to players’ usage and opportunities, not just how many points a player scored and whether they had more fumbles or interceptions than other players. The more times a player touches the ball, the more points they are likely to score. And do not let the anticipation of regret prevent you from making important decisions.\n\n14.4.9 Loss Aversion Bias\nLoss aversion bias is the tendency to avoid losses rather than acquiring equivalent gains. That is, loss aversion (similar to punishment sensitivity, as opposed to reward sensitivity) is the tendency to weigh losses (or punishment) more heavily than gains (or rewards). Thus, the target state matters less than the change (i.e., gains and/or losses) from one’s reference point [e.g., their current roster; Kahneman (2011)]. Loss aversion is a key aspect of prospect theory and suggests that individuals tend to be biased in favor of their reference situation and toward making smaller rather than larger changes (Kahneman, 2011). Loss aversion explains why people tend to favor the status quo; the negative consequences of a change often loom larger than the positive consequences of a change (Kahneman, 2011). Aversion to losses and potential threats may be related to humans’ survival instinct (Kahneman, 2011). With par as a reference point, this may explain why professional golfers tend to putt more accurately for par than for birdie, even for putts of equivalent distance (Kahneman, 2011).\nPrinciples of prospect theory and loss aversion bias are depicted in Figure 14.2.\n\n\n\n\n\nFigure 14.2: Prospect Theory and Loss Aversion: People Tend to Weigh Losses More Heavily Than Gains. That is, the utility function is steeper in the loss (red) region than in the gain (blue) region. Adapted from https://commons.wikimedia.org/wiki/File:Graphique_th%C3%A9orie_des_perspectives.svg.\n\n\nThe loss aversion ratio indicates the extent to which a person weighs losses greater than gains, and for many people is around 2 (typically between 1.5 to 2.5; (Kahneman, 2011). That is, people tend to give as much twice the weight to losses as to gains. The late baseball manager Sparky Anderson once said, “Losing hurts twice as bad as winning feels good.” Loss aversion is different from risk aversion. Loss aversion is exemplified when teams play conservatively so as “not to lose” instead of “to win.” In fantasy football, loss aversion may lead managers to start or hold onto players for too long who were highly drafted yet are underperforming instead of starting a more promising player out of fear of losing potential value from their initial investment. Loss aversion can also influence trade negotiations.\nAn example of loss aversion bias in coaching basketball is removing a star player from a game so you can keep the player for the end of the game (Moskowitz & Wertheim, 2011). As described in Section 27.3.1, an example of loss aversion bias in football is punting on fourth down when, in many cases, it would be advantageous to go for it (Moskowitz & Wertheim, 2011).\n\n14.4.10 Risk Aversion Bias\nRisk aversion bias is the tendency to prefer outcomes with low uncertainty (i.e., risk), even if they offer lower potential rewards compared to outcomes with greater uncertainty but potentially greater rewards. Risk aversion leads people to select safer options but may lead them to miss out on higher-gain opportunities. For instance, risk aversion may lead a fantasy manager to start players who are more steady (i.e., show greater game-to-game consistency) over players who are more volatile (i.e., show greater game-to-game variability) but have higher upside potential.\nIn mixed gambles, in which it is possible for a person to experience either a gain or a loss, loss aversion tends to lead to risk-averse choices (Kahneman, 2011). By contrast, when all of a person’s options are poor, people tend to engage in risk seeking, as has been observed in entrepreneurs and in generals (Kahneman, 2011). In fantasy football, risk seeking may be more likely when a manager has a team full of underperforming players and a weak record.\n\n14.4.11 Primacy Effect Bias\nPrimacy effect bias is the tendency to recall the first information that one encounters better than information presented later on. For instance, primacy effect bias might lead a manager to draft a player based on the first information they encountered about the player.\n\n14.4.12 Recency Effect Bias\nRecency effect bias is the tendency to weigh recent events more than earlier ones. For instance, a manager might observe that a Running Back on the waiver wire performed well in the last two games. Recency effect bias may lead the manager to pick up the player, overvaluing their recent performance. For instance, the manager may not have adequately weighed the player’s overall season performance and the fact that the starting Running Back is returning to the lineup from injury, and that is why the player received more carries in the past two games (i.e., in place of the injured starter).\nRecency effect bias seems a bit at odds with primacy effect bias; however, they can both coexist. People tend to remember the earliest information they encounter in addition to the most recent information, and they tend to forget the information encountered in between.\n\n14.4.13 Framing Effect Bias\nFraming effect bias is the tendency to make decisions based on how information is framed (e.g., based on positive or negative connotations), rather than just the information itself. For instance, consider a weekly column written by two different fantasy football expert commentators. Expert A may write that a Running Back has scored at least 15 fantasy points in 70% of games this season (i.e., positive framing). Expert B may write that the same Running Back has scored fewer than 15 fantasy points in 30% of games this season (i.e., negative framing). Both statements convey the same statistical information; however, a manager might be more inclined to start the player when reading the statement from Expert A than from Expert B because of how the information was framed (i.e., presented). People prefer to make decisions to avoid loss, consistent with loss aversion bias, so the negative framing might lead a person to be less likely to start the player.\nPay attention to how information is framed, knowing that the framing itself could lead you astray. Thus, try to focus on the information itself rather than the framing when making decisions.\n\n14.4.14 Endowment Effect Bias\nEndowment effect bias is the tendency to overvalue merely because one owns it—owning it increases its value to the owner [especially for goods that are not regularly traded; Kahneman (2011)]. That is, a manager tends to value the same player more when they play for their team rather than an opponent’s team. It is related to loss aversion bias and prospect theory; when owning a good (i.e., a person’s reference point), they focus on the loss of the good if they were they to sell or trade it away rather than on the gains from the sale or trade. When trading, the endowment effect bias might lead a manager to demand more to trade away a player than to acquire the same player (R. M. Miller, 2013). For instance, a manager might overvalue a player they drafted in the first round, refusing to trade them even if they could get a better-performing player in return.\nA player’s trade value is not fixed. It makes sense to adjust a player’s trade value based on many factors such as your team’s needs, the needs of your trade partner’s team, the player’s remaining strength of schedule, etc. However, you want to avoid adjusting a player’s value based merely on the fact that he is on your team. Evaluate a variety of trade analyzers to more objectively evaluate players’ worth.\n\n14.4.15 Bandwagon Effect Bias\nThe bandwagon effect bias is the tendency to do or believe things because other people are. It involves social conformity. For instance, consider if a rookie Wide Receiver has a breakout game and he is picked up in many fantasy leagues. A given manager might pick up the player because the player is frequently being picked up in many fantasy leagues, without evaluating whether the player’s success is sustainable.\n\n14.4.16 Dunning–Kruger Effect Bias\n\n“The more you know, the more you know you don’t know.” – Anonymous\n\nThe Dunning–Kruger effect bias is the tendency for people with low ability/competency in a task to overestimate their ability. The Dunning–Kruger effect is depicted in Figures 14.3 and 14.4. For instance, consider a new fantasy manager who experiences some initial wins (often called “beginner’s luck”). They may attribute their successes to their skill rather than to luck. Their overconfidence may lead them to believe they can win the league without much preparation.\n\n\n\n\n\nFigure 14.3: Dunning–Krueger Effect: Confidence as a Function of Competence. People with low competency in a task tend to overestimate their ability. Adapted from https://commons.wikimedia.org/wiki/File:Effet_Dunning-Kruger.svg.\n\n\n\n\n\n\n\nFigure 14.4: Dunning–Krueger Effect: Perceived Performance as a Function of Actual Performance. People who perform poorly in a task tend to overestimate their performance. Adapted from https://commons.wikimedia.org/wiki/File:Dunning-kruger_effect_-_percentile.svg.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-fallacies",
    "href": "cognitive-bias.html#sec-fallacies",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.5 Examples of Fallacies",
    "text": "14.5 Examples of Fallacies\nAs described above, fallacies are mistaken beliefs and flawed reasoning. Fallacies are often due to the use of heuristics and to cognitive biases. Examples of fallacies include:\n\n\nbase rate fallacy (aka base rate neglect)\nregression fallacy\nhot hand fallacy\nsunk cost fallacy\ngambler’s fallacy\nanecdotal fallacy\nnarrative fallacy\nconditional probability fallacy\necological fallacy\n\n\n14.5.1 Base Rate Fallacy\nThe base rate fallacy (aka base rate neglect) is the tendency to ignore information about the general probability of an event in favor of specific information about the event. The base rate is a marginal probability, which is the general probability of an event irrespective of other things. Thus, a “low base-rate” event is an event with a low probability of occurring (i.e., an unlikely event). Focusing on the specific information about the event (and thus underweighting the base rate) involves the representativeness heuristic (Kahneman, 2011)—making judgments based on how similar the event is to a mental prototype regardless of how frequent the event the event actually is. As noted by Kahneman (2011), “people who have information about an individual case rarely feel the need to know the statistics of the class which the case belongs.” (p. 249). In general, people tend to a) overestimate the likelihood of low base-rate events and b) overweight low base-rate events in their decisions (Kahneman, 2011). This is especially true if the unlikely event attracts attention, because of the confirmatory bias of memory—that is, when thinking about the event, you try to make it true in your mind (Kahneman, 2011).\nFor example, the base rate of work-related injury is the general probability of experiencing a work-related injury, irrespective of other factors (e.g., the type of job, the person’s age, the person’s sex). Among the working population in the U.S., the lifetime prevalence of work-related injuries (i.e., the percent of people who will experience a work-related injury at some point in their lives), is ~35% [Free et al. (2020); archived at https://perma.cc/A2L6-WPEH]. Thus, the base rate of work-related injuries in the U.S. is ~35%. The probability of work-related injuries is higher for some occupations (e.g., construction) and for some groups (e.g., men, 55–64-year-olds, Black or Multiracial, who are self-employed and have less than high school education) than others. Nevertheless, if we ignore all of the interacting factors, the general probability of work-related injuries is 35%. If we made a prediction that someone would be highly likely (&gt; 90%) to experience a work-related injury because they are male and self-employed, this would be ignoring the relatively lower base rate of work-related injury. Indeed, even men (36.7%) and self-employed individuals (41.2%) have less than a 50% chance of experiencing a work-related injury.\nThe framing of an event can also alter its availability in mind, judgments of probability, and weighting in decisions (Kahneman, 2011). A more vivid description of an outcome can lead people to be more influenced by its ease of recall (i.e., availability) than its objective probability (Kahneman, 2011). In addition, concrete representations tend to be weighted more heavily than abstract representations (Kahneman, 2011). Abstract representations are framed in terms of “how likely” the event will occur in in terms of chance, risk, probability, or likelihood. Concrete representations are framed in terms of “how many” people will be affected by the event. An example abstract representation is that there is a 0.001% chance of dying from this particular disease. When framed in this way, the risk appears small. Now, consider the same information framed in terms of a concrete representation: 1 in 100,000 people will die from this disease. Due to a phenomenon called denominator neglect, low base-rate events are more heavily weighted when they are framed using a concrete representation than an abstract representation (Kahneman, 2011).\nThere are low base-rate events that tend to be underweighted when making decisions, such as whether to make anticipatory preparations for a future earthquake, housing bubble, or financial crisis (Kahneman, 2011). One major cause of underweighting is if a person has never experienced the rare event (i.e., the earthquake or finanical crisis) (Kahneman, 2011). However, for low base-rate events that people have experienced, the availability and representativeness heuristics tend to lead people to overestimate the likelihood that the unlikely event occurs. In general, many factors—including obsessive concerns about the event, vivid images of the event, concrete representations of the consequences of the event (e.g., “a disease kills 1 person out of every 1,000”), and explicit reminders of the event—contribute to the overweighting of unlikely events (Kahneman, 2011).\nAs applied to fantasy football, consider that you read about a potential sleeper Wide Receiver who had a stellar performance in a preseason game. If you select this player early on in the draft based on this information, this would be ignoring the general probability that most players who have a strong performance in the preseason do not perform as well in the regular season (i.e., base rate neglect). Performance in the preseason is not strongly predictive of performance in the regular season [Schalter (2022); archived at https://perma.cc/FSG2-6AXE].\nMore information on base rates and how to counteract the base rate fallacy in described in Chapter 16.\n\n14.5.2 Regression Fallacy\nThe regression fallacy is the failure to account for the fact that things tend to naturally fluctuate around their mean and that, after an extreme fluctuation, subsequent scores tend to regress (or reverse) to the mean. Regression to the mean is depicted in Figure 14.5. Regression to the mean occurs because performance—both success and failure—is influenced by talent and by luck (Kahneman, 2011). And, unlike talent (or one’s level one the construct of interest), luck is fickle. Regression to the mean can explain why those with depression tend to improve over time without doing anything, and can explain why highly intelligent women tend to marry men who are less intelligent than they are (Kahneman, 2011). Despite peoples’ tendency to ascribe such changes to causal phenomena, regression-related changes do not need a causal explanation—they are a mathematically inevitable consequence that arises from the fact that an outcome is influenced, to some degree, both by one’s level on the construct and by error (Kahneman, 2011).\n\nCodemydata &lt;- data.frame(\n  week = 1:7,\n  fantasyPoints = c(33,7,12,5,11,15,3)\n)\n\nggplot(\n  data = mydata,\n  mapping = aes(\n    x = week,\n    y = fantasyPoints\n  )\n) + \n  geom_point(\n    size = 4\n  ) +\n  geom_line(\n    linewidth = 1\n  ) +\n  geom_abline(\n    aes(\n      intercept = 10,\n      slope = 0,\n      linetype = \"Average ability\"\n    ),\n  ) +\n  scale_linetype_manual(values = \"dashed\") +\n  coord_cartesian(\n    ylim = c(0, NA)) +\n  scale_x_continuous(\n    breaks = 1:7\n  ) +\n  labs(\n    y = \"Fantasy Points\",\n    #title = \"Regression to the Mean\",\n    linetype = NULL\n  ) +\n  annotate(\n    \"segment\",\n    x = 2,\n    xend = 1.75,\n    y = 19,\n    yend = 15,\n    color = \"blue\",\n    linewidth = 1.5,\n    alpha = 0.6,\n    arrow = arrow()) +\n  annotate(\n    \"text\",\n    x = 2,\n    y = 20,\n    label = \"Regression toward the mean\",\n    hjust = 0, # left-justify\n    size = 5) +\n  theme_classic(base_size = 30) +\n  theme(\n    axis.title.y = element_text(angle = 0, vjust = 0.5),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.8, 1))\n\n\n\n\n\n\nFigure 14.5: Regression to the Mean.\n\n\n\n\nAn example of the regression fallacy is the so-called Sports Illustrated or Madden cover jinx curse. The Sports Illustrated or Madden cover jinx curse is the urban legend that players who appear on the cover of Sports Illustrated (the magazine) or Madden (the video game) will perform poorly. But, such a phenomenon can be more simply explained by regression to the mean [Kahneman (2011); G. Smith (2016); archived at https://perma.cc/CZM9-TVFN]. When a player has a superb season, they likely benefited from some degree to good luck, and it is unlikely that they will repeat such a stellar season the following year. Instead, they are likely—at least based on random fluctuation—to regress to their long-term mean.\nApplied to fantasy football, consider that a Quarterback had a 5-touchdown game in Week 1. You are in need of a strong Quarterback, so you drop a solid player to pick him up. However, it is possible that the Quarterback benefited from playing against a week defense in the first game of the season. Future matchups may prove more difficult, and the player is unlikely to sustain such a solid performance consistently throughout the season (i.e., they are likely to regress toward their mean).\n\n14.5.3 Hot Hand Fallacy\nThe “hot hand” is the idea that a player who experiences a successful outcome will have greater chance of success in subsequent attempts. For instance, in basketball, it is widely claimed by coaches, players, and commentators that players who have the hot hand (i.e., who are “on fire”) are more likely to make shots because they made previous shots and are gaining confidence. The hot hand is a supposed example of momentum. However, there is not strong evidence of momentum in sports either at the player or team level; rather, there may be evidence of reversals, the opposite of momentum (Moskowitz & Wertheim, 2011). Streaks certainly exist, but momentum suggests that if a person performs well (or poorly) in a given instance, their performance will influence (and thus predict) their future performance. In general, streaks appear to be largely random variation (i.e., luck/chance) around a player’s mean performance ability.\nEvidence on the hot hand is mixed. Considerable evidence historically has suggested that there is no such thing as a “hot hand” even though many players and fans believe in it (Avugos et al., 2013; Bar-Eli et al., 2006; Gilovich et al., 1985; Moskowitz & Wertheim, 2011; Wetzels et al., 2016). Even though streaks may occur, making a shot may not increase the likelihood of making the next shot; that is, the likelihood of making a field goal seems to be independent of past success (or failure) on recent attempts. Defenses may adjust to guard a “hot” player more closely. However, the findings suggesting no hot hand tend to hold even whether there is no defense (Gilovich et al., 1985). Some recent research, however, has suggested that there may be a small hot hand effect in some contexts (Bocskocsky et al., 2014; Miller & Sanjurjo, 2014; Miller & Sanjurjo, 2024). However, if any such effect exists, the hot hand may be limited to a small subset of players and the effect size of any hot hand effect appears to be small (Pelechrinis & Winston, 2022). Indeed, the slight potential increase in shooting ability tends to be offset by the tendency that “hot” players have to increase their shot difficulty after making a previous shot, so the net hot hand effect appears to be “vanishingly small” (Partnow, 2021).\nIn football, when trying how to distribute the ball among multiple Running Backs, it is not uncommon to hear that a coach wants to give the ball to the Running Back with the “hot hand.” In fantasy football, consider that a player just had a multiple touchdown game. Due to the hot hand fallacy, a manager might continue to start the player because they believe the player is “on fire” and is likely to continue to score at an unsustainable rate.\nIt is important consider whether such a string of strong performances are outliers and if the player may, in future games, regress to the mean. When considering whether strong performances are outliers and may regress to the mean, it is valuable to consider whether the player’s health, skill, or situation has appreciably changed (compared to the player’s earlier, weaker performances). Is the player finally fully healthy? Has the player appreciably improved in some skill that will benefit them in future games? Has the player’s long-term situation improved, such as moving up the depth chart, or receiving more carries/targets that is not tied to a specific opponent or game script? Or, alternatively, do the improvements appear to be driven by transient, game-specific factors, such as a the health of a teammate, the opponents they played, or the game script that ensued? If long-term outlook of the player has appreciably changed due to changes in the fundamentals of a player’s value, such as their health, skill, or situation, it is less likely that such performance improvements will regress over the long run.\n\n14.5.4 Sunk Cost Fallacy\nA sunk cost is a cost (e.g., in money, time, or effort) that has already been incurred and cannot be recovered. For instance, if a person orders an expensive meal at a restaurant, the order is a sunk cost. The sunk cost fallacy is the tendency to continue an endeavor when there is a sunk cost. For instance, when ordering the expensive meal at the restaurant, a person may over-eat so that they feel that they eat their money’s worth of food. The sunk cost fallacy is an example of the commitment bias, an escalation of commitment by which a person continues to commit additional time, effort, or resources to a failing project. People tend to refuse to cut their losses when doing so would admit failure (Kahneman, 2011). It is important to be willing to identify and terminate failing projects to cut one’s losses.\nApplied to fantasy football, consider a situation in which you invest a lot of salary cap or a high draft pick to draft a promising player, but they repeatedly underperform. If you continue to start the player to justify your large investment, instead of benching him in favor of a higher-performing player, you are committing the sunk cost fallacy. It is important to own up to our mistakes; when you make a mistake, the sooner you realize it, own it—for instance, by either benching or dropping the weak-performing player—and move on, the sooner you will be able to replace him with a better-performing player. At the same time, you probably do not want to give too much weight to a player’s performance in any given week, due to random chance that may lead a player to greatly under- or overperform; and whose future peformance is likely to regress (whether positively or negatively) to their mean.\n\n14.5.5 Gambler’s Fallacy\nThe gambler’s fallacy occurs due to an erroneous belief in the law of small numbers. The law of large numbers is a mathematical theorem that the average of a sufficiently large number of independent observations converges to the true value. For instance, if you flip a fair coin 1 million times, it is likely to land heads-up ~50% of the time. The law of small numbers (aka hasty generalization), by contrast, is an erroneous belief that small samples are representative of the populations from which they were drawn. In general, large samples yield more precise results than small samples, whereas extreme results (in either direction—e.g., much higher or much lower than 50% of coins that flip heads-up) are more likely to be obtained from small samples (Kahneman, 2011). For instance, if you flip a coin 10 times, belief in the law of small numbers would lead one to believe that the coin will flip heads-up exactly 5 times out of 10. However, in reality, the chance is less than 1 in 4 (24.6%) that exactly 5 of 10 coin flips turn up heads, as calculated below and as depicted in Figure 14.6:\n\nCodedbinom(\n  x = 5,     # number of coins that flip heads-up\n  size = 10, # how many times you flip a coin\n  prob = 0.5 # probability of a coin flipping heads-up (i.e., fair coin = 50%)\n)\n\n[1] 0.2460938\n\n\nThe stats::dbinom() function in R provides the density of a binomial distribution. A binomial distribution is the probability of a particular number of successes (e.g., coins flipping heads-up) given a certain number of independent trials.\n\nCodeset.seed(52242)\n\nsmallNumFlips &lt;- 10\nlargeNumFlips &lt;- 1000000\nnumSimulations &lt;- 100000\n\nnumHeadsSmallNumFlips &lt;- rbinom(\n  n = numSimulations,\n  size = smallNumFlips,\n  prob = .5\n)\n\nnumHeadsLargeNumFlips &lt;- rbinom(\n  n = numSimulations,\n  size = largeNumFlips,\n  prob = .5\n)\n\nsimulationOfFlippingCoins &lt;- data.frame(\n  numHeadsSmallNumFlips = numHeadsSmallNumFlips,\n  numHeadsLargeNumFlips = numHeadsLargeNumFlips\n)\n\nsimulationOfFlippingCoins &lt;- simulationOfFlippingCoins %&gt;% \n  mutate(\n    proportionHeadsSmallNumFlips = numHeadsSmallNumFlips / smallNumFlips,\n    proportionHeadsLargeNumFlips = numHeadsLargeNumFlips / largeNumFlips,\n    highlight = ifelse(numHeadsSmallNumFlips == 5, \"yes\", \"no\"),\n    extremeSmallNumFlips = ifelse(proportionHeadsSmallNumFlips &lt;= 0.2 | proportionHeadsSmallNumFlips &gt;= 0.8, TRUE, FALSE),\n    extremeLargeNumFlips = ifelse(proportionHeadsLargeNumFlips &lt;= 0.2 | proportionHeadsLargeNumFlips &gt;= 0.8, TRUE, FALSE),\n  )\n\nggplot2::ggplot(\n  data = simulationOfFlippingCoins,\n  mapping = aes(\n    x = numHeadsSmallNumFlips,\n    fill = highlight)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    bins = 11\n  ) +\n  scale_x_continuous(\n    breaks = 0:10\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"yes\" = \"tomato\",\n      \"no\" = \"gray\")\n  ) +\n  labs(\n    x = \"Number of Coins Flipped Heads (out of 10 Coin Flips)\",\n    y = \"Frequency\",\n    title = \"Histogram of Number of Coins that Flip Heads-Up\\nin a Simulation of 10 Coin Flips\\n(with 100,000 Replications).\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 14.6: Histogram of Number of Coins that Flip Heads-Up in a Simulation of 10 Coin Flips (with 100,000 Replications).\n\n\n\n\nAlthough 5 is the modal count of coins that flip heads-up out of 10 flips (i.e., it was more common than any other number), it is less common than the aggregate probability of flipping any number of heads besides 5. The probability of getting any other number of coin flips turning up heads (other than 5) is:\n\nCodedbinom(\n  x = c(0:4, 6:10),\n  size = 10,\n  prob = 0.5\n) %&gt;% sum()\n\n[1] 0.7539062\n\n\nMoreover, among the simulations with only 10 coin flips, nearly 11% of sets had extreme results (≤ 20% heads or ≥ 80% heads):\n\nCodeprop.table(table(simulationOfFlippingCoins$extremeSmallNumFlips))\n\n\n  FALSE    TRUE \n0.89004 0.10996 \n\n\nBy contrast, among the simulations with 1 million coin flips, none of the sets had extreme results:\n\nCodeprop.table(table(simulationOfFlippingCoins$extremeLargeNumFlips))\n\n\nFALSE \n    1 \n\n\nThe gambler’s fallacy is the erroneous belief that future probabilities are influenced by past events, even when the events are independent. For example, a gambler may pay close attention to a particular slot machine. If the slot machine has not paid out in a while, the gambler may believe that the slot machine is about to pay out soon, and may start putting coins in the slots. Or, a gambler on a losing streak may (wrongly) believe that they “are due” and keeps gambling as a result.\nApplied to fantasy football, consider that a Quarterback has had several lousy games in a row. The gambler’s fallacy might lead a manager to start the player under the belief that the player “is due” for a big game, expecting a strong performance from the player merely because the player has not had a good game in a while.\n\n14.5.6 Anecdotal Fallacy\nThe anecdotal fallacy is the error of drawing inferences based on personal experience or singular examples rather than based on stronger forms of evidence. Although people tend to find stories and anecdotes compelling, they are among the weakest forms of evidence, as described in Section 8.6.3. Consider that a fantasy football commentator writes an article about how they won their league by drafting a Quarterback and Tight End with their first two picks, respectively. The anecdotal fallacy might lead a manager who reads the article to believe that they are more likely to win their league if they follow a similar strategy, despite stronger evidence otherwise. It is important to be skeptical of anecdotes and personal experience.\n\n14.5.7 Narrative Fallacy\nThe narrative fallacy is the error of creating a story with cause-and-effect explanations from random events. According to Kahneman (2011), people find stories more compelling if they a) are simple (as opposed to complex), b) are concrete (as opposed to abstract), c) provide a greater attribution to talent, stupidity, and intentions (compared to luck), and d) focus on a few salient events that occurred (rather than the many events that failed to occur). For instance, consider that a player undergoes a challenging start to the season; however, they have higher-performing games in the middle of the season. A fantasy football manager may construct a “causal narrative” that the player overcame obstacles and is now trending toward strong performance for the rest of the season, even though the stronger performance could reflect some natural and random fluctuation.\n\n14.5.8 Conditional Probability Fallacy\nWe describe the conditional probability fallacy in Section 16.3.2 after introducing conditional probability in Section 16.3.1.3.\n\n14.5.9 Ecological Fallacy\nThe ecological fallacy is the error of drawing inferences about an individual from group-level data. For instance, a manager is interested in how age is associated with fantasy performance. They examine the correlation between age and fantasy points and find that age is positively correlated with fantasy points among Quarterbacks. The ecological fallacy would then take this finding to infer that an individual Quarterback is likely to increase in their performance with age. We describe the ecological fallacy more in Section 12.2.1.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasConclusion",
    "href": "cognitive-bias.html#sec-cognitiveBiasConclusion",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.6 Conclusion",
    "text": "14.6 Conclusion\nIn conclusion, there are many heuristics, cognitive biases, and fallacies that people engage in when making judgments and predictions. It is prudent to be aware of these common biases and to work to counteract them. For instance, look for information that challenges or disconfirms your beliefs, and work to incorporate this information into your beliefs. Do your best to pursue observation, data analysis, and data interpretation with an open mind. You never know what important information you might discover if you go in with an open mind. Pay attention to fundamentals of a player’s value, such as their health, skill, or situation when considering whether a player’s performance may regress to the mean. If the strong performances appear to be driven by transient, game-specific factors, such as a the health of a teammate, the opponents they played, or the game script that ensued, future performances may be more likely to regress to the mean. In general, people tend to be overconfident in their predictions. There is considerable luck in fantasy football. Approach the task of prediction with humility; no one is consistently able to accurately predict how well players will perform.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasSessionInfo",
    "href": "cognitive-bias.html#sec-cognitiveBiasSessionInfo",
    "title": "14  Heuristics and Cognitive Biases in Prediction",
    "section": "\n14.7 Session Info",
    "text": "14.7 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.1.0     readr_2.1.5     tidyr_1.3.1     tibble_3.3.0   \n [9] ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_2.0.0     compiler_4.5.1     tidyselect_1.2.1  \n [5] scales_1.4.0       yaml_2.3.10        fastmap_1.2.0      R6_2.6.1          \n [9] labeling_0.4.3     generics_0.1.4     knitr_1.50         htmlwidgets_1.6.4 \n[13] pillar_1.11.0      RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6       \n[17] stringi_1.8.7      xfun_0.53          timechange_0.3.0   cli_3.6.5         \n[21] withr_3.0.2        magrittr_2.0.3     digest_0.6.37      grid_4.5.1        \n[25] hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5        evaluate_1.0.4    \n[29] glue_1.8.0         farver_2.1.2       rmarkdown_2.29     tools_4.5.1       \n[33] pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\n\n\n\nAvugos, S., Köppen, J., Czienskowski, U., Raab, M., & Bar-Eli, M. (2013). The “hot hand” reconsidered: A meta-analytic approach. Psychology of Sport and Exercise, 14(1), 21–27. https://doi.org/10.1016/j.psychsport.2012.07.005\n\n\nBar-Eli, M., Avugos, S., & Raab, M. (2006). Twenty years of “hot hand” research: Review and critique. Psychology of Sport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001\n\n\nBocskocsky, A., Ezekowitz, J., & Stein, C. (2014). The hot hand: A new approach to an old “fallacy.” MIT Sloan Sports Analytics Conference. https://www.sloansportsconference.com/research-papers/the-hot-hand-a-new-approach-to-an-old-fallacy\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nCritcher, C. R., & Rosenzweig, E. L. (2014). The performance heuristic: A misguided reliance on past success when predicting prospects for improvement. Journal of Experimental Psychology: General, 143(2), 480–485. https://doi.org/10.1037/a0034129\n\n\nEnke, B. (2020). What you see is all there is. The Quarterly Journal of Economics, 135(3), 1363–1398. https://doi.org/10.1093/qje/qjaa012\n\n\nFree, H., Groenewold, M. R., & Luckhaupt, S. E. (2020). Lifetime prevalence of self-reported work-related health problems among US workers—United States, 2018. MMWR. Morbidity and Mortality Weekly Report, 69(13), 361–365. https://doi.org/10.15585/mmwr.mm6913a1\n\n\nGet Up ESPN. (2021). @nfldraftscout on the Cowboys’ interest in drafting Kyle Pitts. https://x.com/GetUpESPN/status/1380165126108672001\n\n\nGetty, D., Li, H., Yano, M., Gao, C., & Hosoi, A. E. (2018). Luck and the law: Quantifying chance in fantasy sports and other contests. SIAM Review, 60(4), 869–887. https://doi.org/10.1137/16m1102094\n\n\nGilovich, T., Vallone, R., & Tversky, A. (1985). The hot hand in basketball: On the misperception of random sequences. Cognitive Psychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nKotrba, V. (2020). Heuristics in fantasy sports: Is it profitable to strategize based on favourite of the match? Mind & Society, 19(1), 195–206. https://doi.org/10.1007/s11299-020-00231-7\n\n\nLilienfeld, S. O., Lynn, S. J., & Lohr, J. M. (2015). Science and pseudoscience in clinical psychology: Initial thoughts, reflections, and considerations (S. O. Lilienfeld, S. J. Lynn, & J. M. Lohr, Eds.; 2nd ed., pp. 1–16). Guilford Publications.\n\n\nMiller, J. B., & Sanjurjo, A. (2014). A cold shower for the hot hand fallacy. Innocenzo Gasparini Institute for Economic Research. https://repec.unibocconi.it/igier/igi/wp/2014/518.pdf\n\n\nMiller, J. B., & Sanjurjo, A. (2024). A cold shower for the hot hand fallacy: Robust evidence from controlled settings. The Review of Economics and Statistics, 106(6), 1607–1619. https://doi.org/10.1162/rest_a_01280\n\n\nMiller, R. M. (2013). Cognitive bias in fantasy sports: Is your brain sabotaging your team? Xlibris Press.\n\n\nMoore, D. A., & Healy, P. J. (2008). The trouble with overconfidence. Psychological Review, 115(2), 502–517. https://doi.org/10.1037/0033-295X.115.2.502\n\n\nMoskowitz, T. J., & Wertheim, L. J. (2011). Scorecasting: The hidden influences behind how sports are played and games are won. Three Rivers Press.\n\n\nNivison, A. (2021). Florida TE Kyle Pitts draws comparison to Lebron James. https://247sports.com/article/kyle-pitts-lebron-james-2021-nfl-draft-florida-gators-football-163882176\n\n\nPartnow, S. (2021). The midrange theory: Basketball’s evolution in the age of analytics. Triumph Books.\n\n\nPelechrinis, K., & Winston, W. (2022). The hot hand in the wild. PLOS ONE, 17(1), e0261890. https://doi.org/10.1371/journal.pone.0261890\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nSchalter, T. (2022). The NFL preseason is not predictive — but it can often seem that way. https://fivethirtyeight.com/features/the-nfl-preseason-is-not-predictive-but-it-can-often-seem-that-way\n\n\nSmith, B., Sharma, P., & Hooper, P. (2006). Decision making in online fantasy sports communities. Interactive Technology and Smart Education, 3(4), 347–360. https://doi.org/10.1108/17415650680000072\n\n\nSmith, G. (2016). The Sports Illustrated cover jinx. https://www.psychologytoday.com/us/blog/what-the-luck/201610/the-sports-illustrated-cover-jinx\n\n\nTanney, M. (2021). R in sports analytics. https://www.youtube.com/watch?v=1zCDWtNEucI\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131. https://doi.org/10.1126/science.185.4157.1124\n\n\nWetzels, R., Tutschkow, D., Dolan, C., Sluis, S. van der, Dutilh, G., & Wagenmakers, E.-J. (2016). A Bayesian test for the hot hand phenomenon. Journal of Mathematical Psychology, 72, 200–209. https://doi.org/10.1016/j.jmp.2015.12.003\n\n\nWoodland, L. M., & Woodland, B. M. (2015). The National Football League season wins total betting market: The impact of heuristics on behavior. Southern Economic Journal, 82(1), 38–54. https://doi.org/10.4284/0038-4038-2013.145",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html",
    "href": "actuarial.html",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "",
    "text": "15.1 Getting Started\nThis chapter compares judgment versus actuarial approaches to prediction in fantasy football (and other domains).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-judgmentVsActuarialGettingStarted",
    "href": "actuarial.html#sec-judgmentVsActuarialGettingStarted",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "",
    "text": "15.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-approachesToPrediction",
    "href": "actuarial.html#sec-approachesToPrediction",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.2 Approaches to Prediction",
    "text": "15.2 Approaches to Prediction\nThere are two primary approaches to prediction: human judgment and the actuarial (i.e., statistical) method.\n\n15.2.1 Human Judgment\nUsing the judgment method of prediction, all gathered information is collected and formulated into a prediction in the person’s mind. The person selects, measures, and combines information and produces projections solely according to their experience and judgment. For instance, a proclaimed “fantasy expert” might use their experience, expertise, and judgment to make a prediction about how each player will perform by using whatever information and data they deem to be important, aggregating all of this information in their mind to make the prediction for each player. Professional scouts and coaches use judgment when making predictions or selecting players based on their impressions of the players (Den Hartigh et al., 2018). Experts such as scouts and coaches are often former athletes who are presumed to have unique knowledge of their sport. Because of their expertise, it is possible that they could make observations that a non-expert could not and that could not be captured by statistics (Den Hartigh et al., 2018). As an example in popular media, in the movie, “Trouble with the Curve”, a professional scout makes a judgment about how well a baseball hitter will do in the major leagues from his impressions of the hitter’s ability based on the sound of the ball off the player’s bat.\n\n15.2.2 Actuarial/Statistical Method\nIn the actuarial or statistical method of prediction, information is gathered and combined systematically in an evidence-based statistical prediction formula. The method is based on equations and data, so both are needed.\nAn example of a statistical method of prediction is the Violence Risk Appraisal Guide (Rice et al., 2013). The Violence Risk Appraisal Guide is used in an attempt to predict violence and is used for parole decisions. For instance, the equation might be something like Equation 15.1:\n\\[\n\\scriptsize\n\\text{violence risk} = \\beta \\cdot \\text{conduct disorder} + \\beta \\cdot \\text{substance use} + \\beta \\cdot \\text{suspended from school} + \\beta \\cdot \\text{childhood aggression} + ...\n\\tag{15.1}\\]\nThen, based on their score and the established cutoffs, a person is given a “low risk”, “medium risk”, or “high risk” designation.\nAn actuarial formula for projecting a Running Back’s rushing yards might be something like Equation 15.2:\n\\[\n\\scriptsize\n\\text{rushing yards} = \\beta \\cdot \\text{rushing yards last season} + \\beta \\cdot \\text{age} + \\beta \\cdot \\text{injury history} + \\beta \\cdot \\text{strength of offensive line} + ...\n\\tag{15.2}\\]\nThe beta weights in the actuarial model reflect the relative weight to assign each predictor. For instance, in predicting rushing yards, a player’s historical performance is likely the strongest predictor, whereas injury history might be a relatively weaker predictor. Thus, we might give historical performance a beta of 3 and injury history a beta of 1 to give a player’s historical performance three times more weight than the player’s injury history in predicting their rushing yards. For generating the actuarial model, you could obtain the beta weights for each predictor from multiple regression, from machine learning, or from prior research on the relative importance of each predictor.\nAs an example of using the actuarial approach, Billy Beane, who was the general manager of the Oakland Athletics at the time, wanted to find ways for his team—which had less financial resources than its competitors—to compete with teams that had more money to sign players. Because the team did not have the resources to sign the best players, they had to find to find other ways to find the optimal players that they could afford. So, he used statistical formulas that weight variables, such as on-base percentage and slugging percentage, according to their value for winning games, for the use of selecting players. His approach became well-known based on the Michael Lewis book, “Moneyball: The Art of Winning an Unfair Game”, and the eventual movie, “Moneyball”.\n\n15.2.3 Combining Human Judgment and Statistical Algorithms\nThere are numerous ways in which humans and statistical algorithms could be involved. On one extreme, humans make all judgments. On the other extreme, although humans may be involved in data collection, a statistical formula makes all decisions based on the input data, consistent with an actuarial approach. However, the human judgment and actuarial approaches can be combined in a hybrid way (Dana & Thomas, 2006). For example, to save time and money, a clinical psychologist might use an actuarial approach in all cases, but might only use a judgment approach when the actuarial approach gives a “positive” test. Or, the clinical psychologist might use both human judgment and an actuarial approach independently to see whether they agree. That is, the clinician or expert may make a prediction based on their judgment and might also generate a prediction from an actuarial approach.\nThe challenge is what to do when the human and the algorithm disagree. Hypothetically, humans reviewing and adjusting the results from the statistical algorithm could lead to more accurate prediction. However, human input also could lead to the possibility or exacerbation of biased predictions. In general, with very few exceptions, actuarial approaches are as accurate or more accurate than “expert” judgment (Ægisdóttir et al., 2006; Baird & Wagner, 2000; Dawes et al., 1989; Grove et al., 2000; Grove & Meehl, 1996). This is also likely true with respect to predicting player performance in sports (Den Hartigh et al., 2018). Moreover, the superiority of actuarial approaches to human judgment tends to hold even when the expert is given more information than the actuarial approach (Dawes et al., 1989). In addition, actuarial predictions outperform human judgment even when the human is given the result of the actuarial prediction (Kahneman, 2011). Allowing experts to override actuarial predictions consistently leads to lower predictive accuracy (Garb & Wood, 2019).\nThere is sometimes a misconception that formulas cannot account for qualitative information. However, that is not true. Qualitative information can be scored or coded to be quantified so that it can be included in statistical formulas. For instance, if an expert scout is able to meaningfully assess a player’s cognitive and motivational factors (i.e., the “X factor” or “intangibles”), the scout can score this across multiple players and include these data in the actuarial prediction formula. For instance, the scout could use a rating scale (e.g., 1 = “poor”; 2 = “fair”; 3 = “good”; 4 = “very good”; 5 = “excellent”) to code (i.e., translate) their qualitative judgment into a quantifiable rating that can be integrated with other information in the actuarial formula. That said, the quality of predictions rests on the quality and relevance of the assessment information for the particular prediction decision. If the assessment data are lousy, it is unlikely that a statistical algorithm (or a human for that matter) will make an accurate prediction: “Garbage in, garbage out”. A statistical formula cannot rescue inaccurate assessment data. Likewise, if the input data are based on measurement strategies that are biased against particular individuals or groups, the actuarial formula is not immune from bias.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-errorsInHumanJudgment",
    "href": "actuarial.html#sec-errorsInHumanJudgment",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.3 Errors in Human Judgment",
    "text": "15.3 Errors in Human Judgment\nHuman judgment is naturally subject to errors. Common heuristics, cognitive biases, and fallacies are described in Chapter 14. Judgments of sports athletes have been shown to be biased as a function of many characteristics of the athlete, including whether they are known to have a positive reputation, and their clothing, body language, and skin color (Den Hartigh et al., 2018). Below, I describe a few errors to which human judgment seems particularly prone.\nWhen operating freely, clinicians and medical experts (and humans more generally) tend to overestimate exceptions to the established rules (i.e., the broken leg syndrome). Meehl (1957) acknowledged that there may be some situations where it is glaringly obvious that the statistical formula would be incorrect because it fails to account for an important factor. He called these special cases “broken leg” cases, in which the human should deviate from the formula (i.e., broken leg countervailing). The example goes like this:\n\nIf a sociologist were predicting whether Professor X would go to the movies on a certain night, he might have an equation involving age, academic specialty, and introversion score. The equation might yield a probability of .90 that Professor X goes to the movie tonight. But if the family doctor announced that Professor X had just broken his leg, no sensible sociologist would stick with the equation. Why didn’t the factor of ‘broken leg’ appear in the formula? Because broken legs are very rare, and in the sociologist’s entire sample of 500 criterion cases plus 250 cross-validating cases, he did not come upon a single instance of it. He uses the broken leg datum confidently, because ‘broken leg’ is a subclass of a larger class we may crudely denote as ‘relatively immobilizing illness or injury,’ and movie-attending is a subclass of a larger class of ‘actions requiring moderate mobility.’\n— Meehl (1957, pp. 269–270)\n\nHowever, people too often think that cases where they disagree with the statistical algorithm are broken leg cases. People too often think their case is an exception to the rule. As a result, they too often change the result of the statistical algorithm and are more likely to be wrong than right in doing so. Because actuarial methods are based on actual population levels (i.e., base rates), unique exceptions are not overestimated.\nActuarial predictions are perfectly reliable—they will always return the same conclusion given an identical set of data. The human judge is likely to both disagree with others and with themselves given the same set of symptoms.\nThe decision by an expert (all by all humans) is likely to be influenced by past experiences. Actuarial methods are based on objective algorithms, and past personal experience and personal biases do not factor into any decisions. Humans give weight to less relevant information, and often give too much weight to singular variables. Actuarial formulas do a better job of focusing on relevant variables. Computers are good at factoring in base rates. Humans ignore base rates (base rate neglect).\nComputers are better at accurately weighing predictors and calculating unbiased risk estimates. In an actuarial formula, the relevant predictors are weighted according to their predictive power.\nHumans are typically given no feedback on their judgments. To improve accuracy of judgments, it is important for feedback to be clear, consistent, and timely. Intuition is a form of recognition-based judgment (i.e., recognizing cues that provide access to information in memory). Development of strong intuition depends on the quality and speed of feedback, in addition to having adequate opportunities to practice [i.e., sufficient opportunities to learn the cues; Kahneman (2011)]. The quality and speed of the feedback tend to benefit anesthesiologists who often quickly learn the results of their actions. By contrast, radiologists tend not to receive quality feedback about the accuracy of their diagnoses, including their false-positive and false-negative decisions (Kahneman, 2011).\nIn general, many so-called experts are “pseudo-experts” who do not know the boundaries of their competence—that is, they do not know what they do not know; they have the illusion of validity of their predictions and are overconfident about their predictions (Kahneman, 2011). A person’s confidence is not a good indicator of their accuracy (Kahneman, 2011). Yet, many people arrogantly proclaim to have predictive powers, including in low-validity environments such as fantasy football. Indeed, pundits are more likely to be television guests if they are opinionated, clear, and (overly) confident and make big, bold predictions, because they are more entertaining and their predictions seem more compelling [even though they tend to be less accurate than individuals whose thinking is more complex and less decisive; Kahneman (2011); Silver (2012)]. Consider sports pundits like Stephen A. Smith and Skip Bayless who make bold predictions with uber confidence. Optimism and (over)confidence are valued by society (Kahneman, 2011). Nevertheless, true experts know their limits in terms of knowledge and ability to predict.\nFigure 15.1 is a video of sports pundits, Stephen A. Smith and Skip Bayless, making bold statements and incorrect predictions (TotalProSports.com, 2017):\n\n\n\n\n\nFigure 15.1\n\n\nIntuitions tend to be skilled when a) the environment is regular and predictable, and b) there is opportunity to learn the regularities, cues, and contingencies through extensive practice (Kahneman, 2011). Example domains that meet these conditions supporting intuition include activities such as chess, bridge, and poker, and occupations such as medical providers, athletes, and firefighters. By contrast, fantasy football and other domains such as stock-picking, clinical psychology, and other long-terms forecasts are low-validity environments that are irregular and unpredictable. In environments that do not have stable regularities, intuition cannot be trusted (Kahneman, 2011).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-humansVsComputers",
    "href": "actuarial.html#sec-humansVsComputers",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.4 Humans Versus Computers",
    "text": "15.4 Humans Versus Computers\n\n15.4.1 Advantages of Computers\nHere are some advantages of computers over humans, including “experts”:\n\nComputers can process lots of information simultaneously. So can humans. But computers can to an even greater degree.\nComputers are faster at making calculations.\nGiven the same input, a formula will give the exact same result everytime. Humans’ judgment tends to be inconsistent both across raters and within rater across time, when trying to make judgments or predictions from complex information (Kahneman, 2011). As noted in Section 8.9.3, reliability sets the upper bound for validity, so unreliable judgments cannot be accurate (i.e., valid).\nComputations by computers are error-free (as long as the computations are programmed correctly).\nComputers’ judgments will not be biased by fatigue or emotional responses.\nComputers’ judgments will tend not to be biased in the way that humans’ cognitive biases are. Computers are less likely to be overconfident in their judgments.\nComputers can more accurately weight the set of predictors based on large data sets. Humans tend to give too much weight to singular predictors. Experts may attempt to be clever and to consider complex combinations of predictors, but doing so often reduces validity (Kahneman, 2011). Simple combinations of predictions (e.g., with few predictors and/or predictors of equal weight) often outperform more complex combinations (Dawes, 1979; Kahneman, 2011).\n\n15.4.2 Advantages of Humans\nComputers are bad at some things too. Here are some advantages of humans over computers (as of now):\n\nHumans can be better at identifying patterns in data (but also can mistakenly identify patterns where there are none—i.e., illusory correlation).\nHumans can be flexible and take a different approach if a given approach is not working.\nHumans are better at tasks requiring creativity and imagination, such as developing theories that explain phenomena.\nHumans have the ability to reason, which is especially important when dealing with complex, abstract, or open-ended problems, or problems that have not been faced before (or for which we have insufficient data).\nHumans are better able to learn.\nHumans are better at holistic, gestalt processing, including facial and linguistic processing.\n\nThere may be situations in which a human judgment would do better than an actuarial judgment. One situation where human judgment would be important is when no actuarial method exists for the judgment or prediction. For instance, when no actuarial method exists for the diagnosis or disorder (e.g., suicide), it is up to the clinician/expert. However, we could collect data on the outcomes or on experts’ judgments to develop an actuarial method that will be more reliable than the experts’ judgments. That is, an actuarial method developed based on experts’ judgments will be more accurate than experts’ judgments. In other words, we do not necessarily need outcome data to develop an actuarial method. We could use the player’s data as predictors of the experts’ judgments to develop a structured approach to prediction that weighs factors similarly to experts, but with more reliable predictions.\nAnother situation in which human judgment could outperform a statistical algorithm is in true “broken leg” cases, e.g., important and rare events (edge cases) that are not yet accounted for by the algorithm.\nAnother situation in which human judgment could be preferable is if advanced, complex theories exist. Computers have a difficult time adhering to complex theories, so clinicians/experts may be better suited. However, we do not have any of these complex theories in psychology that are accurate. We would need strong theory informed by data regarding causal influences, and accurate measures to assess them. However, no theories in psychology are that good. Nevertheless, predictive accuracy can be improved when considering theory (Garb & Wood, 2019; Silver, 2012).\nIf the prediction requires complex configural relations that a computer will have a difficult time replicating, an expert’s judgment may be preferred. Although the likelihood that a person can accurately work through these complex relations is theoretically possible, it is highly unlikely. Holistic pattern recognition (such as language and faces) tends to be better by humans than computers. But computers are getting better with holistic pattern recognition through machine learning.\nIn sum, the human seeks to integrate information to make a decision, but is biased.\n\n15.4.3 Comparison of Evidence\nHundreds of studies have examined clinical versus actuarial prediction methods across many disciplines. Despite the romantic notion that professional scouts and experts being able to make accurate predictions (and identify strong players), findings consistently show that actuarial methods are as accurate or more accurate than human judgment/prediction methods (Ægisdóttir et al., 2006; Baird & Wagner, 2000; Dawes et al., 1989; Grove et al., 2000; Grove & Meehl, 1996). “There is no controversy in social science that shows such a large body of qualitatively diverse studies coming out so uniformly…as this one” (Meehl, 1986, pp. 373–374). In general, actuarial methods tend to be about 10% more accurate than human judgment (Grove et al., 2000). Thus, actuarial methods have incremental validity over human judgment. As noted earlier, the superiority of actuarial approaches to clinical judgment tends to hold even when the expert is given more information than the actuarial approach (Dawes et al., 1989). In addition, actuarial predictions outperform human judgment even when the human is given the result of the actuarial prediction (Kahneman, 2011). Allowing clinicians/experts to override actuarial predictions consistently leads to lower predictive accuracy (Garb & Wood, 2019).\nActuarial methods are particularly valuable for criterion-referenced assessment tasks, in which the aim is to predict specific events or outcomes (Garb & Wood, 2019). For instance, actuarial methods have shown promise in predicting violence, criminal recidivism, psychosis onset, course of mental disorders, treatment selection, treatment failure, suicide attempts, and suicide (Garb & Wood, 2019). Actuarial methods are especially important to use in low-validity environments (like fantasy football) in which there is considerable uncertainty and unpredictability (Kahneman, 2011). Actuarial methods may outperform human judgment in low-validity environments because formulas are better able to detect weakly valid cues and to maintain a modest degree of accuracy using such cues consistently (Kahneman, 2011).\nMoreover, actuarial methods are explicit; they can be transparent and lead to informed scientific criticism to improve them. By contrast, human judgment methods are not typically transparent; human judgment relies on mental processes that are often difficult to specify.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-whyHumanJudgmentIsMoreWidelyUsed",
    "href": "actuarial.html#sec-whyHumanJudgmentIsMoreWidelyUsed",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.5 Why Judgment is More Widely Used Than Statistical Formulas",
    "text": "15.5 Why Judgment is More Widely Used Than Statistical Formulas\nDespite actuarial methods being generally more accurate than human judgment, judgment is much more widely used by clinicians/experts. There are several reasons why actuarial methods have not caught on; one reason is professional traditions. Experts in any field do not like to think that a computer could outperform them. Some practitioners argue that judgment/prediction is an “art form” and that using a statistical formula is treating people like a number. However, using an approach (i.e., human judgment) that systematically leads to less accurate decisions and predictions is an ethical problem.\nSome experts do not think that group averages (e.g., in terms of which treatment is most effective) apply to an individual client or player. This invokes the distinction between nomothetic (group-level) inferences and idiographic (individual-level) inferences. However, the scientific evidence and probability theory strongly indicate that it is better to generalize from group-level evidence than throwing out all the evidence and taking the approach of “anything goes.” Clinicians frequently believe the broken leg fallacy, i.e., thinking that your client is an exception to the algorithmic prediction. In most cases, deviating from the statistical formula will result in less accurate predictions. People tend to overestimate the probability of low base rate conditions and events.\nAnother reason why actuarial methods have not caught on is the belief that receiving a treatment is the only thing that matters. But it is an empirical question which treatment is most effective for whom. What if we could do better? For example, we could potentially use a formula to identify the most effective treatment for a client. Some treatments are no better than placebo; other treatments are actually harmful (Lilienfeld, 2007; Williams et al., 2021).\nAnother reason why judgment methods are more widely used than actuarial methods is that so-called “experts” (and people in general) show overconfidence in their predictions—clinicians, experts, and humans in general think they are more accurate than they actually are. We see this when examining their calibration; their predictions tend to be miscalibrated (this is also the case with fantasy football projections, as described in Section Section 17.12). For example, things they report with 80% confidence occur less than 80% of the time, an example of overprecision in their predictions. Humans will sometimes be correct by chance, and they tend to mis-attribute that to their skill; humans tend to remember the successes and forget the failures.\nAnother argument against using actuarial methods is that “no methods exist”. In some cases, that is true—actuarial methods do not yet exist for some prediction problems. However, one can always create an algorithm of the experts’ judgments, even if one does not have access to the outcome information. A model of experts’ responses tends to be more accurate than experts’ judgments themselves because the model gives the same outcome with the same input data—i.e., it is perfectly reliable.\nAnother argument from some clinicians is that, “My job is to understand, not to predict”. But what kind of understanding does not involve predictions? Accurate predictions help in understanding. Knowing how people would perform in different conditions is the same thing as good understanding.\nAnother potential reason clinicians may not use actuarial judgment is time. This can be a legitimate challenge to using actuarial models in some contexts. Actuarial methods can take time to apply—they require assessing the relevant criteria, putting the assessment scores into an actuarial formula, and interpreting the output score. In some clinical situations, such as risk assessments in terms of whether someone is imminently suicidal or homicidal and how to proceed, clinicians must make immediate in-session decisions (Jacinto et al., 2018). Thus, it can be challenging or impractical in some situations to conduct all of the relevant assessments needed to apply actuarial approaches. Moreover, in some workplace settings, such as health care settings, clinicians may be pushed beyond their limits in terms of productivity expectations. In these situations, one possible approach is to exercise one’s best possible judgment in the moment based on the available evidence [i.e., intuitive decision-making processes; Jacinto et al. (2018)], and to check it against an actuarial model after the fact. This is like comparing your decisions in a chess match, after the game ends, to how a computer would have moved in each of the situations. It is valuable to receive feedback on the accuracy of one’s judgments and, when time permits, to evaluate one’s intuitive decision-making against analytical decision-making. However, there are structured approaches to assessing suicide risk that are quick to administer (e.g., Posner et al., 2011).\nNevertheless, the lack of time is perhaps less of a concern in the sports domain. Teams have lots of data readily available on many players. And, for player selection, teams have considerable time to implement statistical models of how players will perform to inform their selections. There is less time during games to make in-game decisions and adjustments from complicated models; that said, teams are gaining greater adeptness with making in-game adjustments and decisions based on analytics. Teams use actuarial charts that inform, for instance, when to go for it on fourth down. Now that many teams are motivated to use analytics to gain a leg up on their competitors, they are finding ways to make the time for it. Clinicians and other medical experts, however, still have considerable room for improvement in terms of their adoption of actuarial approaches to judgment and prediction.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-stepsActuarial",
    "href": "actuarial.html#sec-stepsActuarial",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.6 Steps to Conduct Actuarial Approaches",
    "text": "15.6 Steps to Conduct Actuarial Approaches\nHere are several steps to conduct actuarial approaches (Den Hartigh et al., 2018; Kahneman, 2011):\n\nDetermine a set of relevant variables to measure\nDetermine how you will combine the variables\n\nDo some variables have more weight than other variables?\n\n\nDetermine how the variables will be scored\n\ne.g., a 7-point likert scale\n\n\nCombine the scores based on the pre-specified formula\nUse the final score to make your prediction (for selecting players)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-challengesDataDrivenApproaches",
    "href": "actuarial.html#sec-challengesDataDrivenApproaches",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.7 Challenges of Data-Driven Approaches and How to Address",
    "text": "15.7 Challenges of Data-Driven Approaches and How to Address\nThere are various challenges of data-driven approaches (e.g., machine learning). First, they are sometimes not interpretable or consistent with theory. Sometimes that may because are theories are wrong; other times, it may mean we need to refine our data-driven approach. Second, data-driven approaches tend to overfit the data. Overfitting is described in Section 11.7. Third, as a result of overfitting the data, data-driven approaches tend to show shrinkage.\n\n15.7.1 Shrinkage\nIn general, there is often shrinkage of estimates from training data set to a test data set. Shrinkage is when variables with stronger predictive power in the original data set tend to show somewhat smaller predictive power (smaller regression coefficients and resulting accuracy metrics, such as \\(R^2\\)) when applied to new groups. Shrinkage reflects a model overfitting—i.e., when the model explains error variance by capitalizing on chance. Shrinkage is especially likely when the original sample is small and/or unrepresentative and the number of variables considered for inclusion is large. To help minimize the extent of shrinkage, it is recommended to apply cross-validation.\n\n15.7.2 Cross-Validation\nCross-validation with large, representative samples can help evaluate the amount of shrinkage of estimates, particularly for more complex models such as machine learning models (Ursenbach et al., 2019). Ideally, cross-validation would be conducted with a separate sample (external cross-validation) to see the generalizability of estimates. However, you can also do internal cross-validation. For example, you can perform k-fold cross-validation, where you:\n\nsplit the data set into k groups\nfor each unique group:\n\ntake the group as a hold-out data set (also called a test data set)\ntake the remaining groups as a training data set\nfit a model on the training data set and evaluate it on the test data set\nafter all k-folds have been used as the test data set, and all models have been fit, you average the estimates across the models, which presumably yields more robust, generalizable estimates",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-bestActuarialApproaches",
    "href": "actuarial.html#sec-bestActuarialApproaches",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.8 Best Actuarial Approaches to Prediction",
    "text": "15.8 Best Actuarial Approaches to Prediction\nThe best actuarial models tend to be relatively simple (parsimonious), that can account for one or several of the most important predictors and their optimal weightings, and that account for the base rate of the phenomenon. Multiple regression and/or prior literature can be used to identify the weights of various predictors. Even unit-weighted formulas (formulas whose predictor variables are equally weighted with a weight of one) can sometimes generalize better to other samples than complex weightings (Garb & Wood, 2019; Kahneman, 2011). Differential weightings sometimes capture random variance and over-fit the model, thus leading to predictive accuracy shrinkage in cross-validation samples (Garb & Wood, 2019), as described below. The choice of predictor variables often matters more than their weighting.\nAn emerging technique that holds promise for increasing predictive accuracy of actuarial methods is machine learning (Garb & Wood, 2019). However, one challenge of some machine learning techniques is that they are like a “black box” and are not transparent, which raises ethical concerns (Garb & Wood, 2019). Moreover, machine learning also tends to lead to overfitting and shrinkage. machine learning may be most valuable when the data available are complex and there are many predictor variables (Garb & Wood, 2019), and when the model is validated with cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-conclusion-actuarial",
    "href": "actuarial.html#sec-conclusion-actuarial",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.9 Conclusion",
    "text": "15.9 Conclusion\nIn general, it is better to develop and use structured, actuarial approaches than informal approaches that rely on human judgment or judgment by “so-called” experts. Actuarial approaches to prediction tend to be as accurate or more accurate than expert judgment. Nevertheless, in many domains, human judgment tends to be much more widely used than actuarial approaches.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-judgmentVsActuarialSessionInfo",
    "href": "actuarial.html#sec-judgmentVsActuarialSessionInfo",
    "title": "15  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n15.10 Session Info",
    "text": "15.10 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.5.1    fastmap_1.2.0     cli_3.6.5        \n [5] tools_4.5.1       htmltools_0.5.8.1 yaml_2.3.10       rmarkdown_2.29   \n [9] knitr_1.50        jsonlite_2.0.0    xfun_0.53         digest_0.6.37    \n[13] rlang_1.1.6       evaluate_1.0.4   \n\n\n\n\n\n\nÆgisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S., Anderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K., Walker, B. S., Cohen, G., & Rush, J. D. (2006). The meta-analysis of clinical judgment project: Fifty-six years of accumulated research on clinical versus statistical prediction. The Counseling Psychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875\n\n\nBaird, C., & Wagner, D. (2000). The relative validity of actuarial- and consensus-based risk assessment systems. Children and Youth Services Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDana, J., & Thomas, R. (2006). In defense of clinical judgment … and mechanical prediction. Journal of Behavioral Decision Making, 19(5), 413–428. https://doi.org/10.1002/bdm.537\n\n\nDawes, R. M. (1979). The robust beauty of improper linear models in decision making. American Psychologist, 34(7), 571–582. https://doi.org/10.1037/0003-066X.34.7.571\n\n\nDawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus actuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nGarb, H. N., & Wood, J. M. (2019). Methodological advances in statistical prediction. Psychological Assessment, 31(12), 1456–1466. https://doi.org/10.1037/pas0000673\n\n\nGrove, W. M., & Meehl, P. E. (1996). Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical–statistical controversy. Psychology, Public Policy, and Law, 2(2), 293–323. https://doi.org/10.1037/1076-8971.2.2.293\n\n\nGrove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C. (2000). Clinical versus mechanical prediction: A meta-analysis. Psychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19\n\n\nJacinto, S. B., Lewis, C. C., Braga, J. N., & Scott, K. (2018). A conceptual model for generating and validating in-session clinical judgments. Psychotherapy Research, 28(1), 91–105. https://doi.org/10.1080/10503307.2016.1169329\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nLilienfeld, S. O. (2007). Psychological treatments that cause harm. Perspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1986). Causes and effects of my disturbing little book. Journal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6\n\n\nPosner, K., Brown, G. K., Stanley, B., Brent, D. A., Yershova, K. V., Oquendo, M. A., Currier, G. W., Melvin, G. A., Greenhill, L., Shen, S., & Mann, J. J. (2011). The Columbia–Suicide Severity Rating Scale: Initial validity and internal consistency findings from three multisite studies with adolescents and adults. American Journal of Psychiatry, 168(12), 1266–1277. https://doi.org/10.1176/appi.ajp.2011.10111704\n\n\nRice, M. E., Harris, G. T., & Lang, C. (2013). Validation of and revision to the VRAG and SORAG: The Violence Risk Appraisal Guide—Revised (VRAG-R). Psychological Assessment, 25(3), 951–965. https://doi.org/10.1037/a0032878\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.\n\n\nTotalProSports.com. (2017). 10 most ridiculous things ever said by Stephen A. Smith or Skip Bayless. https://www.youtube.com/watch?v=lTjBuEPcLlc\n\n\nUrsenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D., Kosteniuk, J., & Spiteri, R. J. (2019). Scoring algorithms for a computer-based cognitive screening tool: An illustrative example of overfitting machine learning approaches and the impact on estimates of classification accuracy. Psychological Assessment, 31(11), 1377–1382. https://doi.org/10.1037/pas0000764\n\n\nWilliams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., & Sakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific review of evidential value. Clinical Psychology: Science and Practice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "base-rates.html",
    "href": "base-rates.html",
    "title": "16  Base Rates",
    "section": "",
    "text": "16.1 Getting Started\nThis chapter provides an overview of base rates and the important roles they play in prediction.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesGettingStarted",
    "href": "base-rates.html#sec-baseRatesGettingStarted",
    "title": "16  Base Rates",
    "section": "",
    "text": "16.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"psych\")\nlibrary(\"tidyverse\")\n\n\n\n16.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesOverview",
    "href": "base-rates.html#sec-baseRatesOverview",
    "title": "16  Base Rates",
    "section": "\n16.2 Overview",
    "text": "16.2 Overview\nPredicting player performance is a complex prediction task. Performance is probabilistically influenced by many processes, including processes internal to the player in addition to external processes. Moreover, people’s performance occurs in the context of a dynamic system with nonlinear, probabilistic, and cascading influences that change across time. The ever-changing system makes behavior challenging to predict. And, similar to chaos theory, one small change in the system can lead to large differences later on. Moreover, there are important factors to keep in mind when making predictions.\nLet’s consider a prediction example, assuming the following probabilities:\n\nThe probability of contracting HIV is .3%\nThe probability of a positive test for HIV is 1%\nThe probability of a positive test if you have HIV is 95%\n\nWhat is the probability of HIV if you have a positive test?\nAs we will see, the probability is: \\(\\frac{95\\% \\times .3\\%}{1\\%} = 28.5\\%\\). So based on the above probabilities, if you have a positive test, the probability that you have HIV is 28.5%. Most people tend to vastly overestimate the likelihood that the person has HIV in this example. Why? Because they do not pay enough attention to the base rate (in this example, the base rate of HIV is .3%). Base rates are important to account for when making predictions and people tend to ignore them, a phenomenon called base rate neglect, which is described in Section 14.5.1.\nIn general, people tend to overestimate the likelihood of low base-rate events (Kahneman, 2011). That is, if the base rate of an event or condition—such as schizophrenia—is low (e.g., ~0.5%), people overestimate the likelihood that a person has schizophrenia when given specific information about the person such as their symptoms and history. As an example of people overestimating the likelihood of low base-rate events, Fox & Tversky (1998) asked National Basketball Association (NBA) fans to estimate the probability that each of 8 teams in the playoff would win the playoffs. The median sum of probability judgments for the eight teams was 240% (Fox & Tversky, 1998; Kahneman, 2011) even though, in reality, the true probabilities must sum to 100%. The base rate for a given team to win is 12.5% (i.e., 1/8), whereas people were giving each team (on average) a 30% chance (i.e., 240/8) to win. The finding suggests that people were evaluating each team individually, considering the reasons why that particular team could win, without properly accounting for the total probability (and the base rate). In addition, people tend to overweight unlikely events in their decisions (Kahneman, 2011).\nPeople tend to make judgments based on the representativeness (i.e., similarity to a prototype, or stereotyping) and availability heuristics rather than the base rate (Kahneman, 2011). For instance, professional scouts often make judgments about players based in part on their build and look—i.e., whethey they look the part—rather than just based on their performance (Kahneman, 2011).\nAnother important phenomenon is that low base-rate events (i.e., unlikely events) are difficult to predict accurately. Predictions of lower base rate phenomena tend to be lower than predictions of more common phenomena. For instance, predictions of touchdowns (less common) tend to be less accurate than predictions of yardage (more common).\nIn this chapter, we describe ways to account for base rates in judgments and predictions.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-probability",
    "href": "base-rates.html#sec-probability",
    "title": "16  Base Rates",
    "section": "\n16.3 Issues Around Probability",
    "text": "16.3 Issues Around Probability\n\n16.3.1 Types of Probabilities\nIt is important to distinguish between different types of probabilities: marginal probabilities, joint probabilities, and conditional probabilities.\n\n16.3.1.1 Base Rate (Marginal Probability)\nThe base rate is a marginal probability, which is the general probability of an event irrespective of other things. For instance, the base rate of HIV is the probability of developing HIV. In the U.S., the prevalence rate of HIV is ~0.4% of the adult population [AIDSVu (2022); archived at https://perma.cc/8GE6-GAPC].\nFor instance, we can consider the following marginal probabilities:\n\\(P(C_i)\\) is the probability (i.e., base rate) of a classification, \\(C\\), independent of other things. A base rate is often used as the “prior probability” in a Bayesian model. In our example above, \\(P(C_i)\\) is the base rate (i.e., prevalence) of HIV in the population: \\(P(\\text{HIV}) = .3\\%\\). \\(P(R_i)\\) is the probability (base rate) of a response, \\(R\\), independent of other things. In the example above, \\(P(R_i)\\) is the base rate of a positive test for HIV: \\(P(\\text{positive test}) = 1\\%\\). The base rate of a positive test is known as the positivity rate or selection ratio.\n\n16.3.1.2 Joint Probability\nA joint probability is the probability of two (or more) events occurring simultaneously. For instance, the probability of events \\(A\\) and \\(B\\) both occurring together is \\(P(A, B)\\). A joint probability can be calculated using the marginal probability of each event, as in Equation 16.1:\n\\[\nP(A, B) = P(A) \\cdot P(B)\n\\tag{16.1}\\]\nConversely (and rearranging the terms for the calculation of conditional probability), a joint probability can also be calculated using the conditional probability and marginal probability, as in Equation 16.2:\n\\[\nP(A, B) = P(A | B) \\cdot P(B)\n\\tag{16.2}\\]\n\n16.3.1.3 Conditional Probability\nA conditional probability is the probability of one event occurring given the occurrence of another event. Conditional probabilities are written as: \\(P(A | B)\\). This is read as the probability that event \\(A\\) occurs given that event \\(B\\) occurred. For instance, we can consider the following conditional probabilities:\n\\(P(C | R)\\) is the probability of a classification, \\(C\\), given a response, \\(R\\). In other words, \\(P(C | R)\\) is the probability of having HIV given a positive test: \\(P(\\text{HIV} | \\text{positive test})\\). \\(P(R | C)\\) is the probability of a response, \\(R\\), given a classification, \\(C\\). In the example above, \\(P(R | C)\\) is the probability of having a positive test given that a person has HIV: \\(P(\\text{positive test} | \\text{HIV}) = 95\\%\\).\nA conditional probability can be calculated using the joint probability and marginal probability (base rate), as in Equation 16.3:\n\\[\nP(A, B) = P(A | B) \\cdot P(B)\n\\tag{16.3}\\]\n\n16.3.2 Confusion of the Inverse\nA conditional probability is not the same thing as its reverse (or inverse) conditional probability. Unless the base rate of the two events (\\(C\\) and \\(R\\)) are the same, \\(P(C | R) \\neq P(R | C)\\). However, people frequently make the mistake of thinking that two inverse conditional probabilities are the same. This mistake is known as the “confusion of the inverse”, or the “inverse fallacy”, or the “conditional probability fallacy”. The confusion of inverse probabilities is the logical error of representative thinking that leads people to assume that the probability of \\(C\\) given \\(R\\) is the same as the probability of \\(R\\) given C, even though this is not true. As a few examples to demonstrate the logical fallacy, if 93% of breast cancers occur in high-risk women, this does not mean that 93% of high-risk women will eventually get breast cancer. As another example, if 77% of car accidents take place within 15 miles of a driver’s home, this does not mean that you will get in an accident 77% of times you drive within 15 miles of your home.\nWhich car is the most frequently stolen? It is often the Honda Accord or Honda Civic—probably because they are among the most popular/commonly available cars. The probability that the car is a Honda Accord given that a car was stolen (\\(p(\\text{Honda Accord } | \\text{ Stolen})\\)) is what the media reports and what the police care about. However, that is not what buyers and car insurance companies should care about. Instead, they care about the probability that the car will be stolen given that it is a Honda Accord (\\(p(\\text{Stolen } | \\text{ Honda Accord})\\)).\nApplied to fantasy football, the probability that a given player will be injured given that he is a Running Back (\\(p(\\text{Injured } | \\text{ RB})\\)) is not the same as the probability that a given player is a Running Back given that he is injured (\\(p(\\text{RB } | \\text{ Injured})\\)).\nTo calculate the probability of the inverse conditional, we can leverage Bayesian statistics to calculate the probability of the inverse conditional given a conditional probability (i.e., likelihood) and the marginal probabilities (base rates) of both events. Bayesian statistics is another branch of statistics and is different from frequentist statistics and null hypothesis significance testing. Bayesian statistics is based on Bayes’ theorem, which allows updating our probability estimates based on prior information (e.g., base rate) and new data.\n\n16.3.3 Bayes’ Theorem\n\n16.3.3.1 Standard Formulation\nAn alternative way of calculating a conditional probability is using the inverse conditional probability (instead of the joint probability). This is known as Bayes’ theorem. Bayes’ theorem can help us calculate a conditional probability of some classification, \\(C\\), given some response, \\(R\\), if we know the inverse conditional probability and the base rate (marginal probability) of each. Bayes’ theorem is in Equation 16.4:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}\n\\end{aligned}\n\\tag{16.4}\\]\nOr, equivalently (rearranging the terms):\n\\[\n\\begin{aligned}\n  \\frac{P(C | R)}{P(R | C)} = \\frac{P(C_i)}{P(R_i)}\n\\end{aligned}\n\\tag{16.5}\\]\nOr, equivalently (rearranging the terms):\n\\[\n\\begin{aligned}\n  \\frac{P(C | R)}{P(C_i)} = \\frac{P(R | C)}{P(R_i)}\n\\end{aligned}\n\\tag{16.6}\\]\nMore generally, Bayes’ theorem has been described as:\n\\[\n\\begin{aligned}\n  P(H | E) &= \\frac{P(E | H) \\cdot P(H)}{P(E)} \\\\\n  \\text{posterior probability} &= \\frac{\\text{likelihood} \\times \\text{prior probability}}{\\text{model evidence}}\n\\end{aligned}\n\\tag{16.7}\\]\nwhere \\(H\\) is the hypothesis, and \\(E\\) is the evidence—the new information that was not used in computing the prior probability.\nIn Bayesian terms, the posterior probability is the conditional probability of one event occurring given another event—it is the updated probability after the evidence is considered. In this case, the posterior probability is the probability of the classification occurring (\\(C\\)) given the response (\\(R\\)). The likelihood is the inverse conditional probability—the probability of the response (\\(R\\)) occurring given the classification (\\(C\\)). The prior probability is the marginal probability of the event (i.e., the classification) occurring, before we take into account any new information. The model evidence is the marginal probability of the other event occurring—i.e., the marginal probability of seeing the evidence.\nBayes’ theorem provides the foundation for a paradigm of statistics called Bayesian statistics, which (unlike frequentist statistics) does not use p-values.\nIn the HIV example above, we can calculate the conditional probability of HIV given a positive test using three terms: the conditional probability of a positive test given HIV (i.e., the sensitivity of the test), the base rate of HIV, and the base rate of a positive test for HIV. The conditional probability of HIV given a positive test is in Equation 16.8:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{HIV} | \\text{positive test}) &= \\frac{P(\\text{positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{positive test})} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{base rate of HIV}}{\\text{base rate of positive test}} \\\\\n  &= \\frac{95\\% \\times .3\\%}{1\\%} = \\frac{.95 \\times .003}{.01}\\\\\n  &= 28.5\\%\n\\end{aligned}\n\\tag{16.8}\\]\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .95,\n  pA = .003,\n  pB = .01)\n\n[1] 0.285\n\n\nThus, assuming the probabilities in the example above, the conditional probability of having HIV if a person has a positive test is 28.5%. Given a positive test, chances are higher than not that the person does not have HIV.\nNow let’s see what happens if the person tests positive a second time. We would revise our “prior probability” for HIV from the general prevalence in the population (0.3%) to be the “posterior probability” of HIV given a first positive test (28.5%). This is known as Bayesian updating. We would also update the “evidence” to be the marginal probability of getting a second positive test.\nIf we do not know a marginal probability (i.e., base rate) of an event (e.g., getting a second positive test), we can calculate a marginal probability with the law of total probability using conditional probabilities and the marginal probability of another event (e.g., having HIV). According to the law of total probability, the probability of getting a positive test is the probability that a person with HIV gets a positive test (i.e., sensitivity) times the base rate of HIV plus the probability that a person without HIV gets a positive test (i.e., false positive rate) times the base rate of not having HIV, as in Equation 16.9:\n\\[\n\\begin{aligned}\nP(\\text{not } C_i) &= 1 - P(C_i) \\\\\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\\n  1\\% &= 95\\% \\times .3\\% + P(R | \\text{not } C) \\times 99.7\\% \\\\\n\\end{aligned}\n\\tag{16.9}\\]\nIn this case, we know the marginal probability (\\(P(R_i)\\)), and we can use that to solve for the unknown conditional probability that reflects the false positive rate (\\(P(R | \\text{not } C)\\)), as in Equation 16.10:\n\\[\n\\scriptsize\n\\begin{aligned}\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) && \\\\\n  P(R_i) - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &= P(R | C) \\cdot P(C_i) && \\text{Move } P(R | \\text{not } C) \\text{ to the left side} \\\\\n  - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &= P(R | C) \\cdot P(C_i) - P(R_i) && \\text{Move } P(R_i) \\text{ to the right side} \\\\\n  P(R | \\text{not } C) \\cdot P(\\text{not } C_i) &= P(R_i) - [P(R | C) \\cdot P(C_i)] && \\text{Multiply by } -1 \\\\\n  P(R | \\text{not } C) &= \\frac{P(R_i) - [P(R | C) \\cdot P(C_i)]}{P(\\text{not } C_i)} && \\text{Divide by } P(R | \\text{not } C) \\\\\n  &= \\frac{1\\% - [95\\% \\times .3\\%]}{99.7\\%} = \\frac{.01 - [.95 \\times .003]}{.997}\\\\\n  &= .7171515\\% \\\\\n\\end{aligned}\n\\tag{16.10}\\]\nWe can then estimate the marginal probability of the event, substititing in \\(P(R | \\text{not } C)\\), using the law of total probability. The petersenlab package (Petersen, 2025a) contains the petersenlab::pA() function that estimates the marginal probability of one event, \\(A\\).\n\nCodepetersenlab::pA(\n  pAgivenB = .95,\n  pB = .003,\n  pAgivenNotB = .007171515)\n\n[1] 0.01\n\n\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::pBgivenNotA() function that estimates the probability of one event, \\(B\\), given that another event, \\(A\\), did not occur.\n\nCodepetersenlab::pBgivenNotA(\n  pBgivenA = .95,\n  pA = .003,\n  pB = .01)\n\n[1] 0.007171515\n\n\nWith this conditional probability (\\(P(R | \\text{not } C)\\)), the updated marginal probability of having HIV (\\(P(C_i)\\)), and the updated marginal probability of not having HIV (\\(P(\\text{not } C_i)\\)), we can now calculate an updated estimate of the marginal probability of getting a second positive test. The probability of getting a second positive test is the probability that a person with HIV gets a second positive test (i.e., sensitivity) times the updated probability of HIV plus the probability that a person without HIV gets a second positive test (i.e., false positive rate) times the updated probability of not having HIV, as in Equation 16.11:\n\\[\n\\begin{aligned}\n  P(R_{i}) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\\n  &= 95\\% \\times 28.5\\% + .7171515\\% \\times 71.5\\% = .95 \\times .285 + .007171515 \\times .715 \\\\\n  &= 27.58776\\%\n\\end{aligned}\n\\tag{16.11}\\]\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::pB() function that estimates the marginal probability of one event, \\(B\\).\n\nCodepetersenlab::pB(\n  pBgivenA = .95,\n  pA = .285,\n  pBgivenNotA = .007171515)\n\n[1] 0.2758776\n\n\nWe then substitute the updated marginal probability of HIV (\\(P(C_i)\\)) and the updated marginal probability of getting a second positive test (\\(P(R_i)\\)) into Bayes’ theorem to get the probability that the person has HIV if they have a second positive test (assuming the errors of each test are independent, i.e., uncorrelated), as in Equation 16.12:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{HIV} | \\text{a second positive test}) &= \\frac{P(\\text{a second positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{a second positive test})} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{updated base rate of HIV}}{\\text{updated base rate of positive test}} \\\\\n  &= \\frac{95\\% \\times 28.5\\%}{27.58776\\%} \\\\\n  &= 98.14\\%\n\\end{aligned}\n\\tag{16.12}\\]\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .95,\n  pA = .285,\n  pB = .2758776)\n\n[1] 0.9814135\n\n\nThus, a second positive test greatly increases the posterior probability that the person has HIV from 28.5% to over 98%.\nAs seen in the rearranged formula in Equation 16.5, the ratio of the conditional probabilities is equal to the ratio of the base rates. Thus, it is important to consider base rates. People have a strong tendency to ignore (or give insufficient weight to) base rates when making predictions. The failure to consider the base rate when making predictions when given specific information about a case is known as the base rate fallacy or as base rate neglect. For example, people tend to say that the probability of a rare event is more likely than it actually is given specific information.\nAs seen in the rearranged formula in Equation 16.6, the inverse conditional probabilities (\\(P(C | R)\\) and \\(P(R | C)\\)) are not equal unless the base rates of \\(C\\) and \\(R\\) are the same. If the base rates are not equal, we are making at least some prediction errors. If \\(P(C_i) &gt; P(R_i)\\), our predictions must include some false negatives. If \\(P(R_i) &gt; P(C_i)\\), our predictions must include some false positives.\n\n16.3.3.2 Alternative Formulation\nUsing the law of total probability, we can substitute the calculation of the marginal probability (\\(P(R_i)\\)) into Bayes’ theorem to get an alternative formulation of Bayes’ theorem, as in Equation 16.13:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i)} \\\\\n  &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]}\n\\end{aligned}\n\\tag{16.13}\\]\nInstead of using marginal probability (base rate) of \\(R\\), as in the original formulation of Bayes’ theorem, it uses the conditional probability, \\(P(R|\\text{not } C)\\). Thus, it uses three terms: two conditional probabilities—\\(P(R|C)\\) and \\(P(R|\\text{not } C)\\)—and one marginal probability, \\(P(C_i)\\).\nLet us see how the alternative formulation of Bayes’ theorem applies to the HIV example above. We can calculate the probability of HIV given a positive test using three terms: the conditional probability that a person with HIV gets a positive test (i.e., sensitivity), the conditional probability that a person without HIV gets a positive test (i.e., false positive rate), and the base rate of HIV. Using the \\(P(R|\\text{not } C)\\) calculated in Equation 16.10, the conditional probability of HIV given a single positive test is in Equation 16.14:\n\\[\n\\small\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{base rate of HIV}}{\\text{sensitivity of test} \\times \\text{base rate of HIV} + \\text{false positive rate of test} \\times (1 - \\text{base rate of HIV})} \\\\\n  &= \\frac{95\\% \\times .3\\%}{95\\% \\times .3\\% + .7171515\\% \\times (1 - .3\\%)} = \\frac{.95 \\times .003}{.95 \\times .003 + .007171515 \\times (1 - .003)}\\\\\n  &= 28.5\\%\n\\end{aligned}\n\\tag{16.14}\\]\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .003,\n  pBgivenNotA = .007171515)\n\n[1] 0.285\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .003,\n  pBgivenNotA = pBgivenNotA(\n    pBgivenA = .95,\n    pA = .003,\n    pB = .01))\n\n[1] 0.285\n\n\nTo calculate the conditional probability of HIV given a second positive test, we update our priors because the person has now tested positive for HIV. We update the prior probability of HIV (\\(P(C_i)\\)) based on the posterior probability of HIV after a positive test (\\(P(C | R)\\)) that we calculated above. We can calculate the conditional probability of HIV given a second positive test using three terms: the conditional probability that a person with HIV gets a positive test (i.e., sensitivity; which stays the same), the conditional probability that a person without HIV gets a positive test (i.e., false positive rate; which stays the same), and the updated marginal probability of HIV. The conditional probability of HIV given a second positive test is in Equation 16.15:\n\\[\n\\scriptsize\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{updated base rate of HIV}}{\\text{sensitivity of test} \\times \\text{updated base rate of HIV} + \\text{false positive rate of test} \\times (1 - \\text{updated base rate of HIV})} \\\\\n  &= \\frac{95\\% \\times 28.5\\%}{95\\% \\times 28.5\\% + .7171515\\% \\times (1 - 28.5\\%)} = \\frac{.95 \\times .285}{.95 \\times .285 + .007171515 \\times (1 - .285)}\\\\\n  &= 98.14\\%\n\\end{aligned}\n\\tag{16.15}\\]\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .285,\n  pBgivenNotA = .007171515)\n\n[1] 0.9814134\n\nCodepAgivenB(\n  pBgivenA = .95,\n  pA = .285,\n  pBgivenNotA = pBgivenNotA(\n    pBgivenA = .95,\n    pA = .003,\n    pB = .01))\n\n[1] 0.9814134\n\n\n\n16.3.3.3 Interim Summary\nIn sum, the marginal probability, including the prior probability or base rate, should be weighed heavily in predictions unless there are sufficient data to indicate otherwise, i.e., to update the posterior probability based on new evidence. People tend to ignore base rates (i.e., base rate neglect). In general, people tend to a) overestimate the likelihood of low base-rate events and b) overweight low base-rate events in their decisions (Kahneman, 2011). Bayes’ theorem specifies how prior beliefs (i.e., base rate information) should be integrated with the predictive accuracy of the evidence to make predictions. It thus provides a powerful tool to anchor predictions to the base rate unless sufficient evidence changes the posterior probability (by updating the evidence and prior probability). In general, you should anchor your predictions to the base rate and adjust from there. It is also important to question the validity of the evidence (Kahneman, 2011). As noted by Kahneman (2011), if you have doubts about the quality of the evidence for a particular prediction question, keep your predictions close to the base rate, and modify them only modifying them based on the new information.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-cabExample",
    "href": "base-rates.html#sec-cabExample",
    "title": "16  Base Rates",
    "section": "\n16.4 Cab Example",
    "text": "16.4 Cab Example\nBelow is an example:\n\nA cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data:\n\n85% of the cabs in the city are Green and 15% are Blue.\nA witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time.\n\nWhat is the probability that the cab involved in the accident was Blue rather than Green?\n— Kahneman (2011, p. 166)\n\nThus, we know the following:\n\\[\n\\begin{aligned}\n  P(\\text{Blue}) &= .15 && \\text{prior probability of a Blue cab}\\\\\n  P(\\text{Green}) &= .85 && \\text{prior probability of a Green cab}\\\\\n  P(\\text{Correct}|\\text{Blue}) &= .80 && \\text{probability the witness correctly identifies a Blue cab}\\\\\n  P(\\text{Correct}|\\text{Green}) &= .80 && \\text{probability the witness correctly identifies a Green cab}\\\\\n  P(\\text{Incorrect}|\\text{Blue}) &= .20 && \\text{probability the witness incorrectly identifies a Blue cab}\\\\\n  P(\\text{Incorrect}|\\text{Green}) &= .20 && \\text{probability the witness incorrectly identifies a Green cab}\\\\\n\\end{aligned}\n\\tag{16.16}\\]\nWe want to know the probability that the cab involved in the accident was Blue, given that the witness identified it as Blue (\\(P(\\text{Blue}|\\text{Identified as Blue})\\)). To estimate this probability, we can apply Bayes’ theorem to estimate the posterior probability:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}\\\\\n  P(\\text{Blue}|\\text{Identified as Blue}) &= \\frac{P(\\text{Identified as Blue}|\\text{Blue}) \\cdot P(\\text{Blue})}{P(\\text{Identified as Blue})}\n\\end{aligned}\n\\tag{16.17}\\]\nWe can compute the term in the denominator (\\(P(\\text{Identified as Blue})\\)) using the law of total probability (described in Section 16.3.3).\n\\[\n\\begin{aligned}\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i)\\\\\n  P(R_i) &= P(\\text{Identified as Blue}|\\text{Blue}) \\cdot P(\\text{Blue}) + P(\\text{Identified as Blue}|\\text{Green}) \\cdot P(\\text{Green})\\\\\n  0.29 &= (.80 \\times .15) + (.20 \\times .85) \\\\\n\\end{aligned}\n\\tag{16.18}\\]\n\nCodepetersenlab::pA(\n  pAgivenB = .80,\n  pB = .15,\n  pAgivenNotB = .20)\n\n[1] 0.29\n\n\nWe can now substitute this value into the denominator of Bayes’ theorem to estimate the posterior probability:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}\\\\\n  P(\\text{Blue}|\\text{Identified as Blue}) &= \\frac{P(\\text{Identified as Blue}|\\text{Blue}) \\cdot P(\\text{Blue})}{P(\\text{Identified as Blue})}\\\\\n  0.414 &= \\frac{0.80 \\times 0.15}{0.29}\n\\end{aligned}\n\\tag{16.19}\\]\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .80,\n  pA = .15,\n  pB = .29)\n\n[1] 0.4137931\n\n\nThus, there was a 41.4% probability that the car involved in the accident was Blue rather than Green. However, when faced with this problem, people tend to ignore the base rate and go with the witness (Kahneman, 2011). According to Kahneman (2011), the most frequent response to this question regarding is that there is an 80% that the car was Blue.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-nateSilverExamples",
    "href": "base-rates.html#sec-nateSilverExamples",
    "title": "16  Base Rates",
    "section": "\n16.5 Nate Silver Examples",
    "text": "16.5 Nate Silver Examples\nSilver (2012) provides several examples that leverage the alternative formulation of Bayes’ theorem provided in Equation 16.13 and summarized below:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot [1 - P(C_i)]}\n\\end{aligned}\n\\tag{16.20}\\]\nIn each example, the formula uses three elements to calculate the probability that the hypothesis is true:\n\nthe conditional probability the likelihood of observing the evidence, \\(R\\), given that the hypothesis, \\(C\\), is true (i.e., \\(P(R|C)\\); true positive rate)\nthe conditional probability the likelihood of observing the evidence, \\(R\\), given that the hypothesis, \\(C\\), is false (i.e., \\(P(R | \\text{not } C)\\); false positive rate)\nthe marginal probability (base rate) of the event occurring (i.e., the prior probability of the hypothesis, \\(C\\), being true; \\(P(C_i)\\))\n\nThus, the formula uses the base rate, the true positive rate (sensitivity), and the false positive rate. The ratio of the true positive rate to the false positive rate is called the positive likelihood ratio, and is used in Bayesian updating.\n\n16.5.1 Example 1: Is Your Partner Cheating on You?\nExample 1: You came home and found a strange pair of underwear in your underwear drawer. What is the probability that your partner is cheating on you?\n\nthe prior probability that your partner is cheating on you: 4%\nthe conditional probability of underwear appearing given that your partner is cheating on you: 50%\nthe conditional probability of underwear appearing given that your partner is not cheating on you: 5%\n\n\nCodepAgivenB(\n  pBgivenA = .50,\n  pA = .04,\n  pBgivenNotA = .05)\n\n[1] 0.2941176\n\n\n\n16.5.2 Example 2: Does a Person Have Breast Cancer?\nExample 2: What is the probability that a woman in her 40s has breast cancer if she tested positive on a mammogram?\n\nthe prior probability that she has breast cancer: 1.4%\nthe conditional probability that she has a positive test given that she has breast cancer: 75%\nthe conditional probability that she has a positive test given that she does not have breast cancer: 10%\n\n\nCodepAgivenB(\n  pBgivenA = .75,\n  pA = .014,\n  pBgivenNotA = .10)\n\n[1] 0.09624198\n\n\n\n16.5.3 Example 3: Was it a Terrorist Attack?\n\n16.5.3.1 Example 3A: The First Plane Hit the World Trade Center\nExample 3A: Consider the information we had on 9/11 when the first plane hit the World Trade Center. What is the probability that a terror attack occurred given that the first plane hit the World Trade Center?\n\nthe prior probability that terrorists crash a plane into a Manhattan skyscraper: 0.005%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are attacking Manhattan skyscrapers: 100%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are not attacking Manhattan skyscrapers (i.e., it is an accident): 0.008%\n\n\nCodepAgivenB(\n  pBgivenA = 1,\n  pA = .00005,\n  pBgivenNotA = .00008)\n\n[1] 0.3846272\n\n\n\n16.5.3.2 Example 3B: The Second Plane Hit the World Trade Center\nExample 3B: Now, consider that a second plane just hit the World Trade Center. What is the probability that a terror attack occurred given that a second plane hit the World Trade Center?\n\nthe revised prior probability that terrorists crash a plane into a Manhattan skyscraper (from Example 3A): 38.46272%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are attacking Manhattan skyscrapers: 100%\nthe conditional probability that a plane crashes into a Manhattan skyscraper if terrorists are not attacking Manhattan skyscrapers (i.e., it is an accident): 0.008%\n\n\nCodepAgivenB(\n  pBgivenA = 1,\n  pA = .3846272,\n  pBgivenNotA = .00008)\n\n[1] 0.999872",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRateFantasyFootball",
    "href": "base-rates.html#sec-baseRateFantasyFootball",
    "title": "16  Base Rates",
    "section": "\n16.6 Base Rates Applied to Fantasy Football",
    "text": "16.6 Base Rates Applied to Fantasy Football\nBase rates are also relevant to fantasy football. Unlike yardage (e.g., passing yards, rushing yards, receiving yards), touchdowns occur relatively less frequently. Whereas a solid Wide Receiver may log 100+ receptions and 1,200+ yards in a season, they may have “only” 8–14 receiving touchdowns in a given season. As noted in Section 17.6.6, lower base-rate events—including touchdowns—are harder to predict accurately. As noted by Harris (2012): “NFL statistical projections are basically impossible to get right. (Take it from someone who helps create them for a living.) Yes, we can do a passable job with yardage totals for players who don’t suffer unexpected injuries or depth-chart pratfalls. But so much of fantasy football hinges on touchdowns, and touchdowns are impossibly difficult to predict from season to season (let alone week to week).” (archived at https://perma.cc/4QNH-J2LD). Thus, it is important not to lend too much credence to predictions of touchdowns. Focus on other things that may be more predictable (and that may be indirectly prognostic of touchdowns) such as yards, carries/targets, receptions, depth of targets, red zone carries/targets, short distance carries/targets, etc.\nIn Section 17.6.6, we evaluate the accuracy of predicting touchdowns versus yardage.\nWhen dealing with numeric predictions (rather than categorical outcomes), the equivalent of the base rate is the average value. For instance, the “base rate” of fantasy points for a given position is the average number of fantasy points for that position. We could subdivide even further to identify, for instance, the “base rate” of fantasy points for the Wide Receiver at the top of the depth chart on a team.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRateRookiePerformance",
    "href": "base-rates.html#sec-baseRateRookiePerformance",
    "title": "16  Base Rates",
    "section": "\n16.7 Base Rate of Rookie Performance",
    "text": "16.7 Base Rate of Rookie Performance\nWe examine the base rates of rookie performance compared to performance of non-rookies. Rookies who play tend to be better players who were drafted in early rounds of the National Football League (NFL) draft. Thus, to more fairly compare rookies and non-rookies, we examine only players who were drafted in the first two rounds of the NFL Draft.\n\nCoderookiesDraftedEarly &lt;- player_stats_seasonal %&gt;% \n  filter(years_of_experience == 0) %&gt;% \n  filter(draftround %in% c(1,2)) %&gt;% \n  arrange(player_id, season) %&gt;% \n  group_by(player_id) %&gt;% \n  slice_head(n = 1) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    rookie = 0,\n    rookieLabel = \"Rookie\"\n  ) %&gt;% \n  arrange(player_display_name)\n\nnonRookiesDraftedEarly &lt;- player_stats_seasonal %&gt;% \n  filter(years_of_experience &gt; 0) %&gt;% \n  filter(draftround %in% c(1,2)) %&gt;% \n  mutate(\n    rookie = 1,\n    rookieLabel = \"Non-rookie\"\n  ) %&gt;% \n  arrange(player_display_name)\n\nplayersDraftedEarly &lt;- bind_rows(\n  rookiesDraftedEarly,\n  nonRookiesDraftedEarly\n) %&gt;% \n  mutate(rookieLabel = factor(rookieLabel, levels = c(\"Rookie\",\"Non-rookie\")))\n\n\n\nCodeggplot2::ggplot(\n  data = playersDraftedEarly %&gt;% filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  aes(\n    x = fantasyPoints)) + \n  geom_density(\n    aes(\n      fill = rookieLabel),\n    alpha = 0.5) +\n  facet_wrap( ~ position) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Fantasy Points for Rookies Versus Non-Rookies by Position\"\n  ) +\n  theme_bw() + \n  theme(\n    axis.title.y = element_text(angle = 0, vjust = 0.5), # horizontal y-axis title\n    legend.title = element_blank()) # remove legend title\n\n\n\n\n\n\nFigure 16.1: Density Plot of Fantasy Points for Rookies Versus Non-Rookies by Position Among Players Who Were Drafted in the First Two Rounds of the National Football League Draft.\n\n\n\n\n\n16.7.1 Quarterbacks\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"QB\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"QB\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"QB\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  hist()\n\n\n\n\n\n\nFigure 16.2: Histogram of Fantasy Points Among Rookie Quarterbacks Who Were Drafted in the First Two Rounds of the National Football League Draft.\n\n\n\n\nAmong Quarterbacks who played a full season:\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"QB\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"QB\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\n16.7.2 Running Backs\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  select(rushing_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  select(rushing_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  hist()\n\n\n\n\n\n\nFigure 16.3: Histogram of Fantasy Points Among Rookie Running Backs Who Were Drafted in the First Two Rounds of the National Football League Draft.\n\n\n\n\nAmong Running Backs who played a full season:\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(rushing_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"RB\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(rushing_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\n16.7.3 Wide Receivers\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  select(receiving_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  select(receiving_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  hist()\n\n\n\n\n\n\nFigure 16.4: Histogram of Fantasy Points Among Rookie Wide Receivers Who Were Drafted in the First Two Rounds of the National Football League Draft.\n\n\n\n\nAmong Wide Receivers who played a full season:\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(receiving_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"WR\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(receiving_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\n16.7.4 Tight Ends\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  select(receiving_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  select(receiving_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  select(fantasyPoints) %&gt;% \n  hist()\n\n\n\n\n\n\nFigure 16.5: Histogram of Fantasy Points Among Rookie Tight Ends Who Were Drafted in the First Two Rounds of the National Football League Draft.\n\n\n\n\nAmong Tight Ends who played a full season:\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCodenonRookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(receiving_tds) %&gt;% \n  psych::describe()\n\n\n  \n\n\n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(fantasyPoints) %&gt;% \n  psych::describe()\n\n\n  \n\n\nCoderookiesDraftedEarly %&gt;% \n  filter(position == \"TE\") %&gt;%\n  filter(games &gt;= 16) %&gt;% \n  select(receiving_tds) %&gt;% \n  psych::describe()",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-accountForBaseRates",
    "href": "base-rates.html#sec-accountForBaseRates",
    "title": "16  Base Rates",
    "section": "\n16.8 How to Account for Base Rates",
    "text": "16.8 How to Account for Base Rates\nThere are various ways to account for base rates, including the use of actuarial formulas and the use of Bayesian updating.\n\n16.8.1 Actuarial Formula\nOne approach to account for base rates is to use actuarial formulas (rather than human judgment) to make the predictions. Actuarial formulas based on multiple regression or machine learning can account for the base rate of the event.\n\n16.8.2 Bayesian Updating\nAnother approach to account for base rates is to leverage Bayes’ theorem, using Bayesian updating and the probability nomogram. Bayesian updating is a form of anchoring and adjustment; however, unlike the anchoring and adjustment heuristic, it is a systematic approach to anchoring and adjustment that anchors one’s predictions to the base rate, and then adjusts according to new information. That is, we start with a pretest probability (i.e., base rate) and update our predictions based on the extent of new information (i.e., the likelihood ratio).\nBayesian updating can also be applied to continuous outcomes like fantasy points. For an example of applying Bayesian updating to fantasy points, see Braun (2012) (archived at https://web.archive.org/web/20161028142225/http://www.bayesff.com/bayesian101/). Applying Bayes’ theorem to continuous outcomes, the posterior distribution is equal to the prior distribution times the likelihood (data). For instance, we can start with our prior belief (distribution) for a player’s performance based on, for example, average draft position. Then, we observe the Week 1 performance. For instance, if Tom Brady scores 35 points in Week 1, our likelihood represents the likelihood that Tom Brady would score 35 points (the data). Using Bayesian updating, we can then calculate a posterior distribution that represents our new best prediction moving forward for how many points Tom Brady will score in Week 2. We then observe Week 2, generate a new likelihood and posterior distribution, and use that as our new prior distribution for Week 3, and so on. To keep things simple, we use a binary outcome below to demonstrate Bayesian updating.\nTo perform Bayesian updating involves comparing the relative probability of two outcomes, \\(P(C | R)\\) versus \\(P(\\text{not } C | R)\\). If we want to compare the relative probability of two outcomes, we can use the odds form of Bayes’ theorem, as in Equation 16.21:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{not } C | R) &= \\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)} \\\\\n  \\frac{P(C | R)}{P(\\text{not } C | R)} &= \\frac{\\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}}{\\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)}} \\\\\n  &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)} \\\\\n  &= \\frac{P(C_i)}{P(\\text{not } C_i)} \\times \\frac{P(R | C)}{P(R | \\text{not } C)} \\\\\n  \\text{posterior odds} &= \\text{prior odds} \\times \\text{likelihood ratio}\n\\end{aligned}\n\\tag{16.21}\\]\nAs presented in Equation 16.21, the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. Below, we describe the likelihood ratio.\n\n16.8.2.1 Diagnostic Likelihood Ratio\nA likelihood ratio is the ratio of two probabilities. It can be used to compare the likelihood of two possibilities. The diagnostic likelihood ratio is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect. The diagnostic likelihood ratio is also called the risk ratio. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio.\n\n16.8.2.1.1 Positive Likelihood Ratio (LR+)\nThe positive likelihood ratio (LR+) is the probability that a person with the disease tested positive for the disease (true positive rate) divided by the probability that a person without the disease tested positive for the disease (false positive rate). That is, the positive likelihood ratio (LR+) compares (i.e., is a ratio of) the true positive rate to the false positive rate. Positive likelihood ratio values range from 1 to infinity. Higher values reflect greater accuracy, because it indicates the degree to which a true positive is more likely than a false positive. Testing positive on a test with a high LR+ increases the probability of disease. The formula for calculating the positive likelihood ratio is in Equation 16.22.\n\\[\n\\begin{aligned}\n  \\text{positive likelihood ratio (LR+)} &= \\frac{\\text{TPR}}{\\text{FPR}} \\\\\n  &= \\frac{P(R|C)}{P(R|\\text{not } C)} \\\\\n  &= \\frac{P(R|C)}{1 - P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n\\end{aligned}\n\\tag{16.22}\\]\n\n16.8.2.1.2 Negative Likelihood Ratio (LR−)\nThe negative likelihood ratio (LR−) is the probability that a person with the disease tested negative for the disease (false negative rate) divided by the probability that a person without the disease tested negative for the disease (true negative rate). That is, the negative likelihood ratio (LR−) compares (i.e., is a ratio of) the false negative rate to the true negative rate. Negative likelihood ratio values range from 0 to 1. Smaller values reflect greater accuracy, because it indicates that a false negative is less likely than a true negative. Testing negative on a test with a low LR– decreases the probability of disease. The formula for calculating the negative likelihood ratio is in Equation 16.23.\n\\[\n\\begin{aligned}\n  \\text{negative likelihood ratio } (\\text{LR}-) &= \\frac{\\text{FNR}}{\\text{TNR}} \\\\\n  &= \\frac{P(\\text{not } R|C)}{P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{1 - P(R|C)}{P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{1 - \\text{sensitivity}}{\\text{specificity}}\n\\end{aligned}\n\\tag{16.23}\\]\n\n16.8.2.2 Probability Nomogram\nUsing Bayes’ theorem (described in Section 16.3.3), solving for posttest odds (based on pretest odds and the likelihood ratio, as in Equation 16.21), and converting odds to probabilities, we can use a Fagan probability nomogram to determine the posttest probability following a test result. The calculation of posttest (posterior) probability is described in Section 17.6.10.29. In its calculation, the probability nomogram automatically converts the pretest probability (i.e., base rate) to prior (pretest) odds and the posterior (posttest) odds to posterior probability, so you do not have to. A probability nomogram (aka Fagan nomogram) is a way of visually applying Bayes’ theorem to determine the posttest probability of having a condition based on the pretest (or prior) probability and likelihood ratio, as depicted in Figure 16.6. To use a probability nomogram, connect the dots from the starting probability (left line) with the likelihood ratio (middle line) to see the updated probability. The updated (posttest) probability is where the connecting line crosses the third, right line.\n\n\n\n\n\nFigure 16.6: Probability Nomogram. (Figure retrieved from https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Fagan_nomogram.svg/945px-Fagan_nomogram.svg.png).\n\n\nFor instance, if the starting probability is 0.5% and the likelihood ratio is 10 (e.g., sensitivity = .90, specificity = .91: \\(\\text{likelihood ratio} = \\frac{\\text{sensitivity}}{1 - \\text{specificity}} = \\frac{.9}{1-.91} = 10\\)) from a positive test (i.e., positive likelihood ratio), the updated probability is less than 5%, as depicted in Figure 16.7.\nAn interactive probability nomogram is available from Altarejos & Hayward (2025) at the following link: https://jamaevidence.mhmedical.com/data/calculators/LR_nomogram.html (archived at https://perma.cc/Z3SW-QMJ3).\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::posttestProbability() function that estimates the posttest probability of an event, given the pretest probability and the likelihood ratio, or given the pretest probability and the sensitivity (SN) and specificity (SP) of the test.\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  likelihoodRatio = 10)\n\n[1] 0.04784689\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  SN = .90,\n  SP = .91)\n\n[1] 0.04784689\n\n\nThe function can also estimate the posttest probability of an event given the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):\n\nCodepetersenlab::posttestProbability(\n  TP = 450,\n  TN = 90545,\n  FP = 8955,\n  FN = 50)\n\n[1] 0.04784689\n\n\nWe discuss true positives (TP), true negatives (TN), false positives (FP), false negatives (FN), sensitivity (SN), and specificity (SP) in Section 17.6 (Section 17.6.1 and Section 17.6.7).\nIf the starting probability is 0.5% and the likelihood ratio is 0.11 from a negative test (i.e., negative likelihood ratio), the updated probability is nearly indistinguishable from zero (0.05%).\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  likelihoodRatio = 0.11)\n\n[1] 0.0005524584\n\n\n\n\n\n\n\nFigure 16.7: Probability Nomogram Example. (Figure adapted from https://commons.wikimedia.org/wiki/File:Fagan_nomogram.svg. Also provided in: Petersen (2024) and Petersen (2025b).)\n\n\nA probability nomogram calculator is available from Schwartz (2006) at the following link: http://araw.mede.uic.edu/cgi-bin/testcalc.pl (archived at https://perma.cc/X8TF-7YBX). The petersenlab package (Petersen, 2025a) contains the petersenlab::nomogrammer() function that creates a nomogram plot using the positive and negative likelihood ratio or using the sensitivity (SN) and specificity (SP) of the test, as adapted from Chekroud (2017):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .005,\n  SN = 0.90,\n  SP = 0.91)\n\n\n\n\n\n\n\nThe blue line indicates the posterior probability of the condition given a positive test. The pink line indicates the posterior probability of the condition given a negative test.\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .005,\n  PLR = 10,\n  NLR = 0.11)\n\n\n\n\n\n\n\nThe function can also create a nomogram plot using the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):\n\nCodepetersenlab::nomogrammer(\n  TP = 450,\n  TN = 90545,\n  FP = 8955,\n  FN = 50)\n\n\n\n\n\n\n\nThe function can also create a nomogram plot using the sensitivity (SN) and selection rate of the test. Here is a nomogram plot from the HIV example:\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .003,\n  SN = .95,\n  selectionRate = .01\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from the cab example (Kahneman, 2011):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .15,\n  SN = .80,\n  SP = .80\n)\n\n\n\n\n\n\n\nThe function can also create a nomogram plot using the sensitivity (SN) and false positive rate of the test. Here is a nomogram plot from Example 1 from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .04,\n  SN = .50,\n  FPR = .05\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from Example 2 from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .014,\n  SN = .75,\n  FPR = .10\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from Example 3A from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .00005,\n  SN = 1,\n  FPR = .00008\n)\n\n\n\n\n\n\n\nHere is a nomogram plot from Example 3B from Silver (2012):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .3846272,\n  SN = 1,\n  FPR = .00008\n)\n\n\n\n\n\n\n\n\n16.8.2.3 Informal Updating\nKahneman (2011) provides the following guidance for an informal approach to updating that anchors predictions to the base rate:\n\nStart with the “baseline prediction” (i.e., base rate or average outcome).\nGenerate or identify your “intuitive prediction”—the number that matches your impression of the evidence.\nYour posterior prediction should fall somewhere between the baseline prediction and the intuitive prediction. “In the default case of no useful evidence, you stay with the baseline [prediction]. At the other extreme, you also stay with your initial [i.e., intuitive] prediction. This will happen, of course, only if you remain completely confident in your initial prediction after a critical review of the evidence that supports it. In most cases you will find some reasons to doubt that the correlation between your intuitive judgment and the truth is perfect, and you will end up somewhere between the two poles.” (pp. 191–192). Base the extent of adjustment (from the baseline prediction) on the magnitude of the correlation between your prediction/evidence and the truth, which acts similar to the likelihood ratio. For instance, if the correlation between your prediction/evidence and the truth is .5, move 50% of the difference from the baseline prediction to the intuitive prediction.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesConclusion",
    "href": "base-rates.html#sec-baseRatesConclusion",
    "title": "16  Base Rates",
    "section": "\n16.9 Conclusion",
    "text": "16.9 Conclusion\nFantasy performance—and behavior more generally—is challenging to predict. People commonly demonstrate biases and fallacies when making predictions. People tend to ignore base rates (base rate fallacy) when making predictions. They also tend to confuse inverse conditional probabilities (conditional probability fallacy). Bayes’ theorem provides a way to convert from one conditional probability to its inverse conditional probability using the base rate of each event. There are various ways to account for base rates for more accurate predictions, including through the use of actuarial formulas, Bayesian updating, and more informal approaches. Bayesian updating uses Bayes’ theorem to calculate a posttest probability from a pretest probability and a test result (likelihood ratio). The probability nomogram is a visual approach to Bayesian updating.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesSessionInfo",
    "href": "base-rates.html#sec-baseRatesSessionInfo",
    "title": "16  Base Rates",
    "section": "\n16.10 Session Info",
    "text": "16.10 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n [9] ggplot2_3.5.2     tidyverse_2.0.0   psych_2.5.6       petersenlab_1.2.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   viridisLite_0.4.2  farver_2.1.2       fastmap_1.2.0     \n [5] digest_0.6.37      rpart_4.1.24       timechange_0.3.0   lifecycle_1.0.4   \n [9] cluster_2.1.8.1    magrittr_2.0.3     compiler_4.5.1     rlang_1.1.6       \n[13] Hmisc_5.2-3        tools_4.5.1        yaml_2.3.10        data.table_1.17.8 \n[17] knitr_1.50         labeling_0.4.3     htmlwidgets_1.6.4  mnormt_2.1.1      \n[21] plyr_1.8.9         RColorBrewer_1.1-3 foreign_0.8-90     withr_3.0.2       \n[25] nnet_7.3-20        grid_4.5.1         stats4_4.5.1       lavaan_0.6-19     \n[29] xtable_1.8-4       colorspace_2.1-1   scales_1.4.0       MASS_7.3-65       \n[33] cli_3.6.5          mvtnorm_1.3-3      rmarkdown_2.29     reformulas_0.4.1  \n[37] generics_0.1.4     rstudioapi_0.17.1  reshape2_1.4.4     tzdb_0.5.0        \n[41] minqa_1.2.8        DBI_1.2.3          splines_4.5.1      parallel_4.5.1    \n[45] base64enc_0.1-3    mitools_2.4        vctrs_0.6.5        boot_1.3-31       \n[49] Matrix_1.7-3       jsonlite_2.0.0     hms_1.1.3          Formula_1.2-5     \n[53] htmlTable_2.4.3    glue_1.8.0         nloptr_2.2.1       stringi_1.8.7     \n[57] gtable_0.3.6       quadprog_1.5-8     lme4_1.1-37        pillar_1.11.0     \n[61] htmltools_0.5.8.1  R6_2.6.1           Rdpack_2.6.4       mix_1.0-13        \n[65] evaluate_1.0.4     pbivnorm_0.6.0     lattice_0.22-7     rbibutils_2.3     \n[69] backports_1.5.0    Rcpp_1.1.0         gridExtra_2.3      nlme_3.1-168      \n[73] checkmate_2.3.3    xfun_0.53          pkgconfig_2.0.3   \n\n\n\n\n\n\nAIDSVu. (2022). Understanding the current HIV epidemic in the United States. https://map.aidsvu.org/profiles/nation/usa/overview\n\n\nAltarejos, J., & Hayward, R. (2025). Likelihood ratio nomogram. Centre for Health Evidence. https://jamaevidence.mhmedical.com/data/calculators/LR_nomogram.html\n\n\nBraun, N. (2012). Bayesian football 101. https://web.archive.org/web/20161028142225/http://www.bayesff.com/bayesian101/\n\n\nChekroud, A. (2017). nomogrammer: Fagan’s nomograms with ggplot2. https://github.com/achekroud/nomogrammer\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFox, C. R., & Tversky, A. (1998). A belief-based account of decision under uncertainty. Management Science, 44(7), 879–895. https://doi.org/10.1287/mnsc.44.7.879\n\n\nHarris, C. (2012). How to make VBD work for you. https://www.espn.com/fantasy/football/ffl/story?page=nfldk2k12_vbdwork\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nSchwartz, A. (2006). Diagnostic test calculator. http://araw.mede.uic.edu/cgi-bin/testcalc.pl\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html",
    "href": "evaluating-prediction-accuracy.html",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "",
    "text": "17.1 Getting Started\nThis chapter provides an overview of ways to evaluate the accuracy of predictions. In addition, we evaluate the accuracy of fantasy football projections.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyGettingStarted",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyGettingStarted",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "",
    "text": "17.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"pROC\")\nlibrary(\"magrittr\")\nlibrary(\"viridis\")\nlibrary(\"viridisLite\")\nlibrary(\"msir\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyverse\")\n\n\n\n17.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/players_projections_weekly.RData\")\nload(file = \"./data/players_projections_seasonal.RData\")\nload(file = \"./data/nfl_playerIDs.RData\")\n\n\n\n17.1.3 Specify Options\n\nCodeoptions(scipen = 999) # prevent scientific notation\n\n\n\n17.1.4 Prepare Data\n\n17.1.4.1 Seasonal Projections\nTo evaluate the accuracy of projections, we must first merge projections with actual performance. Below, we merge seasonal projections with actual performance.\n\nCodeplayer_stats_seasonal_subset &lt;- player_stats_seasonal %&gt;% \n  filter(!is.na(player_id))\n\nnfl_playerIDs_subset &lt;- nfl_playerIDs %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  distinct(gsis_id, .keep_all = TRUE) %&gt;% # keep only rows that do not have duplicate values of gsis_id\n  select(-all_of(c(\"team\", \"position\", \"height\", \"weight\", \"age\")))\n\nplayers_projectedPoints_seasonal_combined$season &lt;- as.integer(players_projectedPoints_seasonal_combined$season)\nplayers_projections_seasonal_average_merged$season &lt;- as.integer(players_projections_seasonal_average_merged$season)\n\nplayer_stats_seasonal_subset_IDs &lt;- left_join(\n  player_stats_seasonal_subset,\n  nfl_playerIDs_subset,\n  by = c(\"player_id\" = \"gsis_id\")\n) %&gt;% \n  filter(!is.na(mfl_id))\n\nprojectionsWithActuals_seasonal &lt;- full_join(\n  player_stats_seasonal_subset_IDs,\n  players_projectedPoints_seasonal_combined,\n  by = c(\"mfl_id\" = \"id\", \"season\"),\n  suffix = c(\"\", \"_proj\"),\n)\n\ncrowdAveragedProjectionsWithActuals_seasonal &lt;- full_join(\n  player_stats_seasonal_subset_IDs,\n  players_projections_seasonal_average_merged %&gt;% filter(avg_type == \"average\"),\n  by = c(\"mfl_id\" = \"id\", \"season\"),\n  suffix = c(\"\", \"_proj\"),\n)\n\nprojectionsWithActuals_seasonal &lt;- projectionsWithActuals_seasonal %&gt;% \n  unite(\n    \"player_id_season\",\n    player_id,\n    season,\n    remove = FALSE\n  )\n\ncrowdAveragedProjectionsWithActuals_seasonal &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  unite(\n    \"player_id_season\",\n    player_id,\n    season,\n    remove = FALSE\n  )\n\n\nPlayers in the projectionsWithActuals_seasonal object are (supposed to be) uniquely identified by player_id-season-data_src:\n\nCodeprojectionsWithActuals_seasonal %&gt;% \n  filter(!is.na(player_id)) %&gt;% \n  group_by(player_id, season, data_src) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nPlayers in the crowdAveragedProjectionsWithActuals_seasonal object are (supposed to be) uniquely identified by player_id-season-avg_type:\n\nCodecrowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(!is.na(player_id)) %&gt;% \n  group_by(player_id, season, avg_type) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nWe save the data object for use in other chapters:\n\nCodesave(\n  projectionsWithActuals_seasonal, crowdAveragedProjectionsWithActuals_seasonal,\n  file = \"./data/projectionsWithActuals_seasonal.RData\"\n)\n\n\n\nCodeplayersWithHighProjectedOrActualPoints &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(raw_points &gt; 100 | fantasyPoints &gt; 100) %&gt;% \n  select(player_id_season) %&gt;% \n  pull()\n\nprojectionsWithActuals_seasonal_qb &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"QB\")\n\nprojectionsWithActuals_seasonal_rb &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"RB\")\n\nprojectionsWithActuals_seasonal_wr &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"WR\")\n\nprojectionsWithActuals_seasonal_te &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"TE\")\n\nprojectionsWithActuals_seasonal_k &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position == \"K\")\n\nprojectionsWithActuals_seasonal_dl &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"DL\")\n\nprojectionsWithActuals_seasonal_lb &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"LB\")\n\nprojectionsWithActuals_seasonal_db &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"DB\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_qb &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"QB\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_rb &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"RB\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_wr &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"WR\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_te &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"TE\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_k &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position == \"K\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_dl &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"DL\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_lb &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"LB\")\n\ncrowdAveragedProjectionsWithActuals_seasonal_db &lt;- crowdAveragedProjectionsWithActuals_seasonal %&gt;% \n  filter(position_group == \"DB\")\n\n\n\n17.1.4.2 Weekly Projections\nBelow, we merge weekly projections with actual performance.\n\nCodeplayer_stats_weekly_subset &lt;- player_stats_weekly %&gt;% \n  filter(!is.na(player_id))\n\nnfl_playerIDs_subset &lt;- nfl_playerIDs %&gt;% \n  filter(!is.na(gsis_id)) %&gt;% \n  distinct(gsis_id, .keep_all = TRUE) %&gt;% # keep only rows that do not have duplicate values of gsis_id\n  select(-all_of(c(\"team\", \"position\", \"height\", \"weight\", \"age\")))\n\nplayers_projectedPoints_weekly_combined$season &lt;- as.integer(players_projectedPoints_weekly_combined$season)\nplayers_projections_weekly_average_merged$season &lt;- as.integer(players_projections_weekly_average_merged$season)\n\nplayer_stats_weekly_subset_IDs &lt;- left_join(\n  player_stats_weekly_subset,\n  nfl_playerIDs_subset,\n  by = c(\"player_id\" = \"gsis_id\")\n) %&gt;% \n  filter(!is.na(mfl_id))\n\nprojectionsWithActuals_weekly &lt;- full_join(\n  player_stats_weekly_subset_IDs,\n  players_projectedPoints_weekly_combined,\n  by = c(\"mfl_id\" = \"id\", \"season\", \"week\"),\n  suffix = c(\"\", \"_proj\"),\n)\n\ncrowdAveragedProjectionsWithActuals_weekly &lt;- full_join(\n  player_stats_weekly_subset_IDs,\n  players_projections_weekly_average_merged,\n  by = c(\"mfl_id\" = \"id\", \"season\", \"week\"),\n  suffix = c(\"\", \"_proj\"),\n)\n\nprojectionsWithActuals_weekly &lt;- projectionsWithActuals_weekly %&gt;% \n  unite(\n    \"player_id_season_week\",\n    player_id,\n    season,\n    week,\n    remove = FALSE\n  )\n\ncrowdAveragedProjectionsWithActuals_weekly &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  unite(\n    \"player_id_season_week\",\n    player_id,\n    season,\n    week,\n    remove = FALSE\n  )\n\n\nPlayers in the projectionsWithActuals_weekly object are (supposed to be) uniquely identified by player_id-season-week-data_src:\n\nCodeprojectionsWithActuals_weekly %&gt;% \n  filter(!is.na(player_id)) %&gt;% \n  group_by(player_id, season, week, data_src) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nPlayers in the crowdAveragedProjectionsWithActuals_weekly object are (supposed to be) uniquely identified by player_id-season-week-avg_type:\n\nCodecrowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(!is.na(player_id)) %&gt;% \n  group_by(player_id, season, week, avg_type) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\nWe save the data object for use in other chapters:\n\nCodesave(\n  projectionsWithActuals_weekly, crowdAveragedProjectionsWithActuals_weekly,\n  file = \"./data/projectionsWithActuals_weekly.RData\"\n)\n\n\n\nCodeplayersWithHighProjectedOrActualPoints_weekly &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(raw_points &gt; 6 | fantasyPoints &gt; 6) %&gt;% \n  select(player_id_season_week) %&gt;% \n  pull()\n\nprojectionsWithActuals_weekly_qb &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"QB\")\n\nprojectionsWithActuals_weekly_rb &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"RB\")\n\nprojectionsWithActuals_weekly_wr &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"WR\")\n\nprojectionsWithActuals_weekly_te &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"TE\")\n\nprojectionsWithActuals_weekly_k &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position == \"K\")\n\nprojectionsWithActuals_weekly_dl &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"DL\")\n\nprojectionsWithActuals_weekly_lb &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"LB\")\n\nprojectionsWithActuals_weekly_db &lt;- projectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"DB\")\n\ncrowdAveragedProjectionsWithActuals_weekly_qb &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"QB\")\n\ncrowdAveragedProjectionsWithActuals_weekly_rb &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"RB\")\n\ncrowdAveragedProjectionsWithActuals_weekly_wr &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"WR\")\n\ncrowdAveragedProjectionsWithActuals_weekly_te &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"TE\")\n\ncrowdAveragedProjectionsWithActuals_weekly_k &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position == \"K\")\n\ncrowdAveragedProjectionsWithActuals_weekly_dl &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"DL\")\n\ncrowdAveragedProjectionsWithActuals_weekly_lb &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"LB\")\n\ncrowdAveragedProjectionsWithActuals_weekly_db &lt;- crowdAveragedProjectionsWithActuals_weekly %&gt;% \n  filter(position_group == \"DB\")",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyOverview",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyOverview",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.2 Overview",
    "text": "17.2 Overview\nPredictions can come in different types. Some predictions involve categorical data, whereas other predictions involve continuous data. When dealing with a dichotomous (nominal data that are binary) predictor and outcome variable (or continuous data that have been dichotomized using a cutoff), we can evaluate predictions using a 2x2 table known as a confusion matrix (see Figure 17.2), or with logistic regression models. When dealing with a continuous outcome variable (e.g., ordinal, interval, or ratio data), we can evaluate predictions using multiple regression or similar variants such as structural equation modeling and mixed models.\nIn fantasy football, we most commonly predict continuous outcome variables (e.g., fantasy points, rushing yards). Nevertheless, it is also important to understand principles in the prediction of categorical outcomes variables.\nIn any domain, it is important to evaluate the accuracy of predictions, so we can know how (in)accurate we are, and we can strive to continually improve our predictions. Fantasy performance—and human behavior more general—is incredibly challenging to predict. Indeed, many things in the world, in particular long-term trends, are unpredictable (Kahneman, 2011). In fantasy football, there is considerable luck/chance/randomness. There are relatively few (i.e. 17) games, and there is a sizeable injury risk for each player in a given game. These and other factors combine to render fantasy football predictions not highly accurate. Domains with high uncertainty and unpredictability are considered “low-validity environments” (Kahneman, 2011, p. 223). But, first, let’s learn about the various ways we can evaluate the accuracy of predictions.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-accuracyTypes",
    "href": "evaluating-prediction-accuracy.html#sec-accuracyTypes",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.3 Types of Accuracy",
    "text": "17.3 Types of Accuracy\nThere are two primary dimensions of accuracy: (1) discrimination and (2) calibration. Discrimination and calibration are distinct forms of accuracy. Just because predictions are high in one form of accuracy does not mean that they will be high in the other form of accuracy. As described by Lindhiem et al. (2020), predictions can follow any of the following configurations (and anywhere in between):\n\nhigh discrimination, high calibration\n\nhigh discrimination, low calibration\n\nlow discrimination, high calibration\n\nlow discrimination, low calibration\n\n\nSome general indexes of accuracy combine discrimination and calibration, as described in Section 17.3.3.\nIn addition, accuracy indices can be threshold-dependent or -independent and can be scale-dependent or -independent. Threshold-dependent accuracy indices differ based on the cutoff (i.e., threshold), whereas threshold-independent accuracy indices do not. Thus, raising or lowering the cutoff will change threshold-dependent accuracy indices. Scale-dependent accuracy indices depend on the metric/scale of the data, whereas scale-independent accuracy indices do not. Thus, scale-dependent accuracy indices cannot be directly compared when using measures of differing scales, whereas scale-independent accuracy indices can be compared across data of differing scales.\n\n17.3.1 Discrimination\nWhen dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity, area under the ROC curve) are described in Section 17.6.\n\n17.3.2 Calibration\nWhen dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by Lindhiem et al. (2020).\nCalibration is relevant to all kinds of predictions, including weather forecasts. For instance, on the days that the meteorologist says there is a 60% chance of rain, it should rain about 60% of the time. Calibration is also important for fantasy football predictions. When projections state that a group of players is each expected to score 200 points, their projections would be miscalibrated if those players scored only 150 points on average.\nThere are four general patterns of miscalibration: overextremity, underextremity, overprediction, and underprediction (see Figure 17.15). Overextremity exists when the predicted probabilities are too close to the extremes (zero or one). Underextremity exists when the predicted probabilities are too far away from the extremes. Overprediction exists when the predicted probabilities are consistently greater than the observed probabilities. Underprediction exists when the predicted probabilities are consistently less than the observed probabilities. For a more thorough description of these types of miscalibration, see Lindhiem et al. (2020).\nIndices for evaluating calibration are described in Section 17.7.3.\n\n17.3.3 General Accuracy\nGeneral accuracy indices combine estimates of discrimination and calibration.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionCategorical",
    "href": "evaluating-prediction-accuracy.html#sec-predictionCategorical",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.4 Prediction of Categorical Outcomes",
    "text": "17.4 Prediction of Categorical Outcomes\nTo evaluate the accuracy of our predictions for categorical outcome variables (e.g., binary, dichotomous, or nominal data), we can use either threshold-dependent or threshold-independent accuracy indices.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionContinuous",
    "href": "evaluating-prediction-accuracy.html#sec-predictionContinuous",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.5 Prediction of Continuous Outcomes",
    "text": "17.5 Prediction of Continuous Outcomes\nTo evaluate the accuracy of our predictions for continuous outcome variables (e.g., ordinal, interval, or ratio data), the outcome variable does not have cutoffs, so we would use threshold-independent accuracy indices.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-thresholdDependentAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-thresholdDependentAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.6 Threshold-Dependent Accuracy Indices",
    "text": "17.6 Threshold-Dependent Accuracy Indices\n\n17.6.1 Decision Outcomes\nTo consider how we can evaluate the accuracy of predictions for a categorical outcome, consider an example adapted from Meehl & Rosen (1955). The military conducts a test of its prospective members to screen out applicants who would likely fail basic training. To evaluate the accuracy of our predictions using the test, we can examine a confusion matrix. A confusion matrix is a matrix that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes), as in Figure 17.2: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n\n\n\n\n\nFigure 17.2: A Two-by-Two Confusion Matrix. From Petersen (2024) and Petersen (2025b).\n\n\nWhen discussing the four decision outcomes, “true” means an accurate judgment, whereas “false” means an inaccurate judgment. “Positive” means that the judgment was that the person has the characteristic of interest, whereas “negative” means that the judgment was that the person does not have the characteristic of interest. A true positive is a correct judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually have (or will have) the characteristic. A true negative is a correct judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false positive is an incorrect judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false negative is an incorrect judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do have (or will have) the characteristic.\nAn example of a confusion matrix is in Figure 17.3.\n\n\n\n\n\nFigure 17.3: Example of a Two-by-Two Confusion Matrix. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives. From Petersen (2024) and Petersen (2025b).\n\n\nWith the information in the confusion matrix, we can calculate the marginal sums and the proportion of people in each cell (in parentheses), as depicted in Figure 17.4.\n\n\n\n\n\nFigure 17.4: Example of a Two-by-Two Confusion Matrix With Marginal Sums. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives. From Petersen (2024) and Petersen (2025b).\n\n\nThat is, we can sum across the rows and columns to identify how many people actually showed poor adjustment (\\(n = 100\\)) versus good adjustment (\\(n = 1,900\\)), and how many people were selected to reject (\\(n = 508\\)) versus retain (\\(n = 1,492\\)). If we sum the column of predicted marginal sums (\\(508 + 1,492\\)) or the row of actual marginal sums (\\(100 + 1,900\\)), we get the total number of people (\\(N = 2,000\\)).\nBased on the marginal sums, we can compute the marginal probabilities, as depicted in Figure 17.5.\n\n\n\n\n\nFigure 17.5: Example of a Two-by-Two Confusion Matrix With Marginal Sums and Marginal Probabilities. TP = true positives; TN = true negatives; FP = false positives; FN = false negatives; BR = base rate; SR = selection ratio. From Petersen (2024) and Petersen (2025b).\n\n\nThe marginal probability of the person having the characteristic of interest (i.e., showing poor adjustment) is called the base rate (BR). That is, the base rate is the proportion of people who have the characteristic. It is calculated by dividing the number of people with poor adjustment (\\(n = 100\\)) by the total number of people (\\(N = 2,000\\)): \\(BR = \\frac{FN + TP}{N}\\). Here, the base rate reflects the prevalence of poor adjustment. In this case, the base rate is .05, so there is a 5% chance that an applicant will be poorly adjusted. The marginal probability of good adjustment is equal to 1 minus the base rate of poor adjustment.\nThe marginal probability of predicting that a person has the characteristic (i.e., rejecting a person) is called the selection ratio (SR). The selection ratio is the proportion of people who will be selected (in this case, rejected rather than retained); i.e., the proportion of people who are identified as having the characteristic. The selection ratio is calculated by dividing the number of people selected to reject (\\(n = 508\\)) by the total number of people (\\(N = 2,000\\)): \\(SR = \\frac{TP + FP}{N}\\). In this case, the selection ratio is .25, so 25% of people are rejected. The marginal probability of not selecting someone to reject (i.e., the marginal probability of retaining) is equal to 1 minus the selection ratio.\nThe selection ratio might be something that the test dictates according to its cutoff score. Or, the selection ratio might be imposed by external factors that place limits on how many people you can assign a positive test value. For instance, when deciding whether to treat a client, the selection ratio may depend on how many therapists are available and how many cases can be treated.\n\n17.6.2 Percent Accuracy\nBased on the confusion matrix, we can calculate the prediction accuracy based on the percent accuracy of the predictions. The percent accuracy is the number of correct predictions divided by the total number of predictions, and multiplied by 100. In the context of a confusion matrix, this is calculated as: \\(100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\\). In this case, our percent accuracy was 78%—that is, 78% of our predictions were accurate, and 22% of our predictions were inaccurate.\n\n17.6.3 Percent Accuracy by Chance\n78% sounds pretty accurate. And it is much higher than 50%, so we are doing a pretty good job, right? Well, it is important to compare our accuracy to what accuracy we would expect to get by chance alone, if predictions were made by a random process rather than using a test’s scores. Our selection ratio was 25.4%. How accurate would we be if we randomly selected 25.4% of people to reject? To determine what accuracy we could get by chance alone given the selection ratio and the base rate, we can calculate the chance probability of true positives and the chance probability of true negatives. The probability of a given cell in the confusion matrix is a joint probability—the probability of two events occurring simultaneously. To calculate a joint probability, we multiply the probability of each event.\nSo, to get the chance expectancies of true positives, we would multiply the respective marginal probabilities, as in Equation 17.1:\n\\[\n\\begin{aligned}\n  P(TP) &= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\\n   &= BR \\times SR \\\\\n  &= .05 \\times .254 \\\\\n  &= .0127\n\\end{aligned}\n\\tag{17.1}\\]\nTo get the chance expectancies of true negatives, we would multiply the respective marginal probabilities, as in Equation 17.2:\n\\[\n\\begin{aligned}\n  P(TN) &= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\\n   &= (1 - BR) \\times (1 - SR) \\\\\n  &= .95 \\times .746 \\\\\n  &= .7087\n\\end{aligned}\n\\tag{17.2}\\]\nTo get the percent accuracy by chance, we sum the chance expectancies for the correct predictions (TP and TN): \\(.0127 + .7087 = .7214\\). Thus, the percent accuracy you can get by chance alone is 72%. This is because most of our predictions are to retain people, and the base rate of poor adjustment is quite low (.05). Our measure with 78% accuracy provides only a 6% increment in correct predictions. Thus, you cannot judge how good your judgment or prediction is until you know how you would do by random chance.\nThe chance expectancies for each cell of the confusion matrix are in Figure 17.6.\n\n\n\n\n\nFigure 17.6: Chance Expectancies in Two-by-Two Confusion Matrix. BR = base rate; SR = selection ratio. From Petersen (2024) and Petersen (2025b).\n\n\n\n17.6.4 Predicting from the Base Rate\nNow, let us consider how well you would do if you were to predict from the base rate. Predicting from the base rate is also called “betting from the base rate”, and it involves setting the selection ratio by taking advantage of the base rate so that you go with the most likely outcome in every prediction. Because the base rate is quite low (.05), we could predict from the base rate by selecting no one to reject (i.e., setting the selection ratio at zero). Our percent accuracy by chance if we predict from the base rate would be calculated by multiplying the marginal probabilities, as we did above, but with a new selection ratio, as in Equation 17.3:\n\\[\n\\begin{aligned}\n  P(TP) &= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\\n   &= BR \\times SR \\\\\n  &= .05 \\times 0 \\\\\n  &= 0 \\\\ \\\\\n  P(TN) &= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\\n   &= (1 - BR) \\times (1 - SR) \\\\\n  &= .95 \\times 1 \\\\\n  &= .95\n\\end{aligned}\n\\tag{17.3}\\]\nWe sum the chance expectancies for the correct predictions (TP and TN): \\(0 + .95 = .95\\). Thus, our percent accuracy by predicting from the base rate is 95%. This is damning to our measure because it is a much higher accuracy than the accuracy of our measure. That is, we can be much more accurate than our measure simply by predicting from the base rate and selecting no one to reject.\nGoing with the most likely outcome in every prediction (predicting from the base rate) can be highly accurate (in terms of percent accuracy) as noted by Meehl & Rosen (1955), especially when the base rate is very low or very high. This should serve as an important reminder that we need to compare the accuracy of our measures to the accuracy by (1) random chance and (2) predicting from the base rate. There are several important implications of the impact of base rates on prediction accuracy. One implication is that using the same test in different settings with different base rates will markedly change the accuracy of the test. Oftentimes, using a test will actually decrease the predictive accuracy when the base rate deviates greatly from .50. But percent accuracy is not everything. Percent accuracy treats different kinds of errors as if they are equally important. However, the value we place on different kinds of errors may be different, as described next.\n\n17.6.5 Different Kinds of Errors Have Different Costs\nSome errors have a high cost, and some errors have a low cost. Among the four decision outcomes, there are two types of errors: false positives and false negatives. The extent to which false positives and false negatives are costly depends on the prediction problem. So, even though you can often be most accurate by going with the base rate, it may be advantageous to use a screening instrument despite lower overall accuracy because of the huge difference in costs of false positives versus false negatives in some cases.\nConsider the example of a screening instrument for HIV. False positives would be cases where we said that someone is at high risk of HIV when they are not, whereas false negatives are cases where we said that someone is not at high risk when they actually are. The costs of false positives include a shortage of blood, some follow-up testing, and potentially some anxiety, but that is about it. The costs of false negatives may be people getting HIV. In this case, the costs of false negatives greatly outweigh the costs of false positives, so we use a screening instrument to try to identify the cases at high risk for HIV because of the important consequences of failing to do so, even though using the screening instrument will lower our overall accuracy level.\nAnother example is when the Central Intelligence Agency (CIA) used a screen for protective typists during wartime to try to detect spies. False positives would be cases where the CIA believes that a person is a spy when they are not, and the CIA does not hire them. False negatives would be cases where the CIA believes that a person is not a spy when they actually are, and the CIA hires them. In this case, a false positive would be fine, but a false negative would be really bad.\nHow you weigh the costs of different errors depends considerably on the domain and context. Possible costs of false positives to society include: unnecessary and costly treatment with side effects and sending an innocent person to jail (despite our presumption of innocence in the United States criminal justice system that a person is innocent until proven guilty). Possible costs of false negatives to society include: setting a guilty person free, failing to detect a bomb or tumor, and preventing someone from getting treatment who needs it.\nThe differential costs of different errors also depend on how much flexibility you have in the selection ratio in being able to set a stringent versus loose selection ratio. Consider if there is a high cost of getting rid of people during the selection process. For example, if you must hire 100 people and only 100 people apply for the position, you cannot lose people, so you need to hire even high-risk people. However, if you do not need to hire many people, then you can hire more conservatively.\nAny time the selection ratio differs from the base rate, you will make errors. For example, if you reject 25% of applicants, and the base rate of poor adjustment is 5%, then you are making errors of over-rejecting (false positives). By contrast, if you reject 1% of applicants and the base rate of poor adjustment is 5%, then you are making errors of under-rejecting or over-accepting (false negatives).\n\n17.6.6 Difficulty Predicting Low Base Rate Events\nA low base rate makes it harder to make predictions, and tends to lead to less accurate predictions. For instance, it is very challenging to predict low base rate behaviors, including suicide (Kessler et al., 2020). For this reason, it is likely much more challenging to predict touchdowns—which happen relatively less often—than it is to predict passing/rushing/receiving yards—which are more frequent and continuously distributed.\nHere is the accuracy of the prediction of passing touchdowns versus passing yards among Quarterbacks:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_qb$pass_tds,\n  actual = projectionsWithActuals_weekly_qb$passing_tds,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_qb$pass_yds,\n  actual = projectionsWithActuals_weekly_qb$passing_yards,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThe accuracy for predicting passing yards (\\(R^2 = .19\\)) is higher than the accuracy for predicting passing touchdowns (\\(R^2 = .10\\)).\nHere is the accuracy of the prediction of rushing touchdowns versus rushing yards among Running Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_rb$rush_tds,\n  actual = projectionsWithActuals_weekly_rb$rushing_tds,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_rb$rush_yds,\n  actual = projectionsWithActuals_weekly_rb$rushing_yards,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThe accuracy for predicting rushing yards (\\(R^2 = .44\\)) is higher than the accuracy for predicting rushing touchdowns (\\(R^2 = .12\\)).\nHere is the accuracy of the prediction of receiving touchdowns versus receiving yards among Wide Receivers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_wr$rec_tds,\n  actual = projectionsWithActuals_weekly_wr$receiving_tds,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_wr$rec_yds,\n  actual = projectionsWithActuals_weekly_wr$receiving_yards,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThe accuracy for predicting receiving yards (\\(R^2 = .31\\)) is higher than the accuracy for predicting rushing touchdowns (\\(R^2 = .02\\)) among Wide Receivers.\nHere is the accuracy of the prediction of receiving touchdowns versus receiving yards among Tight Ends:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_te$rec_tds,\n  actual = projectionsWithActuals_weekly_te$receiving_tds,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_te$rec_yds,\n  actual = projectionsWithActuals_weekly_te$receiving_yards,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThe accuracy for predicting receiving yards (\\(R^2 = .26\\)) is higher than the accuracy for predicting rushing touchdowns (\\(R^2 = -.01\\)) among Tight Ends.\nIn sum, the proportion of variance explained by the prediction (i.e., \\(R^2\\)) is much higher for passing/rushing/receiving yards than it is for touchdowns. This is consistent with the notion that prediction accuracy tends to be lower for lower base rate events (like touchdowns) compared to higher base rate events.\n\n17.6.7 Sensitivity, Specificity, PPV, and NPV\nAs described earlier, percent accuracy is not the only important aspect of accuracy. Percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the condition (if the base rate is low) or that everyone has the condition (if the base rate is high). Thus, it is also important to consider other aspects of accuracy, including sensitivity (SN), specificity (SP), positive predictive value (PPV), and negative predictive value (NPV). We want our predictions to be sensitive to be able to detect the characteristic but also to be specific so that we classify only people actually with the characteristic as having the characteristic.\nLet us return to the confusion matrix in Figure 17.5. If we know the frequency of each of the four predicted-actual combinations of the confusion matrix (TP, TN, FP, FN), we can calculate sensitivity, specificity, PPV, and NPV.\nSensitivity is the proportion of those with the characteristic (\\(\\text{TP} + \\text{FN}\\)) that we identified with our measure (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = .86\\). Specificity is the proportion of those who do not have the characteristic (\\(\\text{TN} + \\text{FP}\\)) that we correctly classify as not having the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = .78\\). PPV is the proportion of those who we classify as having the characteristic (\\(\\text{TP} + \\text{FP}\\)) who actually have the characteristic (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = .17\\). NPV is the proportion of those we classify as not having the characteristic (\\(\\text{TN} + \\text{FN}\\)) who actually do not have the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = .99\\).\nSensitivity, specificity, PPV, and NPV are proportions, and their values therefore range from 0 to 1, where higher values reflect greater accuracy. With sensitivity, specificity, PPV, and NPV, we have a good snapshot of how accurate the measure is at a given cutoff. In our case, our measure is good at finding whom to reject (high sensitivity), but it is rejecting too many people who do not need to be rejected (lower PPV due to many FPs). Most people whom we classify as having the characteristic do not actually have the characteristic. However, the fact that we are over-rejecting could be okay depending on our goals, for instance, if we do not care about over-dropping (i.e., the PPV being low).\n\n17.6.7.1 Some Accuracy Estimates Depend on the Cutoff\nSensitivity, specificity, PPV, and NPV differ based on the cutoff (i.e., threshold) for classification. Consider the following example. Aliens visit Earth, and they develop a test to determine whether a berry is edible or inedible.\nFigure 17.7 depicts the distributions of scores by berry type. Note how there are clearly two distinct distributions. However, the distributions overlap to some degree. Thus, any cutoff will have at least some inaccurate classifications. The extent of overlap of the distributions reflects the amount of measurement error of the measure with respect to the characteristic of interest.\n\nCode#No Cutoff\nsampleSize &lt;- 1000\n\nedibleScores &lt;- rnorm(sampleSize, 50, 15)\ninedibleScores &lt;- rnorm(sampleSize, 100, 15)\n\nedibleData &lt;- data.frame(\n  score = c(\n    edibleScores,\n    inedibleScores),\n  type = c(\n    rep(\"edible\", sampleSize),\n    rep(\"inedible\", sampleSize)))\n\ncutoff &lt;- 75\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = edibleData,\n  aes(\n    x = score,\n    ymin = 0,\n    fill = type)) +\n  geom_density(alpha = .5) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(2)[1],\n      viridis::viridis(2)[2])) +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.7: Distribution of Test Scores by Berry Type.\n\n\n\n\nFigure 17.8 depicts the distributions of scores by berry type with a cutoff. The red line indicates the cutoff—the level above which berries are classified by the test as inedible. There are errors on each side of the cutoff. Below the cutoff, there are some false negatives (blue): inedible berries that are inaccurately classified as edible. Above the cutoff, there are some false positives (green): edible berries that are inaccurately classified as inedible. Costs of false negatives could include sickness or death from eating the inedible berries. Costs of false positives could include taking longer to find food, finding insufficient food, and starvation.\n\nCode#Standard Cutoff\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.8: Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nBased on our assessment goals, we might use a different selection ratio by changing the cutoff. Figure 17.9 depicts the distributions of scores by berry type when we raise the cutoff. There are now more false negatives (blue) and fewer false positives (green). If we raise the cutoff (to be more conservative), the number of false negatives increases and the number of false positives decreases. Consequently, as the cutoff increases, sensitivity and NPV decrease (because we have more false negatives), whereas specificity and PPV increase (because we have fewer false positives). A higher cutoff could be optimal if the costs of false positives are considered greater than the costs of false negatives. For instance, if the aliens cannot risk eating the inedible berries because the berries are fatal, and there are sufficient edible berries that can be found to feed the alien colony.\n\nCode#Raise the cutoff\ncutoff &lt;- 85\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.9: Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nFigure 17.10 depicts the distributions of scores by berry type when we lower the cutoff. There are now fewer false negatives (blue) and more false positives (green). If we lower the cutoff (to be more liberal), the number of false negatives decreases and the number of false positives increases. Consequently, as the cutoff decreases, sensitivity and NPV increase (because we have fewer false negatives), whereas specificity and PPV decrease (because we have more false positives). A lower cutoff could be optimal if the costs of false negatives are considered greater than the costs of false positives. For instance, if the aliens cannot risk missing edible berries because they are in short supply relative to the size of the alien colony, and eating the inedible berries would, at worst, lead to minor, temporary discomfort.\n\nCode#Lower the cutoff\ncutoff &lt;- 65\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 17.10: Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nIn sum, sensitivity and specificity differ based on the cutoff for classification. If we raise the cutoff, sensitivity and PPV increase (due to fewer false positives), whereas sensitivity and NPV decrease (due to more false negatives). If we lower the cutoff, sensitivity and NPV increase (due to fewer false negatives), whereas specificity and PPV decrease (due to more false positives). Thus, the optimal cutoff depends on how costly each type of error is: false negatives and false positives. If false negatives are more costly than false positives, we would set a low cutoff. If false positives are more costly than false negatives, we would set a high cutoff.\n\n17.6.8 Set a Cutoff/Threshold\nFor evaluating sensitivity, specificity, positive predictive value, negative predictive value, and the receiver operating characteristic (ROC) curve, we need to make a dichotomous decision/prediction. For this example, our goal is to predict which Running Backs or Wide Receivers will score 300 or more fantasy points. We will predict a player as scoring 300 or more fantasy points for a given projection source if that projection source projected the player to score 300 or more fantasy points. Thus, our cutoff or threshold is 300 or more projected points. The same player has multiple rows in the data file—one for each projection source.\n\nCodeprojectionsWithActuals_seasonal_subset &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group %in% c(\"RB\",\"WR\")) %&gt;% \n  filter(season == 2024)\n\nprojectionsWithActuals_seasonal_subset$prediction &lt;- ifelse(projectionsWithActuals_seasonal_subset$raw_points &gt;= 300, 1, 0)\nprojectionsWithActuals_seasonal_subset$truth &lt;- ifelse(projectionsWithActuals_seasonal_subset$fantasyPoints &gt;= 300, 1, 0)\n\nprojectionsWithActuals_seasonal_subset$predictionFactor &lt;- factor(\n  projectionsWithActuals_seasonal_subset$prediction,\n  levels = c(1,0),\n  labels = c(\"Prediction: Will Score 300+ Points\", \"Prediction: Will Not Score 300+ Points\"))\n\nprojectionsWithActuals_seasonal_subset$truthFactor &lt;- factor(\n  projectionsWithActuals_seasonal_subset$truth,\n  levels = c(1,0),\n  labels = c(\"Truth: Scored 300+ Points\", \"Truth: Did Not Score 300+ Points\")\n)\n\ntable(projectionsWithActuals_seasonal_subset$predictionFactor)\n\n\n    Prediction: Will Score 300+ Points Prediction: Will Not Score 300+ Points \n                                    57                                   2117 \n\nCodetable(projectionsWithActuals_seasonal_subset$truthFactor)\n\n\n       Truth: Scored 300+ Points Truth: Did Not Score 300+ Points \n                              81                             2131 \n\n\n\n17.6.9 Signal Detection Theory\nSignal detection theory (SDT) is a probability-based theory for the detection of a given stimulus (signal) from a stimulus set that includes non-target stimuli (noise). SDT arose through the development of radar (RAdio Detection And Ranging) and sonar (SOund Navigation And Ranging) in World War II based on research on sensory-perception research. The military wanted to determine which objects on radar/sonar were enemy aircraft/submarines, and which were noise (e.g., different object in the environment or even just the weather itself). SDT allowed determining how many errors operators made (how accurate they were) and decomposing errors into different kinds of errors. SDT distinguishes between sensitivity and bias. In SDT, sensitivity (or discriminability) is how well an assessment distinguishes between a target stimulus and non-target stimuli (i.e., how well the assessment detects the target stimulus amid non-target stimuli). Bias is the extent to which the probability of a selection decision from the assessment is higher or lower than the true rate of the target stimulus.\nSome radar/sonar operators were not as sensitive to the differences between signal and noise, due to factors such as age, ability to distinguish gradations of a signal, etc. People who showed low sensitivity (i.e., who were not as successful at distinguishing between signal and noise) were screened out because the military perceived sensitivity as a skill that was not easily taught. By contrast, other operators could distinguish signal from noise, but their threshold was too low or high—they could take in information, but their decisions tended to be wrong due to systematic bias or poor calibration. That is, they systematically over-rejected or under-rejected stimuli. Over-rejecting leads to many false negatives (i.e., saying that a stimulus is safe when it is not). Under-rejecting leads to many false positives (i.e., saying that a stimulus is harmful when it is not). A person who showed good sensitivity but systematic bias was considered more teach-able than a person who showed low sensitivity. Thus, radar and sonar operators were selected based on their sensitivity to distinguish signal from noise, and then were trained to improve the calibration so they reduce their systematic bias and do not systematically over- or under-reject.\nAlthough SDT was originally developed for use in World War II, it now plays an important role in many areas of science and medicine. A medical application of SDT is tumor detection in radiology. Another application of SDT in society is using x-ray to detect bombs or other weapons. An example of applying SDT to fantasy football could be in the prediction (and evaluation) of whether or not a player scores a touchdown in a game.\nSDT metrics of sensitivity include \\(d'\\) (“\\(d\\)-prime”), \\(A\\) (or \\(A'\\)), and the area under the receiver operating characteristic (ROC) curve. SDT metrics of bias include \\(\\beta\\) (beta), \\(c\\), and \\(b\\).\n\n17.6.9.1 Receiver Operating Characteristic (ROC) Curve\nThe x-axis of the ROC curve is the false alarm rate or false positive rate (\\(1 -\\) specificity). The y-axis is the hit rate or true positive rate (sensitivity). We can trace the ROC curve as the combination between sensitivity and specificity at every possible cutoff. At a cutoff of zero (top right of ROC curve), we calculate sensitivity (1.0) and specificity (0) and plot it. At a cutoff of zero, the assessment tells us to make an action for every stimulus (i.e., it is the most liberal). We then gradually increase the cutoff, and plot sensitivity and specificity at each cutoff. As the cutoff increases, sensitivity decreases and specificity increases. We end at the highest possible cutoff, where the sensitivity is 0 and the specificity is 1.0 (i.e., we never make an action; i.e., it is the most conservative). Each point on the ROC curve corresponds to a pair of hit and false alarm rates (sensitivity and specificity) resulting from a specific cutoff value. Then, we can draw lines or a curve to connect the points.\nFigure 17.11 depicts an empirical ROC plot where lines are drawn to connect the hit and false alarm rates, in predicting Running Backs and Wide Receivers who score 300 or more fantasy points.\n\nCodeplot(\n  pROC::roc(\n    truth ~ raw_points,\n    projectionsWithActuals_seasonal_subset),\n  legacy.axes = TRUE,\n  print.auc = TRUE)\n\n\n\n\n\n\nFigure 17.11: Empirical Receiver Operating Characteristic (ROC) Curve for Predicting Running Backs and Wide Receivers that Score 300 or More Fantasy Points.\n\n\n\n\nFigure 17.12 depicts an ROC curve where a smoothed and fitted curve is drawn to connect the hit and false alarm rates, in predicting Running Backs and Wide Receivers who score 300 or more fantasy points.\n\nCodeplot(\n  pROC::roc(\n    truth ~ raw_points,\n    projectionsWithActuals_seasonal_subset,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE)\n\n\n\n\n\n\nFigure 17.12: Smoothed Receiver Operating Characteristic (ROC) Curve for Predicting Running Backs and Wide Receivers that Score 300 or More Fantasy Points.\n\n\n\n\n\n17.6.9.1.1 Area Under the ROC Curve\nROC methods can be used to compare and compute the discriminative power of measurement devices free from the influence of selection ratios, base rates, and costs and benefits. An ROC analysis yields a quantitative index of how well an index predicts a signal of interest or can discriminate between different signals. ROC analysis can help tell us how often our assessment would be correct. If we randomly pick two observations, and we were right once and wrong once, we were 50% accurate. But this would be a useless measure because it reflects chance responding.\nThe geometrical area under the ROC curve reflects the discriminative accuracy of the measure. The index is called the area under the curve (AUC) of an ROC curve. AUC quantifies the discriminative power of an assessment. AUC is the probability that a randomly selected target and a randomly selected non-target is ranked correctly by the assessment method. AUC values range from 0.0 to 1.0, where chance accuracy is 0.5 as indicated by diagonal line in the ROC curve. That is, a measure can be useful to the extent that its ROC curve is above the diagonal line (i.e., its discriminative accuracy is above chance).\nAUC is a threshold-independent accuracy index that applies across all possible cutoff values.\nFigure 17.13 depicts ROC curves with a range of AUC values.\n\nCodeset.seed(52242)\n\nauc60 &lt;- petersenlab::simulateAUC(.60, 50000)\nauc70 &lt;- petersenlab::simulateAUC(.70, 50000)\nauc80 &lt;- petersenlab::simulateAUC(.80, 50000)\nauc90 &lt;- petersenlab::simulateAUC(.90, 50000)\nauc95 &lt;- petersenlab::simulateAUC(.95, 50000)\nauc99 &lt;- petersenlab::simulateAUC(.99, 50000)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc60,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .52,\n  print.auc.y = .61,\n  print.auc.pattern = \"%.2f\")\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc70,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .6,\n  print.auc.y = .67,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc80,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .695,\n  print.auc.y = .735,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc90,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .805,\n  print.auc.y = .815,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc95,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .875,\n  print.auc.y = .865,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc99,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .94,\n  print.auc.y = .94,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\n\n\n\n\n\nFigure 17.13: Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures.\n\n\n\n\nAs an example, given an AUC of .75, this says that the overall score of an individual who has the characteristic in question will be higher 75% of the time than the overall score of an individual who does not have the characteristic. In lay terms, AUC provides the probability that we will classify correctly based on our instrument if we were to randomly pick one good and one bad outcome. AUC is a stronger index of accuracy than percent accuracy, because you can have high percent accuracy just by going with the base rate. AUC tells us how much better than chance a measure is at discriminating outcomes. AUC is useful as a measure of general discriminative accuracy, and it tells us how accurate a measure is at all possible cutoffs. Knowing the accuracy of a measure at all possible cutoffs can be helpful for selecting the optimal cutoff, given the goals of the assessment. In reality, however, we may not be interested in all cutoffs because not all errors are equal in their costs.\nIf we lower the base rate, we would need a larger sample to get enough people to classify into each group. SDT/ROC methods are traditionally about dichotomous decisions (yes/no), not graded judgments. SDT/ROC methods can get messy with ordinal data that are more graded because you would have an AUC curve for each ordinal grouping.\n\n17.6.10 Accuracy Indices\nThere are various accuracy indices we can use to evaluate the accuracy of predictions for categorical outcome variables. We have already described several accuracy indices, including percent accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and area under the ROC curve. We describe these and other indices in greater detail below.\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::accuracyAtCutoff() function that computes many accuracy indices for the prediction of categorical outcome variables. In the example below, we compute the accuracy of predictions for which Running Backs or Wide Receivers will score 300 or more fantasy points, using a cutoff for projected fantasy points of 300.\n\nCodepetersenlab::accuracyAtCutoff(\n  predicted = projectionsWithActuals_seasonal_subset$raw_points,\n  actual = projectionsWithActuals_seasonal_subset$truth,\n  cutoff = 300\n)\n\n\n  \n\n\n\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::accuracyAtEachCutoff() function that computes many accuracy indices for the prediction of categorical outcome variables at each possible cutoff. In the example below, we compute the accuracy of predictions for which Running Backs or Wide Receivers will score 300 or more fantasy points, at each possible cutoff for projected fantasy points.\n\nCodepetersenlab::accuracyAtEachCutoff(\n  predicted = projectionsWithActuals_seasonal_subset$raw_points,\n  actual = projectionsWithActuals_seasonal_subset$truth\n)\n\n\n  \n\n\n\nThere are also test calculators available online:\n\n\nhttp://araw.mede.uic.edu/cgi-bin/testcalc.pl [Schwartz (2006); archived at https://perma.cc/X8TF-7YBX]\n\nhttps://dlrs.shinyapps.io/shinyDLRs (Goodman et al., 2022)\n\n\n\n17.6.10.1 Confusion Matrix aka 2x2 Accuracy Table aka Cross-Tabulation aka Contingency Table\nA confusion matrix (aka 2x2 accuracy table, cross-tabulation table, or contigency table) is a matrix for categorical data that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes). In such a case, the confusion matrix provides a tabular count of each type of accurate cases (true positives and true negatives) versus the number of each type of error (false positives and false negatives), as shown in Figure 17.2. An example of a confusion matrix is in Figure 17.3.\nHere is our 2x2 confusion matrix, predicting which Running Backs or Wide Receivers will score 300 or more fantasy points.\n\nCodeprojectionsWithActuals_seasonal_subset &lt;- projectionsWithActuals_seasonal %&gt;% \n  filter(position_group %in% c(\"RB\",\"WR\")) %&gt;% \n  filter(season == 2024)\n\nprojectionsWithActuals_seasonal_subset$prediction &lt;- ifelse(projectionsWithActuals_seasonal_subset$raw_points &gt;= 300, 1, 0)\nprojectionsWithActuals_seasonal_subset$truth &lt;- ifelse(projectionsWithActuals_seasonal_subset$fantasyPoints &gt;= 300, 1, 0)\n\nprojectionsWithActuals_seasonal_subset$predictionFactor &lt;- factor(\n  projectionsWithActuals_seasonal_subset$prediction,\n  levels = c(1,0),\n  labels = c(\"Prediction: Will Score 300+ Points\", \"Prediction: Will Not Score 300+ Points\"))\n\nprojectionsWithActuals_seasonal_subset$truthFactor &lt;- factor(\n  projectionsWithActuals_seasonal_subset$truth,\n  levels = c(1,0),\n  labels = c(\"Truth: Scored 300+ Points\", \"Truth: Did Not Score 300+ Points\")\n)\n\ntable(projectionsWithActuals_seasonal_subset$predictionFactor)\n\n\n    Prediction: Will Score 300+ Points Prediction: Will Not Score 300+ Points \n                                    57                                   2117 \n\nCodetable(projectionsWithActuals_seasonal_subset$truthFactor)\n\n\n       Truth: Scored 300+ Points Truth: Did Not Score 300+ Points \n                              81                             2131 \n\n\n\n17.6.10.1.1 Number\n\nCodetable(\n  projectionsWithActuals_seasonal_subset$predictionFactor,\n  projectionsWithActuals_seasonal_subset$truthFactor)\n\n                                        \n                                         Truth: Scored 300+ Points\n  Prediction: Will Score 300+ Points                            20\n  Prediction: Will Not Score 300+ Points                        61\n                                        \n                                         Truth: Did Not Score 300+ Points\n  Prediction: Will Score 300+ Points                                   37\n  Prediction: Will Not Score 300+ Points                             2056\n\n\n\n17.6.10.1.2 Number With Margins Added\n\nCodeaddmargins(table(\n  projectionsWithActuals_seasonal_subset$predictionFactor,\n  projectionsWithActuals_seasonal_subset$truthFactor))\n\n                                        \n                                         Truth: Scored 300+ Points\n  Prediction: Will Score 300+ Points                            20\n  Prediction: Will Not Score 300+ Points                        61\n  Sum                                                           81\n                                        \n                                         Truth: Did Not Score 300+ Points  Sum\n  Prediction: Will Score 300+ Points                                   37   57\n  Prediction: Will Not Score 300+ Points                             2056 2117\n  Sum                                                                2093 2174\n\n\n\n17.6.10.1.3 Proportions\n\nCodeprop.table(table(\n  projectionsWithActuals_seasonal_subset$predictionFactor,\n  projectionsWithActuals_seasonal_subset$truthFactor))\n\n                                        \n                                         Truth: Scored 300+ Points\n  Prediction: Will Score 300+ Points                   0.009199632\n  Prediction: Will Not Score 300+ Points               0.028058878\n                                        \n                                         Truth: Did Not Score 300+ Points\n  Prediction: Will Score 300+ Points                          0.017019319\n  Prediction: Will Not Score 300+ Points                      0.945722171\n\n\n\n17.6.10.1.4 Proportions With Margins Added\n\nCodeaddmargins(prop.table(table(\n  projectionsWithActuals_seasonal_subset$predictionFactor,\n  projectionsWithActuals_seasonal_subset$truthFactor)))\n\n                                        \n                                         Truth: Scored 300+ Points\n  Prediction: Will Score 300+ Points                   0.009199632\n  Prediction: Will Not Score 300+ Points               0.028058878\n  Sum                                                  0.037258510\n                                        \n                                         Truth: Did Not Score 300+ Points\n  Prediction: Will Score 300+ Points                          0.017019319\n  Prediction: Will Not Score 300+ Points                      0.945722171\n  Sum                                                         0.962741490\n                                        \n                                                 Sum\n  Prediction: Will Score 300+ Points     0.026218951\n  Prediction: Will Not Score 300+ Points 0.973781049\n  Sum                                    1.000000000\n\n\n\n17.6.10.2 True Positives (TP)\nTrue positives (TPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is correct—that is, the test says that a classification is present, and the classification is present. True positives are also called valid positives (VPs) or hits. Higher values reflect greater accuracy. The formula for true positives is in Equation 17.4:\n\\[\n\\begin{aligned}\n  \\text{TP} &= \\text{BR} \\times \\text{SR} \\times N\n\\end{aligned}\n\\tag{17.4}\\]\n\n17.6.10.3 True Negatives (TN)\nTrue negatives (TNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is correct—that is, the test says that a classification is not present, and the classification is actually not present. True negatives are also called valid negatives (VNs) or correct rejections. Higher values reflect greater accuracy. The formula for true negatives is in Equation 17.5:\n\\[\n\\begin{aligned}\n  \\text{TN} &= (1 - \\text{BR}) \\times (1 - \\text{SR}) \\times N\n\\end{aligned}\n\\tag{17.5}\\]\n\n17.6.10.4 False Positives (FP)\nFalse positives (FPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is incorrect—that is, the test says that a classification is present, and the classification is not present. False positives are also called false alarms (FAs). Lower values reflect greater accuracy. The formula for false positives is in Equation Equation 17.6:\n\\[\n\\begin{aligned}\n  \\text{FP} &= (1 - \\text{BR}) \\times \\text{SR} \\times N\n\\end{aligned}\n\\tag{17.6}\\]\n\n17.6.10.5 False Negatives (FN)\nFalse negatives (FNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is incorrect—that is, the test says that a classification is not present, and the classification is present. False negatives are also called misses. Lower values reflect greater accuracy. The formula for false negatives is in Equation 17.7:\n\\[\n\\begin{aligned}\n  \\text{FN} &= \\text{BR} \\times (1 - \\text{SR}) \\times N\n\\end{aligned}\n\\tag{17.7}\\]\n\n17.6.10.6 Selection Ratio (SR)\nThe selection ratio (SR) is the marginal probability of selection, independent of other things: \\(P(R_i)\\). It is not an index of accuracy, per se. In medicine, the selection ratio is the proportion of people who test positive for the disease. In fantasy football, the selection ratio is the proportion of players who you predict will show a given outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the selection ratio is the proportion of players who you predict will score a touchdown. The formula for calculating the selection ratio is in Equation 17.8.\n\\[\n\\begin{aligned}\n  \\text{SR} &= P(R_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FP}}{N}\n\\end{aligned}\n\\tag{17.8}\\]\n\n17.6.10.7 Base Rate (BR)\nThe base rate (BR) of a classification is its marginal probability, independent of other things: \\(P(C_i)\\). It is not an index of accuracy, per se. In medicine, the base rate of a disease is its prevalence in the population, as in Equation 17.9. Without additional information, the base rate is used as the initial pretest probability. In fantasy football, the base rate is the proportion of players who actually show the particular outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the base rate is the proportion of players who actually score a touchdown in the game. The formula for calculating the selection ratio is in Equation 17.9.\n\\[\n\\begin{aligned}\n  \\text{BR} &= P(C_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FN}}{N}\n\\end{aligned}\n\\tag{17.9}\\]\n\n17.6.10.8 Pretest Odds\nThe pretest odds of a classification can be estimated using the pretest probability (i.e., base rate). To convert a probability to odds, divide the probability by one minus that probability, as in Equation 17.10.\n\\[\n\\begin{aligned}\n  \\text{pretest odds} &= \\frac{\\text{pretest probability}}{1 - \\text{pretest probability}} \\\\\n\\end{aligned}\n\\tag{17.10}\\]\n\n17.6.10.9 Percent Accuracy\nPercent Accuracy is also called overall accuracy. Higher values reflect greater accuracy. The formula for percent accuracy is in Equation 17.11. Percent accuracy has several problems. First, it treats all errors (FP and FN) as equally important. However, in practice, it is rarely the case that false positives and false negatives are equally important. Second, percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the characteristic (if the base rate is low) or that everyone has the characteristic (if the base rate is high). Thus, it is also important to consider other aspects of accuracy.\n\\[\n\\text{Percent Accuracy} = 100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\n\\tag{17.11}\\]\n\n17.6.10.10 Percent Accuracy by Chance\nThe formula for calculating percent accuracy by chance is in Equation 17.12.\n\\[\n\\begin{aligned}\n  \\text{Percent Accuracy by Chance} &= 100\\% \\times [P(\\text{TP}) + P(\\text{TN})] \\\\\n  &= 100\\% \\times \\{(\\text{BR} \\times {\\text{SR}}) + [(1 - \\text{BR}) \\times (1 - \\text{SR})]\\}\n\\end{aligned}\n\\tag{17.12}\\]\n\n17.6.10.11 Percent Accuracy Predicting from the Base Rate\nPredicting from the base rate is going with the most likely outcome in every prediction. If the base rate is less than .50, it would involve predicting that the condition is absent for every case. If the base rate is .50 or above, it would involve predicting that the condition is present for every case. Predicting from the base rate is a special case of percent accuracy by chance when the selection ratio is set to either one (if the base rate \\(\\geq\\) .5) or zero (if the base rate &lt; .5).\n\n17.6.10.12 Relative Improvement Over Chance (RIOC)\nRelative improvement over chance (RIOC) is a prediction’s improvement over chance as a proportion of the maximum possible improvement over chance, as described by Farrington & Loeber (1989). Higher values reflect greater accuracy. The formula for calculating RIOC is in Equation 17.13.\n\\[\n\\begin{aligned}\n  \\text{relative improvement over chance (RIOC)} &= \\frac{\\text{total correct} - \\text{chance correct}}{\\text{maximum correct} - \\text{chance correct}} \\\\\n\\end{aligned}\n\\tag{17.13}\\]\n\n17.6.10.13 Relative Improvement Over Predicting from the Base Rate\nRelative improvement over predicting from the base rate is a prediction’s improvement over predicting from the base rate as a proportion of the maximum possible improvement over predicting from the base rate. Higher values reflect greater accuracy. The formula for calculating relative improvement over predicting from the base rate is in Equation 17.14.\n\\[\n\\scriptsize\n\\begin{aligned}\n  \\text{relative improvement over predicting from base rate} &= \\frac{\\text{total correct} - \\text{correct by predicting from base rate}}{\\text{maximum correct} - \\text{correct by predicting from base rate}} \\\\\n\\end{aligned}\n\\tag{17.14}\\]\n\n17.6.10.14 Sensitivity (SN)\nSensitivity (SN) is also called true positive rate (TPR), hit rate (HR), or recall. Sensitivity is the conditional probability of a positive test given that the person has the condition: \\(P(R|C)\\). Higher values reflect greater accuracy. The formula for calculating sensitivity is in Equation 17.15. As described in Section Section 17.6.7.1, as the cutoff increases (becomes more conservative), sensitivity decreases. As the cutoff decreases, sensitivity increases.\n\\[\n\\begin{aligned}\n  \\text{sensitivity (SN)} &= P(R|C) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{TP}}{N \\times \\text{BR}} = 1 - \\text{FNR}\n\\end{aligned}\n\\tag{17.15}\\]\n\n17.6.10.15 Specificity (SP)\nSpecificity (SP) is also called true negative rate (TNR) or selectivity. Specificity is the conditional probability of a negative test given that the person does not have the condition: \\(P(\\text{not } R|\\text{not } C)\\). Higher values reflect greater accuracy. The formula for calculating specificity is in Equation 17.16. As described in Section Section 17.6.7.1, as the cutoff increases (becomes more conservative), specificity increases. As the cutoff decreases, specificity decreases.\n\\[\n\\begin{aligned}\n  \\text{specificity (SP)} &= P(\\text{not } R|\\text{not } C) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{\\text{TN}}{N (1 - \\text{BR})} = 1 - \\text{FPR}\n\\end{aligned}\n\\tag{17.16}\\]\n\n17.6.10.16 False Negative Rate (FNR)\nThe false negative rate (FNR) is also called the miss rate. The false negative rate is the conditional probability of a negative test given that the person has the condition: \\(P(\\text{not } R|C)\\). Lower values reflect greater accuracy. The formula for calculating false negative rate is in Equation 17.17.\n\\[\n\\begin{aligned}\n  \\text{false negative rate (FNR)} &= P(\\text{not } R|C) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TP}} = \\frac{\\text{FN}}{N \\times \\text{BR}} = 1 - \\text{TPR}\n\\end{aligned}\n\\tag{17.17}\\]\n\n17.6.10.17 False Positive Rate (FPR)\nThe false positive rate (FPR) is also called the false alarm rate (FAR) or fall-out. The false positive rate is the conditional probability of a positive test given that the person does not have the condition: \\(P(R|\\text{not } C)\\). Lower values reflect greater accuracy. The formula for calculating false positive rate is in Equation 17.18:\n\\[\n\\begin{aligned}\n  \\text{false positive rate (FPR)} &= P(R|\\text{not } C) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} = \\frac{\\text{FP}}{N (1 - \\text{BR})} = 1 - \\text{TNR}\n\\end{aligned}\n\\tag{17.18}\\]\n\n17.6.10.18 Positive Predictive Value (PPV)\nThe positive predictive value (PPV) is also called the positive predictive power (PPP) or precision. Many people confuse sensitivity (\\(P(R|C)\\)) with its inverse conditional probability, PPV (\\(P(C|R)\\)). PPV is the conditional probability of having the condition given a positive test: \\(P(C|R)\\). Higher values reflect greater accuracy. The formula for calculating positive predictive value is in Equation 17.19.\nPPV can be low even when sensitivity is high because it depends not only on sensitivity, but also on specificity and the base rate. Because PPV depends on the base rate, PPV is not an intrinsic property of a measure. The same measure will have a different PPV in different contexts with different base rates (Treat & Viken, 2023). As described in Section Section 17.6.7.1, as the base rate increases, PPV increases. As the base rate decreases, PPV decreases. PPV also differs as a function of the cutoff. As described in Section Section 17.6.7.1, as the cutoff increases (becomes more conservative), PPV increases. As the cutoff decreases (becomes more liberal), PPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{positive predictive value (PPV)} &= P(C|R) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{\\text{TP}}{N \\times \\text{SR}}\\\\\n  &= \\frac{\\text{sensitivity} \\times {\\text{BR}}}{\\text{sensitivity} \\times {\\text{BR}} + [(1 - \\text{specificity}) \\times (1 - \\text{BR})]}\n\\end{aligned}\n\\tag{17.19}\\]\n\n17.6.10.19 Negative Predictive Value (NPV)\nThe negative predictive value (NPV) is also called the negative predictive power (NPP). Many people confuse specificity (\\(P(\\text{not } R|\\text{not } C)\\)) with its inverse conditional probability, NPV (\\(P(\\text{not } C| \\text{not } R)\\)). NPV is the conditional probability of not having the condition given a negative test: \\(P(\\text{not } C| \\text{not } R)\\). Higher values reflect greater accuracy. The formula for calculating negative predictive value is in Equation 17.20.\nNPV can be low even when specificity is high because it depends not only on specificity, but also on sensitivity and the base rate. Because NPV depends on the base rate, NPV is not an intrinsic property of a measure. The same measure will have a different NPV in different contexts with different base rates (Treat & Viken, 2023). As described in Section Section 17.6.7.1, as the base rate increases, NPV decreases. As the base rate decreases, NPV increases. NPV also differs as a function of the cutoff. As described in Section Section 17.6.7.1, as the cutoff increases (becomes more conservative), NPV decreases. As the cutoff decreases (becomes more liberal), NPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{negative predictive value (NPV)} &= P(\\text{not } C|\\text{not } R) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{\\text{TN}}{N(\\text{1 - SR})}\\\\\n  &= \\frac{\\text{specificity} \\times (1-{\\text{BR}})}{\\text{specificity} \\times (1-{\\text{BR}}) + [(1 - \\text{sensitivity}) \\times \\text{BR})]}\n\\end{aligned}\n\\tag{17.20}\\]\n\n17.6.10.20 False Discovery Rate (FDR)\nMany people confuse the false positive rate (\\(P(R|\\text{not } C)\\)) with its inverse conditional probability, the false discovery rate (\\(P(\\text{not } C| R)\\)). The false discovery rate (FDR) is the conditional probability of not having the condition given a positive test: \\(P(\\text{not } C| R)\\). Lower values reflect greater accuracy. The formula for calculating false discovery rate is in Equation 17.21.\n\\[\n\\begin{aligned}\n  \\text{false discovery rate (FDR)} &= P(\\text{not } C|R) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TP}} = 1 - \\text{PPV}\n\\end{aligned}\n\\tag{17.21}\\]\n\n17.6.10.21 False Omission Rate (FOR)\nMany people confuse the false negative rate (\\(P(\\text{not } R|C)\\)) with its inverse conditional probability, the false omission rate (\\(P(C|\\text{not } R)\\)). The false omission rate (FOR) is the conditional probability of having the condition given a negative test: \\(P(C|\\text{not } R)\\). Lower values reflect greater accuracy. The formula for calculating false omission rate is in Section 17.6.10.21.\n\\[\n\\begin{aligned}\n  \\text{false omission rate (FOR)} &= P(C|\\text{not } R) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TN}} = 1 - \\text{NPV}\n\\end{aligned}\n\\tag{17.22}\\]\n\n17.6.10.22 Youden’s J Statistic\nYouden’s J statistic is also called Youden’s Index or informedness. Youden’s J statistic is the sum of sensitivity and specificity (and subtracting one). Higher values reflect greater accuracy. The formula for calculating Youden’s J statistic is in Equation 17.23.\n\\[\n\\begin{aligned}\n  \\text{Youden's J statistic} &= \\text{sensitivity} + \\text{specificity} - 1\n\\end{aligned}\n\\tag{17.23}\\]\n\n17.6.10.23 Balanced Accuracy\nBalanced accuracy is the average of sensitivity and specificity. Higher values reflect greater accuracy. The formula for calculating balanced accuracy is in Equation 17.24.\n\\[\n\\begin{aligned}\n  \\text{balanced accuracy} &= \\frac{\\text{sensitivity} + \\text{specificity}}{2}\n\\end{aligned}\n\\tag{17.24}\\]\n\n17.6.10.24 F-Score\nThe F-score combines precision (positive predictive value) and recall (sensitivity), where \\(\\beta\\) indicates how many times more important sensitivity is than the positive predictive value. If sensitivity and the positive predictive value are equally important, \\(\\beta = 1\\), and the F-score is called the \\(F_1\\) score. Higher values reflect greater accuracy. The formula for calculating the F-score is in Equation 17.25.\n\\[\n\\begin{aligned}\n  F_\\beta &= (1 + \\beta^2) \\cdot \\frac{\\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\beta^2 \\cdot \\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{(1 + \\beta^2) \\cdot \\text{TP}}{(1 + \\beta^2) \\cdot \\text{TP} + \\beta^2 \\cdot \\text{FN} + \\text{FP}}\n\\end{aligned}\n\\tag{17.25}\\]\nThe formula for calculating the \\(F_1\\) score is in Equation 17.26.\n\\[\n\\begin{aligned}\n  F_1 &= \\frac{2 \\cdot \\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FN} + \\text{FP}}\n\\end{aligned}\n\\tag{17.26}\\]\n\n17.6.10.25 Matthews Correlation Coefficient (MCC)\nThe Matthews correlation coefficient (MCC) is also called the phi coefficient. It is a correlation coefficient between predicted and observed values from a binary classification. Higher values reflect greater accuracy. The formula for calculating the MCC is in Equation 17.27.\n\\[\n\\begin{aligned}\n  \\text{MCC} &= \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n\\end{aligned}\n\\tag{17.27}\\]\n\n17.6.10.26 Diagnostic Odds Ratio\nThe diagnostic odds ratio is the odds of a positive test among people with the condition relative to the odds of a positive test among people without the condition. Higher values reflect greater accuracy. The formula for calculating the diagnostic odds ratio is in Equation 17.28. If the predictor is bad, the diagnostic odds ratio could be less than one, and values can go up from there. If the diagnostic odds ratio is greater than 2, we take the odds ratio seriously because we are twice as likely to predict accurately than inaccurately. However, the diagnostic odds ratio ignores/hides base rates. When interpreting the diagnostic odds ratio, it is important to keep in mind the practical significance, because otherwise it is not very meaningful. Consider a risk factor that has a diagnostic odds ratio of 3 for tuberculosis, i.e., it puts you at 3 times as likely to develop tuberculosis. The prevalence of tuberculosis is relatively low. Assuming the prevalence of tuberculosis is less than 1/10th of 1%, your risk of developing tuberculosis is still very low even if the risk factor (with a diagnostic odds ratio of 3) is present.\n\\[\n\\begin{aligned}\n  \\text{diagnostic odds ratio} &= \\frac{\\text{TP} \\times \\text{TN}}{\\text{FP} \\times \\text{FN}} \\\\\n  &= \\frac{\\text{sensitivity} \\times \\text{specificity}}{(1 - \\text{sensitivity}) \\times (1 - \\text{specificity})} \\\\\n  &= \\frac{\\text{PPV} \\times \\text{NPV}}{(1 - \\text{PPV}) \\times (1 - \\text{NPV})} \\\\\n  &= \\frac{\\text{LR+}}{\\text{LR}-}\n\\end{aligned}\n\\tag{17.28}\\]\n\n17.6.10.27 Diagnostic Likelihood Ratio\nThe diagnostic likelihood ratio is described in Section 16.8.2.1. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio.\n\n17.6.10.27.1 Positive Likelihood Ratio (LR+)\nThe positive likelihood ratio (LR+) is described in Section 16.8.2.1.1. The formula for calculating the positive likelihood ratio is in Equation 16.22.\n\n17.6.10.27.2 Negative Likelihood Ratio (LR−)\nThe negative likelihood ratio (LR−) is described in Section 16.8.2.1.2. The formula for calculating the negative likelihood ratio is in Equation 16.22.\n\n17.6.10.28 Posttest Odds\nAs presented in Equation 16.21, the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. The posttest odds and posttest probability can be useful to calculate when the pretest probability is different from the pretest probability (or prevalence) of the classification. For instance, you might use a different pretest probability if a test result is already known and you want to know the updated posttest probability after conducting a second test. The formula for calculating posttest odds is in Equation 17.29.\n\\[\n\\begin{aligned}\n  \\text{posttest odds} &= \\text{pretest odds} \\times \\text{likelihood ratio} \\\\\n\\end{aligned}\n\\tag{17.29}\\]\nFor calculating the posttest odds of a true positive compared to a false positive, we use the positive likelihood ratio below. We would use the negative likelihood ratio if we wanted to calculate the posttest odds of a false negative compared to a true negative.\n\n17.6.10.29 Posttest Probability\nThe posttest probability is the probability of having the characteristic given a test result. When the base rate is used as the pretest probability, the posttest probability given a positive test is equal to positive predictive value. To convert odds to a probability, divide the odds by one plus the odds, as is in Equation 17.30.\n\\[\n\\begin{aligned}\n  \\text{posttest probability} &= \\frac{\\text{posttest odds}}{1 + \\text{posttest odds}}\n\\end{aligned}\n\\tag{17.30}\\]\n\n17.6.10.30 Mean Difference Between Predicted and Observed Values\nThe mean difference between predicted values versus observed values at a given cutoff is an index of miscalibration of predictions at that cutoff. It is called “calibration-in-the-small” (as opposed to calibration-in-the-large, which spans all cutoffs). Values closer to zero reflect greater accuracy. Values above zero indicate that the predicted values are, on average, greater than the observed values. Values below zero indicate that the observed values are, on average, greater than the predicted values.\n\n17.6.11 Optimal Cutoff\nThe petersenlab package (Petersen, 2025a) contains the petersenlab::optimalCutoff() function that identifies the cutoff that optimizes each of various accuracy estimates.\n\nCodepetersenlab::optimalCutoff(\n  predicted = projectionsWithActuals_seasonal_subset$raw_points,\n  actual = projectionsWithActuals_seasonal_subset$truth)\n\n\n\n\n  percentAccuracyCutoff percentAccuracyOptimal\n1              370.1761               96.27415\n2              408.2511               96.27415\n  percentAccuracyByChanceCutoff percentAccuracyByChanceOptimal\n1                      408.2511                       96.27415\n  RIOCCutoff RIOCOptimal\n1       0.30           1\n2       0.40           1\n3       1.06           1\n4       1.20           1\n5       1.40           1\n6       1.50           1\n  relativeImprovementOverPredictingFromBaseRateCutoff\n1                                            370.1761\n2                                            408.2511\n  relativeImprovementOverPredictingFromBaseRateOptimal\n1                                                    0\n2                                                    0\n  PPVCutoff PPVOptimal\n1  370.1761        0.5\n  NPVCutoff NPVOptimal\n1      0.30          1\n2      0.40          1\n3      1.06          1\n4      1.20          1\n5      1.40          1\n6      1.50          1\n  youdenJCutoff youdenJOptimal\n1           213      0.7809276\n  balancedAccuracyCutoff balancedAccuracyOptimal\n1                    213               0.8904638\n  f1ScoreCutoff f1ScoreOptimal\n1         279.6      0.4745763\n  mccCutoff mccOptimal\n1     262.8  0.4548917\n  diagnosticOddsRatioCutoff diagnosticOddsRatioOptimal\n1                       199                   282.4242\n  positiveLikelihoodRatioCutoff positiveLikelihoodRatioOptimal\n1                      370.1761                       25.83951\n  negativeLikelihoodRatioCutoff negativeLikelihoodRatioOptimal\n1                          0.30                              0\n2                          0.40                              0\n3                          1.06                              0\n4                          1.20                              0\n5                          1.40                              0\n6                          1.50                              0\n  dPrimeSDTCutoff dPrimeSDTOptimal\n1             199         3.015908\n  betaSDTCutoff betaSDTOptimal\n1          0.30              0\n2          0.40              0\n3          1.06              0\n4          1.20              0\n5          1.40              0\n6          1.50              0\n  cSDTCutoff cSDTOptimal\n1   226.3314 0.001354884\n  aSDTCutoff aSDTOptimal\n1        209   0.9401488\n  bSDTCutoff  bSDTOptimal\n1        0.3 0.0009546539\n  differenceBetweenPredictedAndObservedCutoff\n1                                        0.20\n2                                        0.30\n3                                        0.40\n4                                        1.06\n5                                        1.20\n6                                        1.40\n  differenceBetweenPredictedAndObservedOptimal\n1                                     11.02541\n2                                     11.02541\n3                                     11.02541\n4                                     11.02541\n5                                     11.02541\n6                                     11.02541\n  informationGainCutoff informationGainOptimal\n1                   213             0.07611946",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-thresholdIndependentAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-thresholdIndependentAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.7 Threshold-Independent Accuracy Indices",
    "text": "17.7 Threshold-Independent Accuracy Indices\nThis section describes threshold-independent indexes of accuracy. That is, each index of accuracy described in this section provides a single numerical index of accuracy that aggregates the accuracy across all possible cutoffs. The petersenlab package (Petersen, 2025a) contains the petersenlab::accuracyOverall() function that computes many threshold-independent accuracy indices.\n\n17.7.1 General Prediction Accuracy\nThere are many metrics of general prediction accuracy. When thinking about which metric(s) may be best for a given problem, it is important to consider the purpose of the assessment. The estimates of general prediction accuracy are separated below into scale-dependent and scale-independent accuracy estimates.\n\n17.7.1.1 Scale-Dependent Accuracy Estimates\nThe estimates of prediction accuracy described in this section are scale-dependent. These accuracy estimates depend on the unit of measurement and therefore cannot be compared across measures with different scales or across data sets.\n\n17.7.1.1.1 Mean Error (ME)\nHere, “error” (\\(e\\)) is the difference between the predicted and observed value for a given individual (\\(i\\)). Mean error (ME; also known as bias) is the mean difference between the predicted and observed values across individuals (\\(i\\)), that is, the mean of the errors across individuals (\\(e_i\\)). Values closer to zero reflect greater accuracy. If mean error is above zero, it indicates that predicted values are, on average, greater than observed values (i.e., overestimating errors). If mean error is below zero, it indicates that predicted values are, on average, less than observed values (i.e., underestimating errors). If both over-estimating and under-estimating errors are present, however, they can cancel each other out. As a result, even with a mean error of zero, there can still be considerable error present. Thus, although mean error can be helpful for examining whether predictions systematically under- or over-estimate the actual scores, other forms of accuracy are necessary to examine the extent of error. The formula for mean error is in Equation 17.31:\n\\[\n\\begin{aligned}\n  \\text{mean error} &= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)}{n} \\\\\n  &= \\text{mean}(e_i)\n\\end{aligned}\n\\tag{17.31}\\]\n\n17.7.1.1.2 Mean Absolute Error (MAE)\nMean absolute error (MAE) is the mean of the absolute value of differences between the predicted and observed values across individuals. That is, MAE is the mean of the absolute value of errors (i.e., the mean distance between predicted and observed values). Smaller MAE values (closer to zero) reflect greater accuracy. MAE is preferred over root mean squared error (RMSE) when you want to give equal weight to all errors and when the outliers have considerable impact. The formula for MAE is in Equation 17.32:\n\\[\n\\begin{aligned}\n  \\text{mean absolute error (MAE)} &= \\frac{\\sum\\limits_{i = 1}^n|\\text{predicted}_i - \\text{observed}_i|}{n} \\\\\n  &= \\text{mean}(|e_i|)\n\\end{aligned}\n\\tag{17.32}\\]\n\n17.7.1.1.3 Median Absolute Error (MdAE)\nMedian absolute error (MdAE) is the median of the absolute value of differences between the predicted and observed values across individuals. That is, MdAE is the median of the absolute value of errors (i.e., the median distance between predicted and observed values). Smaller MdAE values (closer to zero) reflect greater accuracy. MdAE indicates the accuracy of the “typical” prediction (i.e., the prediction that is at the 50th percentile in terms of accuracy). The formula for MdAE is in Equation 17.33:\n\\[\n\\begin{aligned}\n  \\text{median absolute error (MdAE)} &= \\frac{\\text{median}(|\\text{predicted}_i - \\text{observed}_i|)}{n} \\\\\n  &= \\text{median}(|e_i|)\n\\end{aligned}\n\\tag{17.33}\\]\n\n17.7.1.1.4 Mean Squared Error (MSE)\nMean squared error (MSE) is the mean of the square of the differences between the predicted and observed values across individuals, that is, the mean of the squared value of errors. Smaller MSE values (closer to zero) reflect greater accuracy. MSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, MSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for MSE is in Equation 17.34:\n\\[\n\\begin{aligned}\n  \\text{mean squared error (MSE)} &= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n} \\\\\n  &= \\text{mean}(e_i^2)\n\\end{aligned}\n\\tag{17.34}\\]\n\n17.7.1.1.5 Root Mean Squared Error (RMSE)\nRoot mean squared error (RMSE) is the square root of the mean of the square of the differences between the predicted and observed values across individuals, that is, the root mean squared value of errors. Smaller RMSE values (closer to zero) reflect greater accuracy. RMSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, RMSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for RMSE is in Equation 17.35:\n\\[\n\\begin{aligned}\n  \\text{root mean squared error (RMSE)} &= \\sqrt{\\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n}} \\\\\n  &= \\sqrt{\\text{mean}(e_i^2)}\n\\end{aligned}\n\\tag{17.35}\\]\n\n17.7.1.2 Scale-Independent Accuracy Estimates\nThe estimates of prediction accuracy described in this section are intended to be scale-independent (unit-free) so the accuracy estimates can be compared across measures with different scales or across data sets (Hyndman & Athanasopoulos, 2021).\n\n17.7.1.2.1 Mean Percentage Error (MPE)\nMean percentage error (MPE) values closer to zero reflect greater accuracy. The formula for percentage error is in Equation 17.36:\n\\[\n\\begin{aligned}\n  \\text{percentage error }(p_i) = \\frac{100\\% \\times (\\text{observed}_i - \\text{predicted}_i)}{\\text{observed}_i}\n\\end{aligned}\n\\tag{17.36}\\]\nWe then take the mean of the percentage errors to get MPE. The formula for MPE is in Equation 17.37:\n\\[\n\\begin{aligned}\n  \\text{mean percentage error (MPE)} &= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i} \\\\\n  &= \\text{mean(percentage error)} \\\\\n  &= \\text{mean}(p_i)\n\\end{aligned}\n\\tag{17.37}\\]\nNote: MPE is undefined when one or more of the observed values equals zero, due to division by zero. The petersenlab::accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.2 Mean Absolute Percentage Error (MAPE)\nSmaller mean absolute percentage error (MAPE) values (closer to zero) reflect greater accuracy. The formula for MAPE is in Equation 17.38. MAPE is asymmetric because it overweights underestimates and underweights overestimates. MAPE can be preferable to symmetric mean absolute percentage error (sMAPE) if there are no observed values of zero and if you want to emphasize the importance of underestimates (relative to overestimates).\n\\[\n\\begin{aligned}\n  \\text{mean absolute percentage error (MAPE)} &= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\Bigg|\\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i}\\Bigg| \\\\\n  &= \\text{mean(|percentage error|)} \\\\\n  &= \\text{mean}(|p_i|)\n\\end{aligned}\n\\tag{17.38}\\]\nNote: MAPE is undefined when one or more of the observed values equals zero, due to division by zero. The petersenlab::accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.3 Symmetric Mean Absolute Percentage Error (sMAPE)\nUnlike MAPE, symmetric mean absolute percentage error (sMAPE) is symmetric because it equally weights underestimates and overestimates. Smaller sMAPE values (closer to zero) reflect greater accuracy. The formula for sMAPE is in Equation 17.39:\n\\[\n\\small\n\\begin{aligned}\n  \\text{symmetric mean absolute percentage error (sMAPE)} = \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{|\\text{predicted}_i - \\text{observed}_i|}{|\\text{predicted}_i| + |\\text{observed}_i|}\n\\end{aligned}\n\\tag{17.39}\\]\nNote: sMAPE is undefined when one or more of the individuals has a prediction–observed combination such that the sum of the absolute value of the predicted value and the absolute value of the observed value equals zero (\\(|\\text{predicted}_i| + |\\text{observed}_i|\\)), due to division by zero. The petersenlab::accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.4 Mean Absolute Scaled Error (MASE)\nMean absolute scaled error (MASE) is described by (Hyndman & Athanasopoulos, 2021). Values closer to zero reflect greater accuracy.\nThe adapted formula for MASE with non-time series data is described by Hyndman (2014) at the following link: https://stats.stackexchange.com/a/108963/20338 (archived at https://perma.cc/G469-8NAJ). Scaled errors are calculated using Equation 17.40:\n\\[\n\\begin{aligned}\n  \\text{scaled error}(q_i) &= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{scaling factor}} \\\\\n  &= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|}\n\\end{aligned}\n\\tag{17.40}\\]\nThen, we calculate the mean of the absolute value of the scaled errors to get MASE, as in Equation 17.41:\n\\[\n\\begin{aligned}\n  \\text{mean absolute scaled error (MASE)} &= \\frac{1}{n} \\sum\\limits_{i = 1}^n |q_i| \\\\\n  &= \\text{mean(|scaled error|)} \\\\\n  &= \\text{mean}(|q_i|)\n\\end{aligned}\n\\tag{17.41}\\]\nNote: MASE is undefined when the scaling factor is zero, due to division by zero. With non-time series data, the scaling factor is the average of the absolute value of individuals’ observed scores minus the average observed score (\\(\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|\\)).\n\n17.7.1.2.5 Root Mean Squared Log Error (RMSLE)\nThe squared log of the accuracy ratio is described by Tofallis (2015). The accuracy ratio is in Equation 17.42:\n\\[\n\\begin{aligned}\n  \\text{accuracy ratio} &= \\frac{\\text{predicted}_i}{\\text{observed}_i}\n\\end{aligned}\n\\tag{17.42}\\]\nHowever, the accuracy ratio is undefined with observed or predicted values of zero, so it is common to modify it by adding 1 to the predictor and denominator, as in Equation 17.43:\n\\[\n\\begin{aligned}\n  \\text{accuracy ratio} &= \\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\n\\end{aligned}\n\\tag{17.43}\\]\nSquaring the log values keeps the values positive, such that smaller values (values closer to zero) reflect greater accuracy. Then we take the mean of the squared log values, which keeps the values positive, and calculate the square root of the mean squared log values to put them back on the (pre-squared) log metric. This is known as the root mean squared log error (RMSLE). Division inside the log is equal to subtraction outside the log. So, the formula can be reformulated with the subtraction of two logs, as in Equation 17.44:\n\\[\n\\scriptsize\n\\begin{aligned}\n  \\text{root mean squared log error (RMSLE)} &= \\sqrt{\\sum\\limits_{i = 1}^n log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2} \\\\\n  &= \\sqrt{\\text{mean}\\Bigg[log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2\\Bigg]} \\\\\n  &= \\sqrt{\\text{mean}\\big[log(\\text{accuracy ratio})^2\\big]} = \\sqrt{\\text{mean}\\Big\\{\\big[log(\\text{predicted}_i + 1) - log(\\text{actual}_i + 1)\\big]^2\\Big\\}}\n\\end{aligned}\n\\tag{17.44}\\]\nRMSLE can be preferable when the scores have a wide range of values and are skewed. RMSLE can help to reduce the impact of outliers. RMSLE gives more weight to smaller errors in the prediction of small observed values, while also penalizing larger errors in the prediction of larger observed values. It overweights underestimates and underweights overestimates.\nThere are other variations of prediction accuracy metrics that use the log of the accuracy ratio. One variation makes it similar to median symmetric percentage error (Morley et al., 2018).\nNote: Root mean squared log error is undefined when one or more predicted values or actual values equals −1. When predicted or actual values are -1, this leads to \\(log(0)\\), which is undefined. The petersenlab::accuracyOverall() function of the petersenlab package (Petersen, 2025a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n17.7.1.2.6 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) is described in Section 11.6.1. The coefficient of determination reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions: \\(R^2 = \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\\). Larger values indicate greater accuracy.\n\\(R^2\\) is commonly estimated in multiple regression, in which multiple predictors are allowed to predict one outcome.\nAn issue with \\(R^2\\) is that it can be artificially inflated by adding more predictors to the model, even if those predictors do not improve the model fit. \\(R^2\\) never decreases when adding predictors to a model, even if the predictors do not provide any predictive power. Thus, to account for the number of predictors in the model, we can use adjusted \\(R^2\\).\n\n17.7.1.2.6.1 Adjusted \\(R^2\\) (\\(R^2_{adj}\\))\nAdjusted \\(R^2\\) (\\(R^2_{adj}\\)) is described in Section 11.6.2. Adjusted \\(R^2\\) is similar to the coefficient of determination, but it accounts for the number of predictors included in the regression model to penalize overfitting. Specifically, adjusted \\(R^2\\) penalizes the number of predictors in the model. Adjusted \\(R^2\\) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictors in the model. Thus, adjusted \\(R^2\\) will increase only if the new predictor improves the model (i.e., explains additional variance in the outcome variable) more than would be expected by chance. Larger values indicate greater accuracy. The formula for adjusted \\(R^2\\) is in Equation 11.5.\n\n17.7.1.2.6.2 Predictive \\(R^2\\) (\\(R^2_{\\text{pred}}\\))\nPredictive \\(R^2\\) (\\(R^2_{\\text{pred}}\\)) is described by Hopper (2014) and Tüzen (2025): https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/ (archived at https://perma.cc/BK8J-HFUK) and https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/ (archived at https://perma.cc/EPP8-VG23). Predictive \\(R^2\\) penalizes overfitting, unlike traditional \\(R^2\\). Larger values indicate greater accuracy. The formula for predictive \\(R^2\\) is in in Equation 17.45:\n\\[\n\\begin{aligned}\n  R^2_{\\text{pred}} &= 1 - \\frac{\\text{PRESS}}{\\text{SS}_\\text{total}}\n\\end{aligned}\n\\tag{17.45}\\]\nwhere PRESS is the predictive residual sum of squares, and \\(\\text{SS}_\\text{total}\\) is the total sum of squares.\nIt is preferable to examine predictive \\(R^2\\) using new data that the model was not trained on—i.e., cross-validation, either in an independent sample (i.e., external cross-validation) or in a hold-out sample (i.e., internal cross-validation on test data).\nFor example, here is an example of estimating predictive \\(R^2\\) using 10-fold cross-validation using the tidymodels ecosystem of packages (Kuhn & Wickham, 2020, 2025).\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Set up Cross-Validation\nfolds_kFold &lt;- rsample::group_vfold_cv(\n  projectionsWithActuals_seasonal,\n  group = player_id, # ensures all rows for a player are in the training set or all in the validation set for each fold\n  v = 10) # 10-fold cross-validation\n\nfolds &lt;- folds_kFold\n\n# Define Recipe (Formula)\nrec &lt;- recipes::recipe(\n  fantasyPoints ~ raw_points, # use all predictors\n  data = projectionsWithActuals_seasonal)\n\n# Define Model\nlm_spec &lt;- parsnip::linear_reg() %&gt;%\n  parsnip::set_engine(\"lm\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\n# Workflow\nlm_wf &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(lm_spec)\n\n# Fit Model with Cross-Validation\ncv_results &lt;- tune::fit_resamples(\n  lm_wf,\n  resamples = folds,\n  metrics = yardstick::metric_set(rmse, mae, rsq),\n  control = tune::control_resamples(save_pred = TRUE)\n)\n\n# View Cross-Validation metrics\ntune::collect_metrics(cv_results)\n\n\n  \n\n\nCode# Fold-level R-squared\ntune::collect_metrics(cv_results, summarize = FALSE) %&gt;%\n  filter(.metric == \"rsq\") %&gt;%\n  select(id, .estimate)\n\n\n  \n\n\n\nHere is an example of estimating predictive \\(R^2\\) with a training and hold-out (testing) data set:\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Create 75/25 split\nsplit &lt;- rsample::group_initial_split(\n  projectionsWithActuals_seasonal,\n  group = player_id,\n  prop = 0.75)\n\n# Fit model on training, evaluate on testing\nlast_fit_results &lt;- tune::last_fit(\n  lm_wf,\n  split = split,\n  metrics = yardstick::metric_set(rmse, mae, rsq)\n)\n\n# View predictive R-squared and other metrics\ntune::collect_metrics(last_fit_results)\n\n\n  \n\n\n\nOr, alternatively:\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Create 75/25 split\nsplit &lt;- rsample::group_initial_split(\n  projectionsWithActuals_seasonal,\n  group = player_id,\n  prop = 0.75)\n\ntraining &lt;- rsample::training(split)\ntesting  &lt;- rsample::testing(split)\n\n# Fit model on training set\ntraining_model &lt;- parsnip::fit(\n  lm_wf,\n  data = training)\n\n# Predict on test set\npredictions &lt;- predict(\n  training_model,\n  new_data = testing) %&gt;%\n  bind_cols(testing)\n\n# Predictive R-squared\nyardstick::rsq(\n  predictions,\n  truth = fantasyPoints,\n  estimate = .pred)\n\n\n\n17.7.2 Discrimination\nWhen dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Threshold-dependent aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity) are described in Section 17.6.\n\n17.7.2.1 Area under the ROC curve (AUC)\nThe area under the ROC curve (AUC) is a general index of discrimination accuracy for a categorical outcome. It is also called the concordance (\\(c\\)) statistic. Larger values reflect greater discrimination accuracy. AUC was estimated using the pROC package (Robin et al., 2011, 2023).\n\n17.7.2.2 Effect Size (\\(\\beta\\)) of Regression\nThe effect size of a predictor, i.e., the standardized regression coefficient is called a beta (\\(\\beta\\)) coefficient, is a general index of discrimination accuracy for a continuous outcome. Larger values reflect greater accuracy. We can obtain standardized regression coefficients by standardizing the predictors and outcome using the base::scale() function in R.\n\n17.7.3 Calibration\nWhen dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by Lindhiem et al. (2020). Calibration can be examined in several ways, including Spiegelhalter’s \\(z\\) (see Section 17.7.3.2), and the mean difference between predicted and observed values at different binned thresholds as depicted graphically with a calibration plot (see Figure 17.15).\n\n17.7.3.1 Calibration Plot\nCalibration plots can be helpful for identifying miscalibration. A calibration plot depicts the predicted probability of an event on the x-axis, and the actual (observed) probability of the event on the y-axis. The predictions are binned into a certain number of groups (commonly 10). The diagonal line reflects predictions that are perfectly calibrated. To the extent that predictions deviate from the diagonal line, the predictions are miscalibrated.\nWell-calibrated predictions are depicted in Figure 17.14:\n\nCode# Specify data\nexamplePredictionsWellCalibrated &lt;- seq(from = 0, to = 1, by = .1)\nexampleOutcomesWellCalibrated &lt;- seq(from = 0, to = 1, by = .1)\n\n# Plot\nplot(\n  examplePredictionsWellCalibrated,\n  exampleOutcomesWellCalibrated,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  type = \"n\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\npoints(\n  examplePredictionsWellCalibrated,\n  exampleOutcomesWellCalibrated,\n  cex = 1.5,\n  col = \"#e41a1c\",\n  lwd = 2,\n  type = \"p\")\n\n\n\n\n\n\nFigure 17.14: Predictions that are Well-Calibrated. That is, the predicted values are close to the observed values.\n\n\n\n\nThe various types of general miscalibration are depicted in Figure 17.15:\n\nCode# Specify data\nexamplePredictions &lt;- seq(from = 0, to = 1, by = .1)\nexampleOutcomes &lt;- c(0, .15, .3, .4, .45, .5, .55, .6, .7, .85, 1)\n\noverPrediction &lt;- c(0, .02, .05, .1, .15, .2, .3, .4, .5, .7, 1)\nunderPrediction &lt;- c(0, .3, .5, .6, .7, .8, .85, .9, .95, .98, 1)\noverExtremity &lt;- c(0, .3, .38, .42, .47, .5, .53, .58, .62, .7, 1)\nunderExtremity &lt;- c(0, .05, .08, .11, .2, .5, .8, .89, .92, .95, 1)\n\n# Plot\npar(\n  mfrow = c(2,2),\n  mar = c(5,4,1,1) + 0.1) #margins: bottom, left, top, right\n\nplot(\n  examplePredictions,\n  overExtremity,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Overextremity\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  underExtremity,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Underextremity\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  overPrediction,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Overprediction\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  underPrediction,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Underprediction\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\n\n\n\n\n\nFigure 17.15: Types of Miscalibration. From Petersen (2024) and Petersen (2025b).\n\n\n\n\nHowever, predictions could also be miscalibrated in more specific ways. For instance, predictions could be well-calibrated at all predicted probabilities except for a given predicted probability (e.g., 20%). Or, the predictions could be miscalibrated but not systematically over- or underpredicted. Thus, it is important to evaluate a calibration plot to evaluate the extent to which the predictions are miscalibrated and the pattern of that miscalibration.\n\n17.7.3.2 Spiegelhalter’s z\n\nSpiegelhalter’s z was calculated using the rms package (Harrell, Jr., 2025). Smaller z values (and larger associated p-values) reflect greater calibration accuracy. A statistically significant Spiegelhalter’s z (p &lt; .05) indicates a significant degree of miscalibration.\n\n17.7.3.3 Calibration for Predicting a Continuous Outcome\nWhen predicting a continuous outcome, calibration of the predicted values in relation to the outcome values can be examined in multiple ways including:\n\nin a calibration plot, the extent to which the intercept is near zero and the slope is near one\nin a calibration plot, the extent to which the 95% confidence interval of the observed value, across all values of the predicted values, includes the diagonal reference line with an intercept of zero and a slope of one\nmean error\nmean absolute error\nmean squared error\nroot mean squared error\n\nWith a plot of the predictions on the x-axis, and the outcomes on the y-axis (i.e., a calibration plot), calibration can be examined graphically as the extent to which the best-fit regression line has an intercept (alpha) close to zero and a slope (beta) close to one (Stevens & Poppe, 2020; Steyerberg & Vergouwe, 2014). The intercept is also called “calibration-in-the-large”, whereas “calibration-in-the-small” refers to the extent to which the predicted values match the observed values at a specific predicted value (e.g., when the weather forecaster says that there is a 10% chance of rain, does it actually rain 10% of the time?). For predictions to be well calibrated, the intercept should be close to zero and the slope should be close to one. If the slope is close to one but the intercept is not close to zero (or the intercept is close to zero but the slope is not close to one), the predictions would not be considered well calibrated. The 95% confidence interval of the observed value, across all values of the predicted values, should include the diagonal reference line whose intercept is zero and whose slope is one. Gold-standard recommendations include examining the predicted values in relation to the observed values using locally estimated scatterplot smoothing (LOESS) (Austin & Steyerberg, 2014), such as in Figure 17.17. We can examine whether the LOESS-based 95% confidence interval of the observed value at every level of the predicted values includes the diagonal reference line (i.e., the actual observed value).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-integratingAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-integratingAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.8 Integrating the Accuracy Indices",
    "text": "17.8 Integrating the Accuracy Indices\nAfter computing the accuracy indices of discrimination and (2) calibration, it is then the task to integrate the indices to determine (a) which are the most accurate predictions for the given goals, and (b) whether additional improvements and refinements to the predictions need to be made. Each of the accuracy indices is computed differently and thus reward (and penalize) predictive (in)accuracy differently. Sometimes, the the accuracy indices will paint a consistent picture regarding which predictions are the most accurate. Other times, the accuracy indices may disagree about which predictions are most accurate.\nIn fantasy football, when evaluating the accuracy of seasonal projections, we care most about accurately distinguishing between higher levels of points (e.g., 200 vs 150) as opposed to lower levels of points (e.g., 0 vs 10). Thus, it can be helpful to punish larger errors more heavily than smaller errors, as RMSE (unlike MAE).\nThus, we would emphasize the following metrics:\n\n\ndiscrimination:\n\nadjusted \\(R^2\\)\n\n\n\ncalibration:\n\ncalibration plot\n\n\n\ngeneral accuracy:\n\nMAE\nRMSE\n\n\n\nIf you focus on only one accuracy index, MAE or RMSE would be a good choice. However, I would also examine a calibration plot to evaluate whether predictions are poorly calibrated at higher levels of points. I would also examine ME—not to compare the accuracy of various predictions per se—but to determine whether predictions are systematically under- or overestimating actual points. If so, predictions may be able to be refined by adding or subtracting a constant to the predictions (or to a subset of the predictions); however, this could worsen other accuracy indices, so it is important to conduct an iterative process of modifying then evaluating, then further modifying and evaluating, etc. It may also be valuable to evaluate the accuracy of various subsets of the predictions. For instance, you might examine the predictive accuracy of players whose projected points are greater than 100, to evaluate the accuracy of predictions specifically to distinguish between players at higher levels of points, which is one of the key goals when selecting which players to draft.\nIf we are making predictions about a categorical variable, we would emphasize the following metrics:\n\n\ndiscrimination:\n\n\narea under the receiver operating characteristic curve (AUC)\nand, secondarily—depending on the particulary cutoff and the relative costs of false positives versus false negatives:\n\nsensitivity\nspecificity\n\npositive predictive value (PPV)\n\nnegative predictive value (NPV)\n\n\n\n\n\ncalibration:\n\ncalibration plot\nSpiegelhalter’s z\nMean difference between observed and predicted values",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-theoryVsEmpiricism",
    "href": "evaluating-prediction-accuracy.html#sec-theoryVsEmpiricism",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.9 Theory Versus Empiricism",
    "text": "17.9 Theory Versus Empiricism\nOne question that inevitably arises when making predictions is the extent to which one should leverage theory versus empiricism. Theory involves conceptual claims of understanding how the causal system works (i.e., what influences what). For example, use of theory in prediction might involve specification of the causal system that influences player performance, measurement of those factors, and the integration of that information to make a prediction. Empiricism involves “letting the data speak for themselves” and is an atheoretical approach. For example, empiricism might involve examining how thousands of variables are associated with the criterion of interest (e.g., fantasy points) and developing the best-fitting model based on those thousands of predictor variables.\nAlthough the atheoretical approach can perform reasonably well, it can be improved by making better use of theory. An empirical result (e.g., a correlation) might not necessarily have a lot of meaning associated with it. As the maxim goes, correlation does not imply causation. Moreover, empiricism can lead to overfitting. So, empiricism is often not enough.\nAs Silver (2012) notes, “The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.” (p. 9). If we understand the variables in the system and how they influence each other, we can predict things more accurately than predicting for the sake of predicting. For instance, we have made great strides in the last decades when it comes to more accurate weather forecasts [Rosalsky (2023); archived at https://perma.cc/PF8P-BT3D], including extreme weather events like hurricanes. These great strides have more to do with a better causal understanding of the weather system and the ability to conduct simulations of the atmosphere than merely because of big data (Silver, 2012). By contrast, other events are still incredibly difficult to predict, including earthquakes, in large part because we do not have a strong understanding of the system (and because we do not have ways of precisely measuring those causes because they occur at a depth below which we are realistically able to drill) (Silver, 2012).\nAt the same time, in the social and behavioral sciences, our theories of the causal processes that influence outcomes are not yet very strong. Indeed, I have misgivings calling them theories because they do not meet the traditional scientific standard for a theory. A scientific theory is an explanation of the natural world that is testable and falsifiable, and that has withstood rigorous scientific testing and scrutiny. In psychology (and other areas of social and behavioral sciences), our “theories” are more like conceptual frameworks. And these conceptual frameworks are often vague, do not make specific predictions of effects and noneffects, and do not hold up consistently when rigorously tested. As described by Meehl (1978):\n\nI consider it unnecessary to persuade you that most so-called “theories” in the soft areas of psychology (clinical, counseling, social, personality, community, and school psychology) are scientifically unimpressive and technologically worthless … Perhaps the easiest way to convince yourself is by scanning the literature of soft psychology over the last 30 years and noticing what happens to theories. Most of them suffer the fate that General MacArthur ascribed to old generals—They never die, they just slowly fade away. In the developed sciences, theories tend either to become widely accepted and built into the larger edifice of well-tested human knowledge or else they suffer destruction in the face of recalcitrant facts and are abandoned, perhaps regretfully as a “nice try.” But in fields like personology and social psychology, this seems not to happen. There is a period of enthusiasm about a new theory, a period of attempted application to several fact domains, a period of disillusionment as the negative data come in, a growing bafflement about inconsistent and unreplicable empirical results, multiple resort to ad hoc excuses, and then finally people just sort of lose interest in the thing and pursue other endeavors.\n— Meehl (1978, pp. 806–807)\n\nEven if we had strong theoretical understanding of the causal system that influences behavior, we would likely still have difficulty making accurate predictions because the field has largely relied on relatively crude instruments. According to one philosophical perspective known as LaPlace’s demon, if we were able to know the exact conditions of everything in the universe, we would be able to know how the conditions would be in the future. This is an example of scientific determinism, where if you know the initial conditions, you also know the future. Other perspectives, such as quantum mechanics and chaos theory, would say that, even if we knew the initial conditions with 100% certainty, there would still be uncertainty in our understanding of the future. But assume, for a moment, that LaPlace’s demon is true. A challenge in the social and behavioral sciences is that we have a relatively poor understanding of the initial conditions of the universe. Thus, our predictions would necessarily be probabilistic, similar to weather forecasts. Despite having a strong understanding of how weather systems behave, we have imperfect understanding of the initial conditions (e.g., the position and movement of all molecules) (Silver, 2012).\nTheories tend to make grand conceptual claims that one observed variable influences another observed variable through a complex chain of intervening processes that are unobservable. Empiricism provides rich lower-level information, but lacks the broader picture. So, it seems, that we need both theory and empiricism. Theory and empiricism can—and should—inform each other.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-testBias",
    "href": "evaluating-prediction-accuracy.html#sec-testBias",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.10 Test Bias",
    "text": "17.10 Test Bias\nTest bias refers to systematic error (in measurement, prediction, etc.) as a function of group membership that leads the same score to have different meaning for different groups. For instance, if the Wonderlic Contemporary Cognitive Ability Test is a strong predictor of performance for Quarterbacks but not for Running Backs, the test is biased. Test bias, including how to identify and address it, is described in Petersen (2025b).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionCategoricalExample",
    "href": "evaluating-prediction-accuracy.html#sec-predictionCategoricalExample",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.11 Example of Predicting a Categorical outcome",
    "text": "17.11 Example of Predicting a Categorical outcome\nLet’s consider an example of predicting a categorical outcome. For this example, let’s predict whether or not a Running Back rushes for 200 yards in a game. To predict that, we will use the number of rushing yards the player had in the previous game.\n\nCodeplayer_stats_weekly_example &lt;- player_stats_weekly %&gt;% \n  filter(position == \"RB\") %&gt;% \n  arrange(player_id, season, week) %&gt;% \n  group_by(player_id, season) %&gt;% \n  mutate(rushing_yards_priorGame = lag(rushing_yards)) %&gt;% \n  ungroup()\n\n\n\nCodeplayer_stats_weekly_example %&gt;% \n  select(player_id, season, week, rushing_yards, rushing_yards_priorGame) %&gt;% \n  head()\n\n\n  \n\n\n\nHere are our accuracy indices at a cutoff of 100. That is, we predict that each player who rushed more than 100 yards in a given game will rush for 200 yards in their next game.\n\nCodepetersenlab::accuracyAtCutoff(\n  predicted = player_stats_weekly_example$rushing_yards_priorGame,\n  actual = player_stats_weekly_example$rush_200_yds,\n  cutoff = 100\n)\n\n\n  \n\n\n\nNow, let’s lower the cutoff to 50 and see how that impacts our predictive accuracy.\n\nCodepetersenlab::accuracyAtCutoff(\n  predicted = player_stats_weekly_example$rushing_yards_priorGame,\n  actual = player_stats_weekly_example$rush_200_yds,\n  cutoff = 50\n)\n\n\n  \n\n\n\nNow, let’s raise the cutoff to 150 and see how that impacts our predictive accuracy.\n\nCodepetersenlab::accuracyAtCutoff(\n  predicted = player_stats_weekly_example$rushing_yards_priorGame,\n  actual = player_stats_weekly_example$rush_200_yds,\n  cutoff = 150\n)\n\n\n  \n\n\n\nWe can also examine the accuracy at each possible cutoff.\n\nCodepetersenlab::accuracyAtEachCutoff(\n  predicted = player_stats_weekly_example$rushing_yards_priorGame,\n  actual = player_stats_weekly_example$rush_200_yds\n)\n\n\n  \n\n\n\nWe can also create a receiver operating characteristic (ROC) curve, as in Figure 17.16.\n\nCoderocCurve &lt;- pROC::roc(\n  data = player_stats_weekly_example,\n  response = rush_200_yds,\n  predictor = rushing_yards_priorGame,\n  smooth = FALSE)\n\n\nHere is the area under the ROC curve (AUC):\n\nCoderocCurve$auc\n\nArea under the curve: 0.7939\n\n\n\nCodepar(pty = \"s\")\n\nplot(\n  rocCurve,\n  legacy.axes = TRUE,\n  print.auc = TRUE)\n\n\n\n\n\n\nFigure 17.16: Example Receiver Operating Characteristic (ROC) Curve.\n\n\n\n\nWe can identify the cutoff that optimizes each of various accuracy estimates.\n\nCodepetersenlab::optimalCutoff(\n  predicted = player_stats_weekly_example$rushing_yards_priorGame,\n  actual = player_stats_weekly_example$rush_200_yds)\n\n[[1]]\n  percentAccuracyCutoff percentAccuracyOptimal\n1                296.01               99.73858\n\n[[2]]\n  percentAccuracyByChanceCutoff percentAccuracyByChanceOptimal\n1                        296.01                       99.73858\n\n[[3]]\n   RIOCCutoff RIOCOptimal\n1         -12           1\n2         -11           1\n3         -10           1\n4          -9           1\n5          -8           1\n6          -7           1\n7          -6           1\n8          -5           1\n9          -4           1\n10         -3           1\n11         -2           1\n12         -1           1\n13          0           1\n\n[[4]]\n  relativeImprovementOverPredictingFromBaseRateCutoff\n1                                              296.01\n  relativeImprovementOverPredictingFromBaseRateOptimal\n1                                                    0\n\n[[5]]\n  PPVCutoff PPVOptimal\n1       228 0.05263158\n\n[[6]]\n   NPVCutoff NPVOptimal\n1        -12          1\n2        -11          1\n3        -10          1\n4         -9          1\n5         -8          1\n6         -7          1\n7         -6          1\n8         -5          1\n9         -4          1\n10        -3          1\n11        -2          1\n12        -1          1\n13         0          1\n\n[[7]]\n  youdenJCutoff youdenJOptimal\n1            33      0.4947791\n\n[[8]]\n  balancedAccuracyCutoff balancedAccuracyOptimal\n1                     33               0.7473895\n\n[[9]]\n  f1ScoreCutoff f1ScoreOptimal\n1           195     0.03314917\n\n[[10]]\n  mccCutoff mccOptimal\n1        59 0.05294265\n\n[[11]]\n  diagnosticOddsRatioCutoff diagnosticOddsRatioOptimal\n1                         2                   27.67096\n\n[[12]]\n  positiveLikelihoodRatioCutoff positiveLikelihoodRatioOptimal\n1                           228                         21.196\n\n[[13]]\n   negativeLikelihoodRatioCutoff negativeLikelihoodRatioOptimal\n1                            -12                              0\n2                            -11                              0\n3                            -10                              0\n4                             -9                              0\n5                             -8                              0\n6                             -7                              0\n7                             -6                              0\n8                             -5                              0\n9                             -4                              0\n10                            -3                              0\n11                            -2                              0\n12                            -1                              0\n13                             0                              0\n\n[[14]]\n  dPrimeSDTCutoff dPrimeSDTOptimal\n1               2         1.573486\n\n[[15]]\n   betaSDTCutoff betaSDTOptimal\n1            -12              0\n2            -11              0\n3            -10              0\n4             -9              0\n5             -8              0\n6             -7              0\n7             -6              0\n8             -5              0\n9             -4              0\n10            -3              0\n11            -2              0\n12            -1              0\n13             0              0\n14           229              0\n15           231              0\n16           233              0\n17           234              0\n18           235              0\n19           236              0\n20           238              0\n21           251              0\n22           253              0\n23           255              0\n24           266              0\n25           278              0\n26           286              0\n27           295              0\n28           296              0\n\n[[16]]\n  cSDTCutoff cSDTOptimal\n1         50 0.004124206\n\n[[17]]\n  aSDTCutoff aSDTOptimal\n1         29    0.833199\n\n[[18]]\n  bSDTCutoff   bSDTOptimal\n1        -12 0.00002944901\n\n[[19]]\n  differenceBetweenPredictedAndObservedCutoff\n1                                           0\n  differenceBetweenPredictedAndObservedOptimal\n1                                            1\n\n[[20]]\n  informationGainCutoff informationGainOptimal\n1                    33             0.00200879",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-accuracyProjections",
    "href": "evaluating-prediction-accuracy.html#sec-accuracyProjections",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.12 Accuracy of Fantasy Projections",
    "text": "17.12 Accuracy of Fantasy Projections\nBelow are estimates of projection accuracy and calibration plots for seasonal and weekly projections. FantasyFootballAnalytics.net also provides tools to evaluate the accuracy of historical projections (Fantasy Football Analytics, 2025): https://fantasyfootballanalytics.net/ffanalytics-web-app-accuracy-tab. For instance, the web application allows evaluating the accuracy of historical projections: https://apps.fantasyfootballanalytics.net.\n\n17.12.1 Seasonal Projections\n\n17.12.1.1 Overall\nHere are the seasonal accuracy indexes for all players (across all positions):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal$raw_points,\n  actual = projectionsWithActuals_seasonal$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nIn the data object, each projection source has a row for each player and season. Thus, the \\(R^2\\) may be higher for this aggregated object than it is for many of the individual sources. To calculate the accuracy by source, we could use the following code:\n\nCodeaccuracy_by_source &lt;- projectionsWithActuals_seasonal %&gt;%\n  filter(!is.na(data_src)) %&gt;% \n  filter(data_src != \"FantasyPros\") %&gt;% # remove crowd-averaged source; we compare to crowd average later\n  group_by(data_src) %&gt;%\n  summarise(\n    accuracy = list(\n      accuracyOverall(\n        predicted = raw_points,\n        actual = fantasyPoints,\n        dropUndefined = TRUE\n      )\n    ),\n    .groups = \"drop\"\n  )\n\naccuracy_by_source\n\n\n  \n\n\nCodeaccuracy_by_source$accuracy\n\n[[1]]\n        ME      MAE MdAE      MSE     RMSE       MPE    MAPE    sMAPE      MASE\n1 19.29565 49.14385 40.4 4111.242 64.11897 -86.14415 246.696 29.22287 0.6441906\n      RMSLE  rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 0.9549752 0.5387409           0.6102684              0.6092561\n  rsquaredPredictiveAsPredictor\n1                     0.6053705\n\n[[2]]\n        ME      MAE     MdAE      MSE     RMSE       MPE     MAPE    sMAPE\n1 6.654441 47.94636 40.17054 3803.477 61.67233 -121.0373 256.4296 26.11913\n     MASE     RMSLE rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 0.65631 0.8843032 0.545926           0.5608127                0.55966\n  rsquaredPredictiveAsPredictor\n1                     0.5550303\n\n[[3]]\n        ME      MAE MdAE      MSE     RMSE       MPE     MAPE    sMAPE\n1 10.22856 36.57324 26.9 2518.884 50.18849 -72.89234 225.2441 24.32337\n       MASE     RMSLE  rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 0.6369759 0.8267402 0.5787538           0.6062782              0.6057214\n  rsquaredPredictiveAsPredictor\n1                     0.6025083\n\n[[4]]\n        ME      MAE MdAE      MSE    RMSE       MPE     MAPE    sMAPE      MASE\n1 11.05553 31.74871 20.9 2070.596 45.5038 -177.9992 294.7969 32.43391 0.6523462\n     RMSLE  rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 1.065329 0.5612591           0.6326647              0.6323668\n  rsquaredPredictiveAsPredictor\n1                      0.630241\n\n[[5]]\n        ME    MAE  MdAE      MSE     RMSE       MPE     MAPE    sMAPE      MASE\n1 13.65172 50.516 40.01 4196.136 64.77759 -241.9184 262.9299 26.58786 0.6761375\n    RMSLE  rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 0.91864 0.5139577           0.5442121               0.542953\n  rsquaredPredictiveAsPredictor\n1                     0.5381388\n\n[[6]]\n        ME     MAE   MdAE      MSE     RMSE       MPE     MAPE    sMAPE\n1 25.00409 49.3935 40.915 4018.046 63.38806 -141.0942 168.0135 32.09659\n       MASE     RMSLE  rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 0.8366983 0.9968876 0.3576512           0.5352534              0.5345719\n  rsquaredPredictiveAsPredictor\n1                     0.5312735\n\n[[7]]\n        ME     MAE  MdAE      MSE     RMSE     MPE     MAPE    sMAPE      MASE\n1 28.11525 54.7398 42.05 5112.366 71.50081 -220.12 368.4965 29.44356 0.7261783\n      RMSLE  rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 0.9929489 0.4173296           0.5695807               0.568439\n  rsquaredPredictiveAsPredictor\n1                     0.5643584\n\n[[8]]\n        ME      MAE     MdAE      MSE     RMSE      MPE    MAPE    sMAPE\n1 33.74628 61.92695 51.69607 6023.084 77.60853 -102.243 115.343 24.26434\n       MASE     RMSLE rsquared rsquaredAsPredictor rsquaredAdjAsPredictor\n1 0.8104313 0.7715701 0.307629           0.4724594              0.4704686\n  rsquaredPredictiveAsPredictor\n1                     0.4638456\n\n\nThe intercept and slope of the best-fit line can be estimated using regression, as in below:\n\nCodesummary(lm(\n  fantasyPoints ~ raw_points,\n  data = projectionsWithActuals_seasonal\n))\n\n\nCall:\nlm(formula = fantasyPoints ~ raw_points, data = projectionsWithActuals_seasonal)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-269.704  -29.749   -4.654   24.988  201.857 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 5.586481   1.284043   4.351            0.0000138 ***\nraw_points  0.811480   0.009213  88.077 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 53.99 on 4936 degrees of freedom\n  (19989 observations deleted due to missingness)\nMultiple R-squared:  0.6111,    Adjusted R-squared:  0.6111 \nF-statistic:  7758 on 1 and 4936 DF,  p-value: &lt; 0.00000000000000022\n\n\nA calibration plot of the seasonal projections for all players (across all positions) is in Figure 17.17. The best-fit LOESS line is the yellow line. The intercept from the best-fit line is positive; the slope is less than one. The predicted values are less than the observed values at lower levels of projected points, whereas the predicted values are higher than the observed values at higher levels of projected points, consistent with the overextremity form of miscalibration. The confidence interval of the observed value (i.e., the purple band) is the interval within which we have 95% confidence that the true observed value would lie for a given predicted value, based on the model. The 95% prediction interval of the observed value (i.e., the dashed red lines) is the interval within which we would expect that 95% of future observations would lie for a given predicted value. We calculate the 95% prediction interval based on a LOESS model, which allows nonlinear associations, using the msir::loess.sd() function of the msir package (Scrucca, 2011, 2020). The black diagonal line indicates the reference line with an intercept of zero and a slope of one. The predictions would be significantly miscalibrated at a given level of the predicted values if the 95% confidence interval of the observed value does not include the reference line at that level of the predicted value. In this case, based on the intercept and slope of the calibration plot in Figure 17.17, the 95% confidence interval of the observed value does not include the reference line (i.e., the actual observed value) at lower or upper levels of the predicted values, so the predictions are miscalibrated at lower and upper levels of the predicted values.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal$raw_points,\n  y = projectionsWithActuals_seasonal$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.17: Calibration Plot for Seasonal Projections. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for players across all positions, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of all players (across all positions):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of players across all positions, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal$points[which(crowdAveragedProjectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.2 Quarterbacks\nHere are the seasonal accuracy indexes for all Quarterbacks:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_qb$raw_points,\n  actual = projectionsWithActuals_seasonal_qb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Quarterbacks is in Figure 17.18.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_qb$raw_points,\n  y = projectionsWithActuals_seasonal_qb$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_qb,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Quarterbacks\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.18: Calibration Plot for Seasonal Projections of Quarterbacks. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Quarterbacks who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_qb$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_qb$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Quarterbacks:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_qb$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_qb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Quarterbacks, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_qb$points[which(crowdAveragedProjectionsWithActuals_seasonal_qb$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_qb$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_qb$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.3 Running Backs\nHere are the seasonal accuracy indexes for all Running Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_rb$raw_points,\n  actual = projectionsWithActuals_seasonal_rb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Running Backs is in Figure 17.19.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_rb$raw_points,\n  y = projectionsWithActuals_seasonal_rb$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_rb,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Running Backs\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.19: Calibration Plot for Seasonal Projections of Running Backs. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Running Backs who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_rb$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_rb$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Running Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_rb$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_rb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Running Backs, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_rb$points[which(crowdAveragedProjectionsWithActuals_seasonal_rb$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_rb$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_rb$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.4 Wide Receivers\nHere are the seasonal accuracy indexes for all Wide Receivers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_wr$raw_points,\n  actual = projectionsWithActuals_seasonal_wr$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Wide Receivers is in Figure 17.20.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_wr$raw_points,\n  y = projectionsWithActuals_seasonal_wr$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_wr,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 100)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Wide Receivers\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.20: Calibration Plot for Seasonal Projections of Wide Receivers. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Wide Receivers who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_wr$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_wr$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Wide Receivers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_wr$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_wr$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Wide Receivers, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_wr$points[which(crowdAveragedProjectionsWithActuals_seasonal_wr$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_wr$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_wr$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.5 Tight Ends\nHere are the seasonal accuracy indexes for all Tight Ends:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_te$raw_points,\n  actual = projectionsWithActuals_seasonal_te$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Tight Ends is in Figure 17.21.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_te$raw_points,\n  y = projectionsWithActuals_seasonal_te$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_te,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 50)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 50)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Tight Ends\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.21: Calibration Plot for Seasonal Projections of Tight Ends. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Tight Ends who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_te$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_te$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Tight Ends:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_te$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_te$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Tight Ends, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_te$points[which(crowdAveragedProjectionsWithActuals_seasonal_te$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_te$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_te$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.6 Kickers\nHere are the seasonal accuracy indexes for all Kickers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_k$raw_points,\n  actual = projectionsWithActuals_seasonal_k$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Kickers is in Figure 17.22.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_k$raw_points,\n  y = projectionsWithActuals_seasonal_k$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_k,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 50)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 50)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Kickers\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.22: Calibration Plot for Seasonal Projections of Kickers. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Kickers who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_k$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_k$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Kickers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_k$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_k$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Kickers, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_k$points[which(crowdAveragedProjectionsWithActuals_seasonal_k$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_k$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_k$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.7 Defensive Linemen\nHere are the seasonal accuracy indexes for all Defensive Linemen:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_dl$raw_points,\n  actual = projectionsWithActuals_seasonal_dl$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Defensive Linemen is in Figure 17.23.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_dl$raw_points,\n  y = projectionsWithActuals_seasonal_dl$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_dl,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 20)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 20)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Defensive Linemen\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.23: Calibration Plot for Seasonal Projections of Defensive Linemen. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Defensive Linemen who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_dl$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_dl$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Defensive Linemen:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_dl$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_dl$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Defensive Linemen, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_dl$points[which(crowdAveragedProjectionsWithActuals_seasonal_dl$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_dl$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_dl$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.8 Linebackers\nHere are the seasonal accuracy indexes for all Linebackers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_lb$raw_points,\n  actual = projectionsWithActuals_seasonal_lb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Linebackers is in Figure 17.24.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_lb$raw_points,\n  y = projectionsWithActuals_seasonal_lb$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_lb,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 20)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 20)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Linebackers\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.24: Calibration Plot for Seasonal Projections of Linebackers. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Linebackers who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_lb$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_lb$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Linebackers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_lb$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_lb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Linebackers, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_lb$points[which(crowdAveragedProjectionsWithActuals_seasonal_lb$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_lb$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_lb$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.1.9 Defensive Backs\nHere are the seasonal accuracy indexes for all Defensive Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_db$raw_points,\n  actual = projectionsWithActuals_seasonal_db$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the seasonal projections for Defensive backs is in Figure 17.25.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_seasonal_db$raw_points,\n  y = projectionsWithActuals_seasonal_db$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_seasonal_db,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 20)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 20)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"loess\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of LOESS model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Seasonal Projections of Defensive Backs\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.25: Calibration Plot for Seasonal Projections of Defensive Backs. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a locally estimated scatterplot smoothing (LOESS) model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points.\n\n\n\n\nHere are the seasonal accuracy indexes for only those Defensive Backs who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_seasonal_db$raw_points[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = projectionsWithActuals_seasonal_db$fantasyPoints[which(projectionsWithActuals_seasonal$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections for Defensive Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_db$points,\n  actual = crowdAveragedProjectionsWithActuals_seasonal_db$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the seasonal accuracy indexes for crowd-averaged projections of Defensive Backs, but only among those who had high projected or actual points (i.e., players who had more than 100 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_seasonal_db$points[which(crowdAveragedProjectionsWithActuals_seasonal_db$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  actual = crowdAveragedProjectionsWithActuals_seasonal_db$fantasyPoints[which(crowdAveragedProjectionsWithActuals_seasonal_db$player_id_season %in% playersWithHighProjectedOrActualPoints)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2 Weekly Projections\n\n17.12.2.1 Overall\nHere are the weekly accuracy indexes for all players (across all positions):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly$raw_points,\n  actual = projectionsWithActuals_weekly$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for all players (across all positions) is in Figure 17.26.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly$raw_points,\n  y = projectionsWithActuals_weekly$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.26: Calibration Plot for Weekly Projections. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for players across all positions, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of all players (across all positions):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of players across all positions, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly$points[which(crowdAveragedProjectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.2 Quarterbacks\nHere are the weekly accuracy indexes for all Quarterbacks:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_qb$raw_points,\n  actual = projectionsWithActuals_weekly_qb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Quarterbacks is in Figure 17.27.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_qb$raw_points,\n  y = projectionsWithActuals_weekly_qb$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_qb,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Quarterbacks\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.27: Calibration Plot for Weekly Projections of Quarterbacks. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Quarterbacks who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_qb$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_qb$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Quarterbacks:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_qb$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_qb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Quarterbacks, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_qb$points[which(crowdAveragedProjectionsWithActuals_weekly_qb$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_qb$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_qb$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.3 Running Backs\nHere are the weekly accuracy indexes for all Running Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_rb$raw_points,\n  actual = projectionsWithActuals_weekly_rb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Running Backs is in Figure 17.28.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_rb$raw_points,\n  y = projectionsWithActuals_weekly_rb$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_rb,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Running Backs\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.28: Calibration Plot for Weekly Projections of Running Backs. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Running Backs who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_rb$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_rb$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Running Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_rb$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_rb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Running Backs, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_rb$points[which(crowdAveragedProjectionsWithActuals_weekly_rb$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_rb$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_rb$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.4 Wide Receivers\nHere are the weekly accuracy indexes for all Wide Receivers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_wr$raw_points,\n  actual = projectionsWithActuals_weekly_wr$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Wide Receivers is in Figure 17.29.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_wr$raw_points,\n  y = projectionsWithActuals_weekly_wr$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_wr,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Wide Receivers\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.29: Calibration Plot for Weekly Projections of Wide Receivers. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Wide Receivers who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_wr$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_wr$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Wide Receivers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_wr$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_wr$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Wide Receivers, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_wr$points[which(crowdAveragedProjectionsWithActuals_weekly_wr$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_wr$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_wr$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.5 Tight Ends\nHere are the weekly accuracy indexes for all Tight Ends:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_te$raw_points,\n  actual = projectionsWithActuals_weekly_te$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Tight Ends is in Figure 17.30.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_te$raw_points,\n  y = projectionsWithActuals_weekly_te$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_te,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Tight Ends\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.30: Calibration Plot for Weekly Projections of Tight Ends. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Tight Ends who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_te$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_te$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Tight Ends:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_te$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_te$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Tight Ends, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_te$points[which(crowdAveragedProjectionsWithActuals_weekly_te$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_te$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_te$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.6 Kickers\nHere are the weekly accuracy indexes for all Kickers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_k$raw_points,\n  actual = projectionsWithActuals_weekly_k$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Kickers is in Figure 17.31.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_k$raw_points,\n  y = projectionsWithActuals_weekly_k$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_k,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Kickers\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.31: Calibration Plot for Weekly Projections of Kickers. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Kickers who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_k$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_k$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Kickers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_k$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_k$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Kickers, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_k$points[which(crowdAveragedProjectionsWithActuals_weekly_k$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_k$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_k$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.7 Defensive Linemen\nHere are the weekly accuracy indexes for all Defensive Linemen:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_dl$raw_points,\n  actual = projectionsWithActuals_weekly_dl$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Defensive Linemen is in Figure 17.32.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_dl$raw_points,\n  y = projectionsWithActuals_weekly_dl$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_dl,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Defensive Linemen\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.32: Calibration Plot for Weekly Projections of Defensive Linemen. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Defensive Linemen who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_dl$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_dl$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Defensive Linemen:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_dl$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_dl$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Defensive Linemen, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_dl$points[which(crowdAveragedProjectionsWithActuals_weekly_dl$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_dl$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_dl$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.8 Linebackers\nHere are the weekly accuracy indexes for all Linebackers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_lb$raw_points,\n  actual = projectionsWithActuals_weekly_lb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Linebackers is in Figure 17.33.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_lb$raw_points,\n  y = projectionsWithActuals_weekly_lb$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_lb,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Linebackers\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.33: Calibration Plot for Weekly Projections of Linebackers. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Linebackers who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_lb$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_lb$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Linebackers:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_lb$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_lb$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Linebackers, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_lb$points[which(crowdAveragedProjectionsWithActuals_weekly_lb$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_lb$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_lb$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.2.9 Defensive Backs\nHere are the weekly accuracy indexes for all Defensive Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_db$raw_points,\n  actual = projectionsWithActuals_weekly_db$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nA calibration plot of the weekly projections for Linebackers is in Figure 17.34.\n\nCode#95 prediction interval based on LOESS model\ncalibrationLoessModel &lt;- msir::loess.sd(\n  x = projectionsWithActuals_weekly_db$raw_points,\n  y = projectionsWithActuals_weekly_db$fantasyPoints,\n  nsigma = qnorm(.975),\n  na.action = \"na.exclude\")\n\ncalibrationLoessPredictionInterval &lt;- data.frame(\n  x = calibrationLoessModel$x,\n  y = calibrationLoessModel$y,\n  lower = calibrationLoessModel$lower,\n  upper = calibrationLoessModel$upper)\n\nplotRangeMin &lt;- min(calibrationLoessModel$lower, na.rm = TRUE)\n\nplotRangeMax &lt;- max(calibrationLoessModel$upper, na.rm = TRUE)\n\nggplot2::ggplot(\n  data = projectionsWithActuals_weekly_db,\n  aes(\n    x = raw_points,\n    y = fantasyPoints)) + \n  coord_cartesian(\n    xlim = c(0, plotRangeMax),\n    ylim = c(plotRangeMin, plotRangeMax),\n    expand = FALSE) +\n  scale_x_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  scale_y_continuous(breaks = seq(0, plotRangeMax, by = 5)) +\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = lower),\n    color = \"red\",\n    linetype = \"dashed\") + #Lower estimate of 95% prediction interval of linear model\n  geom_line(\n    data = calibrationLoessPredictionInterval,\n    aes(\n      x = x,\n      y = upper),\n    color = \"red\",\n    linetype = \"dashed\") + #Upper estimate of 95% prediction interval of linear model\n  geom_smooth(\n    method = \"gam\",\n    color = viridisLite::viridis(3)[3],\n    fill = viridisLite::viridis(3)[1],\n    alpha = 0.7) + #95% confidence interval of GAM model\n  geom_abline(\n    slope = 1,\n    intercept = 0) +\n  labs(\n    x = \"Projected Fantasy Points\",\n    y = \"Actual Fantasy Points\",\n    title = \"Calibration Plot for Weekly Projections of Defensive Backs\"\n  ) +\n  theme_classic() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 17.34: Calibration Plot for Weekly Projections of Defensive Backs. The diagonal black line is the reference line representing perfect calibration. The yellow line is the best-fit line based on a generalized additive model. The purple band is the 95% confidence interval of actual fantasy points. The dashed red lines are the 95% prediction interval of actual fantasy points based on a locally estimated scatterplot smoothing (LOESS) model.\n\n\n\n\nHere are the weekly accuracy indexes for only those Defensive Backs who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = projectionsWithActuals_weekly_db$raw_points[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = projectionsWithActuals_weekly_db$fantasyPoints[which(projectionsWithActuals_weekly$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Defensive Backs:\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_db$points,\n  actual = crowdAveragedProjectionsWithActuals_weekly_db$fantasyPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nHere are the weekly accuracy indexes for crowd-averaged projections of Defensive Backs, but only among those who had high projected or actual points (i.e., players who had more than 6 projected or actual points; the players that are most important to differentiate between):\n\nCodepetersenlab::accuracyOverall(\n  predicted = crowdAveragedProjectionsWithActuals_weekly_db$points[which(crowdAveragedProjectionsWithActuals_weekly_db$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  actual = crowdAveragedProjectionsWithActuals_weekly_db$fantasyPoints[which(crowdAveragedProjectionsWithActuals_weekly_db$player_id_season_week %in% playersWithHighProjectedOrActualPoints_weekly)],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n17.12.3 Summarizing the Accuracy By Position\nWe summarize the accuracy of seasonal projections by position in Tables 17.1 and 17.2.\n\n\n\nTable 17.1: Accuracy of Seasonal Projections By Position.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.55\n15.47\n43.14\n\n\nQuarterbacks\n0.65\n5.88\n57.84\n\n\nRunning Backs\n0.50\n6.52\n50.16\n\n\nWide Receivers\n0.45\n19.93\n50.30\n\n\nTight Ends\n0.48\n5.92\n36.95\n\n\nKickers\n-2.36\n41.59\n44.33\n\n\nDefensive Linemen\n-0.27\n7.78\n16.09\n\n\nLinebackers\n-0.70\n22.89\n35.62\n\n\nDefensive Backs\n-0.67\n24.80\n31.82\n\n\n\n\n\n\n\n\n\n\n\nTable 17.2: Accuracy of Seasonal Projections By Position, Among Players with Greater than 100 Projected or Actual Points.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.35\n21.09\n56.43\n\n\nQuarterbacks\n0.81\n-15.69\n45.10\n\n\nRunning Backs\n0.42\n9.65\n49.60\n\n\nWide Receivers\n0.26\n29.55\n54.84\n\n\nTight Ends\n0.59\n4.28\n34.59\n\n\nKickers\n-3.41\n41.24\n45.84\n\n\nDefensive Linemen\n-0.28\n6.33\n16.27\n\n\nLinebackers\n-0.32\n19.98\n32.14\n\n\nDefensive Backs\n-1.06\n24.32\n30.57\n\n\n\n\n\n\n\n\nWe summarize the accuracy of crowd-averaged seasonal projections by position in Tables 17.3 and 17.4.\n\n\n\nTable 17.3: Accuracy of Crowd-Averaged Seasonal Projections By Position.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.59\n12.27\n30.19\n\n\nQuarterbacks\n0.75\n-0.04\n45.87\n\n\nRunning Backs\n0.61\n4.40\n39.92\n\n\nWide Receivers\n0.60\n14.15\n40.02\n\n\nTight Ends\n0.59\n4.13\n29.70\n\n\nKickers\n-1.94\n35.54\n43.86\n\n\nDefensive Linemen\n0.10\n6.45\n13.64\n\n\nLinebackers\n-0.16\n14.05\n27.17\n\n\nDefensive Backs\n-0.33\n20.47\n28.77\n\n\n\n\n\n\n\n\n\n\n\nTable 17.4: Accuracy of Crowd-Averaged Seasonal Projections By Position, Among Players with Greater than 100 Projected or Actual Points.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.38\n22.73\n50.90\n\n\nQuarterbacks\n0.32\n2.26\n70.33\n\n\nRunning Backs\n0.29\n6.13\n60.89\n\n\nWide Receivers\n0.31\n20.03\n55.20\n\n\nTight Ends\n0.18\n6.32\n49.74\n\n\nKickers\n-2.24\n42.40\n43.06\n\n\nDefensive Linemen\n-4.00\n39.92\n39.92\n\n\nLinebackers\n-1.62\n36.52\n46.90\n\n\nDefensive Backs\n-1.63\n34.57\n37.23\n\n\n\n\n\n\n\n\nWe summarize the accuracy of weekly projections by position in Tables 17.5 and 17.6.\n\n\n\nTable 17.5: Accuracy of Weekly Projections By Position.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.41\n-0.21\n3.87\n\n\nQuarterbacks\n0.18\n-1.49\n6.10\n\n\nRunning Backs\n0.41\n-1.01\n4.51\n\n\nWide Receivers\n0.30\n-0.61\n4.86\n\n\nTight Ends\n0.22\n-0.91\n3.85\n\n\nKickers\n-0.28\n1.51\n3.22\n\n\nDefensive Linemen\n-0.01\n0.22\n1.77\n\n\nLinebackers\n-0.14\n1.05\n2.63\n\n\nDefensive Backs\n-0.17\n0.91\n2.47\n\n\n\n\n\n\n\n\n\n\n\nTable 17.6: Accuracy of Weekly Projections By Position, Among Players with Greater than 6 Projected or Actual Points.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.23\n-0.55\n4.91\n\n\nQuarterbacks\n0.22\n-1.71\n6.12\n\n\nRunning Backs\n0.40\n-1.06\n4.35\n\n\nWide Receivers\n0.32\n-0.56\n4.86\n\n\nTight Ends\n0.23\n-0.95\n3.77\n\n\nKickers\n-0.33\n1.51\n3.17\n\n\nDefensive Linemen\n-0.01\n0.16\n1.87\n\n\nLinebackers\n-0.21\n1.17\n2.72\n\n\nDefensive Backs\n-0.24\n0.89\n2.44\n\n\n\n\n\n\n\n\nWe summarize the accuracy of crowd-averaged seasonal projections by position in Tables 17.7 and 17.8.\n\n\n\nTable 17.7: Accuracy of Crowd-Averaged Weekly Projections By Position.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.46\n0.00\n2.94\n\n\nQuarterbacks\n0.32\n-1.61\n5.75\n\n\nRunning Backs\n0.46\n-0.99\n4.12\n\n\nWide Receivers\n0.37\n-0.63\n4.44\n\n\nTight Ends\n0.26\n-0.96\n3.48\n\n\nKickers\n-0.19\n1.48\n3.14\n\n\nDefensive Linemen\n0.05\n0.12\n1.66\n\n\nLinebackers\n0.10\n0.60\n2.28\n\n\nDefensive Backs\n-0.01\n0.70\n2.28\n\n\n\n\n\n\n\n\n\n\n\nTable 17.8: Accuracy of Crowd-Averaged Weekly Projections By Position, Among Players with Greater than 6 Projected or Actual Points.\n\n\n\n\nPosition\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nAll Positions\n0.24\n-0.42\n4.38\n\n\nQuarterbacks\n-0.03\n-1.87\n6.46\n\n\nRunning Backs\n0.18\n-1.66\n5.63\n\n\nWide Receivers\n0.13\n-1.13\n5.70\n\n\nTight Ends\n-0.17\n-2.04\n5.20\n\n\nKickers\n-0.19\n1.48\n3.14\n\n\nDefensive Linemen\n-0.94\n-2.02\n4.12\n\n\nLinebackers\n-0.57\n0.74\n3.25\n\n\nDefensive Backs\n-0.38\n0.76\n2.87\n\n\n\n\n\n\n\n\nIn general, projections showed moderate accuracy in predicting players’ fantasy performance. Projections showed moderate accuracy in terms of discrimination, based on \\(R^2\\). However, projections were not highly accurate. Projections showed some miscalibration with respect to actual fantasy points. In general, projected showed a modest tendency to underpredict performance at lower levels of projected points and a moderate-to-strong tendency overpredict performance at higher levels of projected points, an example of the overextremity form of miscalibration. As a reminder, the overextremity form of miscalibration is an example of the overprecision form of overconfidence. That is, projection sources appear to be overconfident in their projections of player performance.\nFor seasonal projections, in terms of comparison by position, projections showed greatest discrimination (\\(R^2\\)) for Quarterbacks, followed by Running Backs/Tight Ends, and Wide Receivers. Predictions for defensive players and kickers showed a negative \\(R^2\\), indicating that projections would have been more accurate merely by predicting the mean of the actual values for every observation (i.e., predicting from the base rate). In terms of comparison by projection source, crowd-averaged projections showed greater accuracy than individual projection sources (i.e., “experts”) in terms of discrimination (\\(R^2\\)) and calibration (MAE). This was especially the case when examining individual sources separately (rather than in one combined object).\nFor weekly projections, in terms of comparison by position, projections showed greatest discrimination (\\(R^2\\)) for Running Backs, followed by Wide Receivers, Quarterbacks, and Tight Ends. Predictions for defensive players and kickers showed a negative \\(R^2\\), indicating that projections would have been more accurate merely by predicting the mean of the actual values for every observation (i.e., predicting from the base rate). In terms of comparison by projection source, crowd-averaged projections showed greater accuracy than individual projection sources in terms of discrimination (\\(R^2\\)) and calibration (MAE).\nIn summary, we found that, for both seasonal and weekly projections, crowd-averaged projections showed greater accuracy than individual projection sources (i.e., “experts”), consistent with prior findings [Kartes (2024); archived at https://perma.cc/69F7-LLTN; Petersen (2017); archived at https://perma.cc/BG2W-ANUF]. Nevertheless projections—including individual sources and crowd-averaged—were not highly accurate. Projections tended not to explain more than about two-thirds of the variance in players’ performance, and frequently explained well less than half of the variance, particularly when distinguishing between players with high (i.e, more than 100) projected or actual points—who are the players that are the most important to distinguish, because those are the players that you are most likely trying to decide between when drafting. Thus, although projections for offensive positions (besides Kickers) do better than chance, there remains considerable room for improvement in predictive accuracy. The modest accuracy of projections likely reflects that human behavior is multiply determined and hard to predict.\nPrediction accuracy tended to be higher for seasonal projections than for weekly projections—likely because of the greater number of “trials” (i.e., plays) in a season compared to a game, so more of the random error averages out. The weaker accuracy weekly projections makes betting on daily fantasy sports a gamble.\nThe association between projected and actual fantasy points tended to flatten at the higher levels of projected points, for some positions, including Running Backs, Kickers, and defensive players. A consequence of this pattern, from the perspective of draft strategy, is that it would not make sense to draft players projected to score more than the flattening of the projected–actual curve. For instance, drafting a highly routed Wide Receiver may be safer than drafting a highly ranked Running Back, who may be more prone to injury. This finding is consistent with findings from Petersen (2017, archived at https://perma.cc/BG2W-ANUF), who found that projections were more accurate for Quarterbacks and Wide Receivers than for Running Backs; projections were the least accurate for Kickers, Defensive Backs, and Defense/Special Teams. These finding were also extended by Kartes (2025) (archived at https://perma.cc/R3WS-CAKD) whose evaluation of the accuracy of projections by position found that:\n\nQuarterbacks with preseason ranks of 6–10 showed the greatest systematic underperformance relative to their projections, suggesting that you should either draft an elite Quarterback (ranked 1–5) or wait until later rounds to draft multiple lower-ranked Quarterbacks.\nElite Running Backs (with preseason ranks of 1–5) showed strong underperformance (relative to projections), suggesting that you should draft Running Backs ranked 6 and below.\nWide Receivers showed among the more consistent and predictable patterns in projections, suggesting that it may be safer to draft a top Wide Receiver than top-ranked players at other positions.\nTight Ends showed relative inaccuracy of projections with limited upside concentrated among a few players, which means it is difficult, at the outset, to predict which Tight Ends will excel; based on this, Kartes (2025) suggested avoiding overpaying for mid-tier Tight Ends; to either draft a Tight End early or to wait until later rounds.\n\nThis does not mean that you should intentionally draft a 6th ranked Running Back instead of the top-ranked Running Back. Instead, what it may mean is that, instead of drafting the top-ranked Running Back, you may be better off drafting the top-ranked Wide Receiver and to get a value player at Running Back in a subsequent pick.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-waysToImprovePredictionAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-waysToImprovePredictionAccuracy",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.13 Ways to Improve Prediction Accuracy",
    "text": "17.13 Ways to Improve Prediction Accuracy\nOn the whole, experts’ predictions are inaccurate. Experts’ predictions from many different domains tend to be inaccurate, including political scientists (Tetlock, 2017), physicians (Koehler et al., 2002), clinical psychologists (Oskamp, 1965), stock market traders and corporate financial officers (Skala, 2008), seismologists’ predictions of earthquakes (Hough, 2016), economists’ predictions about the economy (Makridakis et al., 2009), lawyers (Koehler et al., 2002), and business managers (Russo & Schoemaker, 1992). Thus, I would not put much confidence in the predictions by fantasy football pundits. The most common pattern of experts’ predictions is that they show overextremity, that is, their predictions have probability judgments that tend to be too extreme, as described in Section Section 17.3.2. Overextremity of experts’ predictions reflects the overprecision type of overconfidence bias. The degree of confidence of a person’s predictions is often not a good indicator of the accuracy of their predictions [and confidence and prediction accuracy are sometimes inversely associated; Silver (2012)]. Heuristics such as the anchoring and adjustment heuristic, cognitive biases such as confirmation bias (Hoch, 1985; Koriat et al., 1980), fallacies such as the base rate fallacy (Eddy, 1982; Koehler et al., 2002) could contribute to overconfidence of predictions. Poorly calibrated predictions are especially likely when the base rate is very low (e.g., suicide) or when the base rate is very high (Koehler et al., 2002).\nNevertheless, there are some domains that have shown greater predictive accuracy, from which we may learn what practices may lead to greater accuracy. For instance, experts have shown stronger predictive accuracy in weather forecasting (Murphy & Winkler, 1984), horse race betting (Johnson & Bruce, 2001), and playing the card game of bridge (Keren, 1987), but see Koehler et al. (2002) for exceptions.\nHere are some potential ways to improve the accuracy (and honesty) of predictions and judgments:\n\nProvide appropriate anchoring of your predictions to the base rate of the phenomenon you are predicting. To the extent that the base rate of the event you are predicting is low, more extreme evidence should be necessary to consistently and accurately predict that the event will occur. Applying actuarial formulas and Bayes’ theorem can help you appropriately weigh the base rate and evidence.\nInclude multiple predictors, ideally from different measures and measurement methods. Include the predictors with the strongest validity based on theory of the causal process and based on criterion-related validity.\nWhen possible, aggregate multiple perspectives of predictions, especially predictions made independently (from different people/methods/etc.). The “wisdom of the crowd” is often more accurate than individuals’ predictions, including predictions by so-called “experts” (Silver, 2012).\nInstead of aggregating scores from multiple variables, another option is to “unamalgamate” the predictors, that is, to use each variable as a separate predictor, to maximize predictive power (Trafimow et al., in press).\nA goal of prediction is to capture as much signal as possible and as little noise (error) as possible (Silver, 2012). Parsimony (i.e., not having too many predictors) can help reduce the amount of error variance captured by the prediction model. However, to accurately model complex systems like human behavior, complex models may be necessary. However, strong theory of the causal processes and dynamics may be necessary to develop accurate complex models.\nAlthough incorporating theory can be helpful, provide more weight to empiricism than to theory, until our theories and measures are stronger. Ideally, we would use theory to design a model that mirrors the causal system, with accurate measures of each process in the system, so we could make accurate predictions. However, as described in Section 17.9, our psychological theories of the causal processes that influence behavior are not yet very strong. Until we have stronger theories that specify the causal process for a given outcome, and until we have accurate measures of those causal processes, actuarial approaches are likely to be most accurate, as discussed in Chapter 15. At the same time, keep in mind that measures involving human behavior, and their resulting data, are often noisy. As a result, theoretically (conceptually) informed empirical approaches may lead to more accuracy than empiricism alone.\nUse an empirically validated and cross-validated statistical algorithm to combine information from the predictors in a formalized way. Give each predictor appropriate weight in the statistical algorithm, according to its strength of association with the outcome. Use measures with strong reliability and validity for assessing these processes to be used in the algorithm. Cross-validation will help reduce the likelihood that your model is fitting to noise and will maximize the likelihood that the model predicts accurately when applied to new data (i.e., the model’s predictions accurately generalize), as described in Section 15.8.\nWhen presenting your predictions, acknowledge what you do not know.\nExpress your predictions in terms of probabilistic estimates and present the uncertainty in your predictions with confidence intervals [even though bolder, more extreme predictions tend to receive stronger television ratings; Silver (2012)].\nQualify your predictions by identifying and noting counter-examples that would not be well fit by your prediction model, such as extreme cases, edge cases, and “broken leg” (Meehl, 1957) cases.\nProvide clear, consistent, and timely feedback on the outcomes of the predictions to the people making the predictions (Bolger & Önkal-Atay, 2004).\nBe self-critical about your predictions. Update your judgments based on their accuracy, rather than trying to confirm your beliefs (Atanasov et al., 2020).\nIn addition to considering the accuracy of the prediction, consider the quality of the prediction process, especially when random chance is involved to a degree, such as in poker and fantasy football (Silver, 2012).\nWork to identify and mitigate potential blindspots; be aware of cognitive biases and fallacies, such as confirmation bias and the base rate fallacy.\nEvaluate for the possibility of test bias. Correct for any test bias.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyConclusion",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyConclusion",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.14 Conclusion",
    "text": "17.14 Conclusion\nWhen the base rate of a behavior is very low or very high, you can be highly accurate in predicting the behavior by predicting from the base rate. Thus, you cannot judge how accurate your prediction is until you know how accurate your predictions would be by random chance. Moreover, maximizing percent accuracy may not be the ultimate goal because different errors have different costs. Though there are many indices of accuracy, there are two general types of accuracy: discrimination and calibration. Discrimination accuracy is frequently evaluated with the area under the receiver operating characteristic curve, or with sensitivity and specificity, or with standardized regression coefficients or the coefficient of determination. Calibration accuracy is frequently evaluated graphically and with various indices. Sensitivity and specificity depend on the cutoff. It is important to evaluate both discrimination and calibration when evaluating prediction accuracy.\nIn terms of the accuracy of historical fantasy football projections, projections showed moderate accuracy in terms of discrimination in predicting players’ fantasy performance. However, projections were not highly accurate. Projections showed some miscalibration with respect to actual fantasy points. Projection sources appeared to be overconfident in their projections of player performance. In general, projected showed a modest tendency to underpredict performance at lower levels of projected points and a moderate-to-strong tendency overpredict performance at higher levels of projected points, an example of the overextremity, which is an example of the overprecision form of overconfidence. Crowd-averaged projections tend to be more accurate than individual sources of projections.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracySessionInfo",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracySessionInfo",
    "title": "17  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n17.15 Session Info",
    "text": "17.15 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1      readr_2.1.5       \n [5] tidyverse_2.0.0    yardstick_1.3.2    workflowsets_1.1.1 workflows_1.2.0   \n [9] tune_1.3.0         tidyr_1.3.1        tibble_3.3.0       rsample_1.3.1     \n[13] recipes_1.3.1      purrr_1.1.0        parsnip_1.3.2      modeldata_1.5.1   \n[17] infer_1.0.9        ggplot2_3.5.2      dplyr_1.1.4        dials_1.4.1       \n[21] scales_1.4.0       broom_1.0.9        tidymodels_1.3.0   msir_1.3.3        \n[25] viridis_0.6.5      viridisLite_0.4.2  magrittr_2.0.3     pROC_1.19.0.1     \n[29] petersenlab_1.2.0 \n\nloaded via a namespace (and not attached):\n [1] Rdpack_2.6.4        DBI_1.2.3           mnormt_2.1.1       \n [4] gridExtra_2.3       rlang_1.1.6         furrr_0.3.1        \n [7] compiler_4.5.1      mgcv_1.9-3          vctrs_0.6.5        \n[10] reshape2_1.4.4      lhs_1.2.0           quadprog_1.5-8     \n[13] pkgconfig_2.0.3     fastmap_1.2.0       backports_1.5.0    \n[16] labeling_0.4.3      pbivnorm_0.6.0      rmarkdown_2.29     \n[19] tzdb_0.5.0          prodlim_2025.04.28  nloptr_2.2.1       \n[22] xfun_0.53           jsonlite_2.0.0      psych_2.5.6        \n[25] parallel_4.5.1      lavaan_0.6-19       cluster_2.1.8.1    \n[28] R6_2.6.1            stringi_1.8.7       RColorBrewer_1.1-3 \n[31] parallelly_1.45.1   boot_1.3-31         rpart_4.1.24       \n[34] iterators_1.0.14    Rcpp_1.1.0          knitr_1.50         \n[37] future.apply_1.20.0 base64enc_0.1-3     timechange_0.3.0   \n[40] Matrix_1.7-3        splines_4.5.1       nnet_7.3-20        \n[43] tidyselect_1.2.1    rstudioapi_0.17.1   yaml_2.3.10        \n[46] timeDate_4041.110   codetools_0.2-20    listenv_0.9.1      \n[49] lattice_0.22-7      plyr_1.8.9          withr_3.0.2        \n[52] evaluate_1.0.4      foreign_0.8-90      future_1.67.0      \n[55] survival_3.8-3      mclust_6.1.1        pillar_1.11.0      \n[58] foreach_1.5.2       checkmate_2.3.3     stats4_4.5.1       \n[61] reformulas_0.4.1    generics_0.1.4      hms_1.1.3          \n[64] mix_1.0-13          minqa_1.2.8         globals_0.18.0     \n[67] xtable_1.8-4        class_7.3-23        glue_1.8.0         \n[70] Hmisc_5.2-3         tools_4.5.1         data.table_1.17.8  \n[73] lme4_1.1-37         gower_1.0.2         mvtnorm_1.3-3      \n[76] grid_4.5.1          mitools_2.4         rbibutils_2.3      \n[79] ipred_0.9-15        colorspace_2.1-1    nlme_3.1-168       \n[82] htmlTable_2.4.3     Formula_1.2-5       cli_3.6.5          \n[85] DiceDesign_1.10     lava_1.8.1          gtable_0.3.6       \n[88] GPfit_1.0-9         digest_0.6.37       htmlwidgets_1.6.4  \n[91] farver_2.1.2        htmltools_0.5.8.1   lifecycle_1.0.4    \n[94] hardhat_1.4.2       sparsevctrs_0.3.4   MASS_7.3-65        \n\n\n\n\n\n\nAtanasov, P., Witkowski, J., Ungar, L., Mellers, B., & Tetlock, P. (2020). Small steps to accuracy: Incremental belief updaters are better forecasters. Organizational Behavior and Human Decision Processes, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001\n\n\nAustin, P. C., & Steyerberg, E. W. (2014). Graphical assessment of internal and external calibration of logistic regression models by using loess smoothers. Statistics in Medicine, 33(3), 517–535. https://doi.org/10.1002/sim.5941\n\n\nBolger, F., & Önkal-Atay, D. (2004). The effects of feedback on judgmental interval predictions. International Journal of Forecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nEddy, D. M. (1982). Probabilistic reasoning in clinical medicine: Problems and opportunities. In D. Kahneman, P. Slovic, & A. Tversky (Eds.), Judgment under uncertainty: Heuristics and biases (pp. 249–267). Cambridge University Press. https://doi.org/10.1017/CBO9780511809477.019\n\n\nFantasy Football Analytics. (2025). FFAnalytics web app accuracy tab. https://fantasyfootballanalytics.net/ffanalytics-web-app-accuracy-tab\n\n\nFarrington, D. P., & Loeber, R. (1989). Relative improvement over chance (RIOC) and phi as measures of predictive efficiency and strength of association in 2×2 tables. Journal of Quantitative Criminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737\n\n\nGoodman, Z. T., Casline, E., Jensen-Doss, A., Ehrenreich-May, J., & Bainter, S. A. (2022). shinyDLRs: A dashboard to facilitate derivation of diagnostic likelihood ratios. Psychological Assessment, 34(6), 558–569. https://doi.org/10.1037/pas0001114\n\n\nHarrell, Jr., F. E. (2025). rms: Regression modeling strategies. https://doi.org/10.32614/CRAN.package.rms\n\n\nHoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting personal events. Journal of Experimental Psychology: Learning, Memory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719\n\n\nHopper, T. (2014). Can we do better than r-squared? https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared\n\n\nHough, S. E. (2016). Predicting the unpredictable: The tumultuous science of earthquake prediction. Princeton University Press.\n\n\nHyndman, R. J. (2014). Alternative to MAPE when the data is not a time series. https://stats.stackexchange.com/a/108963/20338\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nJohnson, J. E. V., & Bruce, A. C. (2001). Calibration of subjective probability judgments in a naturalistic setting. Organizational Behavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nKartes, J. (2024). Which fantasy football projections are most accurate? https://fantasyfootballanalytics.net/2024/12/which-fantasy-football-projections-are-most-accurate.html\n\n\nKartes, J. (2025). Fantasy football projections: Exploring positional bias in projections. https://fantasyfootballanalytics.net/2025/07/fantasy-football-projections-exploring-positional-bias-in-projections.html\n\n\nKeren, G. (1987). Facing uncertainty in the game of bridge: A calibration study. Organizational Behavior and Human Decision Processes, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1\n\n\nKessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., & Zubizarreta, J. R. (2020). Suicide prediction models: A critical review of recent research with recommendations for the way forward. Molecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0\n\n\nKoehler, D. J., Brenner, L., & Griffin, D. (2002). The calibration of expert judgment: Heuristics and biases beyond the laboratory. In T. Gilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics and biases: The psychology of intuitive judgment. Cambridge University Press. https://doi.org/10.1017/CBO9780511808098.041\n\n\nKoriat, A., Lichtenstein, S., & Fischhoff, B. (1980). Reasons for confidence. Journal of Experimental Psychology: Human Learning and Memory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107\n\n\nKuhn, M., & Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\n\n\nKuhn, M., & Wickham, H. (2025). tidymodels: Easily install and load the tidymodels packages. https://doi.org/10.32614/CRAN.package.tidymodels\n\n\nLindhiem, O., Petersen, I. T., Mentch, L. K., & Youngstrom, E. A. (2020). The importance of calibration in clinical psychology. Assessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055\n\n\nMakridakis, S., Hogarth, R. M., & Gaba, A. (2009). Forecasting and uncertainty in the economic and business world. International Journal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. Journal of Consulting and Clinical Psychology, 46(4), 806–834. https://doi.org/10.1037/0022-006x.46.4.806\n\n\nMeehl, P. E., & Rosen, A. (1955). Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores. Psychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070\n\n\nMorley, S. K., Brito, T. V., & Welling, D. T. (2018). Measures of model performance based on the log accuracy ratio. Space Weather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669\n\n\nMurphy, A. H., & Winkler, R. L. (1984). Probability forecasting in meterology. Journal of the American Statistical Association, 79(387), 489–500. https://doi.org/10.2307/2288395\n\n\nOskamp, S. (1965). Overconfidence in case-study judgments. Journal of Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125\n\n\nPetersen, I. T. (2017). Who has the best fantasy football projections? 2017 update. https://fantasyfootballanalytics.net/2017/03/best-fantasy-football-projections-2017.html\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & Müller, M. (2011). pROC: An open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics, 12, 77. https://doi.org/10.1186/1471-2105-12-77\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & Müller, M. (2023). pROC: Display and analyze ROC curves. https://doi.org/10.32614/CRAN.package.pROC\n\n\nRosalsky, G. (2023). Should we invest more in weather forecasting? It may save your life. https://www.npr.org/sections/money/2023/07/11/1186458991/should-we-invest-more-in-weather-forecasting-it-may-save-your-life\n\n\nRusso, J. E., & Schoemaker, P. J. (1992). Managing overconfidence. Sloan Management Review, 33(2), 7.\n\n\nSchwartz, A. (2006). Diagnostic test calculator. http://araw.mede.uic.edu/cgi-bin/testcalc.pl\n\n\nScrucca, L. (2011). Model-based SIR for dimension reduction. Computational Statistics & Data Analysis, 5(11), 3010–3026. https://doi.org/10.1016/j.csda.2011.05.006\n\n\nScrucca, L. (2020). msir: Model-based sliced inverse regression. https://doi.org/10.32614/CRAN.package.msir\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.\n\n\nSkala, D. (2008). Overconfidence in psychology and finance–an interdisciplinary literature review. Bank i Kredyt, 4, 33–50.\n\n\nStevens, R. J., & Poppe, K. K. (2020). Validation of clinical prediction models: What does the “calibration slope” really measure? Journal of Clinical Epidemiology, 118, 93–99. https://doi.org/10.1016/j.jclinepi.2019.09.016\n\n\nSteyerberg, E. W., & Vergouwe, Y. (2014). Towards better clinical prediction models: Seven steps for development and an ABCD for validation. European Heart Journal, 35(29), 1925–1931. https://doi.org/10.1093/eurheartj/ehu207\n\n\nTetlock, P. E. (2017). Expert political judgment: How good is it? How can we know? - New edition. Princeton University Press.\n\n\nTofallis, C. (2015). A better measure of relative prediction accuracy for model selection and model estimation. Journal of the Operational Research Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103\n\n\nTrafimow, D., Hyman, M. R., & Kostyk, A. (in press). Enhancing predictive power by unamalgamating multi-item scales. Psychological Methods. https://doi.org/10.1037/met0000599\n\n\nTreat, T. A., & Viken, R. J. (2023). Measuring test performance with signal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M. McMullen, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA handbook of research methods in psychology: Foundations, planning, measures, and psychometrics (2nd ed., Vol. 1, pp. 837–858). American Psychological Association. https://doi.org/10.1037/0000318-038\n\n\nTüzen, M. F. (2025). Explained vs. Predictive power: R², adjusted R², and beyond. https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/\n\n\nYahoo! Sports. (2024). How cognitive bias affects your fantasy draft strategy with neuroscience professor Dr. Renee Miller. https://www.youtube.com/watch?v=gmpLFWs5ae0",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "mythbusters.html",
    "href": "mythbusters.html",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "",
    "text": "18.1 Getting Started\nIn this chapter, we put a popular fantasy football belief to the test. We evaluate the widely held belief that players perform better during a contract year.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersGettingStarted",
    "href": "mythbusters.html#sec-mythbustersGettingStarted",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "",
    "text": "18.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"nflreadr\")\nlibrary(\"lme4\")\nlibrary(\"lmerTest\")\nlibrary(\"performance\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")\n\n\n\n18.1.2 Specify Package Options\n\nCodeemm_options(lmerTest.limit = 100000)\nemm_options(pbkrtest.limit = 100000)\n\n\n\n18.1.3 Load Data\n\nCodeload(file = \"./data/nfl_playerContracts.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/nfl_espnQBR_seasonal.RData\")\nload(file = \"./data/nfl_espnQBR_weekly.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 4.4.3.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-contractYear",
    "href": "mythbusters.html#sec-contractYear",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n18.2 Do Players Perform Better in their Contract Year?",
    "text": "18.2 Do Players Perform Better in their Contract Year?\nConsiderable speculation exists regarding whether players perform better in their last year of their contract (i.e., their “contract year”). Fantasy football talking heads and commentators frequently discuss the benefit of selecting players who are in their contract year, because it supposedly means that player has more motivation to perform well so they get a new contract and get paid more. To our knowledge, no peer-reviewed studies have examined this question for football players. One study found that National Basketball Association (NBA) players improved in field goal percentage, points, and player efficiency rating (but not other statistics: rebounds, assists, steals, or blocks) from their pre-contract year to their contract year, and that Major League Baseball (MLB) players improved in runs batted in (RBIs; but not other statistics: batting average, slugging percentage, on base percentage, home runs, fielding percentage) from their pre-contract year to their contract year (White & Sheldon, 2014). Other casual analyses have been examined contract-year performance of National Football League (NFL) players, including articles in 2012 [Bales (2012); archived here] and 2022 [Niles (2022); archived here].\nLet’s examine the question empirically. Our research questions is: Do players perform better in their “contract year” (i.e., the last year of their contract)? Our hypothesis is that players are motivated to get larger contracts (more money), leading players in their contract year to try harder and perform better. If the hypothesis is true, we predict that players who are in their contract year will tend to score more fantasy points than players who are not in their contract year.\nIn order to test this question empirically, we have to make some assumptions/constraints. In this example, we will make the following constraints:\n\nWe will determine a player’s contract year programmatically based on the year the contract was signed. For instance, if a player signed a 3-year contract in 2015, their contract would expire in 2018, and thus their contract year would be 2017. Note: this is a coarse way of determining a player’s contract year because it could depend on when during the year the player’s contract is signed. If we were submitting this analysis as a paper to a scientific journal, it would be important to verify each player’s contract year.\nWe will examine performance in all seasons since 2011, beginning when most data for player contracts are available.\nFor maximum statistical power to detect an effect if a contract year effect exists, we will examine all seasons for a player (since 2011), not just their contract year and their pre-contract year.\nTo ensure a more fair, apples-to-apples comparison of the games in which players played, we will examine per-game performance (except for yards per carry, which is based on \\(\\frac{\\text{rushing yards}}{\\text{carries}}\\) from the entire season).\nWe will examine regular season games only (no postseason).\nTo ensure we do not make generalization about a player’s performance in a season from a small sample, the player has to play at least 5 games in a given season for that player–season combination to be included in analysis.\n\nFor analysis, the same player contributes multiple observations of performance (i.e., multiple seasons) due to the longitudinal nature of the data. Inclusion of multiple data points from the same player would violate the assumption of multiple regression that all observations are independent. Thus, we use mixed-effects models that allow nonindependent observations. In our mixed-effects models, we include a random intercept for each player, to allow our model to account for players’ differing level of performance. We examine two mixed-effects models for each outcome variable: one model that accounts for the effects of age and experience, and one model that does not.\nThe model that does not account for the effects of age and experience includes:\n\nrandom intercepts to allow the model to estimate a different starting point for each player\na fixed effect for whether the player is in a contract year\n\nThe model that accounts for the effects of age and experience includes:\n\nrandom intercepts to allow the model to estimate a different starting point for each player\nrandom linear slopes (i.e., random effect of linear age) to allow the model to estimate a different form of change for each player\na fixed quadratic effect of age to allow for curvilinear effects\na fixed effect of experience\na fixed effect for whether the player is in a contract year\n\n\nCode# Subset to remove players without a year signed\nnfl_playerContracts_subset &lt;- nfl_playerContracts %&gt;% \n  dplyr::filter(!is.na(year_signed) & year_signed != 0)\n\n# Determine the contract year for a given contract\nnfl_playerContracts_subset$contractYear &lt;- nfl_playerContracts_subset$year_signed + nfl_playerContracts_subset$years - 1\n\n# Arrange contracts by player and year_signed\nnfl_playerContracts_subset &lt;- nfl_playerContracts_subset %&gt;%\n  dplyr::group_by(player, position) %&gt;% \n  dplyr::arrange(player, position, -year_signed) %&gt;% \n  dplyr::ungroup()\n\n# Determine if the player played in the original contract year\nnfl_playerContracts_subset &lt;- nfl_playerContracts_subset %&gt;%\n  dplyr::group_by(player, position) %&gt;%\n  dplyr::mutate(\n    next_contract_start = lag(year_signed)) %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::mutate(\n    played_in_contract_year = ifelse(\n      is.na(next_contract_start) | contractYear &lt; next_contract_start,\n      TRUE,\n      FALSE))\n\n# Check individual players\n#nfl_playerContracts_subset %&gt;% \n#  dplyr::filter(player == \"Aaron Rodgers\") %&gt;% \n#  dplyr::select(player:years, contractYear, next_contract_start, played_in_contract_year)\n#\n#nfl_playerContracts_subset %&gt;% \n#  dplyr::filter(player %in% c(\"Jared Allen\", \"Aaron Rodgers\")) %&gt;% \n#  dplyr::select(player:years, contractYear, next_contract_start, played_in_contract_year)\n\n# Subset data\nnfl_playerContractYears &lt;- nfl_playerContracts_subset %&gt;% \n  dplyr::filter(played_in_contract_year == TRUE) %&gt;% \n  dplyr::filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  dplyr::select(player, position, team, contractYear) %&gt;% \n  dplyr::mutate(merge_name = nflreadr::clean_player_names(player, lowercase = TRUE)) %&gt;% \n  dplyr::rename(season = contractYear) %&gt;% \n  dplyr::mutate(contractYear = 1)\n\n# Merge with weekly and seasonal stats data\nplayer_stats_weekly_offense &lt;- player_stats_weekly %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  dplyr::mutate(merge_name = nflreadr::clean_player_names(player_display_name, lowercase = TRUE))\n#nfl_actualStats_offense_seasonal &lt;- nfl_actualStats_offense_seasonal %&gt;% \n#  mutate(merge_name = nflreadr::clean_player_names(player_display_name, lowercase = TRUE))\n\nplayer_statsContracts_offense_weekly &lt;- dplyr::full_join(\n  player_stats_weekly_offense,\n  nfl_playerContractYears,\n  by = c(\"merge_name\", \"position_group\" = \"position\", \"season\")\n) %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\n#player_statsContracts_offense_seasonal &lt;- full_join(\n#  player_stats_seasonal_offense,\n#  nfl_playerContractYears,\n#  by = c(\"merge_name\", \"position_group\" = \"position\", \"season\")\n#) %&gt;% \n#  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\nplayer_statsContracts_offense_weekly$contractYear[which(is.na(player_statsContracts_offense_weekly$contractYear))] &lt;- 0\n#player_statsContracts_offense_seasonal$contractYear[which(is.na(player_statsContracts_offense_seasonal$contractYear))] &lt;- 0\n\n#player_statsContracts_offense_weekly$contractYear &lt;- factor(\n#  player_statsContracts_offense_weekly$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\n#player_statsContracts_offense_seasonal$contractYear &lt;- factor(\n#  player_statsContracts_offense_seasonal$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\nplayer_statsContracts_offense_weekly &lt;- player_statsContracts_offense_weekly %&gt;% \n  dplyr::arrange(merge_name, season, season_type, week)\n\n#player_statsContracts_offense_seasonal &lt;- player_statsContracts_offense_seasonal %&gt;% \n#  arrange(merge_name, season)\n\nplayer_statsContractsSubset_offense_weekly &lt;- player_statsContracts_offense_weekly %&gt;% \n  dplyr::filter(season_type == \"REG\")\n\n#table(nfl_playerContracts$year_signed) # most contract data is available beginning in 2011\n\n# Calculate Per Game Totals\nplayer_statsContracts_seasonal &lt;- player_statsContractsSubset_offense_weekly %&gt;% \n  dplyr::group_by(player_id, season) %&gt;% \n  dplyr::summarise(\n    player_display_name = petersenlab::Mode(player_display_name),\n    position_group = petersenlab::Mode(position_group),\n    age = min(age, na.rm = TRUE),\n    years_of_experience = min(years_of_experience, na.rm = TRUE),\n    rushing_yards = sum(rushing_yards, na.rm = TRUE), # season total\n    carries = sum(carries, na.rm = TRUE), # season total\n    rushing_epa = mean(rushing_epa, na.rm = TRUE),\n    receiving_yards = mean(receiving_yards, na.rm = TRUE),\n    receiving_epa = mean(receiving_epa, na.rm = TRUE),\n    fantasyPoints = sum(fantasyPoints, na.rm = TRUE), # season total\n    contractYear = mean(contractYear, na.rm = TRUE),\n    games = n(),\n    .groups = \"drop_last\"\n  ) %&gt;% \n  dplyr::mutate(\n    player_id = as.factor(player_id),\n    ypc = rushing_yards / carries,\n    contractYear = factor(\n      contractYear,\n      levels = c(0, 1),\n      labels = c(\"no\", \"yes\")\n    ))\n\nplayer_statsContracts_seasonal[sapply(player_statsContracts_seasonal, is.infinite)] &lt;- NA\n\nplayer_statsContracts_seasonal$ageCentered20 &lt;- player_statsContracts_seasonal$age - 20\nplayer_statsContracts_seasonal$ageCentered20Quadratic &lt;- player_statsContracts_seasonal$ageCentered20 ^ 2\n\n# Merge with seasonal fantasy points data\n\n\n\n18.2.1 QB\nFirst, we prepare the data by merging and performing additional processing:\n\nCode# Merge with QBR data\nnfl_espnQBR_weekly$merge_name &lt;- paste(nfl_espnQBR_weekly$name_first, nfl_espnQBR_weekly$name_last, sep = \" \") %&gt;% \n  nflreadr::clean_player_names(., lowercase = TRUE)\n\nnfl_contractYearQBR_weekly &lt;- nfl_playerContractYears %&gt;% \n  dplyr::filter(position == \"QB\") %&gt;% \n  dplyr::full_join(\n    .,\n    nfl_espnQBR_weekly,\n    by = c(\"merge_name\",\"team\",\"season\")\n  )\n\nnfl_contractYearQBR_weekly$contractYear[which(is.na(nfl_contractYearQBR_weekly$contractYear))] &lt;- 0\n#nfl_contractYearQBR_weekly$contractYear &lt;- factor(\n#  nfl_contractYearQBR_weekly$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\nnfl_contractYearQBR_weekly &lt;- nfl_contractYearQBR_weekly %&gt;% \n  dplyr::arrange(merge_name, season, season_type, game_week)\n\nnfl_contractYearQBRsubset_weekly &lt;- nfl_contractYearQBR_weekly %&gt;% \n  dplyr::filter(season_type == \"Regular\") %&gt;% \n  dplyr::arrange(merge_name, season, season_type, game_week) %&gt;% \n  mutate(\n    player = coalesce(player, name_display),\n    position = \"QB\") %&gt;% \n  group_by(merge_name, player_id) %&gt;% \n  fill(player, .direction = \"downup\")\n\n# Merge with age and experience\nnfl_contractYearQBRsubset_weekly &lt;- player_statsContractsSubset_offense_weekly %&gt;% \n  dplyr::filter(position == \"QB\") %&gt;% \n  dplyr::select(merge_name, season, week, age, years_of_experience, fantasyPoints) %&gt;% \n  full_join(\n    nfl_contractYearQBRsubset_weekly,\n    by = c(\"merge_name\",\"season\", c(\"week\" = \"game_week\"))\n  ) %&gt;% select(player_id, season, week, player, everything()) %&gt;% \n  arrange(player_id, season, week)\n\n#hist(nfl_contractYearQBRsubset_weekly$qb_plays) # players have at least 20 dropbacks per game\n\n# Calculate Per Game Totals\nnfl_contractYearQBR_seasonal &lt;- nfl_contractYearQBRsubset_weekly %&gt;% \n  dplyr::group_by(merge_name, season) %&gt;% \n  dplyr::summarise(\n    age = min(age, na.rm = TRUE),\n    years_of_experience = min(years_of_experience, na.rm = TRUE),\n    qbr = mean(qbr_total, na.rm = TRUE),\n    pts_added = mean(pts_added, na.rm = TRUE),\n    epa_pass = mean(pass, na.rm = TRUE),\n    qb_plays = sum(qb_plays, na.rm = TRUE), # season total\n    fantasyPoints = sum(fantasyPoints, na.rm = TRUE), # season total\n    contractYear = mean(contractYear, na.rm = TRUE),\n    games = n(),\n    .groups = \"drop_last\"\n  ) %&gt;% \n  dplyr::mutate(\n    contractYear = factor(\n      contractYear,\n      levels = c(0, 1),\n      labels = c(\"no\", \"yes\")\n    ))\n\nnfl_contractYearQBR_seasonal[sapply(nfl_contractYearQBR_seasonal, is.infinite)] &lt;- NA\n\nnfl_contractYearQBR_seasonal$ageCentered20 &lt;- nfl_contractYearQBR_seasonal$age - 20\nnfl_contractYearQBR_seasonal$ageCentered20Quadratic &lt;- nfl_contractYearQBR_seasonal$ageCentered20 ^ 2\n\nnfl_contractYearQBR_seasonal &lt;- nfl_contractYearQBR_seasonal %&gt;% \n  group_by(merge_name) %&gt;%\n  mutate(player_id = as.factor(as.character(cur_group_id())))\n\nnfl_contractYearQBRsubset_seasonal &lt;- nfl_contractYearQBR_seasonal %&gt;% \n  dplyr::filter(\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\nThen, we analyze the data.\n\n18.2.1.1 Quarterback Rating\nBelow is a mixed model that examines whether a player has a higher QBR per game when they are in a contract year compared to when they are not in a contract year. The first model includes just contract year as a predictor. The second model includes additional covariates, including player age and experience. In terms of Quarterback Rating (QBR), findings from the models indicate that Quarterbacks did not perform significantly better in their contract year.\n\nCodemixedModel_qbr &lt;- lmerTest::lmer(\n  qbr ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_qbr)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: qbr ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 9440.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2451 -0.5403  0.0767  0.5700  3.2399 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 110.5    10.51   \n Residual              199.0    14.11   \nNumber of obs: 1127, groups:  player_id, 262\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)       44.6156     0.8526  237.4894  52.328   &lt;2e-16 ***\ncontractYearyes   -0.1715     1.1601 1008.9925  -0.148    0.883    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.239\n\nCodeperformance::r2(mixedModel_qbr)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.357\n     Marginal R2: 0.000\n\nCodeemmeans::emmeans(mixedModel_qbr, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             44.6 0.853 272     42.9     46.3\n yes            44.4 1.270 774     42.0     46.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_qbr &lt;- lmerTest::lmer(\n  qbr ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_qbr)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: qbr ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 9367.1\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-3.367 -0.513  0.084  0.549  3.266 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   126.9109 11.2655       \n           ageCentered20   0.3855  0.6209  -0.32\n Residual                191.2250 13.8284       \nNumber of obs: 1119, groups:  player_id, 258\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              39.41240    2.25893  194.07184  17.447  &lt; 2e-16 ***\ncontractYearyes           0.01999    1.20153 1000.46464   0.017  0.98673    \nageCentered20             1.52956    0.64905  288.74817   2.357  0.01911 *  \nageCentered20Quadratic   -0.07463    0.02254  106.04139  -3.311  0.00127 ** \nyears_of_experience      -0.19545    0.53974  323.76549  -0.362  0.71750    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.059                     \nageCentrd20 -0.790 -0.071              \nagCntrd20Qd  0.736  0.049 -0.616       \nyrs_f_xprnc  0.238 -0.023 -0.693 -0.094\n\nCodeperformance::r2(mixedModelAge_qbr)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.403\n     Marginal R2: 0.012\n\nCodeemmeans::emmeans(mixedModelAge_qbr, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             44.2 0.905 251     42.5     46.0\n yes            44.3 1.300 718     41.7     46.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.1.2 Points Added\nIn terms of points added, Quarterbacks did not perform better in their contract year.\n\nCodemixedModel_ptsAdded &lt;- lmerTest::lmer(\n  pts_added ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_ptsAdded)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pts_added ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 5132.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.7312 -0.4973  0.0857  0.5453  4.2750 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.574    1.604   \n Residual              4.276    2.068   \nNumber of obs: 1127, groups:  player_id, 262\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)      -0.7982     0.1282 223.7371  -6.226 2.33e-09 ***\ncontractYearyes  -0.1170     0.1705 993.6872  -0.686    0.493    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.233\n\nCodeperformance::r2(mixedModel_ptsAdded)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.376\n     Marginal R2: 0.000\n\nCodeemmeans::emmeans(mixedModel_ptsAdded, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no           -0.798 0.128 272    -1.05   -0.546\n yes          -0.915 0.188 763    -1.28   -0.546\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_ptsAdded &lt;- lmerTest::lmer(\n  pts_added ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_ptsAdded)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pts_added ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 5103.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8882 -0.4961  0.0894  0.5301  4.2771 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   3.57383  1.8905        \n           ageCentered20 0.01126  0.1061   -0.52\n Residual                4.09959  2.0247        \nNumber of obs: 1119, groups:  player_id, 258\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             -1.579520   0.343782 185.015762  -4.595 8.01e-06 ***\ncontractYearyes         -0.103404   0.176353 984.230548  -0.586  0.55778    \nageCentered20            0.197941   0.097544 286.031947   2.029  0.04336 *  \nageCentered20Quadratic  -0.010740   0.003374 106.514781  -3.183  0.00191 ** \nyears_of_experience      0.009662   0.080376 306.571919   0.120  0.90440    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.059                     \nageCentrd20 -0.790 -0.072              \nagCntrd20Qd  0.731  0.054 -0.624       \nyrs_f_xprnc  0.244 -0.023 -0.696 -0.081\n\nCodeperformance::r2(mixedModelAge_ptsAdded)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.416\n     Marginal R2: 0.010\n\nCodeemmeans::emmeans(mixedModelAge_ptsAdded, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no           -0.823 0.134 254    -1.09   -0.559\n yes          -0.927 0.192 714    -1.30   -0.550\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.1.3 Expected Points Added\nIn terms of expected points added (EPA) from passing plays, when not controlling for player age and experience, Quarterbacks performed better in their contract year. However, when controlling for player age and experience, Quarterbacks did not perform significantly better in their contract year.\n\nCodemixedModel_epaPass &lt;- lmerTest::lmer(\n  epa_pass ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: epa_pass ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4784.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0638 -0.5098  0.0388  0.5482  4.4079 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.51     1.584   \n Residual              2.98     1.726   \nNumber of obs: 1127, groups:  player_id, 262\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       1.1290     0.1198 242.6476   9.426  &lt; 2e-16 ***\ncontractYearyes   0.3946     0.1436 977.3789   2.747  0.00612 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.208\n\nCodeperformance::r2(mixedModel_epaPass)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.460\n     Marginal R2: 0.004\n\nCodeemmeans::emmeans(mixedModel_epaPass, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             1.13 0.120 272    0.893     1.36\n yes            1.52 0.167 709    1.196     1.85\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaPass &lt;- lmerTest::lmer(\n  epa_pass ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 | player_id), # removed random slopes to address convergence issue\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: epa_pass ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4755.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1995 -0.4944  0.0481  0.5399  4.3277 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.560    1.600   \n Residual              2.933    1.713   \nNumber of obs: 1119, groups:  player_id, 258\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)  \n(Intercept)             3.318e-01  2.812e-01  9.569e+02   1.180   0.2383  \ncontractYearyes         2.359e-01  1.490e-01  9.964e+02   1.583   0.1137  \nageCentered20           1.374e-01  8.191e-02  7.390e+02   1.677   0.0939 .\nageCentered20Quadratic -4.524e-03  2.587e-03  1.057e+03  -1.749   0.0806 .\nyears_of_experience     1.751e-02  7.151e-02  4.131e+02   0.245   0.8067  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.066                     \nageCentrd20 -0.770 -0.068              \nagCntrd20Qd  0.704  0.054 -0.562       \nyrs_f_xprnc  0.278 -0.028 -0.737 -0.108\n\nCodeperformance::r2(mixedModelAge_epaPass)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.476\n     Marginal R2: 0.019\n\nCodeemmeans::emmeans(mixedModelAge_epaPass, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             1.21 0.123 271    0.965     1.45\n yes            1.44 0.169 694    1.112     1.78\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.1.4 Fantasy Points\nIn terms of fantasy points, Quarterbacks performed significantly worse in their contract year, even controlling for player age and experience.\n\nCodemixedModel_fantasyPtsPass &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_fantasyPtsPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 13300\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7938 -0.5628 -0.0854  0.6325  2.7421 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 6298     79.36   \n Residual              5477     74.01   \nNumber of obs: 1127, groups:  player_id, 262\n\nFixed effects:\n                Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)      111.342      5.749 299.090  19.367  &lt; 2e-16 ***\ncontractYearyes  -29.589      6.205 989.601  -4.769 2.13e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.185\n\nCodeperformance::r2(mixedModel_fantasyPtsPass)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.540\n     Marginal R2: 0.011\n\nCodeemmeans::emmeans(mixedModel_fantasyPtsPass, \"contractYear\")\n\n contractYear emmean   SE  df lower.CL upper.CL\n no            111.3 5.75 271    100.0    122.7\n yes            81.8 7.64 650     66.7     96.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_fantasyPtsPass &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 | player_id), # removed random slopes to address convergence issue\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_fantasyPtsPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 13181.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9481 -0.5697 -0.0770  0.6226  2.6078 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 6574     81.08   \n Residual              5333     73.03   \nNumber of obs: 1119, groups:  player_id, 258\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             138.77595   12.60858  971.08217  11.006  &lt; 2e-16 ***\ncontractYearyes         -23.08409    6.42163  997.45929  -3.595 0.000341 ***\nageCentered20            -8.83704    3.72487  818.10907  -2.372 0.017901 *  \nageCentered20Quadratic   -0.09183    0.11208 1044.34333  -0.819 0.412796    \nyears_of_experience       8.44017    3.34466  523.36645   2.523 0.011915 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.068                     \nageCentrd20 -0.754 -0.065              \nagCntrd20Qd  0.675  0.051 -0.526       \nyrs_f_xprnc  0.304 -0.026 -0.764 -0.111\n\nCodeperformance::r2(mixedModelAge_fantasyPtsPass)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.569\n     Marginal R2: 0.037\n\nCodeemmeans::emmeans(mixedModelAge_fantasyPtsPass, \"contractYear\")\n\n contractYear emmean   SE  df lower.CL upper.CL\n no            110.3 5.96 271     98.5      122\n yes            87.2 7.76 627     71.9      102\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.2 RB\n\nCodeplayer_statsContractsRB_seasonal &lt;- player_statsContracts_seasonal %&gt;% \n  dplyr::filter(\n    position_group == \"RB\",\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\n\n18.2.2.1 Yards Per Carry\nIn terms of yards per carry (YPC), Running Backs did not perform significantly better in their contract year.\n\nCodemixedModel_ypc &lt;- lmerTest::lmer(\n  ypc ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_ypc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ypc ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 6361.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.9762 -0.4002  0.0051  0.4031 14.8072 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.5084   0.713   \n Residual              1.8718   1.368   \nNumber of obs: 1744, groups:  player_id, 531\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)     3.913e+00  5.135e-02 5.086e+02  76.202   &lt;2e-16 ***\ncontractYearyes 8.829e-03  8.155e-02 1.677e+03   0.108    0.914    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.374\n\nCodeperformance::r2(mixedModel_ypc)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.214\n     Marginal R2: 0.000\n\nCodeemmeans::emmeans(mixedModel_ypc, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no             3.91 0.0514  631     3.81     4.01\n yes            3.92 0.0786 1257     3.77     4.08\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_ypc &lt;- lmerTest::lmer(\n  ypc ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_ypc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ypc ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 6346.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-7.7936 -0.3853 -0.0070  0.3843 14.2145 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.39621  0.6295        \n           ageCentered20 0.01043  0.1021   -0.28\n Residual                1.79738  1.3407        \nNumber of obs: 1742, groups:  player_id, 529\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             4.223e+00  1.705e-01  7.173e+02  24.772   &lt;2e-16 ***\ncontractYearyes         1.093e-01  8.800e-02  1.602e+03   1.243    0.214    \nageCentered20          -5.148e-02  5.996e-02  8.149e+02  -0.859    0.391    \nageCentered20Quadratic -2.817e-03  4.225e-03  4.257e+02  -0.667    0.505    \nyears_of_experience     1.444e-02  3.780e-02  4.721e+02   0.382    0.703    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.151                     \nageCentrd20 -0.891 -0.173              \nagCntrd20Qd  0.795  0.127 -0.800       \nyrs_f_xprnc  0.057 -0.066 -0.355 -0.198\n\nCodeperformance::r2(mixedModelAge_ypc)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.267\n     Marginal R2: 0.017\n\nCodeemmeans::emmeans(mixedModelAge_ypc, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no             3.86 0.0547  555     3.76     3.97\n yes            3.97 0.0822 1231     3.81     4.13\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.2.2 Expected Points Added\nIn terms of expected points added (EPA) from rushing plays, Running Backs did not perform significantly better in their contract year.\n\nCodemixedModel_epaRush &lt;- lmerTest::lmer(\n  rushing_epa ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: rushing_epa ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 5057.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.5544 -0.5021  0.0793  0.5799  3.4236 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.09945  0.3154  \n Residual              0.97437  0.9871  \nNumber of obs: 1744, groups:  player_id, 531\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       -0.64551    0.03172  652.34985 -20.349   &lt;2e-16 ***\ncontractYearyes    0.03646    0.05649 1741.99500   0.645    0.519    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.441\n\nCodeperformance::r2(mixedModel_epaRush)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.093\n     Marginal R2: 0.000\n\nCodeemmeans::emmeans(mixedModel_epaRush, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no           -0.646 0.0317  650   -0.708   -0.583\n yes          -0.609 0.0512 1162   -0.710   -0.509\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaRush &lt;- lmerTest::lmer(\n  rushing_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: rushing_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 5062.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6136 -0.4966  0.0707  0.5823  3.4224 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.173096 0.41605       \n           ageCentered20 0.002046 0.04523  -0.67\n Residual                0.958271 0.97891       \nNumber of obs: 1742, groups:  player_id, 529\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)            -6.602e-01  1.179e-01  4.018e+02  -5.597 4.04e-08 ***\ncontractYearyes         7.124e-02  5.994e-02  1.545e+03   1.189  0.23475    \nageCentered20           5.052e-02  3.974e-02  4.083e+02   1.271  0.20434    \nageCentered20Quadratic -1.233e-03  2.753e-03  2.098e+02  -0.448  0.65469    \nyears_of_experience    -5.986e-02  2.252e-02  4.869e+02  -2.658  0.00812 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.148                     \nageCentrd20 -0.902 -0.197              \nagCntrd20Qd  0.818  0.154 -0.838       \nyrs_f_xprnc  0.030 -0.053 -0.307 -0.191\n\nCodeperformance::r2(mixedModelAge_epaRush)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.112\n     Marginal R2: 0.008\n\nCodeemmeans::emmeans(mixedModelAge_epaRush, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no           -0.658 0.0326  581   -0.722   -0.594\n yes          -0.587 0.0532 1159   -0.691   -0.482\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.2.3 Fantasy Points\nIn terms of fantasy points, Running Backs performed significantly worse in their contract year, even controlling for player age and experience.\n\nCodemixedModel_fantasyPtsRush &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_fantasyPtsRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 20809.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1340 -0.5057 -0.1729  0.4156  3.8988 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 3546     59.55   \n Residual              3042     55.16   \nNumber of obs: 1844, groups:  player_id, 548\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       81.495      3.055  654.865  26.672  &lt; 2e-16 ***\ncontractYearyes  -13.576      3.466 1589.483  -3.917 9.36e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.235\n\nCodeperformance::r2(mixedModel_fantasyPtsRush)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.541\n     Marginal R2: 0.005\n\nCodeemmeans::emmeans(mixedModel_fantasyPtsRush, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             81.5 3.06  597     75.5     87.5\n yes            67.9 4.05 1246     60.0     75.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_fantasyPtsRush &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_fantasyPtsRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 20671.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4371 -0.4920 -0.1588  0.4013  3.6457 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   6841.8   82.715        \n           ageCentered20   54.6    7.389   -0.75\n Residual                2641.7   51.398        \nNumber of obs: 1842, groups:  player_id, 546\n\nFixed effects:\n                        Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)              70.0016     8.8203  729.4479   7.936 7.85e-15 ***\ncontractYearyes          -9.3663     3.6074 1564.7556  -2.596  0.00951 ** \nageCentered20             1.5209     3.0282  968.8867   0.502  0.61560    \nageCentered20Quadratic   -1.1212     0.1823  514.8215  -6.149 1.57e-09 ***\nyears_of_experience      12.0528     2.0883  659.2541   5.772 1.21e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.170                     \nageCentrd20 -0.868 -0.171              \nagCntrd20Qd  0.716  0.160 -0.740       \nyrs_f_xprnc  0.264 -0.065 -0.551 -0.086\n\nCodeperformance::r2(mixedModelAge_fantasyPtsRush)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.613\n     Marginal R2: 0.062\n\nCodeemmeans::emmeans(mixedModelAge_fantasyPtsRush, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             80.7 3.08  599     74.6     86.7\n yes            71.3 4.04 1227     63.4     79.2\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.3 WR/TE\n\nCodeplayer_statsContractsWRTE_seasonal &lt;- player_statsContracts_seasonal %&gt;% \n  dplyr::filter(\n    position_group %in% c(\"WR\",\"TE\"),\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\n\n18.2.3.1 Receiving Yards\nIn terms of receiving yards, Wide Receivers/Tight Ends performed significantly worse in their contract year, even controlling for player age and experience.\n\nCodemixedModel_receivingYards &lt;- lmerTest::lmer(\n  receiving_yards ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_receivingYards)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: receiving_yards ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 32762.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8528 -0.5278 -0.1103  0.5063  4.5623 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 280.4    16.74   \n Residual              182.7    13.52   \nNumber of obs: 3845, groups:  player_id, 1087\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)       25.1251     0.5908 1296.1433  42.528  &lt; 2e-16 ***\ncontractYearyes   -4.0728     0.5567 3257.3106  -7.316  3.2e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.234\n\nCodeperformance::r2(mixedModel_receivingYards)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.608\n     Marginal R2: 0.007\n\nCodeemmeans::emmeans(mixedModel_receivingYards, \"contractYear\")\n\n contractYear emmean    SE   df lower.CL upper.CL\n no             25.1 0.591 1189     24.0     26.3\n yes            21.1 0.711 2075     19.7     22.4\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_receivingYards &lt;- lmerTest::lmer(\n  receiving_yards ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_receivingYards)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nreceiving_yards ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 32324.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9759 -0.5212 -0.0962  0.4752  3.8093 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   546.051  23.368        \n           ageCentered20   5.812   2.411   -0.69\n Residual                136.886  11.700        \nNumber of obs: 3843, groups:  player_id, 1086\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              14.70081    1.57144 1529.88027   9.355  &lt; 2e-16 ***\ncontractYearyes          -3.03800    0.54699 3035.02757  -5.554 3.03e-08 ***\nageCentered20             2.45054    0.53041 2226.85462   4.620 4.05e-06 ***\nageCentered20Quadratic   -0.43105    0.02688 1513.89053 -16.036  &lt; 2e-16 ***\nyears_of_experience       3.34628    0.40668 1329.37210   8.228 4.48e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.122                     \nageCentrd20 -0.844 -0.150              \nagCntrd20Qd  0.659  0.071 -0.648       \nyrs_f_xprnc  0.336  0.030 -0.663 -0.067\n\nCodeperformance::r2(mixedModelAge_receivingYards)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.745\n     Marginal R2: 0.112\n\nCodeemmeans::emmeans(mixedModelAge_receivingYards, \"contractYear\")\n\n contractYear emmean    SE   df lower.CL upper.CL\n no             24.0 0.622 1199     22.8     25.2\n yes            20.9 0.713 1898     19.5     22.3\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.3.2 Expected Points Added\nIn terms of expected points added (EPA) from receiving plays, Wide Receivers/Tight Ends performed significantly worse in their contract year, even controlling for player age and experience.\n\nCodemixedModel_epaReceiving &lt;- lmerTest::lmer(\n  receiving_epa ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: receiving_epa ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 12590.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.5745 -0.5715 -0.0389  0.5376  3.8959 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.555    0.745   \n Residual              1.301    1.141   \nNumber of obs: 3770, groups:  player_id, 1070\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)        0.65910    0.03347 1440.32757  19.692  &lt; 2e-16 ***\ncontractYearyes   -0.16319    0.04527 3585.01245  -3.605 0.000316 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.358\n\nCodeperformance::r2(mixedModel_epaReceiving)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.301\n     Marginal R2: 0.003\n\nCodeemmeans::emmeans(mixedModel_epaReceiving, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no            0.659 0.0335 1268    0.593    0.725\n yes           0.496 0.0457 2454    0.406    0.585\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaReceiving &lt;- lmerTest::lmer(\n  receiving_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nreceiving_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 12573.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.4226 -0.5652 -0.0324  0.5258  3.8586 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.9379   0.96844       \n           ageCentered20 0.0055   0.07416  -0.66\n Residual                1.2482   1.11724       \nNumber of obs: 3769, groups:  player_id, 1069\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             3.514e-01  1.040e-01  1.117e+03   3.379 0.000752 ***\ncontractYearyes        -1.635e-01  4.791e-02  3.496e+03  -3.413 0.000651 ***\nageCentered20           7.729e-02  3.395e-02  1.207e+03   2.277 0.022982 *  \nageCentered20Quadratic -9.348e-03  1.879e-03  4.726e+02  -4.974 9.18e-07 ***\nyears_of_experience     6.149e-02  2.301e-02  1.144e+03   2.672 0.007654 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.143                     \nageCentrd20 -0.871 -0.203              \nagCntrd20Qd  0.758  0.117 -0.741       \nyrs_f_xprnc  0.202  0.041 -0.524 -0.121\n\nCodeperformance::r2(mixedModelAge_epaReceiving)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.338\n     Marginal R2: 0.013\n\nCodeemmeans::emmeans(mixedModelAge_epaReceiving, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no            0.657 0.0346 1214    0.589    0.725\n yes           0.494 0.0467 2498    0.402    0.585\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.3.3 Fantasy Points\nIn terms of fantasy points, Wide Receivers/Tight Ends performed significantly worse in their contract year, even controlling for player age and experience.\n\nCodemixedModel_fantasyPtsReceiving &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_fantasyPtsReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 42562.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2503 -0.5270 -0.1438  0.4785  4.5956 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2935     54.18   \n Residual              2473     49.73   \nNumber of obs: 3845, groups:  player_id, 1087\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       76.244      1.978 1334.194  38.543  &lt; 2e-16 ***\ncontractYearyes  -14.199      2.032 3340.122  -6.989 3.32e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.258\n\nCodeperformance::r2(mixedModel_fantasyPtsReceiving)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.546\n     Marginal R2: 0.008\n\nCodeemmeans::emmeans(mixedModel_fantasyPtsReceiving, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             76.2 1.98 1207     72.4     80.1\n yes            62.0 2.44 2199     57.3     66.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_fantasyPtsReceiving &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_fantasyPtsReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nfantasyPoints ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 42196.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0668 -0.4967 -0.1292  0.4563  4.9582 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   5863.26  76.572        \n           ageCentered20   60.76   7.795   -0.71\n Residual                1960.17  44.274        \nNumber of obs: 3843, groups:  player_id, 1086\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              43.34064    5.49900 1469.85276   7.882 6.25e-15 ***\ncontractYearyes         -10.48463    2.03309 3169.40708  -5.157 2.66e-07 ***\nageCentered20             7.30197    1.83894 2147.02160   3.971 7.40e-05 ***\nageCentered20Quadratic   -1.37759    0.09638 1290.30852 -14.293  &lt; 2e-16 ***\nyears_of_experience      11.34115    1.36400 1283.33031   8.315 2.32e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.128                     \nageCentrd20 -0.852 -0.161              \nagCntrd20Qd  0.684  0.080 -0.675       \nyrs_f_xprnc  0.306  0.032 -0.629 -0.076\n\nCodeperformance::r2(mixedModelAge_fantasyPtsReceiving)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.679\n     Marginal R2: 0.103\n\nCodeemmeans::emmeans(mixedModelAge_fantasyPtsReceiving, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             72.5 2.07 1204     68.5     76.6\n yes            62.1 2.45 2055     57.3     66.9\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\n18.2.4 QB/RB/WR/TE\n\nCodeplayer_statsContractsQBRBWRTE_seasonal &lt;- player_statsContracts_seasonal %&gt;% \n  dplyr::filter(\n    position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"),\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\n\n18.2.4.1 Fantasy Points\nIn terms of fantasy points, Quarterbacks/Running Backs/Wide Receivers/Tight Ends performed significantly worse in their contract year, even controlling for player age and experience.\n\nCodemixedModel_fantasyPts &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + position_group + (1 | player_id),\n  data = player_statsContractsQBRBWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_fantasyPts)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + position_group + (1 | player_id)\n   Data: player_statsContractsQBRBWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 71203.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7593 -0.5149 -0.1412  0.4757  4.2028 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 3367     58.02   \n Residual              2940     54.22   \nNumber of obs: 6338, groups:  player_id, 1801\n\nFixed effects:\n                 Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       153.731      5.297 1921.969  29.024   &lt;2e-16 ***\ncontractYearyes   -16.267      1.757 5506.301  -9.258   &lt;2e-16 ***\nposition_groupRB  -71.612      6.023 1903.445 -11.890   &lt;2e-16 ***\nposition_groupTE  -92.593      6.341 1890.741 -14.601   &lt;2e-16 ***\nposition_groupWR  -68.644      5.855 1902.453 -11.724   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY pst_RB pst_TE\ncontrctYrys -0.083                     \npostn_grpRB -0.874  0.013              \npostn_grpTE -0.829 -0.003  0.729       \npostn_grpWR -0.898  0.002  0.790  0.750\n\nCodeperformance::r2(mixedModel_fantasyPts)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.578\n     Marginal R2: 0.095\n\nCodeemmeans::emmeans(mixedModel_fantasyPts, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             95.5 1.90 1911     91.8     99.3\n yes            79.3 2.28 3302     74.8     83.7\n\nResults are averaged over the levels of: position_group \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_fantasyPts &lt;- lmerTest::lmer(\n  fantasyPoints ~ contractYear + position_group + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsQBRBWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_fantasyPts)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: fantasyPoints ~ contractYear + position_group + ageCentered20 +  \n    ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 |  \n    player_id)\n   Data: player_statsContractsQBRBWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 70714.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6807 -0.4959 -0.1259  0.4469  4.4881 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   6282.61  79.263        \n           ageCentered20   67.43   8.211   -0.69\n Residual                2428.40  49.279        \nNumber of obs: 6334, groups:  player_id, 1798\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             133.39528    6.92252 2745.68858  19.270  &lt; 2e-16 ***\ncontractYearyes         -11.30969    1.78900 5319.62142  -6.322  2.8e-10 ***\nposition_groupRB        -71.11907    6.11655 1837.86928 -11.627  &lt; 2e-16 ***\nposition_groupTE        -92.24588    6.41645 1795.94420 -14.376  &lt; 2e-16 ***\nposition_groupWR        -68.57668    5.94960 1828.90822 -11.526  &lt; 2e-16 ***\nageCentered20             3.16804    1.47524 3313.76803   2.147   0.0318 *  \nageCentered20Quadratic   -1.12349    0.07419 1794.90207 -15.143  &lt; 2e-16 ***\nyears_of_experience      11.62581    1.13823 2207.50866  10.214  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY pst_RB pst_TE pst_WR agCn20 agC20Q\ncontrctYrys  0.102                                          \npostn_grpRB -0.710 -0.024                                   \npostn_grpTE -0.656 -0.023  0.735                            \npostn_grpWR -0.734 -0.034  0.794  0.755                     \nageCentrd20 -0.555 -0.148  0.018 -0.009  0.033              \nagCntrd20Qd  0.439  0.083 -0.017 -0.004 -0.022 -0.646       \nyrs_f_xprnc  0.181  0.005  0.023  0.033  0.005 -0.645 -0.088\n\nCodeperformance::r2(mixedModelAge_fantasyPts)\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.678\n     Marginal R2: 0.151\n\nCodeemmeans::emmeans(mixedModelAge_fantasyPts, \"contractYear\")\n\n contractYear emmean   SE   df lower.CL upper.CL\n no             90.9 1.97 1853       87     94.7\n yes            79.6 2.32 3175       75     84.1\n\nResults are averaged over the levels of: position_group \nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersConclusion",
    "href": "mythbusters.html#sec-mythbustersConclusion",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n18.3 Conclusion",
    "text": "18.3 Conclusion\nThere is a widely held belief that NFL players perform better in the last year of the contract because they are motivated to gain another contract. There is some evidence in the NBA and MLB that players tend to perform better in their contract year. We evaluated this possibility among NFL players who were Quarterbacks, Running Backs, Wide Receivers, or Tight Ends. We evaluated a wide range of performance indexes, including Quarterback Rating, yards per carry, points added, expected points added, receiving yards, and fantasy points. None of the positions showed significantly better performance in their contract year for any of the performance indexes. By contrast, if anything, players tended to perform more poorly during their contract year, as operationalized by fantasy points, receiving yards (WR/TE), and EPA from receiving plays (WR/TE), even when controlling for player and age experience. In sum, we did not find evidence in support of the contract year hypothesis and consider this myth debunked. However, we are open to this possibility being reexamined in new ways or with additional performance metrics.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersSessionInfo",
    "href": "mythbusters.html#sec-mythbustersSessionInfo",
    "title": "18  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n18.4 Session Info",
    "text": "18.4 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4    forcats_1.0.0      stringr_1.5.1      dplyr_1.1.4       \n [5] purrr_1.1.0        readr_2.1.5        tidyr_1.3.1        tibble_3.3.0      \n [9] ggplot2_3.5.2      tidyverse_2.0.0    emmeans_1.11.2     performance_0.15.0\n[13] lmerTest_3.1-3     lme4_1.1-37        Matrix_1.7-3       nflreadr_1.4.1    \n[17] petersenlab_1.2.0 \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    psych_2.5.6         viridisLite_0.4.2  \n [4] farver_2.1.2        fastmap_1.2.0       TH.data_1.1-3      \n [7] digest_0.6.37       rpart_4.1.24        timechange_0.3.0   \n[10] estimability_1.5.1  lifecycle_1.0.4     cluster_2.1.8.1    \n[13] survival_3.8-3      magrittr_2.0.3      compiler_4.5.1     \n[16] rlang_1.1.6         Hmisc_5.2-3         tools_4.5.1        \n[19] yaml_2.3.10         data.table_1.17.8   knitr_1.50         \n[22] htmlwidgets_1.6.4   mnormt_2.1.1        plyr_1.8.9         \n[25] RColorBrewer_1.1-3  multcomp_1.4-28     withr_3.0.2        \n[28] foreign_0.8-90      numDeriv_2016.8-1.1 nnet_7.3-20        \n[31] grid_4.5.1          stats4_4.5.1        lavaan_0.6-19      \n[34] xtable_1.8-4        colorspace_2.1-1    scales_1.4.0       \n[37] MASS_7.3-65         insight_1.4.0       cli_3.6.5          \n[40] mvtnorm_1.3-3       rmarkdown_2.29      reformulas_0.4.1   \n[43] generics_0.1.4      rstudioapi_0.17.1   tzdb_0.5.0         \n[46] reshape2_1.4.4      minqa_1.2.8         DBI_1.2.3          \n[49] cachem_1.1.0        splines_4.5.1       parallel_4.5.1     \n[52] base64enc_0.1-3     mitools_2.4         vctrs_0.6.5        \n[55] sandwich_3.1-1      boot_1.3-31         jsonlite_2.0.0     \n[58] hms_1.1.3           pbkrtest_0.5.5      Formula_1.2-5      \n[61] htmlTable_2.4.3     glue_1.8.0          nloptr_2.2.1       \n[64] codetools_0.2-20    stringi_1.8.7       gtable_0.3.6       \n[67] quadprog_1.5-8      pillar_1.11.0       htmltools_0.5.8.1  \n[70] R6_2.6.1            Rdpack_2.6.4        mix_1.0-13         \n[73] evaluate_1.0.4      pbivnorm_0.6.0      lattice_0.22-7     \n[76] rbibutils_2.3       backports_1.5.0     broom_1.0.9        \n[79] memoise_2.0.1       Rcpp_1.1.0          coda_0.19-4.1      \n[82] gridExtra_2.3       nlme_3.1-168        checkmate_2.3.3    \n[85] xfun_0.53           zoo_1.8-14          pkgconfig_2.0.3    \n\n\n\n\n\n\nBales, J. (2012). 2012 contract year players and the myth of increased production. https://www.4for4.com/2012/preseason/2012-contract-year-players-and-myth-increased-production\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nNiles, B. (2022). Do players perform better in fantasy football in a contract year? https://www.4for4.com/2022/preseason/do-players-perform-better-fantasy-football-contract-year\n\n\nWhite, M. H., & Sheldon, K. M. (2014). The contract year syndrome in the NBA and MLB: A classic undermining pattern. Motivation and Emotion, 38(2), 196–205. https://doi.org/10.1007/s11031-013-9389-7",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "machine-learning.html",
    "href": "machine-learning.html",
    "title": "19  Machine Learning",
    "section": "",
    "text": "19.1 Getting Started\nThis chapter provides an overview of machine learning.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-machineLearningGettingStarted",
    "href": "machine-learning.html#sec-machineLearningGettingStarted",
    "title": "19  Machine Learning",
    "section": "",
    "text": "19.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"future\")\nlibrary(\"missRanger\")\nlibrary(\"powerjoin\")\nlibrary(\"tidymodels\")\nlibrary(\"LongituRF\")\nlibrary(\"gpboost\")\nlibrary(\"effectsize\")\nlibrary(\"tidyverse\")\nlibrary(\"knitr\")\n\n\n\n19.1.2 Load Data\n\nCode# Downloaded Data - Processed\nload(file = \"./data/nfl_players.RData\")\nload(file = \"./data/nfl_teams.RData\")\nload(file = \"./data/nfl_rosters.RData\")\nload(file = \"./data/nfl_rosters_weekly.RData\")\nload(file = \"./data/nfl_schedules.RData\")\nload(file = \"./data/nfl_combine.RData\")\nload(file = \"./data/nfl_draftPicks.RData\")\nload(file = \"./data/nfl_depthCharts.RData\")\n#load(file = \"./data/nfl_pbp.RData\")\n#load(file = \"./data/nfl_4thdown.RData\")\n#load(file = \"./data/nfl_participation.RData\")\n#load(file = \"./data/nfl_actualFantasyPoints_weekly.RData\")\nload(file = \"./data/nfl_injuries.RData\")\nload(file = \"./data/nfl_snapCounts.RData\")\nload(file = \"./data/nfl_espnQBR_seasonal.RData\")\nload(file = \"./data/nfl_espnQBR_weekly.RData\")\nload(file = \"./data/nfl_nextGenStats_weekly.RData\")\nload(file = \"./data/nfl_advancedStatsPFR_seasonal.RData\")\nload(file = \"./data/nfl_advancedStatsPFR_weekly.RData\")\nload(file = \"./data/nfl_playerContracts.RData\")\nload(file = \"./data/nfl_ftnCharting.RData\")\nload(file = \"./data/nfl_playerIDs.RData\")\nload(file = \"./data/nfl_rankings_draft.RData\")\nload(file = \"./data/nfl_rankings_weekly.RData\")\nload(file = \"./data/nfl_expectedFantasyPoints_weekly.RData\")\n#load(file = \"./data/nfl_expectedFantasyPoints_pbp.RData\")\n\n# Calculated Data - Processed\nload(file = \"./data/nfl_actualStats_career.RData\")\nload(file = \"./data/nfl_actualStats_seasonal.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\n\n19.1.3 Specify Options\n\nCodeoptions(scipen = 999) # prevent scientific notation",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-machineLearningOverview",
    "href": "machine-learning.html#sec-machineLearningOverview",
    "title": "19  Machine Learning",
    "section": "\n19.2 Overview of Machine Learning",
    "text": "19.2 Overview of Machine Learning\nMachine learning takes us away from focusing on causal inference. Machine learning does not care about which processes are causal—i.e., which processes influence the outcome. Instead, machine learning cares about prediction—it cares about a predictor variable to the extent that it increases predictive accuracy regardless of whether it is causally related to the outcome.\nMachine learning can be useful for leveraging big data and lots of predictor variable to develop predictive models with greater accuracy. However, many machine learning techniques are black boxes—it is often unclear how or why certain predictions are made, which can make it difficult to interpret the model’s decisions and understand the underlying relationships between variables. Machine learning tends to be a data-driven, atheoretical technique. This can result in overfitting. Thus, when estimating machine learning models, it is common to keep a hold-out sample for use in cross-validation to evaluate the extent of shrinkage of model coefficients. The data that the model is trained on is known as the “training data”. The data that the model was not trained on but is then is independently tested on—i.e., the hold-out sample—is the “test data”. Shrinkage occurs when predictor variables explain some random error variance in the original model. When the model is applied to an independent sample (i.e., the test data), the predictive model will likely not perform quite as well, and the regressions coefficients will tend to get smaller (i.e., shrink).\nIf the test data were collected as part of the same processes as the original data and were merely held out for purposes of analysis, this is called internal cross-validation. If the test data were collected separately from the original data used to train the model, this is called external cross-validation.\nMost machine learning methods were developed with cross-sectional data in mind. That is, they assume that each person has only one observation on the outcome variable. However, with longitudinal data, each person has multiple observations on the outcome variable.\nWhen performing machine learning, various approaches may help address this:\n\ntransform data from long to wide form, so that each person has only one row\nwhen designing the training and test sets, keep all measurements from the same person in the same data object (either the training or test set); do not have some measurements from a given person in the training set and other measurements from the same person in the test set\nuse a machine learning approach that accounts for the clustered/nested nature of the data",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-machineLearningTypes",
    "href": "machine-learning.html#sec-machineLearningTypes",
    "title": "19  Machine Learning",
    "section": "\n19.3 Types of Machine Learning",
    "text": "19.3 Types of Machine Learning\nThere are many approaches to machine learning. This chapter discusses several key ones:\n\nsupervised learning\n\ncontinuous outcome (i.e., regression)\n\nlinear regression\nleast absolute shrinkage and selection option (LASSO) regression\nridge regression\nelastic net regression\nrandom forest\n\n\ncategorical outcome (i.e., classification)\n\nlogistic regression\nsupport vector machine\nrandom forest\nboosting\n\ntree boosting\n\n\n\n\n\n\nunsupervised learning\n\ncluster analysis\nprincipal component analysis\nfactor analysis\n\n\nsemi-supervised learning\nreinforcement learning\n\ndeep learning\n\n\nensemble\n\nEnsemble machine learning methods combine multiple machine learning approaches with the goal that combining multiple approaches might lead to more accurate predictions than any one method might be able to achieve on its own.\n\n19.3.1 Supervised Learning\nSupervised learning involves learning from data where the correct classification or outcome is known. For instance, predicting how many points a player will score is a supervised learning task, because there is a ground truth—the actual number of points scored—that can be used to train and evaluate the model.\nUnlike linear and logistic regression, various machine learning techniques can handle multicollinearity, including LASSO regression, ridge regression, and elastic net regression. Least absolute shrinkage and selection option (LASSO) regression performs selection of which predictor variables to keep in the model by shrinking some coefficients to zero, effectively removing them from the model. Ridge regression shrinks the coefficients of predictor variables toward zero, but not to zero, so it does not perform selection of which predictor variables to retain; this allows it to yield stable estimates for multiple correlated predictor variables in the context of multicollinearity. Elastic net involves a combination of LASSO and ridge regression; it performs selection of which predictor variables to keep by shrinking the coefficients of some predictor variables to zero (like LASSO, for variable selection), and it shrinks the coefficients of some predictor variables toward zero (like ridge, for handling multicollinearity among correlated predictors).\nUnless interactions or nonlinear terms are specified, linear, logistic, LASSO, ridge, and elastic net regression assume additive and linear associations between the predictors and outcome. That is, they do not automatically account for interactions among the predictor variables or for nonlinear associations between the predictor variables and the outcome variable (unless interaction terms or nonlinear transformations are explicitly included). By contrast, random forests and tree boosting methods automatically account for interactions and nonlinear associations between predictors and the outcome variable. These models recursively partition the data in ways that capture complex patterns without the need to manually specify interaction or polynomial terms.\n\n19.3.2 Unsupervised Learning\nUnsupervised learning involves learning from data without known classifications. Unsupervised learning is used to discover hidden patterns, groupings, or structures in the data. For instance, if we want to identify different subtypes of Wide Receivers based on their playing style or performance metrics, or uncover underlying dimensions in a large dataset, we would use an unsupervised learning approach.\nWe describe cluster analysis in Chapter 21. We describe principal component analysis in Chapter 23.\n\n19.3.3 Semi-supervised Learning\nSemi-supervised learning combines supervised learning and unsupervised learning by training the model on some data for which the classification is known and some data for which the classification is not known.\n\n19.3.4 Reinforcement Learning\nReinforcement learning involves an agent learning to make decisions by interacting with the environment. Through trial and error, the agent receives feedback in the form of rewards or penalties and learns a strategy that maximizes the cumulative reward over time.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-machineLearningDataProcessing",
    "href": "machine-learning.html#sec-machineLearningDataProcessing",
    "title": "19  Machine Learning",
    "section": "\n19.4 Data Processing",
    "text": "19.4 Data Processing\nSeveral data processing steps are necessary to get the data in the form necessary for machine learning.\n\n19.4.1 Prepare Data for Merging\nFirst, we apply several steps. We subset to the positions and variables of interest. We also rename columns and change variable types to make sure they match the column names and types across objects, which will be important later when we merge the data.\n\nCode# Prepare data for merging\n\n#nfl_actualFantasyPoints_player_weekly &lt;- nfl_actualFantasyPoints_player_weekly %&gt;% \n#  rename(gsis_id = player_id)\n#\n#nfl_actualFantasyPoints_player_seasonal &lt;- nfl_actualFantasyPoints_player_seasonal %&gt;% \n#  rename(gsis_id = player_id)\n\nplayer_stats_seasonal_offense &lt;- player_stats_seasonal %&gt;% \n  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  rename(gsis_id = player_id)\n\nplayer_stats_weekly_offense &lt;- player_stats_weekly %&gt;% \n  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  rename(gsis_id = player_id)\n\nnfl_expectedFantasyPoints_weekly &lt;- nfl_expectedFantasyPoints_weekly %&gt;% \n  rename(gsis_id = player_id)\n\n## Rename other variables to ensure common names\n\n## Ensure variables with the same name have the same type\nnfl_players &lt;- nfl_players %&gt;% \n  mutate(\n    birth_date = as.Date(birth_date),\n    jersey_number = as.character(jersey_number),\n    gsis_it_id = as.character(gsis_it_id),\n    years_of_experience = as.integer(years_of_experience))\n\nplayer_stats_seasonal_offense &lt;- player_stats_seasonal_offense %&gt;% \n  mutate(\n    birth_date = as.Date(birth_date),\n    jersey_number = as.character(jersey_number),\n    gsis_it_id = as.character(gsis_it_id))\n\nnfl_rosters &lt;- nfl_rosters %&gt;% \n  mutate(\n    draft_number = as.integer(draft_number))\n\nnfl_rosters_weekly &lt;- nfl_rosters_weekly %&gt;% \n  mutate(\n    draft_number = as.integer(draft_number))\n\nnfl_depthCharts &lt;- nfl_depthCharts %&gt;% \n  mutate(\n    season = as.integer(season))\n\nnfl_expectedFantasyPoints_weekly &lt;- nfl_expectedFantasyPoints_weekly %&gt;% \n  mutate(\n    season = as.integer(season),\n    receptions = as.integer(receptions)) %&gt;% \n  distinct(gsis_id, season, week, .keep_all = TRUE) # drop duplicated rows\n\n## Rename variables\nnfl_draftPicks &lt;- nfl_draftPicks %&gt;%\n  rename(\n    games_career = games,\n    pass_completions_career = pass_completions,\n    pass_attempts_career = pass_attempts,\n    pass_yards_career = pass_yards,\n    pass_tds_career = pass_tds,\n    pass_ints_career = pass_ints,\n    rush_atts_career = rush_atts,\n    rush_yards_career = rush_yards,\n    rush_tds_career = rush_tds,\n    receptions_career = receptions,\n    rec_yards_career = rec_yards,\n    rec_tds_career = rec_tds,\n    def_solo_tackles_career = def_solo_tackles,\n    def_ints_career = def_ints,\n    def_sacks_career = def_sacks\n  )\n\n## Subset variables\nnfl_expectedFantasyPoints_weekly &lt;- nfl_expectedFantasyPoints_weekly %&gt;% \n  select(gsis_id:position, contains(\"_exp\"), contains(\"_diff\"), contains(\"_team\")) #drop \"raw stats\" variables (e.g., rec_yards_gained) so they don't get coalesced with actual stats\n\n# Check duplicate ids\nplayer_stats_seasonal_offense %&gt;% \n  group_by(gsis_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodenfl_advancedStatsPFR_seasonal %&gt;% \n  group_by(gsis_id, season) %&gt;% \n  filter(n() &gt; 1, !is.na(gsis_id)) %&gt;% \n  select(gsis_id, pfr_id, season, team, everything()) %&gt;% \n  head()\n\n\n  \n\n\n\nBelow, we identify shared variable names across objects to be merged to make sure we account for them in merging:\n\nCodedplyr::intersect(\n  names(nfl_players),\n  names(nfl_draftPicks))\n\n[1] \"gsis_id\"  \"position\"\n\nCodelength(na.omit(nfl_players$position)) # use by default (more cases)\n\n[1] 21360\n\nCodelength(na.omit(nfl_draftPicks$position))\n\n[1] 2855\n\nCodedplyr::intersect(\n  names(player_stats_seasonal_offense),\n  names(nfl_advancedStatsPFR_seasonal))\n\n[1] \"gsis_id\" \"season\"  \"team\"    \"age\"    \n\nCodelength(na.omit(player_stats_seasonal_offense$season)) # use by default (more cases)\n\n[1] 14859\n\nCodelength(na.omit(nfl_advancedStatsPFR_seasonal$season))\n\n[1] 10395\n\nCodelength(na.omit(player_stats_seasonal_offense$team)) # use by default (more cases)\n\n[1] 14858\n\nCodelength(na.omit(nfl_advancedStatsPFR_seasonal$team))\n\n[1] 10395\n\nCodelength(na.omit(player_stats_seasonal_offense$age)) # use by default (more cases)\n\n[1] 14859\n\nCodelength(na.omit(nfl_advancedStatsPFR_seasonal$age))\n\n[1] 10325\n\nCodedplyr::intersect(\n  names(nfl_rosters_weekly),\n  names(nfl_expectedFantasyPoints_weekly))\n\n[1] \"gsis_id\"   \"season\"    \"week\"      \"position\"  \"full_name\"\n\nCodelength(na.omit(nfl_rosters_weekly$season)) # use by default (more cases)\n\n[1] 845134\n\nCodelength(na.omit(nfl_expectedFantasyPoints_weekly$season))\n\n[1] 100272\n\nCodelength(na.omit(nfl_rosters_weekly$week)) # use by default (more cases)\n\n[1] 841942\n\nCodelength(na.omit(nfl_expectedFantasyPoints_weekly$week))\n\n[1] 100272\n\nCodelength(na.omit(nfl_rosters_weekly$position)) # use by default (more cases)\n\n[1] 845101\n\nCodelength(na.omit(nfl_expectedFantasyPoints_weekly$position))\n\n[1] 97815\n\nCodelength(na.omit(nfl_rosters_weekly$full_name)) # use by default (more cases)\n\n[1] 845118\n\nCodelength(na.omit(nfl_expectedFantasyPoints_weekly$full_name))\n\n[1] 97815\n\n\n\n19.4.2 Merge Data\nTo perform machine learning, we need all of the predictor variables and the outcome variable in the same data file. Thus, we must merge data files. To merge data, we use the powerjoin package (Fabri, 2022), which allows coalescing variables with the same name from two different objects. We specify coalesce_xy, which means that—for variables that have the same name across both objects—it keeps the value from object 1 (if present); if not, it keeps the value from object 2. We first merge variables from objects that have the same structure—player data (i.e., id form), seasonal data (i.e., id-season form), or weekly data (i.e., id-season-week form).\n\nCode# Create lists of objects to merge, depending on data structure: id; or id-season; or id-season-week\nplayerListToMerge &lt;- list(\n  nfl_players %&gt;% filter(!is.na(gsis_id)),\n  nfl_draftPicks %&gt;% filter(!is.na(gsis_id)) %&gt;% select(-season)\n)\n\nplayerSeasonListToMerge &lt;- list(\n  player_stats_seasonal_offense %&gt;% filter(!is.na(gsis_id), !is.na(season)),\n  nfl_advancedStatsPFR_seasonal %&gt;% filter(!is.na(gsis_id), !is.na(season))\n)\n\nplayerSeasonWeekListToMerge &lt;- list(\n  nfl_rosters_weekly %&gt;% filter(!is.na(gsis_id), !is.na(season), !is.na(week)),\n  #nfl_actualStats_offense_weekly,\n  nfl_expectedFantasyPoints_weekly %&gt;% filter(!is.na(gsis_id), !is.na(season), !is.na(week))\n  #nfl_advancedStatsPFR_weekly,\n)\n\nplayerSeasonWeekPositionListToMerge &lt;- list(\n  nfl_depthCharts %&gt;% filter(!is.na(gsis_id), !is.na(season), !is.na(week))\n)\n\n# Merge data\nplayerMerged &lt;- playerListToMerge %&gt;% \n  reduce(\n    powerjoin::power_full_join,\n    by = c(\"gsis_id\"),\n    conflict = coalesce_xy) # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2\n\nplayerSeasonMerged &lt;- playerSeasonListToMerge %&gt;% \n  reduce(\n    powerjoin::power_full_join,\n    by = c(\"gsis_id\",\"season\"),\n    conflict = coalesce_xy) # where the objects have the same variable name (e.g., team), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2\n\nplayerSeasonWeekMerged &lt;- playerSeasonWeekListToMerge %&gt;% \n  reduce(\n    powerjoin::power_full_join,\n    by = c(\"gsis_id\",\"season\",\"week\"),\n    conflict = coalesce_xy) # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2\n\n\nTo prepare for merging player data with seasonal data, we identify shared variable names across the objects:\n\nCodedplyr::intersect(\n  names(playerSeasonMerged),\n  names(playerMerged))\n\n [1] \"gsis_id\"                  \"position\"                \n [3] \"position_group\"           \"first_name\"              \n [5] \"last_name\"                \"esb_id\"                  \n [7] \"display_name\"             \"rookie_year\"             \n [9] \"college_conference\"       \"current_team_id\"         \n[11] \"draft_club\"               \"draft_number\"            \n[13] \"draftround\"               \"entry_year\"              \n[15] \"football_name\"            \"gsis_it_id\"              \n[17] \"headshot\"                 \"jersey_number\"           \n[19] \"short_name\"               \"smart_id\"                \n[21] \"status\"                   \"status_description_abbr\" \n[23] \"status_short_description\" \"uniform_number\"          \n[25] \"height\"                   \"weight\"                  \n[27] \"college_name\"             \"birth_date\"              \n[29] \"suffix\"                   \"years_of_experience\"     \n[31] \"pfr_player_name\"          \"team\"                    \n[33] \"age\"                     \n\n\nThen we merge the player data with the seasonal data:\n\nCodeseasonalData &lt;- powerjoin::power_full_join(\n  playerSeasonMerged,\n  playerMerged %&gt;% select(-age, -years_of_experience, -team, -team_abbr, -team_seq, -current_team_id), # drop variables from id objects that change from year to year (and thus are not necessarily accurate for a given season)\n  by = \"gsis_id\",\n  conflict = coalesce_xy # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2\n) %&gt;% \n  filter(!is.na(season)) %&gt;% \n  select(gsis_id, season, player_display_name, position, team, games, everything())\n\n\nTo prepare for merging player and seasonal data with weekly data, we identify shared variable names across the objects:\n\nCodedplyr::intersect(\n  names(playerSeasonWeekMerged),\n  names(seasonalData))\n\n [1] \"gsis_id\"                 \"season\"                 \n [3] \"week\"                    \"team\"                   \n [5] \"jersey_number\"           \"status\"                 \n [7] \"first_name\"              \"last_name\"              \n [9] \"birth_date\"              \"height\"                 \n[11] \"weight\"                  \"college\"                \n[13] \"pfr_id\"                  \"headshot_url\"           \n[15] \"status_description_abbr\" \"football_name\"          \n[17] \"esb_id\"                  \"gsis_it_id\"             \n[19] \"smart_id\"                \"entry_year\"             \n[21] \"rookie_year\"             \"draft_club\"             \n[23] \"draft_number\"            \"position\"               \n\n\nThen we merge the player and seasonal data with the weekly data:\n\nCodeseasonalAndWeeklyData &lt;- powerjoin::power_full_join(\n  playerSeasonWeekMerged,\n  seasonalData,\n  by = c(\"gsis_id\",\"season\"),\n  conflict = coalesce_xy # where the objects have the same variable name (e.g., position), keep the values from object 1, unless it's NA, in which case use the relevant value from object 2\n) %&gt;% \n  filter(!is.na(week)) %&gt;% \n  select(gsis_id, season, week, full_name, position, team, everything())\n\n\n\nCode# Duplicate cases\nseasonalData %&gt;% \n  group_by(gsis_id, season) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\nCodeseasonalAndWeeklyData %&gt;% \n  group_by(gsis_id, season, week) %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  head()\n\n\n  \n\n\n\n\n19.4.3 Additional Processing\nFor purposes of machine learning, we set all character and logical columns to factors.\n\nCode# Convert character and logical variables to factors\nseasonalData &lt;- seasonalData %&gt;% \n  mutate(\n    across(\n      where(is.character),\n      as.factor\n    ),\n    across(\n      where(is.logical),\n      as.factor\n    )\n  )\n\n\n\n19.4.4 Fill in Missing Data for Static Variables\nFor variables that are not expected to change, such as a player’s name and position, we fill in missing values by using a player’s value on those variables from other rows in the data.\n\nCodeseasonalData &lt;- seasonalData %&gt;% \n  arrange(gsis_id, season) %&gt;% \n  group_by(gsis_id) %&gt;% \n  fill(\n    player_name, player_display_name, pos, position, position_group,\n    .direction = \"downup\") %&gt;% \n  ungroup()\n\n\n\n19.4.5 Create New Data Object for Merging with Later Predictions\nWe create a new data object that contains the latest seasonal data, for merging with later predictions.\n\nCodenewData_seasonal &lt;- seasonalData %&gt;% \n  filter(season == max(season, na.rm = TRUE))\n\n\n\n19.4.6 Lag Fantasy Points\nTo develop a machine learning model that uses a player’s performance metrics in a given season for predicting the player’s fantasy points in the subsequent season, we need to include the player’s fantasy points from the subsequent season in the same row as the previous season’s performance metrics. Thus, we need to create a lagged variable for fantasy points. That way, 2024 fantasy points are in the same row as 2023 performance metrics, 2023 fantasy points are in the same row as 2023 performance metrics, and so on. We call this the lagged fantasy points variable (fantasyPoints_lag). We also retain the original same-year fantasy points variable (fantasyPoints) so it can be used as predictor of their subsequent-year fantasy points.\n\nCodeseasonalData_lag &lt;- seasonalData %&gt;% \n  arrange(gsis_id, season) %&gt;% \n  group_by(gsis_id) %&gt;% \n  mutate(\n    fantasyPoints_lag = lead(fantasyPoints)\n  ) %&gt;% \n  ungroup()\n\nseasonalData_lag %&gt;% \n  select(gsis_id, player_display_name, season, fantasyPoints, fantasyPoints_lag) # verify that lagging worked as expected\n\n\n  \n\n\n\n\n19.4.7 Subset to Predictor Variables and Outcome Variable\nThen, we drop variables that we do not want to include in the model as our predictor or outcome variable. Thus, all of the variables in the object are our predictor and outcome variables.\n\nCodeseasonalData_lag %&gt;% select_if(~class(.) == \"Date\")\n\n\n  \n\n\nCodeseasonalData_lag %&gt;% select_if(is.character)\n\n\n  \n\n\nCodeseasonalData_lag %&gt;% select_if(is.factor)\n\n\n  \n\n\nCodeseasonalData_lag %&gt;% select_if(is.logical)\n\n\n  \n\n\nCodedropVars &lt;- c(\n  \"birth_date\", \"loaded\", \"full_name\", \"player_name\", \"player_display_name\", \"display_name\", \"suffix\", \"headshot_url\", \"player\", \"pos\",\n  \"espn_id\", \"sportradar_id\", \"yahoo_id\", \"rotowire_id\", \"pff_id\", \"fantasy_data_id\", \"sleeper_id\", \"pfr_id\",\n  \"pfr_player_id\", \"cfb_player_id\", \"pfr_player_name\", \"esb_id\", \"gsis_it_id\", \"smart_id\",\n  \"college\", \"college_name\", \"team_abbr\", \"current_team_id\", \"college_conference\", \"draft_club\", \"status_description_abbr\",\n  \"status_short_description\", \"short_name\", \"headshot\", \"uniform_number\", \"jersey_number\", \"first_name\", \"last_name\",\n  \"football_name\", \"team\")\n\nseasonalData_lag_subset &lt;- seasonalData_lag %&gt;% \n  dplyr::select(-any_of(dropVars))\n\n\n\n19.4.8 Separate by Position\nThen, we separate the objects by position, so we can develop different machine learning models for each position.\n\nCodeseasonalData_lag_subsetQB &lt;- seasonalData_lag_subset %&gt;% \n  filter(position == \"QB\") %&gt;% \n  select(\n    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,\n    height, weight, rookie_year, draft_number,\n    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,\n    completions:rushing_2pt_conversions, special_teams_tds, contains(\".pass\"), contains(\".rush\"))\n\nseasonalData_lag_subsetRB &lt;- seasonalData_lag_subset %&gt;% \n  filter(position == \"RB\") %&gt;% \n  select(\n    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,\n    height, weight, rookie_year, draft_number,\n    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,\n    carries:special_teams_tds, contains(\".rush\"), contains(\".rec\"))\n\nseasonalData_lag_subsetWR &lt;- seasonalData_lag_subset %&gt;% \n  filter(position == \"WR\") %&gt;% \n  select(\n    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,\n    height, weight, rookie_year, draft_number,\n    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,\n    carries:special_teams_tds, contains(\".rush\"), contains(\".rec\"))\n\nseasonalData_lag_subsetTE &lt;- seasonalData_lag_subset %&gt;% \n  filter(position == \"TE\") %&gt;% \n  select(\n    gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic,\n    height, weight, rookie_year, draft_number,\n    fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag,\n    carries:special_teams_tds, contains(\".rush\"), contains(\".rec\"))\n\n\n\n19.4.9 Split into Test and Training Data\nBecause machine learning can leverage many predictors, it is at high risk of overfitting—explaining error variance that would not generalize to new data, such as data for new players or future seasons. Thus, it is important to develop and tune the machine learning model so as not to overfit the model. In machine learning, it is common to use cross-validation where we train the model on a subset of the observations, and we evaluate how well the model generalizes to unseen (e.g., “hold-out”) observations. Then, we select the model parameters by how well the model generalizes to the hold-out data, so we are selecting a model that maximizes accuracy and generalizability (i.e., parsimony).\nFor internal cross-validation, it is common to divide the data into three subsets:\n\ntraining data\nvalidation data\ntest data\n\nThe training set is used to fit the model. It is usually the largest portion of the data. We fit various models to the training set based on which parameters we want to evaluate (e.g., how many trees to use in a tree-boosting model).\nThe models fit with the training set are then evaluated using the unseen observations in the validation set. The validation set is used to tune the model parameters and prevent overfitting. We select the model parameters that yield the greatest accuracy in the validation set. In k-fold cross-validation, the validation set rotates across folds, thus replacing the need for a separate validation set.\nThe test set is used after model training and tuning to evaluate the model’s generalizability to unseen data.\nBelow, we split the data into test and training data. Our ultimate goal is to predict next year’s fantasy points. However, to do that effectively, we must first develop a model for which we can evaluate its accuracy against historical fantasy points (because we do not yet know players will score in the future). We want to include all current/active players in our training data, so that our predictions of their future performance can be accounted for by including their prior data in the model. Thus, we use retired players as our hold-out (test) data. We split our data into 80% training data and 20% testing data. The 20% testing data thus includes all retired players, but not all retired players are in the testing data.\nThen, for the analysis, we can either a) use rotating folds (as the case for k-fold and leave-one-out [LOO] cross-validation) for which a separate validation set (from the training set) is not needed, as we do in Section 19.6, or we can b) subdivide the training set into an inner training set and validation set, as we do in Section 19.8.5.4.\n\nCodeseasonalData_lag_qb_all &lt;- seasonalData_lag_subsetQB\nseasonalData_lag_rb_all &lt;- seasonalData_lag_subsetRB\nseasonalData_lag_wr_all &lt;- seasonalData_lag_subsetWR\nseasonalData_lag_te_all &lt;- seasonalData_lag_subsetTE\n\nset.seed(52242) # for reproducibility (to keep the same train/holdout players)\n\nactiveQBs &lt;- unique(seasonalData_lag_qb_all$gsis_id[which(seasonalData_lag_qb_all$season == max(seasonalData_lag_qb_all$season, na.rm = TRUE))])\nretiredQBs &lt;- unique(seasonalData_lag_qb_all$gsis_id[which(seasonalData_lag_qb_all$gsis_id %ni% activeQBs)])\nnumQBs &lt;- length(unique(seasonalData_lag_qb_all$gsis_id))\nqbHoldoutIDs &lt;- sample(retiredQBs, size = ceiling(.2 * numQBs)) # holdout 20% of players\n\nactiveRBs &lt;- unique(seasonalData_lag_rb_all$gsis_id[which(seasonalData_lag_rb_all$season == max(seasonalData_lag_rb_all$season, na.rm = TRUE))])\nretiredRBs &lt;- unique(seasonalData_lag_rb_all$gsis_id[which(seasonalData_lag_rb_all$gsis_id %ni% activeRBs)])\nnumRBs &lt;- length(unique(seasonalData_lag_rb_all$gsis_id))\nrbHoldoutIDs &lt;- sample(retiredRBs, size = ceiling(.2 * numRBs)) # holdout 20% of players\n\nset.seed(52242) # for reproducibility (to keep the same train/holdout players); added here to prevent a downstream error with predict.missRanger() due to missingness; this suggests that an error can arise from including a player in the holdout sample who has missingness in particular variables; would be good to identify which player(s) in the holdout sample evoke that error to identify the kinds of missingness that yield the error\n\nactiveWRs &lt;- unique(seasonalData_lag_wr_all$gsis_id[which(seasonalData_lag_wr_all$season == max(seasonalData_lag_wr_all$season, na.rm = TRUE))])\nretiredWRs &lt;- unique(seasonalData_lag_wr_all$gsis_id[which(seasonalData_lag_wr_all$gsis_id %ni% activeWRs)])\nnumWRs &lt;- length(unique(seasonalData_lag_wr_all$gsis_id))\nwrHoldoutIDs &lt;- sample(retiredWRs, size = ceiling(.2 * numWRs)) # holdout 20% of players\n\nactiveTEs &lt;- unique(seasonalData_lag_te_all$gsis_id[which(seasonalData_lag_te_all$season == max(seasonalData_lag_te_all$season, na.rm = TRUE))])\nretiredTEs &lt;- unique(seasonalData_lag_te_all$gsis_id[which(seasonalData_lag_te_all$gsis_id %ni% activeTEs)])\nnumTEs &lt;- length(unique(seasonalData_lag_te_all$gsis_id))\nteHoldoutIDs &lt;- sample(retiredTEs, size = ceiling(.2 * numTEs)) # holdout 20% of players\n  \nseasonalData_lag_qb_train &lt;- seasonalData_lag_qb_all %&gt;% \n  filter(gsis_id %ni% qbHoldoutIDs)\nseasonalData_lag_qb_test &lt;- seasonalData_lag_qb_all %&gt;% \n  filter(gsis_id %in% qbHoldoutIDs)\n\nseasonalData_lag_rb_train &lt;- seasonalData_lag_rb_all %&gt;% \n  filter(gsis_id %ni% rbHoldoutIDs)\nseasonalData_lag_rb_test &lt;- seasonalData_lag_rb_all %&gt;% \n  filter(gsis_id %in% rbHoldoutIDs)\n\nseasonalData_lag_wr_train &lt;- seasonalData_lag_wr_all %&gt;% \n  filter(gsis_id %ni% wrHoldoutIDs)\nseasonalData_lag_wr_test &lt;- seasonalData_lag_wr_all %&gt;% \n  filter(gsis_id %in% wrHoldoutIDs)\n\nseasonalData_lag_te_train &lt;- seasonalData_lag_te_all %&gt;% \n  filter(gsis_id %ni% teHoldoutIDs)\nseasonalData_lag_te_test &lt;- seasonalData_lag_te_all %&gt;% \n  filter(gsis_id %in% teHoldoutIDs)\n\n\n\n19.4.10 Impute the Missing Data\nMany of the machine learning approaches described in this chapter require no missing observations in order for a case to be included in the analysis. In this section, we demonstrate one approach to imputing missing data. Here is a vignette demonstrating how to impute missing data using missForest(): https://rpubs.com/lmorgan95/MissForest (archived at: https://perma.cc/6GB4-2E22). Below, we impute the training data (and all data) separately by position. We then use the imputed training data to make out-of-sample predictions to fill in the missing data for the testing data. We do not want to impute the training and testing data together so that we can keep them separate for the purposes of cross-validation. However, we impute all data (training and test data together) for purposes of making out-of-sample predictions from the machine learning models to predict players’ performance next season (when actuals are not yet available for evaluating their accuracy). To impute data, we use the missRanger package (Mayer, 2024).\n\n\n\n\n\n\nNote 19.1: Impute missing data for machine learning\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCode# QBs\nseasonalData_lag_qb_all_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_qb_all,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\n\nVariables to impute:        fantasy_points, fantasy_points_ppr, special_teams_tds, passing_epa, pacr, rushing_epa, fantasyPoints_lag, passing_cpoe, rookie_year, draft_number, gs, pass_attempts.pass, throwaways.pass, spikes.pass, drops.pass, bad_throws.pass, times_blitzed.pass, times_hurried.pass, times_hit.pass, times_pressured.pass, batted_balls.pass, on_tgt_throws.pass, rpo_plays.pass, rpo_yards.pass, rpo_pass_att.pass, rpo_pass_yards.pass, rpo_rush_att.pass, rpo_rush_yards.pass, pa_pass_att.pass, pa_pass_yards.pass, att.rush, yds.rush, td.rush, x1d.rush, ybc.rush, yac.rush, brk_tkl.rush, att_br.rush, drop_pct.pass, bad_throw_pct.pass, on_tgt_pct.pass, pressure_pct.pass, ybc_att.rush, yac_att.rush, pocket_time.pass\nVariables used to impute:   gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic, height, weight, rookie_year, draft_number, fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag, completions, attempts, passing_yards, passing_tds, passing_interceptions, sacks_suffered, sack_yards_lost, sack_fumbles, sack_fumbles_lost, passing_air_yards, passing_yards_after_catch, passing_first_downs, passing_epa, passing_cpoe, passing_2pt_conversions, pacr, carries, rushing_yards, rushing_tds, rushing_fumbles, rushing_fumbles_lost, rushing_first_downs, rushing_epa, rushing_2pt_conversions, special_teams_tds, pocket_time.pass, pass_attempts.pass, throwaways.pass, spikes.pass, drops.pass, bad_throws.pass, times_blitzed.pass, times_hurried.pass, times_hit.pass, times_pressured.pass, batted_balls.pass, on_tgt_throws.pass, rpo_plays.pass, rpo_yards.pass, rpo_pass_att.pass, rpo_pass_yards.pass, rpo_rush_att.pass, rpo_rush_yards.pass, pa_pass_att.pass, pa_pass_yards.pass, drop_pct.pass, bad_throw_pct.pass, on_tgt_pct.pass, pressure_pct.pass, ybc_att.rush, yac_att.rush, att.rush, yds.rush, td.rush, x1d.rush, ybc.rush, yac.rush, brk_tkl.rush, att_br.rush\n\n    fntsy_  fnts__  spcl__  pssng_p pacr    rshng_  fntsP_  pssng_c rok_yr  drft_n  gs  pss_t.  thrww.  spks.p  drps.p  bd_th.  tms_b.  tms_hr. tms_ht. tms_p.  bttd_.  on_tgt_t.   rp_pl.  rp_yr.  rp_pss_t.   rp_pss_y.   rp_rsh_t.   rp_rsh_y.   p_pss_t.    p_pss_y.    att.rs  yds.rs  td.rsh  x1d.rs  ybc.rs  yc.rsh  brk_t.  att_b.  drp_p.  bd_t_.  on_tgt_p.   prss_.  ybc_t.  yc_tt.  pckt_.\niter 1: 0.0054  0.0024  0.7924  0.1919  0.7612  0.3628  0.4789  0.4133  0.0224  0.5216  0.0271  0.0134  0.3024  0.7659  0.1304  0.0541  0.0758  0.1759  0.1820  0.0370  0.3238  0.0291  0.2952  0.1812  0.0885  0.0867  0.2627  0.2563  0.1093  0.0902  0.0580  0.0645  0.1732  0.0524  0.0578  0.1795  0.3524  0.3428  0.7447  0.5158  0.0824  0.6803  0.3529  0.5758  0.8111  \niter 2: 0.0044  0.0048  0.8304  0.2002  0.7926  0.3736  0.4801  0.4289  0.0488  0.6139  0.0188  0.0090  0.2883  0.7481  0.0764  0.0385  0.0718  0.1231  0.1329  0.0337  0.2760  0.0113  0.0548  0.0814  0.0765  0.0990  0.1989  0.2841  0.0707  0.0952  0.0396  0.0386  0.1606  0.0492  0.0525  0.1220  0.2541  0.3556  0.7468  0.4937  0.0827  0.6610  0.3465  0.5796  0.8134  \niter 3: 0.0049  0.0046  0.8690  0.1986  0.7810  0.3641  0.4774  0.4360  0.0528  0.6123  0.0188  0.0088  0.2867  0.7538  0.0767  0.0393  0.0734  0.1261  0.1374  0.0343  0.2741  0.0119  0.0524  0.0816  0.0748  0.1008  0.2184  0.2811  0.0691  0.0926  0.0389  0.0413  0.1640  0.0511  0.0585  0.1255  0.2510  0.3609  0.7477  0.5108  0.0858  0.6426  0.3588  0.5734  0.8300  \n\nCodeseasonalData_lag_qb_all_imp\n\nmissRanger object. Extract imputed data via $data\n- best iteration: 2 \n- best average OOB imputation error: 0.2524825 \n\nCodedata_all_qb &lt;- seasonalData_lag_qb_all_imp$data\ndata_all_qb$fantasyPointsMC_lag &lt;- scale(data_all_qb$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_all_qb_matrix &lt;- data_all_qb %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\nnewData_qb &lt;- data_all_qb %&gt;% \n  filter(season == max(season, na.rm = TRUE)) %&gt;% \n  select(-fantasyPoints_lag, -fantasyPointsMC_lag)\nnewData_qb_matrix &lt;- data_all_qb_matrix[\n  data_all_qb_matrix[, \"season\"] == max(data_all_qb_matrix[, \"season\"], na.rm = TRUE), # keep only rows with the most recent season\n  , # all columns\n  drop = FALSE]\n\ndropCol_qb &lt;- which(colnames(newData_qb_matrix) %in% c(\"fantasyPoints_lag\",\"fantasyPointsMC_lag\"))\nnewData_qb_matrix &lt;- newData_qb_matrix[, -dropCol_qb, drop = FALSE]\n\nseasonalData_lag_qb_train_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_qb_train,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\n\nVariables to impute:        fantasy_points, fantasy_points_ppr, special_teams_tds, passing_epa, pacr, rushing_epa, fantasyPoints_lag, passing_cpoe, rookie_year, draft_number, gs, pass_attempts.pass, throwaways.pass, spikes.pass, drops.pass, bad_throws.pass, times_blitzed.pass, times_hurried.pass, times_hit.pass, times_pressured.pass, batted_balls.pass, on_tgt_throws.pass, rpo_plays.pass, rpo_yards.pass, rpo_pass_att.pass, rpo_pass_yards.pass, rpo_rush_att.pass, rpo_rush_yards.pass, pa_pass_att.pass, pa_pass_yards.pass, att.rush, yds.rush, td.rush, x1d.rush, ybc.rush, yac.rush, brk_tkl.rush, att_br.rush, drop_pct.pass, bad_throw_pct.pass, on_tgt_pct.pass, pressure_pct.pass, ybc_att.rush, yac_att.rush, pocket_time.pass\nVariables used to impute:   gsis_id, season, games, gs, years_of_experience, age, ageCentered20, ageCentered20Quadratic, height, weight, rookie_year, draft_number, fantasy_points, fantasy_points_ppr, fantasyPoints, fantasyPoints_lag, completions, attempts, passing_yards, passing_tds, passing_interceptions, sacks_suffered, sack_yards_lost, sack_fumbles, sack_fumbles_lost, passing_air_yards, passing_yards_after_catch, passing_first_downs, passing_epa, passing_cpoe, passing_2pt_conversions, pacr, carries, rushing_yards, rushing_tds, rushing_fumbles, rushing_fumbles_lost, rushing_first_downs, rushing_epa, rushing_2pt_conversions, special_teams_tds, pocket_time.pass, pass_attempts.pass, throwaways.pass, spikes.pass, drops.pass, bad_throws.pass, times_blitzed.pass, times_hurried.pass, times_hit.pass, times_pressured.pass, batted_balls.pass, on_tgt_throws.pass, rpo_plays.pass, rpo_yards.pass, rpo_pass_att.pass, rpo_pass_yards.pass, rpo_rush_att.pass, rpo_rush_yards.pass, pa_pass_att.pass, pa_pass_yards.pass, drop_pct.pass, bad_throw_pct.pass, on_tgt_pct.pass, pressure_pct.pass, ybc_att.rush, yac_att.rush, att.rush, yds.rush, td.rush, x1d.rush, ybc.rush, yac.rush, brk_tkl.rush, att_br.rush\n\n    fntsy_  fnts__  spcl__  pssng_p pacr    rshng_  fntsP_  pssng_c rok_yr  drft_n  gs  pss_t.  thrww.  spks.p  drps.p  bd_th.  tms_b.  tms_hr. tms_ht. tms_p.  bttd_.  on_tgt_t.   rp_pl.  rp_yr.  rp_pss_t.   rp_pss_y.   rp_rsh_t.   rp_rsh_y.   p_pss_t.    p_pss_y.    att.rs  yds.rs  td.rsh  x1d.rs  ybc.rs  yc.rsh  brk_t.  att_b.  drp_p.  bd_t_.  on_tgt_p.   prss_.  ybc_t.  yc_tt.  pckt_.\niter 1: 0.0061  0.0028  0.8162  0.1897  0.5083  0.3633  0.4726  0.4456  0.0242  0.4723  0.0283  0.0141  0.2939  0.7728  0.1343  0.0558  0.0744  0.1757  0.1818  0.0381  0.3288  0.0351  0.2921  0.1846  0.0860  0.0894  0.2737  0.2661  0.1127  0.0900  0.0586  0.0644  0.1800  0.0574  0.0639  0.1792  0.3570  0.3486  0.7646  0.5313  0.0868  0.7084  0.3533  0.5933  0.8466  \niter 2: 0.0052  0.0052  0.8304  0.1937  0.5621  0.3715  0.4614  0.4586  0.0505  0.5647  0.0192  0.0092  0.2953  0.7530  0.0800  0.0393  0.0725  0.1170  0.1355  0.0343  0.2771  0.0121  0.0555  0.0731  0.0713  0.0979  0.2073  0.2943  0.0698  0.0911  0.0416  0.0399  0.1683  0.0527  0.0577  0.1262  0.2474  0.3582  0.7719  0.5165  0.0900  0.6862  0.3642  0.5926  0.8400  \niter 3: 0.0053  0.0051  0.8261  0.2008  0.5551  0.3571  0.4727  0.4410  0.0551  0.5658  0.0188  0.0092  0.2859  0.7460  0.0807  0.0402  0.0739  0.1202  0.1393  0.0351  0.2808  0.0114  0.0595  0.0705  0.0775  0.1051  0.2163  0.2935  0.0718  0.0921  0.0426  0.0400  0.1719  0.0535  0.0534  0.1225  0.2498  0.3484  0.7502  0.5100  0.0884  0.6609  0.3672  0.5852  0.8440  \niter 4: 0.0054  0.0051  0.6928  0.1979  0.5598  0.3732  0.4771  0.4349  0.0506  0.5691  0.0189  0.0085  0.2891  0.7456  0.0785  0.0395  0.0737  0.1210  0.1353  0.0335  0.2836  0.0117  0.0566  0.0778  0.0743  0.1055  0.2131  0.2964  0.0697  0.0912  0.0396  0.0395  0.1611  0.0531  0.0597  0.1258  0.2600  0.3560  0.8062  0.5032  0.0973  0.6739  0.3698  0.5875  0.8485  \niter 5: 0.0052  0.0055  0.8355  0.1965  0.5664  0.3710  0.4743  0.4604  0.0520  0.5598  0.0193  0.0091  0.2852  0.7474  0.0800  0.0405  0.0722  0.1213  0.1366  0.0344  0.2788  0.0118  0.0555  0.0756  0.0746  0.0986  0.2190  0.2765  0.0695  0.0932  0.0390  0.0425  0.1650  0.0509  0.0576  0.1305  0.2556  0.3509  0.7738  0.5051  0.0969  0.6902  0.3640  0.6007  0.8326  \n\nCodeseasonalData_lag_qb_train_imp\n\nmissRanger object. Extract imputed data via $data\n- best iteration: 4 \n- best average OOB imputation error: 0.2482278 \n\nCodedata_train_qb &lt;- seasonalData_lag_qb_train_imp$data\ndata_train_qb$fantasyPointsMC_lag &lt;- scale(data_train_qb$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_train_qb_matrix &lt;- data_train_qb %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\n\nseasonalData_lag_qb_test_imp &lt;- predict(\n  object = seasonalData_lag_qb_train_imp,\n  newdata = seasonalData_lag_qb_test,\n  seed = 52242)\n\ndata_test_qb &lt;- seasonalData_lag_qb_test_imp\ndata_test_qb_matrix &lt;- data_test_qb %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\n\n\n\nCode# RBs\nseasonalData_lag_rb_all_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_rb_all,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\nseasonalData_lag_rb_all_imp\n\ndata_all_rb &lt;- seasonalData_lag_rb_all_imp$data\ndata_all_rb$fantasyPointsMC_lag &lt;- scale(data_all_rb$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_all_rb_matrix &lt;- data_all_rb %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\nnewData_rb &lt;- data_all_rb %&gt;% \n  filter(season == max(season, na.rm = TRUE)) %&gt;% \n  select(-fantasyPoints_lag, -fantasyPointsMC_lag)\nnewData_rb_matrix &lt;- data_all_rb_matrix[\n  data_all_rb_matrix[, \"season\"] == max(data_all_rb_matrix[, \"season\"], na.rm = TRUE), # keep only rows with the most recent season\n  , # all columns\n  drop = FALSE]\n\ndropCol_rb &lt;- which(colnames(newData_rb_matrix) %in% c(\"fantasyPoints_lag\",\"fantasyPointsMC_lag\"))\nnewData_rb_matrix &lt;- newData_rb_matrix[, -dropCol_rb, drop = FALSE]\n\nseasonalData_lag_rb_train_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_rb_train,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\nseasonalData_lag_rb_train_imp\n\ndata_train_rb &lt;- seasonalData_lag_rb_train_imp$data\ndata_train_rb$fantasyPointsMC_lag &lt;- scale(data_train_rb$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_train_rb_matrix &lt;- data_train_rb %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\n\nseasonalData_lag_rb_test_imp &lt;- predict(\n  object = seasonalData_lag_rb_train_imp,\n  newdata = seasonalData_lag_rb_test,\n  seed = 52242)\n\ndata_test_rb &lt;- seasonalData_lag_rb_test_imp\ndata_test_rb_matrix &lt;- data_test_rb %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\n\n\n\nCode# WRs\nseasonalData_lag_wr_all_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_wr_all,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\nseasonalData_lag_wr_all_imp\n\ndata_all_wr &lt;- seasonalData_lag_wr_all_imp$data\ndata_all_wr$fantasyPointsMC_lag &lt;- scale(data_all_wr$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_all_wr_matrix &lt;- data_all_wr %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\nnewData_wr &lt;- data_all_wr %&gt;% \n  filter(season == max(season, na.rm = TRUE)) %&gt;% \n  select(-fantasyPoints_lag, -fantasyPointsMC_lag)\nnewData_wr_matrix &lt;- data_all_wr_matrix[\n  data_all_wr_matrix[, \"season\"] == max(data_all_wr_matrix[, \"season\"], na.rm = TRUE), # keep only rows with the most recent season\n  , # all columns\n  drop = FALSE]\n\ndropCol_wr &lt;- which(colnames(newData_wr_matrix) %in% c(\"fantasyPoints_lag\",\"fantasyPointsMC_lag\"))\nnewData_wr_matrix &lt;- newData_wr_matrix[, -dropCol_wr, drop = FALSE]\n\nseasonalData_lag_wr_train_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_wr_train,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\nseasonalData_lag_wr_train_imp\n\ndata_train_wr &lt;- seasonalData_lag_wr_train_imp$data\ndata_train_wr$fantasyPointsMC_lag &lt;- scale(data_train_wr$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_train_wr_matrix &lt;- data_train_wr %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\n\nseasonalData_lag_wr_test_imp &lt;- predict(\n  object = seasonalData_lag_wr_train_imp,\n  newdata = seasonalData_lag_wr_test,\n  seed = 52242)\n\ndata_test_wr &lt;- seasonalData_lag_wr_test_imp\ndata_test_wr_matrix &lt;- data_test_wr %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\n\n\n\nCode# TEs\nseasonalData_lag_te_all_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_te_all,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\nseasonalData_lag_te_all_imp\n\ndata_all_te &lt;- seasonalData_lag_te_all_imp$data\ndata_all_te$fantasyPointsMC_lag &lt;- scale(data_all_te$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_all_te_matrix &lt;- data_all_te %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\nnewData_te &lt;- data_all_te %&gt;% \n  filter(season == max(season, na.rm = TRUE)) %&gt;% \n  select(-fantasyPoints_lag, -fantasyPointsMC_lag)\nnewData_te_matrix &lt;- data_all_te_matrix[\n  data_all_te_matrix[, \"season\"] == max(data_all_te_matrix[, \"season\"], na.rm = TRUE), # keep only rows with the most recent season\n  , # all columns\n  drop = FALSE]\n\ndropCol_te &lt;- which(colnames(newData_te_matrix) %in% c(\"fantasyPoints_lag\",\"fantasyPointsMC_lag\"))\nnewData_te_matrix &lt;- newData_te_matrix[, -dropCol_te, drop = FALSE]\n\nseasonalData_lag_te_train_imp &lt;- missRanger::missRanger(\n  seasonalData_lag_te_train,\n  pmm.k = 5,\n  verbose = 2,\n  seed = 52242,\n  keep_forests = TRUE)\n\nseasonalData_lag_te_train_imp\n\ndata_train_te &lt;- seasonalData_lag_te_train_imp$data\ndata_train_te$fantasyPointsMC_lag &lt;- scale(data_train_te$fantasyPoints_lag, scale = FALSE) # mean-centered\ndata_train_te_matrix &lt;- data_train_te %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()\n\nseasonalData_lag_te_test_imp &lt;- predict(\n  object = seasonalData_lag_te_train_imp,\n  newdata = seasonalData_lag_te_test,\n  seed = 52242)\n\ndata_test_te &lt;- seasonalData_lag_te_test_imp\ndata_test_te_matrix &lt;- data_test_te %&gt;%\n  mutate(across(where(is.factor), ~ as.numeric(as.integer(.)))) %&gt;% \n  as.matrix()",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-coresParallel",
    "href": "machine-learning.html#sec-coresParallel",
    "title": "19  Machine Learning",
    "section": "\n19.5 Identify Cores for Parallel Processing",
    "text": "19.5 Identify Cores for Parallel Processing\n\nCodenum_cores &lt;- parallelly::availableCores() - 1\nnum_true_cores &lt;- parallelly::availableCores(logical = FALSE) - 1\n\n\n\nCodenum_cores\n\nsystem \n     4 \n\n\nWe use the future package (Bengtsson, 2025) for parallel (faster) processing.\n\nCodefuture::plan(future::multisession, workers = num_cores)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-folds",
    "href": "machine-learning.html#sec-folds",
    "title": "19  Machine Learning",
    "section": "\n19.6 Set up the Cross-Validation Folds",
    "text": "19.6 Set up the Cross-Validation Folds\nIn the examples below, we predict the future fantasy points of Quarterbacks. However, the examples could be applied to any of the positions. There are various approaches to cross-validation. In the examples below, we use k-fold cross-validation. However, we also provide the code to apply leave-one-out (LOO) cross-validation.\n\n19.6.1 k-Fold Cross-Validation\nk-fold cross-validation partitions the data into k folds (subsets). In each of the k iterations, the model is trained on \\(k - 1\\) folds and is evaluated on the remaining fold. For example, in a 10-fold cross-validation (i.e., \\(k = 10\\)), as used below, the model is trained 10 times, each time leaving out a different 10% of the data for validation. k-fold cross-validation is widely used because it tends to yield stable estimates of model performance, by balancing bias and variance. It is also computationally efficient, requiring only k model fits to evaluate model performance.\nWe set up the k folds using the rsample::group_vfold_cv() function of the rsample package (Frick, Chow, et al., 2025).\n\nCodeset.seed(52242) # for reproducibility\n\nfolds_kFold &lt;- rsample::group_vfold_cv(\n  data_train_qb,\n  group = gsis_id, # ensures all rows for a player are in the training set or all in the validation set for each fold\n  v = 10) # 10-fold cross-validation\n\n\n\n19.6.2 Leave-One-Out (LOO) Cross-Validation\nLeave-one-out (LOO) cross-validation partitions the data into n folds, where n is the sample size. In each of the n iterations, the model is trained on \\(n - 1\\) observations and is evaluated on the one left out. For example, in a LOO cross-validation with 100 players, the model is trained 100 times, each time leaving out a different player for validation. LOO cross-validation is especially useful when the dataset is small—too small to form reliable training sets in k-fold cross-validation (e.g., with \\(k = 5\\) or \\(k = 10\\), which divide the sample into 5 or 10 folds, respectively). However, LOO tends to be less computationally efficient because it requires more model fits than k-fold cross-validation. LOO tends to have low bias, producing performance estimates closer to those obtained when fitting the model to the full dataset, because each model is trained on nearly all the data. However, LOO also tends to have high variance in its error estimates, because each validation fold contains only a single observation, making those estimates more sensitive to individual data points.\nWe set up the LOO folds using the rsample::loo_cv() function of the rsample package (Frick, Chow, et al., 2025).\n\nCodeset.seed(52242) # for reproducibility\n\nfolds_loo &lt;- rsample::loo_cv(data_train_qb)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-fittingModels-regression",
    "href": "machine-learning.html#sec-fittingModels-regression",
    "title": "19  Machine Learning",
    "section": "\n19.7 Fitting the Traditional Linear Regression Models",
    "text": "19.7 Fitting the Traditional Linear Regression Models\nWe describe linear regression in Chapter 11.\n\n19.7.1 Regression with One Predictor\nBelow, we fit a linear regression model with one predictor and evaluate it with cross-validation. We also evaluate its accuracy on the hold-out (test) data. For each of the models, we fit and evaluate the models using the tidymodels ecosystem of packages (Kuhn & Wickham, 2020, 2025). we specify our model formula using the recipes::recipe() function of the recipes package (Kuhn, Wickham, et al., 2025). We define the model using the parsnip::linear_reg(), parsnip::set_engine(), and parsnip::set_mode() functions of the parsnip package (Kuhn & Vaughan, 2025). We specify the workflow using the workflows::workflow(), workflows::add_recipe(), and workflows::add_model() functions of the workflows package (Vaughan & Couch, 2025). We fit the cross-validation model using the tune::fit_resamples() function of the tune package (Kuhn, 2025). We specify the accuracy metrics to evaluate using the yardstick::metric_set() function of the yardstick package (Kuhn, Vaughan, et al., 2025). We fit the final model using the workflows::fit() function of the workflows package (Vaughan & Couch, 2025). We evaluate the accuracy of the model’s predictions on the test data using the petersenlab::accuracyOverall() of the petersenlab package (Petersen, 2025a).\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Set up Cross-Validation\nfolds &lt;- folds_kFold\n\n# Define Recipe (Formula)\nrec &lt;- recipes::recipe(\n  fantasyPoints_lag ~ fantasyPoints,\n  data = data_train_qb)\n\n# Define Model\nlm_spec &lt;- parsnip::linear_reg() %&gt;%\n  parsnip::set_engine(\"lm\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\n# Workflow\nlm_wf &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(lm_spec)\n\n# Fit Model with Cross-Validation\ncv_results &lt;- tune::fit_resamples(\n  lm_wf,\n  resamples = folds,\n  metrics = yardstick::metric_set(rmse, mae, rsq),\n  control = tune::control_resamples(save_pred = TRUE)\n)\n\n# View Cross-Validation metrics\ntune::collect_metrics(cv_results)\n\n\n  \n\n\nCode# Fit Final Model on Training Data\nfinal_model &lt;- workflows::fit(\n  lm_wf,\n  data = data_train_qb)\n\n# View Coefficients\nfinal_model %&gt;% \n  workflows::extract_fit_parsnip() %&gt;% \n  broom::tidy()\n\n\n  \n\n\nCodefinal_model %&gt;%\n  workflows::extract_fit_parsnip() %&gt;% \n  effectsize::standardize_parameters()\n\n\n  \n\n\nCode# Predict on Test Data\ndf &lt;- data_test_qb %&gt;%\n  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)\n\n# Evaluate Accuracy of Predictions\npetersenlab::accuracyOverall(\n  predicted = df$pred,\n  actual = df$fantasyPoints_lag,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThere was modest shrinkage from the training model to the test model: the \\(R^2\\) for the model on the training data was 0.46; the \\(R^2\\) for the same model applied to the test data was 0.44.\nFigure 19.1 depicts the predicted versus actual fantasy points for the model on the test data.\n\nCode# Calculate combined range for axes\naxis_limits &lt;- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)\n\nggplot(\n  df,\n  aes(\n    x = pred,\n    y = fantasyPoints_lag)) +\n  geom_point(\n    size = 2,\n    alpha = 0.6) +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"blue\",\n    linetype = \"dashed\") +\n  coord_equal(\n    xlim = axis_limits,\n    ylim = axis_limits) +\n  labs(\n    title = \"Predicted vs Actual Fantasy Points (Test Data)\",\n    x = \"Predicted Fantasy Points\",\n    y = \"Actual Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 19.1: Predicted Versus Actual Fantasy Points for Regression Model with One Predictor (Player Age).\n\n\n\n\nBelow are the model predictions for next year’s fantasy points:\n\nCodenewData_qb %&gt;%\n  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %&gt;% \n  left_join(\n    .,\n    nfl_playerIDs %&gt;% select(gsis_id, name),\n    by = \"gsis_id\"\n  ) %&gt;% \n  select(name, fantasyPoints_lag) %&gt;% \n  arrange(-fantasyPoints_lag)\n\n\n  \n\n\n\n\n19.7.2 Regression with Multiple Predictors\nBelow, we fit a linear regression model with multiple predictors and evaluate it with cross-validation. We also evaluate its accuracy on the hold-out (test) data.\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Set up Cross-Validation\nfolds &lt;- folds_kFold\n\n# Define Recipe (Formula)\nrec &lt;- recipes::recipe(\n  fantasyPoints_lag ~ ., # use all predictors\n  data = data_train_qb %&gt;% select(-gsis_id, -fantasyPointsMC_lag))\n\n# Define Model\nlm_spec &lt;- parsnip::linear_reg() %&gt;%\n  parsnip::set_engine(\"lm\") %&gt;%\n  parsnip::set_mode(\"regression\")\n\n# Workflow\nlm_wf &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(lm_spec)\n\n# Fit Model with Cross-Validation\ncv_results &lt;- tune::fit_resamples(\n  lm_wf,\n  resamples = folds,\n  metrics = yardstick::metric_set(rmse, mae, rsq),\n  control = tune::control_resamples(save_pred = TRUE)\n)\n\n# View Cross-Validation metrics\ntune::collect_metrics(cv_results)\n\n\n  \n\n\nCode# Fit Final Model on Training Data\nfinal_model &lt;- workflows::fit(\n  lm_wf,\n  data = data_train_qb)\n\n# View Coefficients\nfinal_model %&gt;% \n  workflows::extract_fit_parsnip() %&gt;% \n  broom::tidy()\n\n\n  \n\n\nCodefinal_model %&gt;%\n  workflows::extract_fit_parsnip() %&gt;% \n  effectsize::standardize_parameters()\n\n\n  \n\n\nCode# Predict on Test Data\ndf &lt;- data_test_qb %&gt;%\n  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)\n\n# Evaluate Accuracy of Predictions\npetersenlab::accuracyOverall(\n  predicted = df$pred,\n  actual = df$fantasyPoints_lag,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThere was modest shrinkage from the training model to the test model: the \\(R^2\\) for the model on the training data was 0.43; the \\(R^2\\) for the same model applied to the test data was 0.38.\nFigure 19.2 depicts the predicted versus actual fantasy points for the model on the test data.\n\nCode# Calculate combined range for axes\naxis_limits &lt;- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)\n\nggplot(\n  df,\n  aes(\n    x = pred,\n    y = fantasyPoints_lag)) +\n  geom_point(\n    size = 2,\n    alpha = 0.6) +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"blue\",\n    linetype = \"dashed\") +\n  coord_equal(\n    xlim = axis_limits,\n    ylim = axis_limits) +\n  labs(\n    title = \"Predicted vs Actual Fantasy Points (Test Data)\",\n    x = \"Predicted Fantasy Points\",\n    y = \"Actual Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 19.2: Predicted Versus Actual Fantasy Points for Regression Model with Multiple Predictors.\n\n\n\n\nBelow are the model predictions for next year’s fantasy points:\n\nCodenewData_qb %&gt;%\n  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %&gt;% \n  left_join(\n    .,\n    nfl_playerIDs %&gt;% select(gsis_id, name),\n    by = \"gsis_id\"\n  ) %&gt;% \n  select(name, fantasyPoints_lag) %&gt;% \n  arrange(-fantasyPoints_lag)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-fittingModels-machineLearning",
    "href": "machine-learning.html#sec-fittingModels-machineLearning",
    "title": "19  Machine Learning",
    "section": "\n19.8 Fitting the Machine Learning Models",
    "text": "19.8 Fitting the Machine Learning Models\n\n19.8.1 Least Absolute Shrinkage and Selection Option (LASSO)\nBelow, we fit a LASSO model. We evaluate it and tune its penalty parameter with cross-validation. The penalty parameter in a LASSO model controls the strength of regularization applied to the model coefficients. Smaller penalty values result in less regularization, allowing the model to retain more predictors with larger (nonzero) coefficients. This typically reduces bias but increases variance of the model’s predictions, as the model may overfit to the training data by including irrelevant or weak predictors. Larger penalty values apply stronger regularization, shrinking more coefficients exactly to zero. This encourages a sparser model that may increase bias (by excluding useful predictors) but reduces variance and improves generalizability by simplifying the model and reducing overfitting.\nAfter tuning the model, we evaluate its accuracy on the hold-out (test) data.\nThe LASSO models were fit using the glmnet package (Friedman et al., 2010, 2025; Tay et al., 2023).\nFor the machine learning models, we perform the parameter tuning using the tune::tune(), tune::tune_grid(), tune::select_best(), and tune::finalize_workflow() functions of the tune package (Kuhn, 2025). We specify the grid of possible parameter values using the dials::grid_regular() function of the dials package (Kuhn & Frick, 2025).\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Set up Cross-Validation\nfolds &lt;- folds_kFold\n\n# Define Recipe (Formula)\nrec &lt;- recipes::recipe(\n  fantasyPoints_lag ~ ., # use all predictors\n  data = data_train_qb %&gt;% select(-gsis_id, -fantasyPointsMC_lag))\n\n# Define Model\nlasso_spec &lt;- \n  parsnip::linear_reg(\n    penalty = tune::tune(),\n    mixture = 1) %&gt;%\n  parsnip::set_engine(\"glmnet\")\n\n# Workflow\nlasso_wf &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(lasso_spec)\n\n# Define grid of penalties to try (log scale is typical)\npenalty_grid &lt;- dials::grid_regular(\n  dials::penalty(range = c(-4, -1)),\n  levels = 20)\n\n# Tune the Penalty Parameter\ncv_results &lt;- tune::tune_grid(\n  lasso_wf,\n  resamples = folds,\n  grid = penalty_grid,\n  metrics = yardstick::metric_set(rmse, mae, rsq),\n  control = tune::control_grid(save_pred = TRUE)\n)\n\n# View Cross-Validation metrics\ntune::collect_metrics(cv_results)\n\n\n  \n\n\nCode# Identify best penalty\ntune::select_best(cv_results, metric = \"rmse\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"mae\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"rsq\")\n\n\n  \n\n\nCodebest_penalty &lt;- tune::select_best(cv_results, metric = \"mae\")\n\n# Finalize Workflow with Best Penalty\nfinal_wf &lt;- tune::finalize_workflow(\n  lasso_wf,\n  best_penalty)\n\n# Fit Final Model on Training Data\nfinal_model &lt;- workflows::fit(\n  final_wf,\n  data = data_train_qb)\n\n# View Coefficients\nfinal_model %&gt;% \n  workflows::extract_fit_parsnip() %&gt;% \n  broom::tidy()\n\n\n  \n\n\nCode# Predict on Test Data\ndf &lt;- data_test_qb %&gt;%\n  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)\n\n# Evaluate Accuracy of Predictions\npetersenlab::accuracyOverall(\n  predicted = df$pred,\n  actual = df$fantasyPoints_lag,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThere was modest shrinkage from the training model to the test model: the \\(R^2\\) for the model on the training data was 0.46; the \\(R^2\\) for the same model applied to the test data was 0.4.\nFigure 19.3 depicts the predicted versus actual fantasy points for the model on the test data.\n\nCode# Calculate combined range for axes\naxis_limits &lt;- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)\n\nggplot(\n  df,\n  aes(\n    x = pred,\n    y = fantasyPoints_lag)) +\n  geom_point(\n    size = 2,\n    alpha = 0.6) +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"blue\",\n    linetype = \"dashed\") +\n  coord_equal(\n    xlim = axis_limits,\n    ylim = axis_limits) +\n  labs(\n    title = \"Predicted vs Actual Fantasy Points (Test Data)\",\n    x = \"Predicted Fantasy Points\",\n    y = \"Actual Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 19.3: Predicted Versus Actual Fantasy Points for Least Absolute Shrinkage and Selection Option (Lasso) Model.\n\n\n\n\nBelow are the model predictions for next year’s fantasy points:\n\nCodenewData_qb %&gt;%\n  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %&gt;% \n  left_join(\n    .,\n    nfl_playerIDs %&gt;% select(gsis_id, name),\n    by = \"gsis_id\"\n  ) %&gt;% \n  select(name, fantasyPoints_lag) %&gt;% \n  arrange(-fantasyPoints_lag)\n\n\n  \n\n\n\n\n19.8.2 Ridge Regression\nBelow, we fit a ridge regression model. We evaluate it and tune its penalty parameter with cross-validation. The penalty parameter in a ridge regression model controls the amount of regularization applied to the model’s coefficients. Smaller penalty values allow coefficients to remain large, resulting in a model that closely fits the training data. This may reduce bias but increases the risk of overfitting, especially in the presence of multicollinearity or many weak predictors. Larger penalty values shrink coefficients toward zero (though not exactly to zero), which reduces model complexity. This typically increases bias slightly but reduces variance of the model’s predictions, making the model more stable and better suited for generalization to new data.\nAfter tuning the model, we also evaluate its accuracy on the hold-out (test) data.\nThe ridge regression models were fit using the glmnet package (Friedman et al., 2010, 2025; Tay et al., 2023).\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Set up Cross-Validation\nfolds &lt;- folds_kFold\n\n# Define Recipe (Formula)\nrec &lt;- recipes::recipe(\n  fantasyPoints_lag ~ ., # use all predictors\n  data = data_train_qb %&gt;% select(-gsis_id, -fantasyPointsMC_lag))\n\n# Define Model\nridge_spec &lt;- \n  parsnip::linear_reg(\n    penalty = tune::tune(),\n    mixture = 0) %&gt;%\n  parsnip::set_engine(\"glmnet\")\n\n# Workflow\nridge_wf &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(ridge_spec)\n\n# Define grid of penalties to try (log scale is typical)\npenalty_grid &lt;- dials::grid_regular(\n  dials::penalty(range = c(-4, -1)),\n  levels = 20)\n\n# Tune the Penalty Parameter\ncv_results &lt;- tune::tune_grid(\n  ridge_wf,\n  resamples = folds,\n  grid = penalty_grid,\n  metrics = yardstick::metric_set(rmse, mae, rsq),\n  control = tune::control_grid(save_pred = TRUE)\n)\n\n# View Cross-Validation metrics\ntune::collect_metrics(cv_results)\n\n\n  \n\n\nCode# Identify best penalty\ntune::select_best(cv_results, metric = \"rmse\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"mae\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"rsq\")\n\n\n  \n\n\nCodebest_penalty &lt;- tune::select_best(cv_results, metric = \"mae\")\n\n# Finalize Workflow with Best Penalty\nfinal_wf &lt;- tune::finalize_workflow(\n  ridge_wf,\n  best_penalty)\n\n# Fit Final Model on Training Data\nfinal_model &lt;- workflows::fit(\n  final_wf,\n  data = data_train_qb)\n\n# View Coefficients\nfinal_model %&gt;% \n  workflows::extract_fit_parsnip() %&gt;% \n  broom::tidy()\n\n\n  \n\n\nCode# Predict on Test Data\ndf &lt;- data_test_qb %&gt;%\n  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)\n\n# Evaluate Accuracy of Predictions\npetersenlab::accuracyOverall(\n  predicted = df$pred,\n  actual = df$fantasyPoints_lag,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThere was modest shrinkage from the training model to the test model: the \\(R^2\\) for the model on the training data was 0.47; the \\(R^2\\) for the same model applied to the test data was 0.42.\nFigure 19.4 depicts the predicted versus actual fantasy points for the model on the test data.\n\nCode# Calculate combined range for axes\naxis_limits &lt;- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)\n\nggplot(\n  df,\n  aes(\n    x = pred,\n    y = fantasyPoints_lag)) +\n  geom_point(\n    size = 2,\n    alpha = 0.6) +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"blue\",\n    linetype = \"dashed\") +\n  coord_equal(\n    xlim = axis_limits,\n    ylim = axis_limits) +\n  labs(\n    title = \"Predicted vs Actual Fantasy Points (Test Data)\",\n    x = \"Predicted Fantasy Points\",\n    y = \"Actual Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 19.4: Predicted Versus Actual Fantasy Points for Ridge Regression Model.\n\n\n\n\nBelow are the model predictions for next year’s fantasy points:\n\nCodenewData_qb %&gt;%\n  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %&gt;% \n  left_join(\n    .,\n    nfl_playerIDs %&gt;% select(gsis_id, name),\n    by = \"gsis_id\"\n  ) %&gt;% \n  select(name, fantasyPoints_lag) %&gt;% \n  arrange(-fantasyPoints_lag)\n\n\n  \n\n\n\n\n19.8.3 Elastic Net\nBelow, we fit an elastic net model. We evaluate it and tune its penalty and mixture parameters with cross-validation.\nThe penalty parameter controls the overall strength of regularization applied to the model’s coefficients. Smaller penalty values allow coefficients to remain large, which can reduce bias but increase variance of the model’s predictions and can increase the risk of overfitting. Larger penalty values shrink coefficients more aggressively, leading to simpler models with potentially higher bias but lower variance of predictions. This regularization helps prevent overfitting, especially when the model includes many predictors or multicollinearity.\nThe mixture parameter controls the balance between LASSO and ridge regularization. It ranges from 0 to 1: A value of 0 applies pure ridge regression, which shrinks all coefficients but keeps them in the model. A value of 1 applies pure LASSO, which can shrink some coefficients exactly to zero, effectively performing variable selection. Values between 0 and 1 apply a combination: ridge-like smoothing and LASSO-like sparsity. Smaller mixture values favor shrinkage without variable elimination, whereas larger values favor sparser solutions by excluding weak predictors.\nAfter tuning the model, we also evaluate its accuracy on the hold-out (test) data.\nThe elastic net models were fit using the glmnet package (Friedman et al., 2010, 2025; Tay et al., 2023).\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Set up Cross-Validation\nfolds &lt;- folds_kFold\n\n# Define Recipe (Formula)\nrec &lt;- recipes::recipe(\n  fantasyPoints_lag ~ ., # use all predictors\n  data = data_train_qb %&gt;% select(-gsis_id, -fantasyPointsMC_lag))\n\n# Define Model\nenet_spec &lt;- \n  parsnip::linear_reg(\n    penalty = tune::tune(),\n    mixture = tune::tune()) %&gt;%\n  parsnip::set_engine(\"glmnet\")\n\n# Workflow\nenet_wf &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(enet_spec)\n\n# Define a regular grid for both penalty and mixture\ngrid_enet &lt;- dials::grid_regular(\n  dials::penalty(range = c(-4, -1)),\n  dials::mixture(range = c(0, 1)),\n  levels = c(20, 5) # 20 penalty values × 5 mixture values\n)\n\n# Tune the Grid\ncv_results &lt;- tune::tune_grid(\n  enet_wf,\n  resamples = folds,\n  grid = grid_enet,\n  metrics = yardstick::metric_set(rmse, mae, rsq),\n  control = tune::control_grid(save_pred = TRUE)\n)\n\n# View Cross-Validation metrics\ntune::collect_metrics(cv_results)\n\n\n  \n\n\nCode# Identify best penalty\ntune::select_best(cv_results, metric = \"rmse\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"mae\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"rsq\")\n\n\n  \n\n\nCodebest_penalty &lt;- tune::select_best(cv_results, metric = \"mae\")\n\n# Finalize Workflow with Best Penalty\nfinal_wf &lt;- tune::finalize_workflow(\n  enet_wf,\n  best_penalty)\n\n# Fit Final Model on Training Data\nfinal_model &lt;- workflows::fit(\n  final_wf,\n  data = data_train_qb)\n\n# View Coefficients\nfinal_model %&gt;% \n  workflows::extract_fit_parsnip() %&gt;% \n  broom::tidy()\n\n\n  \n\n\nCode# Predict on Test Data\ndf &lt;- data_test_qb %&gt;%\n  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)\n\n# Evaluate Accuracy of Predictions\npetersenlab::accuracyOverall(\n  predicted = df$pred,\n  actual = df$fantasyPoints_lag,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThere was modest shrinkage from the training model to the test model: the \\(R^2\\) for the model on the training data was 0.47; the \\(R^2\\) for the same model applied to the test data was 0.42.\nFigure 19.5 depicts the predicted versus actual fantasy points for the model on the test data.\n\nCode# Calculate combined range for axes\naxis_limits &lt;- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)\n\nggplot(\n  df,\n  aes(\n    x = pred,\n    y = fantasyPoints_lag)) +\n  geom_point(\n    size = 2,\n    alpha = 0.6) +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"blue\",\n    linetype = \"dashed\") +\n  coord_equal(\n    xlim = axis_limits,\n    ylim = axis_limits) +\n  labs(\n    title = \"Predicted vs Actual Fantasy Points (Test Data)\",\n    x = \"Predicted Fantasy Points\",\n    y = \"Actual Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 19.5: Predicted Versus Actual Fantasy Points for Elastic Net Model.\n\n\n\n\nBelow are the model predictions for next year’s fantasy points:\n\nCodenewData_qb %&gt;%\n  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %&gt;% \n  left_join(\n    .,\n    nfl_playerIDs %&gt;% select(gsis_id, name),\n    by = \"gsis_id\"\n  ) %&gt;% \n  select(name, fantasyPoints_lag) %&gt;% \n  arrange(-fantasyPoints_lag)\n\n\n  \n\n\n\n\n19.8.4 Random Forest Machine Learning\n\n19.8.4.1 Assuming Independent Observations\nBelow, we fit a random forest model. We evaluate it and tune two parameters with cross-validation. The first parameter is mtry, which controls the number of predictors randomly sampled at each split in a decision tree. Smaller mtry values increase tree diversity by forcing trees to consider different subsets of predictors. This typically reduces the variance of the overall model’s predictions (because the final prediction is averaged over more diverse trees) but may increase bias if strong predictors are frequently excluded. Larger mtry allow more predictors to be considered at each split, making trees more similar to each other. This can reduce bias but often increases variance of the model’s predictions, because the trees are more correlated and less effective at error cancellation when averaged.\nThe second parameter is min_n, which controls the minimum number of observations that must be present in a node for a split to be attempted. Smaller min_n values allow trees to grow deeper and capture more fine-grained patterns in the training data. This typically reduces bias but increases variance of the overall model’s predictions, because deeper trees are more likely to overfit to noise in the training data. Larger min_n values restrict the depth of the trees by requiring more data to justify a split. This can reduce variance by producing simpler, more generalizable trees—but may increase bias if the trees are unable to capture important structure in the data.\nAfter tuning the model, we evaluate its accuracy on the hold-out (test) data.\nThe random forest models were fit using the ranger package (Wright, 2024; Wright & Ziegler, 2017). We specify the grid of possible parameter values using the dials::grid_random(), dials::update.parameters(), dials::mtry(), and dials::min_n() functions of the dials package (Kuhn & Frick, 2025) and the hardhat::extract_parameter_set_dials() function of the hardhat package (Frick, Vaughan, et al., 2025).\n\nCode# Set seed for reproducibility\nset.seed(52242)\n\n# Set up Cross-Validation\nfolds &lt;- folds_kFold\n\n# Define Recipe (Formula)\nrec &lt;- recipes::recipe(\n  fantasyPoints_lag ~ ., # use all predictors\n  data = data_train_qb %&gt;% select(-gsis_id, -fantasyPointsMC_lag))\n\n# Define Model\nrf_spec &lt;- \n  parsnip::rand_forest(\n    mtry = tune::tune(),\n    min_n = tune::tune(),\n    trees = 500) %&gt;%\n  parsnip::set_mode(\"regression\") %&gt;%\n  parsnip::set_engine(\n    \"ranger\",\n    importance = \"impurity\")\n\n# Workflow\nrf_wf &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(rec) %&gt;%\n  workflows::add_model(rf_spec)\n\n# Create Grid\nn_predictors &lt;- recipes::prep(rec) %&gt;%\n  recipes::juice() %&gt;%\n  dplyr::select(-fantasyPoints_lag) %&gt;%\n  ncol()\n\n# Dynamically define ranges based on data\nrf_params &lt;- hardhat::extract_parameter_set_dials(rf_spec) %&gt;%\n  dials:::update.parameters(\n    mtry = dials::mtry(range = c(1L, n_predictors)),\n    min_n = dials::min_n(range = c(2L, 10L))\n  )\n\nrf_grid &lt;- dials::grid_random(rf_params, size = 15) #dials::grid_regular(rf_params, levels = 5)\n\n# Tune the Grid\ncv_results &lt;- tune::tune_grid(\n  rf_wf,\n  resamples = folds,\n  grid = rf_grid,\n  metrics = yardstick::metric_set(rmse, mae, rsq),\n  control = tune::control_grid(save_pred = TRUE)\n)\n\n# View Cross-Validation metrics\ntune::collect_metrics(cv_results)\n\n\n  \n\n\nCode# Identify best penalty\ntune::select_best(cv_results, metric = \"rmse\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"mae\")\n\n\n  \n\n\nCodetune::select_best(cv_results, metric = \"rsq\")\n\n\n  \n\n\nCodebest_penalty &lt;- tune::select_best(cv_results, metric = \"mae\")\n\n# Finalize Workflow with Best Penalty\nfinal_wf &lt;- tune::finalize_workflow(\n  rf_wf,\n  best_penalty)\n\n# Fit Final Model on Training Data\nfinal_model &lt;- workflows::fit(\n  final_wf,\n  data = data_train_qb)\n\n# View Feature Importance\nrf_fit &lt;- final_model %&gt;% \n  workflows::extract_fit_parsnip()\n\nrf_fit\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~8L,      x), num.trees = ~500, min.node.size = min_rows(~3L, x), importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      1582 \nNumber of independent variables:  73 \nMtry:                             8 \nTarget node size:                 3 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       6292.873 \nR squared (OOB):                  0.5165346 \n\nCoderanger_obj &lt;- rf_fit$fit\n\nranger_obj$variable.importance\n\n                   season                     games                        gs \n              158486.2281               311601.5550               508011.8724 \n      years_of_experience                       age             ageCentered20 \n              153382.2301               267708.3635               279843.6026 \n   ageCentered20Quadratic                    height                    weight \n              275022.6554                94161.2553               191252.7328 \n              rookie_year              draft_number            fantasy_points \n              140253.5439               272494.7684              1419670.4892 \n       fantasy_points_ppr             fantasyPoints               completions \n             1293483.9743              1312301.3737               827216.4193 \n                 attempts             passing_yards               passing_tds \n              646841.5869               917999.3053               857088.9137 \n    passing_interceptions            sacks_suffered           sack_yards_lost \n              130485.7228               161120.2954               220249.8113 \n             sack_fumbles         sack_fumbles_lost         passing_air_yards \n               76489.5596                52392.6083               299007.8777 \npassing_yards_after_catch       passing_first_downs               passing_epa \n              329812.9334               939795.2571               431328.9812 \n             passing_cpoe   passing_2pt_conversions                      pacr \n              177761.1008                35523.1896               168910.0901 \n                  carries             rushing_yards               rushing_tds \n              240997.7562               181847.7875                57066.7128 \n          rushing_fumbles      rushing_fumbles_lost       rushing_first_downs \n               54868.5816                33249.7074               118475.4108 \n              rushing_epa   rushing_2pt_conversions         special_teams_tds \n              212956.2609                26187.3427                  585.1349 \n         pocket_time.pass        pass_attempts.pass           throwaways.pass \n              117380.4026               542011.1159               181719.6877 \n              spikes.pass                drops.pass           bad_throws.pass \n               56148.3695               317853.6846               331561.0155 \n       times_blitzed.pass        times_hurried.pass            times_hit.pass \n              347944.2411               194260.1925               192719.6026 \n     times_pressured.pass         batted_balls.pass        on_tgt_throws.pass \n              441632.1558                92205.6330               362699.7319 \n           rpo_plays.pass            rpo_yards.pass         rpo_pass_att.pass \n              161827.5832               166390.5923               141437.2823 \n      rpo_pass_yards.pass         rpo_rush_att.pass       rpo_rush_yards.pass \n              134159.9270                64700.4095               108135.7323 \n         pa_pass_att.pass        pa_pass_yards.pass             drop_pct.pass \n              274342.4437               247080.0647               160183.4104 \n       bad_throw_pct.pass           on_tgt_pct.pass         pressure_pct.pass \n              195925.8780               132741.7748               167263.9786 \n             ybc_att.rush              yac_att.rush                  att.rush \n              169961.6625               155900.6714               229358.3533 \n                 yds.rush                   td.rush                  x1d.rush \n              158863.0638                65818.0750               155380.6876 \n                 ybc.rush                  yac.rush              brk_tkl.rush \n              156091.4085               153192.0666                49373.0228 \n              att_br.rush \n               60286.9067 \n\nCode# Predict on Test Data\ndf &lt;- data_test_qb %&gt;%\n  mutate(pred = predict(final_model, new_data = data_test_qb)$.pred)\n\n# Evaluate Accuracy of Predictions\npetersenlab::accuracyOverall(\n  predicted = df$pred,\n  actual = df$fantasyPoints_lag,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThere was modest shrinkage from the training model to the test model: the \\(R^2\\) for the model on the training data was 0.47; the \\(R^2\\) for the same model applied to the test data was 0.43.\nFigure 19.6 depicts the predicted versus actual fantasy points for the model on the test data.\n\nCode# Calculate combined range for axes\naxis_limits &lt;- range(c(df$pred, df$fantasyPoints_lag), na.rm = TRUE)\n\nggplot(\n  df,\n  aes(\n    x = pred,\n    y = fantasyPoints_lag)) +\n  geom_point(\n    size = 2,\n    alpha = 0.6) +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"blue\",\n    linetype = \"dashed\") +\n  coord_equal(\n    xlim = axis_limits,\n    ylim = axis_limits) +\n  labs(\n    title = \"Predicted vs Actual Fantasy Points (Test Data)\",\n    x = \"Predicted Fantasy Points\",\n    y = \"Actual Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 19.6: Predicted Versus Actual Fantasy Points for Random Forest Model.\n\n\n\n\nBelow are the model predictions for next year’s fantasy points:\n\nCodenewData_qb %&gt;%\n  mutate(fantasyPoints_lag = predict(final_model, new_data = newData_qb)$.pred) %&gt;% \n  left_join(\n    .,\n    nfl_playerIDs %&gt;% select(gsis_id, name),\n    by = \"gsis_id\"\n  ) %&gt;% \n  select(name, fantasyPoints_lag) %&gt;% \n  arrange(-fantasyPoints_lag)\n\n\n  \n\n\n\nNow we can stop the parallel backend:\n\nCodefuture::plan(future::sequential)\n\n\n\n19.8.4.2 Accounting for Longitudinal Data\nThe above approaches to machine learning assume that the observations are independent across rows. However, in our case, this assumption does not hold because the data are longitudinal—each player has multiple seasons, and each row corresponds to a unique player–season combination. In the approaches below, using a random forest model (this section) and a tree-boosting model (in Section 19.8.5), we address this by explicitly accounting for the nesting of longitudinal observations within players.\nApproaches to estimating random forest models with longitudinal data are described by Hu & Szymczak (2023). Below, we fit longitudinal random forest models using the LongituRF::MERF() function of the LongituRF package (Capitaine, 2020).\n\nCodesmerf &lt;- LongituRF::MERF(\n  X = data_train_qb %&gt;% dplyr::select(season:att_br.rush) %&gt;% as.matrix(), # predictors of the fixed effects\n  Y = data_train_qb[,c(\"fantasyPoints_lag\")] %&gt;% as.matrix(), # outcome variable\n  Z = data_train_qb %&gt;% dplyr::mutate(constant = 1) %&gt;% dplyr::select(constant, passing_yards, passing_tds, passing_interceptions, passing_epa, pacr) %&gt;% as.matrix(), # predictors of the random effects\n  id = data_train_qb[,c(\"gsis_id\")] %&gt;% as.matrix(), # player ID (for nesting)\n  time = data_train_qb[,c(\"ageCentered20\")] %&gt;% as.matrix(), # time variable\n  ntree = 500,\n  sto = \"BM\")\n\n[1] \"stopped after 11 iterations.\"\n\nCodesmerf$forest # the fitted random forest (obtained at the last iteration)\n\n\nCall:\n randomForest(x = X, y = ystar, ntree = ntree, mtry = mtry, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 25\n\n          Mean of squared residuals: 165.2049\n                    % Var explained: 98.49\n\nCodesmerf$random_effects %&gt;% data.frame() # the predicted random effects for each player\n\n\n  \n\n\nCodesmerf$omega %&gt;% data.frame() # the predicted stochastic processes\n\n\n  \n\n\nCodesmerf$OOB # OOB error at each iteration\n\n [1] 154.9823 110.8246 105.2359 108.3674 127.9415 145.0449 145.5596 158.7728\n [9] 160.9756 151.7372 165.2049\n\n\n\nCodeplot(smerf$Vraisemblance)\n\n\n\n\n\n\nFigure 19.7: Evolution of the Log-Likelihood.\n\n\n\n\nThe following code generates an error, for some reason. This issue has been posted on the GitHub repository: https://github.com/sistm/LongituRF/issues/5. Hopefully the package maintainer will help address the issue.\n\nCode# Predict on Test Data\npredict(\n  smerf,\n  X = data_test_qb %&gt;% dplyr::select(season:att_br.rush) %&gt;% as.matrix(),\n  Z = data_train_qb %&gt;% dplyr::mutate(constant = 1) %&gt;% dplyr::select(constant, passing_yards, passing_tds, passing_interceptions, passing_epa, pacr) %&gt;% as.matrix(),\n  id = data_test_qb[,c(\"gsis_id\")] %&gt;% as.matrix(),\n  time = data_test_qb[,c(\"ageCentered20\")] %&gt;% as.matrix())\n\nError in Z[w, , drop = FALSE] %*% object$random_effects[k, ]: non-conformable arguments\n\n\n\n19.8.5 Combining Tree-Boosting with Mixed Models\nTo combine tree-boosting with mixed models, we use the gpboost package (Sigrist et al., 2025).\nAdapted from here: https://towardsdatascience.com/mixed-effects-machine-learning-for-longitudinal-panel-data-with-gpboost-part-iii-523bb38effc\n\n19.8.5.1 Process Data\nIf using a gamma distribution, it requires positive-only values:\n\nCodedata_train_qb_matrix[,\"fantasyPoints_lag\"][data_train_qb_matrix[,\"fantasyPoints_lag\"] &lt;= 0] &lt;- 0.01\n\n\n\n19.8.5.2 Specify Predictor Variables\n\nCodepred_vars_qb &lt;- data_train_qb_matrix %&gt;% \n  as_tibble() %&gt;% \n  select(-fantasyPoints_lag, -fantasyPointsMC_lag, -ageCentered20, ageCentered20Quadratic) %&gt;% # -gsis_id\n  names()\n\npred_vars_qb_categorical &lt;- \"gsis_id\" # to specify categorical predictors\n\n\n\n19.8.5.3 Specify General Model Options\n\nCodemodel_likelihood &lt;- \"gamma\" # gaussian\nnrounds &lt;- 2000 # maximum number of boosting iterations (i.e., number of trees built sequentially); more rounds = potentially better learning, but also greater risk of overfitting\n\n\n\n19.8.5.4 Identify Optimal Tuning Parameters\nFor identifying the optimal tuning parameters for boosting, we partition the training data into inner training data and validation data. We randomly split the training data into 80% inner training data and 20% held-out validation data. We then use the mean absolute error as our index of prediction accuracy on the held-out validation data. We use the gpboost::gpb.Dataset() function to specify the data, the gpboost::GPModel() function to specify the model, and the gpboost::gpb.grid.search.tune.parameters() function to identify the optimal tuning parameters for that model.\n\nCode# Partition training data into inner training data and validation data\nntrain_qb &lt;- dim(data_train_qb_matrix)[1]\n\nset.seed(52242)\nvalid_tune_idx_qb &lt;- sample.int(ntrain_qb, as.integer(0.2*ntrain_qb)) # validation set\n\nfolds_qb &lt;- list(valid_tune_idx_qb)\n\n# Specify parameter grid, gp_model, and gpb.Dataset\nparam_grid_qb &lt;- list(\n  \"learning_rate\" = c(0.2, 0.1, 0.05, 0.01), # the step size used when updating predictions after each boosting round (high values make big updates, which can speed up learning but risk overshooting; low values are usually more accurate but require more rounds)\n  \"max_depth\" = c(3, 5, 7), # maximum depth (levels) of each decision tree; deeper trees capture more complex patterns and interactions but risk overfitting; shallower trees tend to generalize better\n  \"min_data_in_leaf\" = c(10, 50, 100), # minimum number of training examples in a leaf node; higher values = more regularization (simpler trees)\n  \"lambda_l2\" = c(0, 1, 5)) # L2 regularization penalty for large weights in tree splits; adds a \"cost\" for complexity; helps prevent overfitting by shrinking the contribution of each tree\n\nother_params_qb &lt;- list(\n  num_leaves = 2^6) # maximum number of leaves per tree; controls the maximum complexity of each tree (along with max_depth); more leaves = more expressive models, but can overfit if min_data_in_leaf is too small; num_leaves must be consistent with max_depth, because deeper trees naturally support more leaves; max is: 2^n, where n is the largest max_depth\n\ngp_model_qb &lt;- gpboost::GPModel(\n  group_data = data_train_qb_matrix[,\"gsis_id\"],\n  likelihood = model_likelihood,\n  group_rand_coef_data = cbind(\n    data_train_qb_matrix[,\"ageCentered20\"],\n    data_train_qb_matrix[,\"ageCentered20Quadratic\"]),\n  ind_effect_group_rand_coef = c(1,1))\n\ngp_data_qb &lt;- gpboost::gpb.Dataset(\n  data = data_train_qb_matrix[,pred_vars_qb],\n  categorical_feature = pred_vars_qb_categorical,\n  label = data_train_qb_matrix[,\"fantasyPoints_lag\"]) # could instead use mean-centered variable (fantasyPointsMC_lag) and add mean back afterward\n\n# Find optimal tuning parameters\nopt_params_qb &lt;- gpboost::gpb.grid.search.tune.parameters(\n  param_grid = param_grid_qb,\n  params = other_params_qb,\n  num_try_random = NULL,\n  folds = folds_qb,\n  data = gp_data_qb,\n  gp_model = gp_model_qb,\n  nrounds = nrounds,\n  early_stopping_rounds = 50, # stops training early if the model hasn’t improved on the validation set in 50 rounds; prevents overfitting and saves time\n  verbose_eval = 1,\n  metric = \"mae\")\n\nError in fd$booster$update(fobj = fobj): [GPBoost] [Fatal] NaN occured in gradient wrt covariance / auxiliary parameter number 1 (counting starts at 1, total nb. par. = 4) \n\nCodeopt_params_qb\n\nError: object 'opt_params_qb' not found\n\n\nWe use the optimal parameters identified during tuning. We use a low learning rate (0.1) to avoid overfitting. We set some light regularization (lambda_l2) for better generalization. We also set the maximum tree depth (max_depth) at 5 to capture complex (up to 5-way) interactions, and set the maximum number of terminal nodes (num_leaves) per tree at \\(2^6\\) (64)—though the number will not reach greater than \\(2^{\\text{max depth}} = 2^5 = 32\\). We set the minimum number of samples in any leaf (min_data_in_leaf) to be 100.\n\n19.8.5.5 Specify Model and Tuning Parameters\n\nCodegp_model_qb &lt;- gpboost::GPModel(\n  group_data = data_train_qb_matrix[,\"gsis_id\"],\n  likelihood = model_likelihood,\n  group_rand_coef_data = cbind(\n    data_train_qb_matrix[,\"ageCentered20\"],\n    data_train_qb_matrix[,\"ageCentered20Quadratic\"]),\n  ind_effect_group_rand_coef = c(1,1))\n\ngp_data_qb &lt;- gpboost::gpb.Dataset(\n  data = data_train_qb_matrix[,pred_vars_qb],\n  categorical_feature = pred_vars_qb_categorical,\n  label = data_train_qb_matrix[,\"fantasyPoints_lag\"])\n\nparams_qb &lt;- list(\n  learning_rate = 0.1,\n  max_depth = 5,\n  min_data_in_leaf = 100,\n  lambda_l2 = 5,\n  num_leaves = 2^6,\n  num_threads = num_cores)\n\nnrounds_qb &lt;- 84 # identify optimal number of trees through iteration and cross-validation\n\n#gp_model_qb$set_optim_params(params = list(optimizer_cov = \"nelder_mead\")) # to speed up model estimation\n\n\n\n19.8.5.6 Fit Model\nWe use the gpboost::gpb.train() function to fit the specified model with the specified tuning parameters and dataset.\n\nCodegp_model_fit_qb &lt;- gpboost::gpb.train(\n  data = gp_data_qb,\n  gp_model = gp_model_qb,\n  nrounds = nrounds_qb,\n  params = params_qb) # verbose = 0\n\n[GPBoost] [Info] Total Bins 8709\n[GPBoost] [Info] Number of data points in the train set: 1582, number of used features: 73\n[GPBoost] [Info] [GPBoost with gamma likelihood]: initscore=4.800459\n[GPBoost] [Info] Start training from score 4.800459\n\n\n\n19.8.5.7 Model Results\n\nCodesummary(gp_model_qb) # estimated random effects model\n\n=====================================================\nModel summary:\nNb. observations: 1582\nNb. groups: 316 (Group_1)\nCovariance parameters (random effects):\n                       Param.\nGroup_1                     0\nGroup_1_rand_coef_nb_1      0\nGroup_1_rand_coef_nb_2      0\n-----------------------------------------------------\nAdditional parameters:\n      Param.\nshape 0.6343\n=====================================================\n\n\nWe evaluate the importance of features using the gpboost::gpb.importance() and gpboost::gpb.plot.importance() functions:\n\nCodegp_model_qb_importance &lt;- gpboost::gpb.importance(gp_model_fit_qb)\ngp_model_qb_importance\n\n\n  \n\n\n\n\nCodegpboost::gpb.plot.importance(gp_model_qb_importance)\n\n\n\n\n\n\nFigure 19.8: Importance of Features (Predictors) in Tree Boosting Machine Learning Model.\n\n\n\n\n\n19.8.5.8 Evaluate Accuracy of Model on Training Data\n\nCode# Model-implied predictions\npred_train_qb &lt;- predict(\n  gp_model_fit_qb,\n  data = data_train_qb_matrix[, pred_vars_qb],\n  group_data_pred = data_train_qb_matrix[, \"gsis_id\"],\n  group_rand_coef_data_pred = cbind(\n    data_train_qb_matrix[, \"ageCentered20\"],\n    data_train_qb_matrix[, \"ageCentered20Quadratic\"]),\n  predict_var = FALSE,\n  pred_latent = FALSE)\n\npetersenlab::accuracyOverall(\n  predicted = pred_train_qb[[\"response_mean\"]],\n  actual = data_train_qb_matrix[, \"fantasyPoints_lag\"],\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\n\n19.8.5.9 Evaluate Accuracy of Model on Test Data\n\nCode# Test Model on Test Data\npred_test_qb &lt;- predict(\n  gp_model_fit_qb,\n  data = data_test_qb_matrix[,pred_vars_qb],\n  group_data_pred = data_test_qb_matrix[,\"gsis_id\"],\n  group_rand_coef_data_pred = cbind(\n    data_test_qb_matrix[,\"ageCentered20\"],\n    data_test_qb_matrix[,\"ageCentered20Quadratic\"]),\n  predict_var = FALSE,\n  pred_latent = FALSE)\n\ny_pred_test_qb &lt;- pred_test_qb[[\"response_mean\"]] # if outcome is mean-centered, add mean(data_train_qb_matrix[,\"fantasyPoints_lag\"])\n\npredictedVsActual &lt;- data.frame(\n  predictedPoints = y_pred_test_qb,\n  actualPoints = data_test_qb_matrix[,\"fantasyPoints_lag\"]\n)\n\npredictedVsActual\n\n\n  \n\n\nCodepetersenlab::accuracyOverall(\n  predicted = predictedVsActual$predictedPoints,\n  actual = predictedVsActual$actualPoints,\n  dropUndefined = TRUE\n)\n\n\n  \n\n\n\nThere was moderate shrinkage from the training model to the test model: the \\(R^2\\) for the model on the training data was 0.65; the \\(R^2\\) for the same model applied to the test data was 0.44.\nFigure 19.9 depicts the predicted versus actual fantasy points for the model on the test data.\n\nCode# Calculate combined range for axes\naxis_limits &lt;- range(c(predictedVsActual$predictedPoints, predictedVsActual$actualPoints), na.rm = TRUE)\n\nggplot(\n  predictedVsActual,\n  aes(\n    x = predictedPoints,\n    y = actualPoints)) +\n  geom_point(\n    size = 2,\n    alpha = 0.6) +\n  geom_abline(\n    slope = 1,\n    intercept = 0,\n    color = \"blue\",\n    linetype = \"dashed\") +\n  coord_equal(\n    xlim = axis_limits,\n    ylim = axis_limits) +\n  labs(\n    title = \"Predicted vs Actual Fantasy Points (Test Data)\",\n    x = \"Predicted Fantasy Points\",\n    y = \"Actual Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 19.9: Predicted Versus Actual Fantasy Points for Tree-Boosting Model.\n\n\n\n\n\n19.8.5.10 Generate Predictions for Next Season\n\nCode# Generate model predictions for next season\npred_nextYear_qb &lt;- predict(\n  gp_model_fit_qb,\n  data = newData_qb_matrix[,pred_vars_qb],\n  group_data_pred = newData_qb_matrix[,\"gsis_id\"],\n  group_rand_coef_data_pred = cbind(\n    newData_qb_matrix[,\"ageCentered20\"],\n    newData_qb_matrix[,\"ageCentered20Quadratic\"]),\n  predict_var = FALSE,\n  pred_latent = FALSE)\n\nnewData_qb$fantasyPoints_lag &lt;- pred_nextYear_qb$response_mean\n\n# Merge with player names\nnewData_qb &lt;- left_join(\n  newData_qb,\n  nfl_playerIDs %&gt;% select(gsis_id, name),\n  by = \"gsis_id\"\n)\n\nnewData_qb %&gt;% \n  arrange(-fantasyPoints_lag) %&gt;% \n  select(name, fantasyPoints_lag, fantasyPoints)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-summarizingModels-machineLearning",
    "href": "machine-learning.html#sec-summarizingModels-machineLearning",
    "title": "19  Machine Learning",
    "section": "\n19.9 Summarizing the Machine Learning Models",
    "text": "19.9 Summarizing the Machine Learning Models\nWe examined multiple machine learning models that included many predictor variables to identify the model that generates the most accurate prediction of Quarterbacks’ future fantasy points. We also compared them to simpler models, including a regression model with one predictor and a regression model with multiple predictors to deteremine the incremental validity of the machine learning models above and beyond the simpler models. A summary of the accuracy of the machine learning models is in Table 19.1.\n\n\n\nTable 19.1: Summary of Machine Learning Models.\n\n\n\n\n\n\n\n\n\n\nModel\n\\(R^2\\)\nMean Error\nMean Absolute Error\n\n\n\nRegression with One Predictor\n0.44\n7.91\n57.08\n\n\nRegressions with Multiple Predictors\n0.38\n9.49\n61.38\n\n\nLASSO\n0.40\n9.52\n60.52\n\n\nRidge Regression\n0.42\n8.71\n59.65\n\n\nElastic Net\n0.42\n8.71\n59.65\n\n\nRandom Forest\n0.43\n10.87\n58.79\n\n\nTree-Boosting\n0.44\n-10.05\n56.41\n\n\n\n\n\n\n\n\nThe most accurate models, in terms of discrimination (\\(R^2\\)), had an \\(R^2\\) of 0.44, and it was achieved using one of two approaches—a tree-boosting model with many predictors or a regression model with one predictor: the player’s fantasy points in the previous season. The regression model with one predictor was more accurate (in terms of \\(R^2\\)) than a regression model with multiple predictors and the other machine learning models that leveraged multiple predictors. This suggests that the other predictors examined did not have incremental validity—they did not show utility above and beyond a player’s fantasy points in predicting their future fantasy points. If anything, considering the additional predictor variables showed decremental validity—the inclusion of the additional variables worsened prediction, possibly by weakening the predictive association of the previous season’s fantasy points and misattributing its predictive effect to other, related predictors in the model. This is consistent with the common finding that simple models (and parsimony) are often preferable. As an example, Youngstrom et al. (2018) found that simple models did just as well and in some cases better than LASSO models in classifying bipolar disorder.\n\nThe most accurate model in terms of calibration (mean absolute error) was the tree-boosting model, which had a mean absolute error of 56.41, and was just slightly more accurate than the regression model with one predictor. However, even in the most accurate model, the predicted values were over 56 points away from the actual values, suggesting that the model-predicted values were not particularly accurate. In sum, the regression model with one predictor was the most accurate in terms of differentiating between players (\\(R^2\\)), whereas the tree-boosting model was the most accurate in terms of how close the predicted values were to the actual values.\nAll models explained less than half of the variance in players’ future fantasy points. Thus, there remains considerable variance to be explained. The considerable unexplained variance suggests that we are missing key variables that are important predictors of future fantasy performance. It is also consistent with the conclusion from many other domains in psychology that human behavior is challenging to predict (Petersen, 2025b). We could likely improve our predictive accuracy by including projections, but it is unclear what information projections are or are not based on. For instance, if projections already account for the prior season’s statistics and fantasy points, then including both may not buy you much in terms of increased predictive accuracy.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-machineLearningConclusion",
    "href": "machine-learning.html#sec-machineLearningConclusion",
    "title": "19  Machine Learning",
    "section": "\n19.10 Conclusion",
    "text": "19.10 Conclusion\nMachine learning attempts to maximize prediction accuracy. There are various types of machine learning models. Supervised learning involves learning from data where the correct classification or outcome is known. Unsupervised learning involves learning from data without known classifications. Machine learning can incorporate many predictor variables, so it is important to perform cross-validation to avoid overfitting. We examined multiple machine learning models to identify the model that generates the most accurate prediction of Quarterbacks’ future fantasy points. All models explained less than half of the variance in players’ fantasy points. The most accurate model, in terms of discrimination (\\(R^2\\)), was achieved using regression with one predictor: the player’s fantasy points in the previous season. The most accurate model in terms of calibration (mean absolute error) was the tree-boosting model, which accounted for the longitudinal nature of the data. The considerable unexplained variance suggests that we are missing key variables that are important predictors of future fantasy performance.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine-learning.html#sec-machineLearningSessionInfo",
    "href": "machine-learning.html#sec-machineLearningSessionInfo",
    "title": "19  Machine Learning",
    "section": "\n19.11 Session Info",
    "text": "19.11 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] glmnet_4.1-10      Matrix_1.7-3       knitr_1.50         lubridate_1.9.4   \n [5] forcats_1.0.0      stringr_1.5.1      readr_2.1.5        tidyverse_2.0.0   \n [9] effectsize_1.0.1   gpboost_1.6.1      R6_2.6.1           LongituRF_0.9     \n[13] yardstick_1.3.2    workflowsets_1.1.1 workflows_1.2.0    tune_1.3.0        \n[17] tidyr_1.3.1        tibble_3.3.0       rsample_1.3.1      recipes_1.3.1     \n[21] purrr_1.1.0        parsnip_1.3.2      modeldata_1.5.1    infer_1.0.9       \n[25] ggplot2_3.5.2      dplyr_1.1.4        dials_1.4.1        scales_1.4.0      \n[29] broom_1.0.9        tidymodels_1.3.0   powerjoin_0.1.0    missRanger_2.6.1  \n[33] future_1.67.0      petersenlab_1.2.0 \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3   shape_1.4.6.1        rstudioapi_0.17.1   \n  [4] jsonlite_2.0.0       datawizard_1.2.0     magrittr_2.0.3      \n  [7] TH.data_1.1-3        estimability_1.5.1   farver_2.1.2        \n [10] nloptr_2.2.1         rmarkdown_2.29       vctrs_0.6.5         \n [13] minqa_1.2.8          base64enc_0.1-3      sparsevctrs_0.3.4   \n [16] htmltools_0.5.8.1    Formula_1.2-5        parallelly_1.45.1   \n [19] htmlwidgets_1.6.4    sandwich_3.1-1       plyr_1.8.9          \n [22] zoo_1.8-14           emmeans_1.11.2       lifecycle_1.0.4     \n [25] iterators_1.0.14     pkgconfig_2.0.3      fastmap_1.2.0       \n [28] rbibutils_2.3        digest_0.6.37        colorspace_2.1-1    \n [31] furrr_0.3.1          Hmisc_5.2-3          labeling_0.4.3      \n [34] latex2exp_0.9.6      randomForest_4.7-1.2 RJSONIO_2.0.0       \n [37] timechange_0.3.0     compiler_4.5.1       withr_3.0.2         \n [40] htmlTable_2.4.3      backports_1.5.0      DBI_1.2.3           \n [43] psych_2.5.6          MASS_7.3-65          lava_1.8.1          \n [46] tools_4.5.1          pbivnorm_0.6.0       ranger_0.17.0       \n [49] foreign_0.8-90       future.apply_1.20.0  nnet_7.3-20         \n [52] doFuture_1.1.2       glue_1.8.0           quadprog_1.5-8      \n [55] nlme_3.1-168         grid_4.5.1           checkmate_2.3.3     \n [58] cluster_2.1.8.1      reshape2_1.4.4       generics_0.1.4      \n [61] gtable_0.3.6         tzdb_0.5.0           class_7.3-23        \n [64] hms_1.1.3            data.table_1.17.8    foreach_1.5.2       \n [67] pillar_1.11.0        mitools_2.4          splines_4.5.1       \n [70] lhs_1.2.0            lattice_0.22-7       FNN_1.1.4.1         \n [73] survival_3.8-3       tidyselect_1.2.1     mix_1.0-13          \n [76] reformulas_0.4.1     gridExtra_2.3        stats4_4.5.1        \n [79] xfun_0.53            hardhat_1.4.2        timeDate_4041.110   \n [82] stringi_1.8.7        DiceDesign_1.10      yaml_2.3.10         \n [85] boot_1.3-31          evaluate_1.0.4       codetools_0.2-20    \n [88] cli_3.6.5            rpart_4.1.24         xtable_1.8-4        \n [91] parameters_0.28.0    Rdpack_2.6.4         lavaan_0.6-19       \n [94] Rcpp_1.1.0           globals_0.18.0       coda_0.19-4.1       \n [97] parallel_4.5.1       gower_1.0.2          bayestestR_0.16.1   \n[100] GPfit_1.0-9          lme4_1.1-37          listenv_0.9.1       \n[103] viridisLite_0.4.2    mvtnorm_1.3-3        ipred_0.9-15        \n[106] prodlim_2025.04.28   insight_1.4.0        rlang_1.1.6         \n[109] multcomp_1.4-28      mnormt_2.1.1        \n\n\n\n\n\n\nBengtsson, H. (2025). ‘future‘: Unified parallel and distributed processing in ‘R‘ for everyone. https://doi.org/10.32614/CRAN.package.future\n\n\nCapitaine, L. (2020). LongituRF: Random forests for longitudinal data. https://doi.org/10.32614/CRAN.package.LongituRF\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFabri, A. (2022). powerjoin: Extensions of dplyr and fuzzyjoin join functions. https://doi.org/10.32614/CRAN.package.powerjoin\n\n\nFrick, H., Chow, F., Kuhn, M., Mahoney, M., Silge, J., & Wickham, H. (2025). rsample: General resampling infrastructure. https://doi.org/10.32614/CRAN.package.rsample\n\n\nFrick, H., Vaughan, D., & Kuhn, M. (2025). hardhat: Construct modeling packages. https://doi.org/10.32614/CRAN.package.hardhat\n\n\nFriedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1–22. https://doi.org/10.18637/jss.v033.i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., & Yang, J. (2025). glmnet: Lasso and elastic-net regularized generalized linear models. https://doi.org/10.32614/CRAN.package.glmnet\n\n\nHu, J., & Szymczak, S. (2023). A review on longitudinal data analysis with random forest. Briefings in Bioinformatics, 24(2). https://doi.org/10.1093/bib/bbad002\n\n\nKuhn, M. (2025). tune: Tidy tuning tools. https://doi.org/10.32614/CRAN.package.tune\n\n\nKuhn, M., & Frick, H. (2025). dials: Tools for creating tuning parameter values. https://doi.org/10.32614/CRAN.package.dials\n\n\nKuhn, M., & Vaughan, D. (2025). parsnip: A common API to modeling and analysis functions. https://doi.org/10.32614/CRAN.package.parsnip\n\n\nKuhn, M., Vaughan, D., & Hvitfeldt, E. (2025). yardstick: Tidy characterizations of model performance. https://doi.org/10.32614/CRAN.package.yardstick\n\n\nKuhn, M., & Wickham, H. (2020). Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\n\n\nKuhn, M., & Wickham, H. (2025). tidymodels: Easily install and load the tidymodels packages. https://doi.org/10.32614/CRAN.package.tidymodels\n\n\nKuhn, M., Wickham, H., & Hvitfeldt, E. (2025). recipes: Preprocessing and feature engineering steps for modeling. https://doi.org/10.32614/CRAN.package.recipes\n\n\nMayer, M. (2024). missRanger: Fast imputation of missing values. https://doi.org/10.32614/CRAN.package.missRanger\n\n\nPetersen, I. T. (2025a). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nSigrist, F., Gyger, T., & Kuendig, P. (2025). gpboost: Combining tree-boosting with gaussian process and mixed effects models. https://doi.org/10.32614/CRAN.package.gpboost\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic net regularization paths for all generalized linear models. Journal of Statistical Software, 106(1), 1–31. https://doi.org/10.18637/jss.v106.i01\n\n\nVaughan, D., & Couch, S. (2025). workflows: Modeling workflows. https://doi.org/10.32614/CRAN.package.workflows\n\n\nWright, M. N. (2024). ranger: A fast implementation of random forests. https://doi.org/10.32614/CRAN.package.ranger\n\n\nWright, M. N., & Ziegler, A. (2017). ranger: A fast implementation of random forests for high dimensional data in C++ and R. Journal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01\n\n\nYoungstrom, E. A., Halverson, T. F., Youngstrom, J. K., Lindhiem, O., & Findling, R. L. (2018). Evidence-based assessment from simple clinical judgments to statistical learning: Evaluating a range of options using pediatric bipolar disorder as a diagnostic challenge. Clinical Psychological Science, 6(2), 243–265. https://doi.org/10.1177/2167702617741845",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html",
    "href": "modern-portfolio-theory.html",
    "title": "20  Modern Portfolio Theory",
    "section": "",
    "text": "20.1 Getting Started\nThis chapter provides an overview of modern portfolio theory.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheoryGettingStarted",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheoryGettingStarted",
    "title": "20  Modern Portfolio Theory",
    "section": "",
    "text": "20.1.1 Load Packages\n\nCodelibrary(\"quantmod\")\nlibrary(\"TTR\")\nlibrary(\"fPortfolio\")\nlibrary(\"NMOF\")\nlibrary(\"GGally\")\nlibrary(\"tidyverse\")\n\n\n\n20.1.2 Load Data\n\nCodeload(file = \"./data/players_projectedPoints_seasonal.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/nfl_actualFantasyPoints_weekly.RData\")\nload(file = \"./data/nfl_playerIDs.RData\")\nload(file = \"./data/nfl_schedules.RData\")",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheoryOverview",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheoryOverview",
    "title": "20  Modern Portfolio Theory",
    "section": "\n20.2 Overview",
    "text": "20.2 Overview",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-likeStockPicking",
    "href": "modern-portfolio-theory.html#sec-likeStockPicking",
    "title": "20  Modern Portfolio Theory",
    "section": "\n20.3 Fantasy Football is Like Stock Picking",
    "text": "20.3 Fantasy Football is Like Stock Picking\nSelecting players for your fantasy team is like picking stocks. In both fantasy football and the stock market, your goal is to pick assets (i.e., players/stocks) that will perform best and that others undervalue. But what is the best way to do that? Below, we discuss approaches to picking players/stocks.\n\n20.3.1 The Wisdom of the Crowd (or Market)\nIn picking players, there are various approaches one could take. You could do lots of research to pick players/stocks with strong fundamentals that you think will do particularly well next year. By picking these players/stocks, you are predicting that they will outperform their expectations. However, all of your information is likely already reflected in the current valuation of the player/stock, so your prediction is basically a gamble. This is evidenced by the fact that people do not reliably beat the crowd/market.\nEven so-called experts do not beat the market reliably. There is little consistency in the performance of mutual fund managers over time. In the book, “The Drunkard’s Walk: How Randomness Rules Our Lives”, Mlodinow (2008) reported essentially no correlation between performance of the top mutual funds in a five-year period with their performance over the subsequent five years. That is, the best funds in a one period were not necessarily the best funds in another period. This suggests that mutual fund managers differ in great part because of luck or chance rather than reliable skill. In any given year, some mutual funds will do better than other mutual funds. But this overperformance in a given year likely reflects more randomness than skill. That is likely why a cat beat professional investors in a stock market challenge [Goldstein (2013); archived at https://perma.cc/R3XU-K6J8]. That is, “few stock pickers, if any, have the skill needed to beat the market consistently, year after year” (Kahneman, 2011, p. 214). Although our sample size is much smaller with fantasy football projections, there also appears to be little consistency in fantasy football sites’ rank in accuracy over time [Kartes (2024); archived at https://perma.cc/69F7-LLTN; Petersen (2017); archived at https://perma.cc/BG2W-ANUF], suggesting that the projection sources are not reliably better than each other (or the crowd) over time.\nThe market reflects all of the knowledge of the crowd. One common misconception is that if you go with the market, you will receive “average” returns (by “average”, I mean that you will be in the 50th percentile among investors). This is not true—it has been shown that most mutual funds (about 80%) underperform the average returns of the stock market. So, by going with the market average, you will likely perform better than the “average” fund/investor. Consistent with this, crowd-averaged fantasy football projections tend to be more accurate than any individual’s projection [Kartes (2024); archived at https://perma.cc/69F7-LLTN; Petersen (2017); archived at https://perma.cc/BG2W-ANUF]. This evidence is consistent with the notion of the wisdom of the crowd, described in Section 26.3. Moreover, even if the stock market is relatively accurate (“efficient”) in terms of valuing stocks based on all (publicly) available information (i.e., the efficient market hypothesis), your fantasy football league is likely not. Thus, it may be effective to use crowd-based projections to identify players who are undervalued by your league.\n\n20.3.2 Diversification\nModern portfolio theory (mean-variance theory) is a framework for determining the optimal composition of an investment portfolio to maximize expected returns for a given level of risk. Here, risk refers to the variability (e.g., standard deviation or variance) of returns across time. Given two portfolios with the same expected returns over time, people tend to prefer the “safer” portfolio—that is, the portfolio with less variability/volatility across time. One of the powerful notions of modern portfolio theory is that, through diversification, one can achieve lower risk with the same expected returns. In investing, diversification involves owning multiple asset classes (e.g., domestic and international stocks and bonds), with the goal of having asset classes that are either uncorrelated or negatively correlated. That is, owning different types of assets is safer than owning only one type. If you have too much money in one asset and that asset tanks, you will lose your money. In other words, you do not want to put all of your eggs in one basket. By owning different asset classes, you can limit your downside risk without sacrificing much in terms of expected return. In sum, the goal of diversification is to reduce risk (not to increase returns) and thus to increase risk-adjusted returns, by reducing the amount of risk needed for a given level of return.\nThis lesson can also apply to fantasy football. When assembling a team, you are essentially putting together a portfolio of assets (i.e., team of players). As with stocks, each player has an expected return (i.e., projection) and a degree of risk. In fantasy football, a player’s risk might be quantified in terms of the variability of projected scores for a player across projection sources (e.g., Projection Source A, Source B, Source C, etc.), or as historical game-to-game variability. Variability of projected scores for a player across projection sources could reflect the uncertainty of projections for a player. Variability of historical (actual) fantasy points across games could reflect many factors, including risks due to injuries, situational changes (e.g., being traded to a new team or changes in team composition such as due to the acquisition of new players on the team), game scripts, and the tendency for the player to be “boom-or-bust” (e.g., if they are highly dependent on scoring touchdowns or long receptions for fantasy points). All things equal, we want to minimize our risk for a given level of expected returns. That way, we have the best chance of winning any given week. For the same level of expected returns, higher risk teams might have a few amazing games, but their teams might fall flat in other weeks. That is, for a given (high) rate of return, you are best off in the long run (i.e., over the course of a season) with a lower risk team compared to a higher risk team [Hitchings (2012); archived at https://perma.cc/NE35-G6LR].\nIn terms of diversification, it can be helpful to diversify in multiple ways. First, it can be helpful not to rely on just one or two “stud” players. If they are on bye or have a down week, your team is more likely to suffer. Also, there are risks in picking multiple offensive players from the same team. If you draft your starting Quarterback and Wide Receiver from the same team (e.g., the Cowboys), you are exposing your fantasy team to considerable risk. For instance, if you have the Quarterback and Wide Receiver from the same team, and the team has a poor offensive outing, that will have a greater impact. You can limit your downside risk by diversifying—drafting players from different teams. That way if the Cowboys’ offense does poorly in a given week, your fantasy team will not be as affected. Having multiple players on a juggernaut offense can be a boon, but it can be challenging to predict which offense will lead the league.\nHowever, sometimes having two players on the same team might be beneficial because some positions may be uncorrelated or even negatively correlated, which can also reduce risk. For instance, the performance of the Tight End and Running Back on the same team tends to be slightly negatively correlated, so it might not be a bad idea to start the Tight End and Running Back from the same team. For a correlation matrix of all positions on the team, see: https://assets-global.website-files.com/5f1af76ed86d6771ad48324b/607a4434a565aa7763bd1312_AndyAsh-Sharpstack-RPpaper.pdf [Sherman & Goldner (2021); archived at https://perma.cc/JQ6G-KSRT] and https://www.4for4.com/2018/preseason/definitive-guide-stacking-draftkings [4for4 Staff (2018); archived at https://perma.cc/JZH3-ZM5V]. We generate a correlation matrix of positions in Section 20.3.3.\nAnother important idea from modern portfolio theory is that, if you want to achieve higher returns, you may be able to by accepting additional—and the right combination of—risk. In general, risk is positively correlated with return. That is, receiving higher returns generally requires taking on additional risk—at least as long as we stay along the efficient frontier, described next. Diversification, by contrast, can limit your upside (by also limiting your downside). In other words, if your goal is to win your fantasy league, you may need to be willing to carry additional risk (e.g., drafting a Quarterback and Wide Receiver from the same team), knowing full well the possibility that it will not work out as predicted. However, winning a fantasy football league requires multiple predictions to work out as expected, which can benefit from situations where the bets were correlated. If you are going to lean into variability, you might take several steps: a) draft multiple players from the same team (aka “stacking”; e.g., Quarterback and Wide Receiver) or go all in on a high-powered offense and b) target high-risk, high-reward players (i.e., players with a high “ceiling”), such as rookies, sleepers, and players coming off injuries, trades, and suspensions.\n\n20.3.3 Stacking\nStacking involves selecting players on the same team (e.g., multiple players on the Dallas Cowboys). Having two players from the same team increases the variance of the fantasy points scored, for better or for worse, because their performance is linked [i.e., can depend on one another; Lee & Liu (2022)]. Thus, when the players’ respective team performs well, the players may score many points, greatly increasing the likelihood that a fantasy team wins; however, if the players’ respective team does not perform well, the players may score few points, thus increasing the likelihood that the fantasy team loses. It is a higher-risk strategy—but one that can pay off if the players are on a high-scoring offense. To help inform the process of stacking, we evaluate the inter-correlation of performance in a given week across players on the same team and on opposing teams. We determine QB1, RB1, RB2, WR1, WR2, WR3, TE1, and K1 for each combination of team and season based on how many fantasy points a player scored over the entire season.\n\nCode# Identify the QB1, RB1, RB2, WR1, WR2, WR3, TE1, and K1 for each team-season combination\nqb1ByTeamSeasonPosition &lt;- player_stats_seasonal %&gt;% \n  filter(!is.na(player_id) & !is.na(team) & !is.na(season) & !is.na(position_group) & !is.na(fantasyPoints)) %&gt;% \n  filter(position_group %in% c(\"QB\")) %&gt;% \n  group_by(team, season, position_group) %&gt;% \n  arrange(-fantasyPoints) %&gt;% \n  slice_max(\n    fantasyPoints,\n    with_ties = FALSE) %&gt;%\n  ungroup() %&gt;% \n  arrange(season, team, position_group) %&gt;% \n  select(team, season, position_group, player_id) %&gt;% \n  mutate(position = \"QB1\")\n\nrbsByTeamSeasonPosition &lt;- player_stats_seasonal %&gt;% \n  filter(!is.na(player_id) & !is.na(team) & !is.na(season) & !is.na(position_group) & !is.na(fantasyPoints)) %&gt;% \n  filter(position_group %in% c(\"RB\")) %&gt;% \n  group_by(team, season, position_group) %&gt;% \n  arrange(-fantasyPoints) %&gt;% \n  slice_max(\n    fantasyPoints,\n    n = 2,\n    with_ties = FALSE) %&gt;% \n  mutate(position = paste0(\"RB\", row_number())) %&gt;% \n  ungroup() %&gt;% \n  arrange(season, team, position_group) %&gt;% \n  select(team, season, position_group, player_id, position)\n\nwrsByTeamSeasonPosition &lt;- player_stats_seasonal %&gt;% \n  filter(!is.na(player_id) & !is.na(team) & !is.na(season) & !is.na(position_group) & !is.na(fantasyPoints)) %&gt;% \n  filter(position_group %in% c(\"WR\")) %&gt;% \n  group_by(team, season, position_group) %&gt;% \n  arrange(-fantasyPoints) %&gt;% \n  slice_max(\n    fantasyPoints,\n    n = 3,\n    with_ties = FALSE) %&gt;% \n  mutate(position = paste0(\"WR\", row_number())) %&gt;% \n  ungroup() %&gt;% \n  arrange(season, team, position_group) %&gt;% \n  select(team, season, position_group, player_id, position)\n\nte1ByTeamSeasonPosition &lt;- player_stats_seasonal %&gt;% \n  filter(!is.na(player_id) & !is.na(team) & !is.na(season) & !is.na(position_group) & !is.na(fantasyPoints)) %&gt;% \n  filter(position_group %in% c(\"TE\")) %&gt;% \n  group_by(team, season, position_group) %&gt;% \n  arrange(-fantasyPoints) %&gt;% \n  slice_max(\n    fantasyPoints,\n    with_ties = FALSE) %&gt;%\n  ungroup() %&gt;% \n  arrange(season, team, position_group) %&gt;% \n  select(team, season, position_group, player_id) %&gt;% \n  mutate(position = \"TE1\")\n\nk1ByTeamSeasonPosition &lt;- player_stats_seasonal %&gt;% \n  filter(!is.na(player_id) & !is.na(team) & !is.na(season) & !is.na(position_group) & !is.na(fantasyPoints)) %&gt;% \n  filter(position_group %in% c(\"K\")) %&gt;% \n  group_by(team, season, position_group) %&gt;% \n  arrange(-fantasyPoints) %&gt;% \n  slice_max(\n    fantasyPoints,\n    with_ties = FALSE) %&gt;%\n  ungroup() %&gt;% \n  arrange(season, team, position_group) %&gt;% \n  select(team, season, position_group, player_id) %&gt;% \n  mutate(position = \"K1\")\n\ntopRankedPlayersByTeamSeasonPosition &lt;- dplyr::bind_rows(\n  qb1ByTeamSeasonPosition,\n  rbsByTeamSeasonPosition,\n  wrsByTeamSeasonPosition,\n  te1ByTeamSeasonPosition,\n  k1ByTeamSeasonPosition\n)\n\nfantasyPointsForTopRankedPlayers_weekly &lt;- dplyr::left_join(\n  topRankedPlayersByTeamSeasonPosition,\n  player_stats_weekly %&gt;% select(team, season, week, player_id, fantasyPoints, game_id),\n  by = c(\"team\",\"season\",\"player_id\")\n)\n\n# add game_id to dst data\n\n# first attempt: match on home_team\ndst_with_game_home &lt;- nfl_actualFantasyPoints_dst_weekly %&gt;%\n  left_join(\n    nfl_schedules %&gt;% select(game_id, season, week, home_team),\n    by = c(\"season\", \"week\", \"team\" = \"home_team\")\n  )\n\n# second attempt: match on away_team\ndst_with_game_away &lt;- nfl_actualFantasyPoints_dst_weekly %&gt;%\n  left_join(\n    nfl_schedules %&gt;% select(game_id, season, week, away_team),\n    by = c(\"season\", \"week\", \"team\" = \"away_team\")\n  )\n\n# Combine the two (one will have NA for game_id, the other will have the match)\ndst_with_game_id &lt;- dplyr::bind_rows(\n  dst_with_game_home,\n  dst_with_game_away) %&gt;%\n  filter(!is.na(game_id)) # keep only matched rows\n\ndst_with_game_id_subset &lt;- dst_with_game_id %&gt;% \n  select(team, season, week, fantasyPoints, game_id) %&gt;%\n  mutate(\n    position = \"DST\",\n    position_group = \"DST\")\n\nfantasyPointsSameTeam_weekly &lt;- dplyr::bind_rows(\n  fantasyPointsForTopRankedPlayers_weekly,\n  dst_with_game_id_subset\n) %&gt;% \n  select(-player_id) %&gt;% \n  filter(!is.na(game_id))\n\nfantasyPointsOpponent_weekly &lt;- fantasyPointsSameTeam_weekly %&gt;%\n  rename(\n    opp_team = team,\n    opp_fantasyPoints = fantasyPoints\n  )\n\nfantasyPointsCombined_weekly &lt;- fantasyPointsSameTeam_weekly %&gt;%\n  inner_join(\n    fantasyPointsOpponent_weekly %&gt;% select(game_id, opp_team, position, opp_fantasyPoints),\n    by = c(\"game_id\", \"position\")\n  ) %&gt;%\n  filter(team != opp_team) # exclude matching to own team\n\n# Pivot to wide format for team positions\nteam_points_wide &lt;- fantasyPointsCombined_weekly %&gt;%\n  select(team, season, week, position, fantasyPoints) %&gt;%\n  tidyr::pivot_wider(\n    names_from = position,\n    values_from = fantasyPoints)\n\n# Pivot to wide format for opponent positions\nopp_points_wide &lt;- fantasyPointsCombined_weekly %&gt;%\n  select(team, season, week, position, opp_fantasyPoints) %&gt;%\n  tidyr::pivot_wider(\n    names_from = position,\n    values_from = opp_fantasyPoints,\n    names_prefix = \"opp_\")\n\n# Join both so each row has team and opponent positions side by side\ncombined_points_wide &lt;- dplyr::left_join(\n  team_points_wide,\n  opp_points_wide,\n  by = c(\"team\", \"season\", \"week\")\n)\n\n\nHere is a correlation matrix of players’ weekly performance across positions within a given team and their opponent:\n\nCodecor(\n  combined_points_wide %&gt;% select(QB1:opp_DST),\n  use = \"pairwise.complete.obs\") %&gt;% \n  round(., 2)\n\n          QB1   RB1   RB2   WR1   WR2   WR3   TE1    K1   DST opp_QB1 opp_RB1\nQB1      1.00  0.08  0.08  0.41  0.35  0.25  0.32  0.24 -0.01    0.19    0.01\nRB1      0.08  1.00 -0.13 -0.01 -0.03 -0.03 -0.02  0.17  0.04    0.01   -0.02\nRB2      0.08 -0.13  1.00  0.02 -0.01 -0.01  0.01  0.11  0.02    0.02   -0.05\nWR1      0.41 -0.01  0.02  1.00  0.07  0.01  0.00  0.13 -0.03    0.11    0.01\nWR2      0.35 -0.03 -0.01  0.07  1.00  0.02 -0.02  0.10 -0.03    0.09    0.02\nWR3      0.25 -0.03 -0.01  0.01  0.02  1.00  0.03  0.05 -0.03    0.11    0.01\nTE1      0.32 -0.02  0.01  0.00 -0.02  0.03  1.00  0.07 -0.04    0.09    0.00\nK1       0.24  0.17  0.11  0.13  0.10  0.05  0.07  1.00  0.18   -0.07   -0.11\nDST     -0.01  0.04  0.02 -0.03 -0.03 -0.03 -0.04  0.18  1.00   -0.24   -0.05\nopp_QB1  0.19  0.01  0.02  0.11  0.09  0.11  0.09 -0.07 -0.24    1.00    0.08\nopp_RB1  0.01 -0.02 -0.05  0.01  0.02  0.01  0.00 -0.11 -0.05    0.08    1.00\nopp_RB2  0.02 -0.05  0.00  0.00  0.00  0.00  0.03 -0.06 -0.03    0.08   -0.13\nopp_WR1  0.11  0.01  0.00  0.07  0.06  0.06  0.04  0.01 -0.02    0.41   -0.01\nopp_WR2  0.09  0.02  0.00  0.06  0.06  0.05  0.06  0.02  0.01    0.35   -0.03\nopp_WR3  0.11  0.01  0.00  0.06  0.05  0.05  0.03  0.01  0.01    0.25   -0.03\nopp_TE1  0.09  0.00  0.03  0.04  0.06  0.03  0.05  0.00 -0.02    0.32   -0.02\nopp_K1  -0.07 -0.11 -0.06  0.01  0.02  0.01  0.00 -0.20 -0.18    0.24    0.17\nopp_DST -0.24 -0.05 -0.03 -0.02  0.01  0.01 -0.02 -0.18 -0.03   -0.01    0.04\n        opp_RB2 opp_WR1 opp_WR2 opp_WR3 opp_TE1 opp_K1 opp_DST\nQB1        0.02    0.11    0.09    0.11    0.09  -0.07   -0.24\nRB1       -0.05    0.01    0.02    0.01    0.00  -0.11   -0.05\nRB2        0.00    0.00    0.00    0.00    0.03  -0.06   -0.03\nWR1        0.00    0.07    0.06    0.06    0.04   0.01   -0.02\nWR2        0.00    0.06    0.06    0.05    0.06   0.02    0.01\nWR3        0.00    0.06    0.05    0.05    0.03   0.01    0.01\nTE1        0.03    0.04    0.06    0.03    0.05   0.00   -0.02\nK1        -0.06    0.01    0.02    0.01    0.00  -0.20   -0.18\nDST       -0.03   -0.02    0.01    0.01   -0.02  -0.18   -0.03\nopp_QB1    0.08    0.41    0.35    0.25    0.32   0.24   -0.01\nopp_RB1   -0.13   -0.01   -0.03   -0.03   -0.02   0.17    0.04\nopp_RB2    1.00    0.02   -0.01   -0.01    0.01   0.11    0.02\nopp_WR1    0.02    1.00    0.07    0.01    0.00   0.13   -0.03\nopp_WR2   -0.01    0.07    1.00    0.02   -0.02   0.10   -0.03\nopp_WR3   -0.01    0.01    0.02    1.00    0.03   0.05   -0.03\nopp_TE1    0.01    0.00   -0.02    0.03    1.00   0.07   -0.04\nopp_K1     0.11    0.13    0.10    0.05    0.07   1.00    0.18\nopp_DST    0.02   -0.03   -0.03   -0.03   -0.04   0.18    1.00\n\n\nIn Figure 20.1, we depict a correlation matrix plot using the GGally::ggcorr() function of the GGally (Schloerke et al., 2025) package.\n\nCodeGGally::ggcorr(\n  combined_points_wide %&gt;% select(QB1:opp_DST),\n  label = TRUE,\n  label_size = 3)\n\n\n\n\n\n\nFigure 20.1: Correlation Matrix Plot.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-efficientFrontier",
    "href": "modern-portfolio-theory.html#sec-efficientFrontier",
    "title": "20  Modern Portfolio Theory",
    "section": "\n20.4 The Efficient Frontier of a Stock Portfolio",
    "text": "20.4 The Efficient Frontier of a Stock Portfolio\nThe ultimate goal in fantasy football is to draft players for your starting lineup that provide the most projected points (i.e., the highest returns) and the smallest downside risk. That is, your goal is to achieve the optimal portfolio at a given level of risk, depending on how much risk you are willing to tolerate. One of the key tools in modern portfolio theory for identifying the optimal portfolio (for a given risk level) is the efficient frontier. The efficient frontier is a visual depiction of the maximum expected returns for a given level of risk (where risk is the variability in returns over time). The efficient frontier is helpful for identifying the optimal portfolio—the optimal combination and weighting of assets—for a given risk level. Anything below the efficient frontier is considered inefficient (i.e., lower-than-maximum returns for a given level of risk).\nIn the example below, we use historical returns (since 2012) as the expected future returns. However, using historical returns as the expected future returns is risky because, as described in the common disclaimer, “Past performance does not guarantee future results.” If you select a relatively short period of historical returns, you may be selecting a period when the stock performed particularly well. When evaluating historical returns it is preferable to evaluate long time horizons and to evaluate how the stock performed during period of both boom (i.e., “bull markets”) and bust (i.e., “bear markets”, such as in a recession).\n\n20.4.1 Download Historical Stock Prices\nWe download historical stock prices using the quantmod package (Ryan & Ulrich, 2024):\n\nCodesymbols &lt;- c(\n  \"AAPL\",  # Apple\n  \"MSFT\",  # Microsoft\n  \"GOOGL\", # Google\n  \"AMZN\",  # Amazon\n  \"META\",  # Meta/Facebook\n  \"V\",     # Visa\n  \"DIS\",   # Disney\n  \"NKE\",   # Nike\n  \"TSLA\")  # Tesla\n\nquantmod::getSymbols(symbols)\n\n[1] \"AAPL\"  \"MSFT\"  \"GOOGL\" \"AMZN\"  \"META\"  \"V\"     \"DIS\"   \"NKE\"   \"TSLA\" \n\n\n\n20.4.2 Calculate Stock Returns\nWe download closing prices using the quantmod::Cl() function of the quantmod package (Ryan & Ulrich, 2024). We calculate returns from the closing prices using the TTR::ROC() function of the TTR package (Ulrich, 2023).\n\nCodeprices &lt;- do.call(\n  merge,\n  lapply(\n    symbols,\n    function(sym) quantmod::Cl(get(sym))))\n\nreturns &lt;- na.omit(\n  TTR::ROC(\n    prices,\n    type = \"discrete\"))\n\nreturns_ts &lt;- timeSeries::as.timeSeries(returns)\n\n\n\nCodeminYear &lt;- format(as.Date(row.names(returns_ts)), \"%Y\") %&gt;% \n  na.omit %&gt;% \n  as.integer %&gt;% \n  min()\n\n\n\n20.4.3 Create Portfolio\nWe use the fPortfolio package (Wuertz et al., 2023) to determine the optimal portfolio. We specify the portfolio specifications using the fPortfolio::portfolioSpec() function.\n\nCodeportfolioSpec &lt;- fPortfolio::portfolioSpec()\n\nfPortfolio::setNFrontierPoints(portfolioSpec) &lt;- 1000\n\n\n\n20.4.4 Determine the Efficient Frontier\nWe determine the efficient frontier using the fPortfolio::portfolioFrontier() function.\n\nCodeefficientFrontier &lt;- fPortfolio::portfolioFrontier(\n  returns_ts,\n  spec = portfolioSpec)\n\nefficientFrontier\n\n\nTitle:\n MV Portfolio Frontier \n Estimator:         covEstimator \n Solver:            solveRquadprog \n Optimize:          minRisk \n Constraints:       LongOnly \n Portfolio Points:  5 of 1000 \n\nPortfolio Weights:\n     AAPL.Close MSFT.Close GOOGL.Close AMZN.Close META.Close V.Close DIS.Close\n1        0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    1.0000\n250      0.1021     0.1677      0.1075     0.0528     0.0085  0.3446    0.1139\n500      0.0000     0.2119      0.0000     0.1285     0.1087  0.2806    0.0000\n750      0.0000     0.0677      0.0000     0.1445     0.2012  0.0000    0.0000\n1000     0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    0.0000\n     NKE.Close TSLA.Close\n1       0.0000     0.0000\n250     0.0718     0.0311\n500     0.0000     0.2704\n750     0.0000     0.5866\n1000    0.0000     1.0000\n\nCovariance Risk Budgets:\n     AAPL.Close MSFT.Close GOOGL.Close AMZN.Close META.Close V.Close DIS.Close\n1        0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    1.0000\n250      0.1028     0.1732      0.1093     0.0568     0.0094  0.3429    0.0982\n500      0.0000     0.1489      0.0000     0.1052     0.0972  0.1672    0.0000\n750      0.0000     0.0229      0.0000     0.0650     0.1051  0.0000    0.0000\n1000     0.0000     0.0000      0.0000     0.0000     0.0000  0.0000    0.0000\n     NKE.Close TSLA.Close\n1       0.0000     0.0000\n250     0.0633     0.0440\n500     0.0000     0.4816\n750     0.0000     0.8069\n1000    0.0000     1.0000\n\nTarget Returns and Risks:\n       mean    Cov   CVaR    VaR\n1    0.0004 0.0167 0.0379 0.0234\n250  0.0009 0.0130 0.0308 0.0200\n500  0.0013 0.0171 0.0388 0.0269\n750  0.0018 0.0254 0.0563 0.0380\n1000 0.0022 0.0364 0.0787 0.0520\n\nDescription:\n Tue Aug 26 19:56:15 2025 by user:  \n\n\n\nCode# Extract the coordinates of individual assets\nasset_means &lt;- colMeans(returns)\nasset_sd &lt;- apply(returns, 2, sd)\n\n# Add some padding to plot limits (so ticker symbols don't get cut off)\nxlim &lt;- range(asset_sd) * c(0.9, 1.1)\nylim &lt;- range(asset_means) * c(0.9, 1.1)\n\nxlim[1] &lt;- 0\nylim[1] &lt;- 0\n\n# Set scientific notation penalty\noptions(scipen = 999)\n\nplot(\n  efficientFrontier,\n  which = c(\n    1,  # efficient frontier\n    3,  # tangency portfolio\n    4), # risk/return of individual assets\n  control = list(\n    xlim = xlim,\n    ylim = ylim\n  ))\n\n# Add text labels for individual assets\npoints(\n  asset_sd,\n  asset_means,\n  col = \"red\",\n  pch = 19)\n\ntext(\n  asset_sd,\n  asset_means,\n  labels = symbols,\n  pos = 4,\n  cex = 0.8,\n  col = \"black\")\n\n\n\n\n\n\nFigure 20.2: Efficient Frontier for a Stock Portfolio.\n\n\n\n\n\n20.4.5 Identify the Optimal Weights\n\n20.4.5.1 Tangency Portfolio\nThe tangency portfolio is the portfolio with the highest Sharpe ratio—i.e., the highest ratio of return to risk. In other words, it is the portfolio with the greatest risk-adjusted returns. We identify the tangency portfolio using the fPortfolio::tangencyPortfolio() function.\n\nCode# Find the tangency portfolio (portfolio with the highest Sharpe ratio)\ntangencyPortfolio &lt;- fPortfolio::tangencyPortfolio(\n  data = returns_ts,\n  spec = portfolioSpec)\n\n\nWe extract the optimal weights for each asset at the tangency portfolio using the fPortfolio::getWeights() function.\n\nCode# Extract optimal weights\ntangencyPortfolio_optimalWeights &lt;- fPortfolio::getWeights(tangencyPortfolio)\ntangencyPortfolio_optimalWeights\n\n AAPL.Close  MSFT.Close GOOGL.Close  AMZN.Close  META.Close     V.Close \n 0.01730899  0.21429056  0.02762759  0.11567618  0.08632023  0.32699495 \n  DIS.Close   NKE.Close  TSLA.Close \n 0.00000000  0.00000000  0.21178150 \n\nCode# Output the results\nsummary(tangencyPortfolio)\n\n\nTitle:\n MV Tangency Portfolio \n Estimator:         covEstimator \n Solver:            solveRquadprog \n Optimize:          minRisk \n Constraints:       LongOnly \n\nPortfolio Weights:\n AAPL.Close  MSFT.Close GOOGL.Close  AMZN.Close  META.Close     V.Close \n     0.0173      0.2143      0.0276      0.1157      0.0863      0.3270 \n  DIS.Close   NKE.Close  TSLA.Close \n     0.0000      0.0000      0.2118 \n\nCovariance Risk Budgets:\n AAPL.Close  MSFT.Close GOOGL.Close  AMZN.Close  META.Close     V.Close \n     0.0126      0.1705      0.0208      0.1045      0.0840      0.2279 \n  DIS.Close   NKE.Close  TSLA.Close \n     0.0000      0.0000      0.3797 \n\nTarget Returns and Risks:\n  mean    Cov   CVaR    VaR \n0.0012 0.0159 0.0363 0.0247 \n\nDescription:\n Tue Aug 26 19:56:15 2025 by user:  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20.4.5.2 Portfolio with Max Return at a Given Risk Level\nIf the fPortfolio::maxreturnPortfolio() function worked as expected, the following could should work for determining the portfolio with the maximum return at each of various risk levels. However, there is a known bug in the fportfolio package (Wuertz et al., 2023) that the optimal portfolio (based on maximum returns) does not change when changing the target risk level, suggesting that it is not taking into account the target risk level specified by the user (see here: https://stackoverflow.com/q/78784306/2029527).\n\nCode# Define target risk levels\ntargetRisks &lt;- seq(0, 0.3, by = 0.01)\n\n# Initialize storage for optimal portfolios\noptimalPortfolios &lt;- list()\noptimalWeights_list &lt;- list()\n\n# Find optimal weightings for each target risk level\nfor (risk in targetRisks) {\n  # Create a portfolio optimization specification with the target risk\n  portfolioSpec &lt;- fPortfolio::portfolioSpec()\n  fPortfolio::setTargetRisk(portfolioSpec) &lt;- risk\n  \n  # Solve for the maximum return at this target risk\n  optimal_portfolio &lt;- fPortfolio::maxreturnPortfolio(\n    returns_ts,\n    spec = portfolioSpec)\n  \n  # Store the optimal portfolio\n  optimalPortfolios[[as.character(risk)]] &lt;- optimal_portfolio\n  \n  # Store the optimal portfolio weights with risk level\n  optimal_weights &lt;- fPortfolio::getWeights(optimal_portfolio)\n  optimalWeights_list[[as.character(risk)]] &lt;- c(RiskLevel = risk, optimal_weights)\n}\n\noptimalWeightsByRisk &lt;- dplyr::bind_rows(optimalWeights_list)\noptimalWeightsByRisk",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-efficientFrontierFantasy",
    "href": "modern-portfolio-theory.html#sec-efficientFrontierFantasy",
    "title": "20  Modern Portfolio Theory",
    "section": "\n20.5 The Efficient Frontier of a Fantasy Team",
    "text": "20.5 The Efficient Frontier of a Fantasy Team\nIn fantasy football, the efficient frontier can be helpful for identifying the optimal players to draft for a given risk level (and potentially within the salary cap). It can also be helpful for identifying potential trades. In this way, modern portfolio theory and the efficient frontier can be helpful for arbitrage—buying and selling the same asset (in this case, player) to take advantage of different prices for the same asset. That is, you could buy low and, for players who outperform expectations, sell high—in the form of a trade.\n\n20.5.1 Based on Variability Across Projection Sources\nWe can examine the efficient frontier of fantasy performance based on the mean and variability of players’ projected fantasy points across projection sources.\n\nCodeall_proj &lt;- dplyr::bind_rows(players_projectedPoints_seasonal)\n\nall_proj &lt;- all_proj %&gt;% \n  rename(projectedPoints = raw_points)\n\n\n\nCodeall_proj_summary &lt;- all_proj %&gt;% \n  group_by(id) %&gt;% \n  summarise(\n    mean = mean(projectedPoints, na.rm = TRUE),\n    sd = sd(projectedPoints, na.rm = TRUE),\n    var = var(projectedPoints, na.rm = TRUE)\n  )\n\nall_proj_summary &lt;- all_proj_summary %&gt;% \n  left_join(\n    nfl_playerIDs[,c(\"mfl_id\",\"name\",\"merge_name\",\"position\",\"team\")],\n    by = c(\"id\" = \"mfl_id\")\n  ) %&gt;% \n  select(name, team, position, everything()) %&gt;% \n  arrange(-mean)\n\n\nAs shown below, the correlation between mean and standard deviation of projected points is positive. That is, greater expected returns (i.e., mean of projected fantasy points) are associated with greater risk or volatility (i.e., standard deviation of projected fantasy points). Thus, to obtain a lineup that scores greater fantasy points, it may be necessary to take on greater risk. Moreover, you would not want take take on more risk for the same number of expected fantasy points; in addition, you would not want to score fewer fantasy points for the same amount of risk [Hitchings (2012); archived at https://perma.cc/NE35-G6LR].\n\nCodecor.test(\n  ~ mean + sd,\n  data = all_proj_summary\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  mean and sd\nt = 10.273, df = 995, p-value &lt; 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2524313 0.3647383\nsample estimates:\n      cor \n0.3096644 \n\n\nA scatterplot of the expected returns versus risk, based on projected fantasy points, is in Figure 20.3.\n\nCodeplot_projectedPointsMeanVsSD &lt;- ggplot2::ggplot(\n  data = all_proj_summary,\n  aes(\n    x = sd,\n    y = mean)) +\n  geom_point(\n    aes(\n      text = name, # add player name for mouse over tooltip\n      label = position)) + # add season for mouse over tooltip\n  #geom_smooth() +\n  coord_cartesian(\n    xlim = c(0,NA),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Standard Deviation of Player's Projected Fantasy Points (Season) Across Sources\",\n    y = \"Mean of Player's Projected Fantasy Points (Season) Across Sources\",\n    title = \"Mean vs Standard Deviation of Players' Projected Fantasy Points\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(plot_projectedPointsMeanVsSD)\n\n\n\n\n\n\nFigure 20.3: Mean of Projected Fantasy Points For Each Player as a Function of the Standard Deviation of the Projected Fantasy Points Across Sources.\n\n\n\nNow let’s consider the expected returns versus risk for a combination of players. The formula for the variance (and standard deviation) of the sum of two variables is in Equation 20.1:\n\\[\n\\begin{aligned}\n  \\text{Var}(x + y) &= \\text{Var}(x) + \\text{Var}(y) + 2\\text{Cov}(x, y)\\\\\n  s^2_{(x + y)} &= s^2_x + s^2_y +  2\\text{Cov}(x, y)\\\\\n  \\text{Std Dev}(x + y) &= \\sqrt{\\text{Var}(x) + \\text{Var}(y) + 2\\text{Cov}(x, y)}\\\\\n  s_{(x + y)} &= \\sqrt{s^2_x + s^2_y + 2\\text{Cov}(x, y)}\\\\\n\\end{aligned}\n\\tag{20.1}\\]\nIf we assume that players’ performance is uncorrelated with one another, the covariance between the players is zero, so the formula simplifies to Equation 20.2:\n\\[\n\\begin{aligned}\n  \\text{Var}(x + y) &= \\text{Var}(x) + \\text{Var}(y)\\\\\n  s^2_{(x + y)} &= s^2_x + s^2_y\\\\\n  \\text{Std Dev}(x + y) &= \\sqrt{\\text{Var}(x) + \\text{Var}(y)}\\\\\n  s_{(x + y)} &= \\sqrt{s^2_x + s^2_y}\\\\\n\\end{aligned}\n\\tag{20.2}\\]\nThat is, if players’ performance is independent of each other, the variance of two players’ points is merely the sum of their variances; their standard deviation is then the square root of that. In reality, players’ performance is not truly independent—particularly for players on the same team or, for a given game, for players on opposing teams. However, for players not playing in the same game, it is a reasonable assumption to make and it greatly simplifies the math. If you wanted to, you could account for players’ covariance by using Equation 20.1. An example variance-covariance matrix of players’ performance is in Section 20.5.2.\nBelow is code for obtaining the expected returns and risk for various combinations of Quarterback, Running Back, and Wide Receiver.\n\nCodehiLo_permutations &lt;- expand.grid(\n  rep(list(c(\"Hi\", \"Lo\")), 3),\n  stringsAsFactors = FALSE\n)\n\nqbRbWr_projections &lt;- data.frame(\n  condition = c(\"Hi\", \"Lo\"),\n  qb_name = c(\"Josh Allen\", \"Matthew Stafford\"),\n  rb_name = c(\"Saquon Barkley\", \"Joe Mixon\"),\n  wr_name = c(\"Ja'Marr Chase\", \"Courtland Sutton\")\n)\n\nqbRbWr_projections$qb_mean &lt;- c(\n  all_proj_summary$mean[which(all_proj_summary$position == \"QB\" & all_proj_summary$name == qbRbWr_projections$qb_name[1])],\n  all_proj_summary$mean[which(all_proj_summary$position == \"QB\" & all_proj_summary$name == qbRbWr_projections$qb_name[2])])\n\nqbRbWr_projections$qb_sd &lt;- c(\n  all_proj_summary$sd[which(all_proj_summary$position == \"QB\" & all_proj_summary$name == qbRbWr_projections$qb_name[1])],\n  all_proj_summary$sd[which(all_proj_summary$position == \"QB\" & all_proj_summary$name == qbRbWr_projections$qb_name[2])])\n\nqbRbWr_projections$rb_mean &lt;- c(\n  all_proj_summary$mean[which(all_proj_summary$position == \"RB\" & all_proj_summary$name == qbRbWr_projections$rb_name[1])],\n  all_proj_summary$mean[which(all_proj_summary$position == \"RB\" & all_proj_summary$name == qbRbWr_projections$rb_name[2])])\n\nqbRbWr_projections$rb_sd &lt;- c(\n  all_proj_summary$sd[which(all_proj_summary$position == \"RB\" & all_proj_summary$name == qbRbWr_projections$rb_name[1])],\n  all_proj_summary$sd[which(all_proj_summary$position == \"RB\" & all_proj_summary$name == qbRbWr_projections$rb_name[2])])\n\nqbRbWr_projections$wr_mean &lt;- c(\n  all_proj_summary$mean[which(all_proj_summary$position == \"WR\" & all_proj_summary$name == qbRbWr_projections$wr_name[1])],\n  all_proj_summary$mean[which(all_proj_summary$position == \"WR\" & all_proj_summary$name == qbRbWr_projections$wr_name[2])])\n\nqbRbWr_projections$wr_sd &lt;- c(\n  all_proj_summary$sd[which(all_proj_summary$position == \"WR\" & all_proj_summary$name == qbRbWr_projections$wr_name[1])],\n  all_proj_summary$sd[which(all_proj_summary$position == \"WR\" & all_proj_summary$name == qbRbWr_projections$wr_name[2])])\n\nqbRbWr_projections$qb_lastName &lt;- sapply(strsplit(qbRbWr_projections$qb_name, \" \"), function(x) tail(x, 1))\nqbRbWr_projections$rb_lastName &lt;- sapply(strsplit(qbRbWr_projections$rb_name, \" \"), function(x) tail(x, 1))\nqbRbWr_projections$wr_lastName &lt;- sapply(strsplit(qbRbWr_projections$wr_name, \" \"), function(x) tail(x, 1))\n\nteam_projections &lt;- data.frame(\n  team = apply(hiLo_permutations, 1, paste0, collapse = \"\"),\n  team_name = NA,\n  team_mean = NA,\n  team_sd = NA,\n  qb = hiLo_permutations$Var1,\n  rb = hiLo_permutations$Var2,\n  wr = hiLo_permutations$Var3,\n  qb_name = NA,\n  qb_lastName = NA,\n  qb_mean = NA,\n  qb_sd = NA,\n  rb_name = NA,\n  rb_lastName = NA,\n  rb_mean = NA,\n  rb_sd = NA,\n  wr_name = NA,\n  wr_lastName = NA,\n  wr_mean = NA,\n  wr_sd = NA\n)\n\nteam_projections$qb_name[which(team_projections$qb == \"Hi\")] &lt;- qbRbWr_projections$qb_name[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$qb_name[which(team_projections$qb == \"Lo\")] &lt;- qbRbWr_projections$qb_name[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$qb_lastName[which(team_projections$qb == \"Hi\")] &lt;- qbRbWr_projections$qb_lastName[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$qb_lastName[which(team_projections$qb == \"Lo\")] &lt;- qbRbWr_projections$qb_lastName[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$qb_mean[which(team_projections$qb == \"Hi\")] &lt;- qbRbWr_projections$qb_mean[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$qb_mean[which(team_projections$qb == \"Lo\")] &lt;- qbRbWr_projections$qb_mean[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$qb_sd[which(team_projections$qb == \"Hi\")] &lt;- qbRbWr_projections$qb_sd[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$qb_sd[which(team_projections$qb == \"Lo\")] &lt;- qbRbWr_projections$qb_sd[which(qbRbWr_projections$condition == \"Lo\")]\n\nteam_projections$rb_name[which(team_projections$rb == \"Hi\")] &lt;- qbRbWr_projections$rb_name[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$rb_name[which(team_projections$rb == \"Lo\")] &lt;- qbRbWr_projections$rb_name[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$rb_lastName[which(team_projections$rb == \"Hi\")] &lt;- qbRbWr_projections$rb_lastName[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$rb_lastName[which(team_projections$rb == \"Lo\")] &lt;- qbRbWr_projections$rb_lastName[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$rb_mean[which(team_projections$rb == \"Hi\")] &lt;- qbRbWr_projections$rb_mean[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$rb_mean[which(team_projections$rb == \"Lo\")] &lt;- qbRbWr_projections$rb_mean[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$rb_sd[which(team_projections$rb == \"Hi\")] &lt;- qbRbWr_projections$rb_sd[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$rb_sd[which(team_projections$rb == \"Lo\")] &lt;- qbRbWr_projections$rb_sd[which(qbRbWr_projections$condition == \"Lo\")]\n\nteam_projections$wr_name[which(team_projections$wr == \"Hi\")] &lt;- qbRbWr_projections$wr_name[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$wr_name[which(team_projections$wr == \"Lo\")] &lt;- qbRbWr_projections$wr_name[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$wr_lastName[which(team_projections$wr == \"Hi\")] &lt;- qbRbWr_projections$wr_lastName[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$wr_lastName[which(team_projections$wr == \"Lo\")] &lt;- qbRbWr_projections$wr_lastName[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$wr_mean[which(team_projections$wr == \"Hi\")] &lt;- qbRbWr_projections$wr_mean[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$wr_mean[which(team_projections$wr == \"Lo\")] &lt;- qbRbWr_projections$wr_mean[which(qbRbWr_projections$condition == \"Lo\")]\nteam_projections$wr_sd[which(team_projections$wr == \"Hi\")] &lt;- qbRbWr_projections$wr_sd[which(qbRbWr_projections$condition == \"Hi\")]\nteam_projections$wr_sd[which(team_projections$wr == \"Lo\")] &lt;- qbRbWr_projections$wr_sd[which(qbRbWr_projections$condition == \"Lo\")]\n\nteam_projections$team_name &lt;- apply(\n  team_projections[, c(\"qb_lastName\", \"rb_lastName\", \"wr_lastName\")],\n  1,\n  paste,\n  collapse = \"/\"\n)\n\nteam_projections$team_mean &lt;- rowSums(team_projections[,c(\"qb_mean\",\"rb_mean\",\"wr_mean\")])\nteam_projections$team_sd &lt;- rowSums(team_projections[,c(\"qb_sd\",\"rb_sd\",\"wr_sd\")])\n\n\nIn Figure 20.4, we depict the combined mean of projected fantasy points for a Quarterback/Running Back/Wide Receiver combination as a function of the standard deviation of their projected fantasy points across sources.\n\nCodeggplot2::ggplot(\n  data = team_projections,\n  aes(\n    x = team_sd,\n    y = team_mean,\n    label = team_name)) +\n  geom_point() +\n  ggrepel::geom_text_repel() +\n  labs(\n    x = \"Standard Deviation of Players' Projected Fantasy Points (Season) Across Sources\",\n    y = \"Mean of Players' Projected Fantasy Points (Season) Across Sources\",\n    title = \"Mean vs Standard Deviation of Players' Projected Fantasy Points\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 20.4: Mean of Projected Fantasy Points For a Quarterback/Running Back/Wide Receiver Combination as a Function of the Standard Deviation of the Projected Fantasy Points Across Sources.\n\n\n\n\nNow let’s generate the efficient frontier across all players.\n\nCodeall_proj_summary_noNA &lt;- all_proj_summary %&gt;% \n  filter(!is.na(id) & !is.na(mean) & !is.na(var))\n\nall_proj_summary_noNA_removeZeroVar &lt;- all_proj_summary_noNA %&gt;% \n  filter(var &gt; 0)\n\n\nBelow, we use the NMOF::mvFrontier() function from the NMOF package (Gilli et al., 2019; Schumann, 2011--2024, 2024) to compute the efficient frontier.\n\nCodeconst_cor &lt;- function(rho, numAssets) {\n  C &lt;- array(rho, dim = c(numAssets, numAssets))\n  diag(C) &lt;- 1\n  C\n}\n\nvarCovMatrix &lt;- diag(all_proj_summary_noNA_removeZeroVar$sd) %*% const_cor(0, nrow(all_proj_summary_noNA_removeZeroVar)) %*% diag(all_proj_summary_noNA_removeZeroVar$sd) # create a variance-covariance matrix assuming assets/players are uncorrelated\n\nefficientFrontierData &lt;- NMOF::mvFrontier(\n  all_proj_summary_noNA_removeZeroVar$mean,\n  varCovMatrix,\n  wmin = 0,\n  wmax = 1,\n  n = 50)\n\n\nWhen you evaluate all possible combinations of players, you can obtain the expected returns and standard deviations for each player combination. Based on that, you can determine the maximum number of fantasy points (i.e., the best possible team) at each level of risk. The best expected returns at each level of risk is known as the efficient frontier. The efficient frontier of projected fantasy points across sources is in Figure 20.5.\n\nCodeplot(\n  efficientFrontierData$volatility,\n  efficientFrontierData$return,\n  pch = 19,\n  cex = 0.5,\n  type = \"o\",\n  xlab = \"Standard Deviation of Projected Fantasy Points Across Sources\",\n  ylab = \"Projected Fantasy Points\",\n  main = \"Efficient Frontier of Projected Fantasy Points Across Sources\")\n\n\n\n\n\n\nFigure 20.5: Efficient Frontier of Projected Fantasy Points Across Sources.\n\n\n\n\n\n20.5.2 Based on Historical Game-to-Game Variability\nWe can also examine the efficient frontier of fantasy performance based on the mean and variability of players’ projected week-to-week fantasy points [Hitchings (2012); archived at https://perma.cc/JQ6G-KSRT].\n\nCodeplayer_stats_weekly_recent &lt;- player_stats_weekly %&gt;% \n  filter(season == max(season))\n\nplayer_stats_weekly_recent_summary &lt;- player_stats_weekly_recent %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(\n    mean = mean(fantasyPoints, na.rm = TRUE),\n    sd = sd(fantasyPoints, na.rm = TRUE),\n    var = var(fantasyPoints, na.rm = TRUE)\n  )\n\nplayer_stats_weekly_recent_summary &lt;- player_stats_weekly_recent_summary %&gt;% \n  left_join(\n    nfl_playerIDs[,c(\"gsis_id\",\"name\",\"merge_name\",\"position\",\"team\")],\n    by = c(\"player_id\" = \"gsis_id\")\n  ) %&gt;% \n  select(name, team, position, everything()) %&gt;% \n  arrange(-mean)\n\n\n\nCodecor.test(\n  ~ mean + sd,\n  data = player_stats_weekly_recent_summary\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  mean and sd\nt = 237.14, df = 8953, p-value &lt; 0.00000000000000022\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9258926 0.9315841\nsample estimates:\n      cor \n0.9287931 \n\n\nHere is the code to generate the variance-covariance matrix of players’ performance (it is very large because there are many players):\n\nCodedataForVarCovMatrix &lt;- player_stats_weekly_recent %&gt;% \n  select(player_id, week, fantasyPoints) %&gt;% \n  pivot_wider(\n    names_from = player_id,\n    values_from = fantasyPoints\n  )\n\n\n\nCodecov(\n  dataForVarCovMatrix,\n  use = \"pairwise.complete.obs\")\n\n\nA scatterplot of the expected returns versus risk, based on weekly fantasy points, is in Figure 20.6.\n\nCodeplot_fantasyPointsMeanVsSD &lt;- ggplot2::ggplot(\n  data = player_stats_weekly_recent_summary,\n  aes(\n    x = sd,\n    y = mean)) +\n  geom_point(\n    aes(\n      text = name, # add player name for mouse over tooltip\n      label = position)) + # add season for mouse over tooltip\n  #geom_smooth() +\n  coord_cartesian(\n    xlim = c(0,NA),\n    ylim = c(0,NA),\n    expand = FALSE) +\n  labs(\n    x = \"Standard Deviation of Player's Fantasy Points From Week-to-Week\",\n    y = \"Mean of Player's Fantasy Points Across Weeks\",\n    title = \"Mean vs Standard Deviation of Players' Fantasy Points\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(plot_fantasyPointsMeanVsSD)\n\n\n\n\n\n\nFigure 20.6: Mean of Weekly Fantasy Points For Each Player as a Function of the Week-to-Week Standard Deviation of their Fantasy Points.\n\n\n\nNow let’s consider the expected returns versus risk for a combination of players. Below is code for obtaining the expected returns and risk for various combinations of Quarterback, Running Back, and Wide Receiver.\n\nCodeqbRbWr_projections2 &lt;- data.frame(\n  condition = c(\"Hi\", \"Lo\"),\n  qb_name = c(\"Josh Allen\", \"Patrick Mahomes\"),\n  rb_name = c(\"Saquon Barkley\", \"David Montgomery\"),\n  wr_name = c(\"Ja'Marr Chase\", \"Stefon Diggs\")\n)\n\nqbRbWr_projections2$qb_mean &lt;- c(\n  player_stats_weekly_recent_summary$mean[which(player_stats_weekly_recent_summary$position == \"QB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$qb_name[1])],\n  player_stats_weekly_recent_summary$mean[which(player_stats_weekly_recent_summary$position == \"QB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$qb_name[2])])\n\nqbRbWr_projections2$qb_sd &lt;- c(\n  player_stats_weekly_recent_summary$sd[which(player_stats_weekly_recent_summary$position == \"QB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$qb_name[1])],\n  player_stats_weekly_recent_summary$sd[which(player_stats_weekly_recent_summary$position == \"QB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$qb_name[2])])\n\nqbRbWr_projections2$rb_mean &lt;- c(\n  player_stats_weekly_recent_summary$mean[which(player_stats_weekly_recent_summary$position == \"RB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$rb_name[1])],\n  player_stats_weekly_recent_summary$mean[which(player_stats_weekly_recent_summary$position == \"RB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$rb_name[2])])\n\nqbRbWr_projections2$rb_sd &lt;- c(\n  player_stats_weekly_recent_summary$sd[which(player_stats_weekly_recent_summary$position == \"RB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$rb_name[1])],\n  player_stats_weekly_recent_summary$sd[which(player_stats_weekly_recent_summary$position == \"RB\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$rb_name[2])])\n\nqbRbWr_projections2$wr_mean &lt;- c(\n  player_stats_weekly_recent_summary$mean[which(player_stats_weekly_recent_summary$position == \"WR\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$wr_name[1])],\n  player_stats_weekly_recent_summary$mean[which(player_stats_weekly_recent_summary$position == \"WR\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$wr_name[2])])\n\nqbRbWr_projections2$wr_sd &lt;- c(\n  player_stats_weekly_recent_summary$sd[which(player_stats_weekly_recent_summary$position == \"WR\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$wr_name[1])],\n  player_stats_weekly_recent_summary$sd[which(player_stats_weekly_recent_summary$position == \"WR\" & player_stats_weekly_recent_summary$name == qbRbWr_projections2$wr_name[2])])\n\nqbRbWr_projections2$qb_lastName &lt;- sapply(strsplit(qbRbWr_projections2$qb_name, \" \"), function(x) tail(x, 1))\nqbRbWr_projections2$rb_lastName &lt;- sapply(strsplit(qbRbWr_projections2$rb_name, \" \"), function(x) tail(x, 1))\nqbRbWr_projections2$wr_lastName &lt;- sapply(strsplit(qbRbWr_projections2$wr_name, \" \"), function(x) tail(x, 1))\n\nteam_projections2 &lt;- data.frame(\n  team = apply(hiLo_permutations, 1, paste0, collapse = \"\"),\n  team_name = NA,\n  team_mean = NA,\n  team_sd = NA,\n  qb = hiLo_permutations$Var1,\n  rb = hiLo_permutations$Var2,\n  wr = hiLo_permutations$Var3,\n  qb_name = NA,\n  qb_lastName = NA,\n  qb_mean = NA,\n  qb_sd = NA,\n  rb_name = NA,\n  rb_lastName = NA,\n  rb_mean = NA,\n  rb_sd = NA,\n  wr_name = NA,\n  wr_lastName = NA,\n  wr_mean = NA,\n  wr_sd = NA\n)\n\nteam_projections2$qb_name[which(team_projections2$qb == \"Hi\")] &lt;- qbRbWr_projections2$qb_name[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$qb_name[which(team_projections2$qb == \"Lo\")] &lt;- qbRbWr_projections2$qb_name[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$qb_lastName[which(team_projections2$qb == \"Hi\")] &lt;- qbRbWr_projections2$qb_lastName[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$qb_lastName[which(team_projections2$qb == \"Lo\")] &lt;- qbRbWr_projections2$qb_lastName[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$qb_mean[which(team_projections2$qb == \"Hi\")] &lt;- qbRbWr_projections2$qb_mean[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$qb_mean[which(team_projections2$qb == \"Lo\")] &lt;- qbRbWr_projections2$qb_mean[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$qb_sd[which(team_projections2$qb == \"Hi\")] &lt;- qbRbWr_projections2$qb_sd[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$qb_sd[which(team_projections2$qb == \"Lo\")] &lt;- qbRbWr_projections2$qb_sd[which(qbRbWr_projections2$condition == \"Lo\")]\n\nteam_projections2$rb_name[which(team_projections2$rb == \"Hi\")] &lt;- qbRbWr_projections2$rb_name[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$rb_name[which(team_projections2$rb == \"Lo\")] &lt;- qbRbWr_projections2$rb_name[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$rb_lastName[which(team_projections2$rb == \"Hi\")] &lt;- qbRbWr_projections2$rb_lastName[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$rb_lastName[which(team_projections2$rb == \"Lo\")] &lt;- qbRbWr_projections2$rb_lastName[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$rb_mean[which(team_projections2$rb == \"Hi\")] &lt;- qbRbWr_projections2$rb_mean[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$rb_mean[which(team_projections2$rb == \"Lo\")] &lt;- qbRbWr_projections2$rb_mean[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$rb_sd[which(team_projections2$rb == \"Hi\")] &lt;- qbRbWr_projections2$rb_sd[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$rb_sd[which(team_projections2$rb == \"Lo\")] &lt;- qbRbWr_projections2$rb_sd[which(qbRbWr_projections2$condition == \"Lo\")]\n\nteam_projections2$wr_name[which(team_projections2$wr == \"Hi\")] &lt;- qbRbWr_projections2$wr_name[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$wr_name[which(team_projections2$wr == \"Lo\")] &lt;- qbRbWr_projections2$wr_name[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$wr_lastName[which(team_projections2$wr == \"Hi\")] &lt;- qbRbWr_projections2$wr_lastName[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$wr_lastName[which(team_projections2$wr == \"Lo\")] &lt;- qbRbWr_projections2$wr_lastName[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$wr_mean[which(team_projections2$wr == \"Hi\")] &lt;- qbRbWr_projections2$wr_mean[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$wr_mean[which(team_projections2$wr == \"Lo\")] &lt;- qbRbWr_projections2$wr_mean[which(qbRbWr_projections2$condition == \"Lo\")]\nteam_projections2$wr_sd[which(team_projections2$wr == \"Hi\")] &lt;- qbRbWr_projections2$wr_sd[which(qbRbWr_projections2$condition == \"Hi\")]\nteam_projections2$wr_sd[which(team_projections2$wr == \"Lo\")] &lt;- qbRbWr_projections2$wr_sd[which(qbRbWr_projections2$condition == \"Lo\")]\n\nteam_projections2$team_name &lt;- apply(\n  team_projections2[, c(\"qb_lastName\", \"rb_lastName\", \"wr_lastName\")],\n  1,\n  paste,\n  collapse = \"/\"\n)\n\nteam_projections2$team_mean &lt;- rowSums(team_projections2[,c(\"qb_mean\",\"rb_mean\",\"wr_mean\")])\nteam_projections2$team_sd &lt;- rowSums(team_projections2[,c(\"qb_sd\",\"rb_sd\",\"wr_sd\")])\n\n\nIn Figure 20.7, we depict the combined mean of projected fantasy points for a Quarterback/Running Back/Wide Receiver combination as a function of the standard deviation of their projected fantasy points across sources.\n\nCodeggplot2::ggplot(\n  data = team_projections2,\n  aes(\n    x = team_sd,\n    y = team_mean,\n    label = team_name)) +\n  geom_point() +\n  ggrepel::geom_text_repel() +\n  labs(\n    x = \"Standard Deviation of Players' Fantasy Points From Week-to-Week\",\n    y = \"Mean of Players' Fantasy Points Across Weeks\",\n    title = \"Mean vs Standard Deviation of Players' Projected Fantasy Points\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 20.7: Mean of Weekly Fantasy Points For a Quarterback/Running Back/Wide Receiver Combination as a Function of the Week-to-Week Standard Deviation of their Fantasy Points.\n\n\n\n\nNow let’s generate the efficient frontier across all players.\n\nCodeplayer_stats_weekly_recent_summary_noNA &lt;- player_stats_weekly_recent_summary %&gt;% \n  filter(!is.na(player_id) & !is.na(mean) & !is.na(var))\n\nplayer_stats_weekly_recent_summary_noNA_removeZeroVar &lt;- player_stats_weekly_recent_summary_noNA %&gt;% \n  filter(var &gt; 0)\n\n\nBelow, we use the NMOF::mvFrontier() function from the NMOF package (Gilli et al., 2019; Schumann, 2011--2024, 2024) to compute the efficient frontier.\n\nCodevarCovMatrix2 &lt;- diag(player_stats_weekly_recent_summary_noNA_removeZeroVar$sd) %*% const_cor(0, nrow(player_stats_weekly_recent_summary_noNA_removeZeroVar)) %*% diag(player_stats_weekly_recent_summary_noNA_removeZeroVar$sd) # create a variance-covariance matrix assuming assets/players are uncorrelated\n\nefficientFrontierData2 &lt;- NMOF::mvFrontier(\n  player_stats_weekly_recent_summary_noNA_removeZeroVar$mean,\n  varCovMatrix2,\n  wmin = 0,\n  wmax = 1,\n  n = 50)\n\n\nThe efficient frontier of fantasy points across weeks is in Figure 20.8.\n\nCodeplot(\n  efficientFrontierData2$volatility,\n  efficientFrontierData2$return,\n  pch = 19,\n  cex = 0.5,\n  type = \"o\",\n  xlab = \"Standard Deviation of Fantasy Points Across Weeks\",\n  ylab = \"Weekly Fantasy Points\",\n  main = \"Efficient Frontier of Fantasy Points Across Weeks\")\n\n\n\n\n\n\nFigure 20.8: Efficient Frontier of Fantasy Points Across Weeks.\n\n\n\n\nIn these examples, we have computed the efficient frontier across all players. However, it would be more relevant to establish the efficient frontier for the possible combinations of players (e.g., one Quarterback, two Running Backs, two Wide Receivers, one Tight End, one Kicker, etc.). Nevertheless, the above examples illustrate how one might do this; one could extend the examples to consider whole teams rather than just three players at a time.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheoryConclusion",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheoryConclusion",
    "title": "20  Modern Portfolio Theory",
    "section": "\n20.6 Conclusion",
    "text": "20.6 Conclusion\nIn summary, fantasy football is similar to stock picking. You are most likely to pick the best players if you go with the wisdom of the crowd (e.g., average projections across projection sources) and diversify. Players—like stocks—have expected returns and risk associated with them. You can think of constructing your team like constructing a stock portfolio, where your goal is to gain the maximum expected returns for a given level of risk. The greater risk you are willing to take, the greater the potential returns, but also the lower the potential downside if the players underperform. Thus, you may want your team to be composed of a diverse combination of some higher risk players and some lower risk players. We demonstrate the efficient frontier as a valuable tool for identifying the combination of players with the maximum expected returns relative to their risk. Most projections are public information, so you might wonder whether using crowd projections gains you anything because everybody else has access to public information. However, this is also the case with stocks, and people still consistently perform best over time when they go with the market. Nevertheless, crowd projections are not highly accurate. And fantasy football is a game, so feel free to have fun and deviate from the crowd! However, you may be just as (if not more) likely to be wrong by deviating from the crowd.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "modern-portfolio-theory.html#sec-modernPortfolioTheorySessionInfo",
    "href": "modern-portfolio-theory.html#sec-modernPortfolioTheorySessionInfo",
    "title": "20  Modern Portfolio Theory",
    "section": "\n20.7 Session Info",
    "text": "20.7 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n [4] dplyr_1.1.4         purrr_1.1.0         readr_2.1.5        \n [7] tidyr_1.3.1         tibble_3.3.0        tidyverse_2.0.0    \n[10] GGally_2.4.0        ggplot2_3.5.2       NMOF_2.10-1        \n[13] fPortfolio_4023.84  fAssets_4023.85     fBasics_4041.97    \n[16] timeSeries_4041.111 timeDate_4041.110   quantmod_0.4.28    \n[19] TTR_0.24.4          xts_0.14.1          zoo_1.8-14         \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    viridisLite_0.4.2   farver_2.1.2       \n [4] S7_0.2.0            bitops_1.0-9        lazyeval_0.2.2     \n [7] fastmap_1.2.0       RCurl_1.98-1.17     XML_3.99-0.19      \n[10] digest_0.6.37       timechange_0.3.0    lifecycle_1.0.4    \n[13] mvnormtest_0.1-9-3  Rsolnp_2.0.1        magrittr_2.0.3     \n[16] kernlab_0.9-33      compiler_4.5.1      rlang_1.1.6        \n[19] tools_4.5.1         igraph_2.1.4        yaml_2.3.10        \n[22] data.table_1.17.8   knitr_1.50          sn_2.1.1           \n[25] labeling_0.4.3      htmlwidgets_1.6.4   mnormt_2.1.1       \n[28] curl_7.0.0          RColorBrewer_1.1-3  withr_3.0.2        \n[31] numDeriv_2016.8-1.1 grid_4.5.1          stats4_4.5.1       \n[34] future_1.67.0       globals_0.18.0      scales_1.4.0       \n[37] MASS_7.3-65         cli_3.6.5           mvtnorm_1.3-3      \n[40] rmarkdown_2.29      Rglpk_0.6-5.1       generics_0.1.4     \n[43] future.apply_1.20.0 robustbase_0.99-4-1 httr_1.4.7         \n[46] tzdb_0.5.0          energy_1.7-12       parallel_4.5.1     \n[49] vctrs_0.6.5         boot_1.3-31         jsonlite_2.0.0     \n[52] slam_0.1-55         hms_1.1.3           ggrepel_0.9.6      \n[55] listenv_0.9.1       crosstalk_1.2.1     rneos_0.4-0        \n[58] plotly_4.11.0       spatial_7.3-18      glue_1.8.0         \n[61] parallelly_1.45.1   DEoptimR_1.1-4      ggstats_0.10.0     \n[64] codetools_0.2-20    ecodist_2.1.3       stringi_1.8.7      \n[67] gtable_0.3.6        fMultivar_4031.84   quadprog_1.5-8     \n[70] gsl_2.1-8           pillar_1.11.0       htmltools_0.5.8.1  \n[73] truncnorm_1.0-9     R6_2.6.1            evaluate_1.0.4     \n[76] lattice_0.22-7      Rcpp_1.1.0          fCopulae_4022.85   \n[79] xfun_0.53           pkgconfig_2.0.3    \n\n\n\n\n\n\n4for4 Staff. (2018). The definitive guide to stacking on DraftKings. https://www.4for4.com/2018/preseason/definitive-guide-stacking-draftkings\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGilli, M., Maringer, D., & Schumann, E. (2019). Numerical methods and optimization in finance (Second). Elsevier/Academic Press. https://www.enricoschumann.net/NMOF\n\n\nGoldstein, J. (2013). Cat beats investors in stock market challenge. https://www.npr.org/sections/money/2013/01/14/169326326/housecat-beats-investors-in-stock-market-challenge\n\n\nHitchings, J. (2012). Moneyball: Using modern portfolio theory to win your fantasy sports league. https://eng.wealthfront.com/2012/01/17/moneyball-using-modern-portfolio-theory-to-win-your-fantasy-sports-league\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nKartes, J. (2024). Which fantasy football projections are most accurate? https://fantasyfootballanalytics.net/2024/12/which-fantasy-football-projections-are-most-accurate.html\n\n\nLee, M. D., & Liu, S. (2022). Drafting strategies in fantasy football: A study of competitive sequential human decision making. Judgment and Decision Making, 17(4), 691–719. https://doi.org/10.1017/S1930297500008901\n\n\nMlodinow, L. (2008). The drunkard’s walk: How randomness rules our lives. Pantheon Books.\n\n\nPetersen, I. T. (2017). Who has the best fantasy football projections? 2017 update. https://fantasyfootballanalytics.net/2017/03/best-fantasy-football-projections-2017.html\n\n\nRyan, J. A., & Ulrich, J. M. (2024). quantmod: Quantitative financial modelling framework. https://doi.org/10.32614/CRAN.package.quantmod\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., & Crowley, J. (2025). GGally: Extension to ggplot2. https://doi.org/10.32614/CRAN.package.GGally\n\n\nSchumann, E. (2011--2024). Numerical methods and optimization in finance (NMOF) manual. Package version 2.10-1). https://enricoschumann.net/NMOF\n\n\nSchumann, E. (2024). NMOF: Numerical methods and optimization in finance. https://doi.org/10.32614/CRAN.package.NMOF\n\n\nSherman, A., & Goldner, K. (2021). Sharpstack: Cholesky correlations for building better lineups. https://assets-global.website-files.com/5f1af76ed86d6771ad48324b/607a4434a565aa7763bd1312_AndyAsh-Sharpstack-RPpaper.pdf\n\n\nUlrich, J. (2023). TTR: Technical trading rules. https://doi.org/10.32614/CRAN.package.TTR\n\n\nWuertz, D., Setz, T., Chalabi, Y., & Theussl, S. (2023). fPortfolio: Rmetrics - portfolio selection and optimization. https://doi.org/10.32614/CRAN.package.fPortfolio",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html",
    "href": "cluster-analysis.html",
    "title": "21  Cluster Analysis",
    "section": "",
    "text": "21.1 Getting Started\nThis chapter provides an overview of cluster analysis.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#sec-clusterAnalysisGettingStarted",
    "href": "cluster-analysis.html#sec-clusterAnalysisGettingStarted",
    "title": "21  Cluster Analysis",
    "section": "",
    "text": "21.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"nflreadr\")\nlibrary(\"mclust\")\nlibrary(\"plotly\")\nlibrary(\"tidyverse\")\n\n\n\n21.1.2 Load Data\n\nCodeload(file = \"./data/nfl_players.RData\")\nload(file = \"./data/nfl_combine.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/nfl_advancedStatsPFR_seasonal.RData\")\nload(file = \"./data/nfl_actualStats_career.RData\")\n\n\n\n21.1.3 Overview\nWhereas factor analysis evaluates how variables do or do not hang together—in terms of their associations and non-associations, cluster analysis evaluates how people are or or not similar—in terms of their scores on one or more variables. The goal of cluster analysis is to identify distinguishable subgroups of people. The people within a subgroup are expected to be more similar to each other than they are to people in other subgroups. For instance, we might expect that there are distinguishable subtypes of Wide Receivers: possession, deep threats, and slot-type Wide Receivers. Possession Wide Receivers tend to be taller and heavier, with good hands who catch the ball at a high rate. Deep threat Wide Receivers tend to be fast. Slot-type Wide Receivers tend to be small, quick, and agile. In order to identify these clusters of Wide Receivers, we might conduct a cluster analysis with variables relating to the players’ height, weight, percent of (catchable) targets caught, air yards received, and various metrics from the National Football League (NFL) Combine, including their times in the 40-yard dash, 20-yard shuttle run, and three cone drill.\nThere are many approaches to cluster analysis, including model-based clustering, density-based clustering, centroid-based clustering, hierarchical clustering (aka connectivity-based clustering), etc. An overview of approaches to cluster analysis in R is provided by Kassambara (2017). In this chapter, we focus on examples using model-based clustering with the R package mclust (Fraley et al., 2024; Scrucca et al., 2023), which uses Gaussian finite mixture modeling. The various types of mclust models are provided here: https://mclust-org.github.io/mclust/reference/mclustModelNames.html.\n\n21.1.4 Tiers of Prior Season Fantasy Points\n\n21.1.4.1 Prepare Data\n\nCoderecentSeason &lt;- max(player_stats_seasonal$season, na.rm = TRUE) # also works: nflreadr::most_recent_season()\nrecentSeason\n\n[1] 2024\n\nCodeplayer_stats_seasonal_offense_recent &lt;- player_stats_seasonal %&gt;% \n  filter(season == recentSeason) %&gt;% \n  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\nplayer_stats_seasonal_offense_recentQB &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"QB\")\n\nplayer_stats_seasonal_offense_recentRB &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"RB\")\n\nplayer_stats_seasonal_offense_recentWR &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"WR\")\n\nplayer_stats_seasonal_offense_recentTE &lt;- player_stats_seasonal_offense_recent %&gt;% \n  filter(position_group == \"TE\")\n\n\n\n21.1.4.2 Identify the Optimal Number of Tiers by Position\n\n21.1.4.2.1 Quarterbacks\nWe can perform a cluster analysis using the mclust::mclustBIC(), mclust::mclustICL(), and mclust::mclustBootstrapLRT() functions of the mclust package (Fraley et al., 2024; Scrucca et al., 2023).\n\nCodetiersQB_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = 1:9\n)\n\ntiersQB_bic\n\nBayesian Information Criterion (BIC): \n          E         V\n1 -982.9038 -982.9038\n2 -964.3518 -927.2534\n3 -973.0978 -930.5327\n4 -971.2212 -912.2067\n5 -971.1002 -924.2192\n6 -979.8174 -928.6330\n7 -974.9956 -949.0362\n8 -981.8676 -955.9257\n9 -990.5409 -963.9506\n\nTop 3 models based on the BIC criterion: \n      V,4       V,5       V,2 \n-912.2067 -924.2192 -927.2534 \n\nCodesummary(tiersQB_bic)\n\nBest BIC values:\n               V,4        V,5        V,2\nBIC      -912.2067 -924.21918 -927.25337\nBIC diff    0.0000  -12.01245  -15.04664\n\nCodeplot(tiersQB_bic)\n\n\n\n\n\n\nCodetiersQB_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = 1:9\n)\n\ntiersQB_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n           E         V\n1  -982.9038 -982.9038\n2  -972.1069 -933.7840\n3 -1039.9236 -945.9954\n4 -1040.3715 -927.2426\n5 -1033.2208 -945.8315\n6 -1061.5988 -935.2325\n7 -1056.6193 -993.1199\n8 -1065.2675 -976.8222\n9 -1088.2374 -986.9286\n\nTop 3 models based on the ICL criterion: \n      V,4       V,2       V,6 \n-927.2426 -933.7840 -935.2325 \n\nCodesummary(tiersQB_icl)\n\nBest ICL values:\n               V,4        V,2         V,6\nICL      -927.2426 -933.78400 -935.232482\nICL diff    0.0000   -6.54137   -7.989849\n\nCodeplot(tiersQB_icl)\n\n\n\n\n\n\nCodetiersQB_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\nnumTiersQB &lt;- as.numeric(summary(tiersQB_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersQB_boostrap\n\n------------------------------------------------------------- \nBootstrap sequential LRT for the number of mixture components \n------------------------------------------------------------- \nModel        = V \nReplications = 999 \n              LRTS bootstrap p-value\n1 vs 2   68.720575             0.001\n2 vs 3    9.790787             0.045\n3 vs 4   31.396105             0.001\n4 vs 5    1.057678             0.674\n\nCodeplot(\n  tiersQB_boostrap,\n  G = numTiersQB - 1)\n\n\n\n\n\n\n\n\n21.1.4.2.2 Running Backs\n\nCodetiersRB_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  G = 1:9\n)\n\ntiersRB_bic\n\nBayesian Information Criterion (BIC): \n          E         V\n1 -1888.714 -1888.714\n2 -1817.804 -1769.298\n3 -1827.956 -1699.724\n4 -1817.083 -1701.580\n5 -1827.203 -1708.617\n6 -1837.331 -1719.106\n7 -1817.623 -1721.044\n8 -1827.752 -1735.666\n9 -1834.919 -1746.427\n\nTop 3 models based on the BIC criterion: \n      V,3       V,4       V,5 \n-1699.724 -1701.580 -1708.617 \n\nCodesummary(tiersRB_bic)\n\nBest BIC values:\n               V,3          V,4          V,5\nBIC      -1699.724 -1701.580264 -1708.616531\nBIC diff     0.000    -1.855914    -8.892182\n\nCodeplot(tiersRB_bic)\n\n\n\n\n\n\nCodetiersRB_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  G = 1:9\n)\n\ntiersRB_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n          E         V\n1 -1888.714 -1888.714\n2 -1823.200 -1793.185\n3 -1991.232 -1728.105\n4 -1974.495 -1745.695\n5 -2074.939 -1750.066\n6 -2123.855 -1757.956\n7 -2081.524 -1765.455\n8 -2133.100 -1796.801\n9 -2136.424 -1795.120\n\nTop 3 models based on the ICL criterion: \n      V,3       V,4       V,5 \n-1728.105 -1745.695 -1750.066 \n\nCodesummary(tiersRB_icl)\n\nBest ICL values:\n               V,3         V,4         V,5\nICL      -1728.105 -1745.69534 -1750.06574\nICL diff     0.000   -17.58998   -21.96037\n\nCodeplot(tiersRB_icl)\n\n\n\n\n\n\nCodenumTiersRB &lt;- 3\n\n\nThe model-based bootstrap clustering of Running Backs’ fantasy points is unable to run due to an error:\n\nCodetiersRB_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\n\nThus, we cannot use the following code, which would otherwise summarize the model results, specify the number of tiers, and plot model comparisons:\n\nCodenumTiersRB &lt;- as.numeric(summary(tiersRB_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersRB_boostrap\nplot(\n  tiersRB_boostrap,\n  G = numTiersRB - 1)\n\n\n\n21.1.4.2.3 Wide Receivers\n\nCodetiersWR_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  G = 1:9\n)\n\ntiersWR_bic\n\nBayesian Information Criterion (BIC): \n          E         V\n1 -2761.531 -2761.531\n2 -2703.730 -2574.337\n3 -2714.665 -2561.183\n4 -2690.946 -2551.896\n5 -2701.848 -2559.810\n6 -2679.348 -2566.401\n7 -2690.252 -2567.887\n8 -2693.451 -2579.761\n9 -2704.412 -2594.502\n\nTop 3 models based on the BIC criterion: \n      V,4       V,5       V,3 \n-2551.896 -2559.810 -2561.183 \n\nCodesummary(tiersWR_bic)\n\nBest BIC values:\n               V,4          V,5          V,3\nBIC      -2551.896 -2559.809568 -2561.182771\nBIC diff     0.000    -7.913781    -9.286984\n\nCodeplot(tiersWR_bic)\n\n\n\n\n\n\nCodetiersWR_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  G = 1:9\n)\n\ntiersWR_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n          E         V\n1 -2761.531 -2761.531\n2 -2728.952 -2597.147\n3 -2967.945 -2623.521\n4 -2909.051 -2643.926\n5 -3004.434 -2652.681\n6 -2995.921 -2665.160\n7 -3044.355 -2642.838\n8 -3043.060 -2662.966\n9 -3081.954 -2680.271\n\nTop 3 models based on the ICL criterion: \n      V,2       V,3       V,7 \n-2597.147 -2623.521 -2642.838 \n\nCodesummary(tiersWR_icl)\n\nBest ICL values:\n               V,2         V,3         V,7\nICL      -2597.147 -2623.52084 -2642.83833\nICL diff     0.000   -26.37432   -45.69181\n\nCodeplot(tiersWR_icl)\n\n\n\n\n\n\nCodetiersWR_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\nnumTiersWR &lt;- as.numeric(summary(tiersWR_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersWR_boostrap\n\n------------------------------------------------------------- \nBootstrap sequential LRT for the number of mixture components \n------------------------------------------------------------- \nModel        = V \nReplications = 999 \n               LRTS bootstrap p-value\n1 vs 2   203.573535             0.001\n2 vs 3    29.532613             0.001\n3 vs 4    25.665741             0.001\n4 vs 5     8.464976             0.074\n\nCodeplot(\n  tiersWR_boostrap,\n  G = numTiersWR - 1)\n\n\n\n\n\n\n\n\n21.1.4.2.4 Tight Ends\n\nCodetiersTE_bic &lt;- mclust::mclustBIC(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  G = 1:9\n)\n\ntiersTE_bic\n\nBayesian Information Criterion (BIC): \n          E         V\n1 -1416.311 -1416.311\n2 -1382.530 -1330.306\n3 -1392.221 -1305.417\n4 -1401.914 -1304.670\n5 -1370.398 -1314.375\n6 -1380.110 -1322.054\n7 -1387.386 -1329.543\n8 -1397.037 -1343.259\n9 -1406.769 -1349.787\n\nTop 3 models based on the BIC criterion: \n      V,4       V,3       V,5 \n-1304.670 -1305.417 -1314.375 \n\nCodesummary(tiersTE_bic)\n\nBest BIC values:\n              V,4           V,3          V,5\nBIC      -1304.67 -1305.4171376 -1314.374518\nBIC diff     0.00    -0.7472878    -9.704669\n\nCodeplot(tiersTE_bic)\n\n\n\n\n\n\nCodetiersTE_icl &lt;- mclust::mclustICL(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  G = 1:9\n)\n\ntiersTE_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n          E         V\n1 -1416.311 -1416.311\n2 -1393.104 -1350.405\n3 -1524.763 -1331.375\n4 -1592.916 -1341.536\n5 -1569.134 -1358.678\n6 -1611.364 -1360.491\n7 -1616.459 -1360.443\n8 -1650.436 -1392.210\n9 -1687.470 -1383.417\n\nTop 3 models based on the ICL criterion: \n      V,3       V,4       V,2 \n-1331.375 -1341.536 -1350.405 \n\nCodesummary(tiersTE_icl)\n\nBest ICL values:\n               V,3         V,4         V,2\nICL      -1331.375 -1341.53615 -1350.40527\nICL diff     0.000   -10.16078   -19.02991\n\nCodeplot(tiersTE_icl)\n\n\n\n\n\n\nCodetiersTE_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  modelName = \"V\") # variable/unequal variance (for univariate data)\n\nnumTiersTE &lt;- as.numeric(summary(tiersTE_boostrap)[,\"Length\"][1]) # or could specify the number of teams manually\n\ntiersTE_boostrap\n\n------------------------------------------------------------- \nBootstrap sequential LRT for the number of mixture components \n------------------------------------------------------------- \nModel        = V \nReplications = 999 \n               LRTS bootstrap p-value\n1 vs 2   100.537455             0.001\n2 vs 3    39.421427             0.001\n3 vs 4    15.279849             0.005\n4 vs 5     4.827893             0.237\n\nCodeplot(\n  tiersTE_boostrap,\n  G = numTiersTE - 1)\n\n\n\n\n\n\n\n\n21.1.4.3 Fit the Cluster Model to the Optimal Number of Tiers\n\n21.1.4.3.1 Quarterbacks\nIn our data, all of the following models are equivalent—i.e., they result in the same unequal variance model with a 4-cluster solution—but they arrive there in different ways. We can fit the cluster model to the optimal number of tiers using the mclust::Mclust() function.\n\nCodemclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = numTiersQB,\n)\n\nmclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = 4,\n)\n\nmclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n)\n\nmclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  x = tiersQB_bic\n)\n\n\nLet’s fit one of these:\n\nCodeclusterModelQBs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentQB$fantasyPoints,\n  G = numTiersQB,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelQBs$classification)\n\n\n 1  2  3  4 \n11 20 26 21 \n\n\n\n21.1.4.3.2 Running Backs\n\nCodeclusterModelRBs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentRB$fantasyPoints,\n  G = numTiersRB,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelRBs$classification)\n\n\n 1  2  3 \n39 61 58 \n\n\n\n21.1.4.3.3 Wide Receivers\n\nCodeclusterModelWRs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentWR$fantasyPoints,\n  G = numTiersWR,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelWRs$classification)\n\n\n 1  2  3  4 \n58 51 68 58 \n\n\n\n21.1.4.3.4 Tight Ends\n\nCodeclusterModelTEs &lt;- mclust::Mclust(\n  data = player_stats_seasonal_offense_recentTE$fantasyPoints,\n  G = numTiersTE,\n)\n\n\nHere are the number of players that are in each of the four clusters (i.e., tiers):\n\nCodetable(clusterModelTEs$classification)\n\n\n 1  2  3  4 \n24 32 29 42 \n\n\n\n21.1.4.4 Plot the Tiers\nWe can merge the player’s classification into the dataset and plot each player’s classification.\n\n21.1.4.4.1 Quarterbacks\n\nCodeplayer_stats_seasonal_offense_recentQB$tier &lt;- clusterModelQBs$classification\n\nplayer_stats_seasonal_offense_recentQB &lt;- player_stats_seasonal_offense_recentQB %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentQB$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentQB$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_qbTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentQB,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Quarterback Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nplotly::ggplotly(plot_qbTiers)\n\n\n\n\n\n\nFigure 21.1: Quarterback Fantasy Points by Tier.\n\n\n\n\n21.1.4.4.2 Running Backs\n\nCodeplayer_stats_seasonal_offense_recentRB$tier &lt;- clusterModelRBs$classification\n\nplayer_stats_seasonal_offense_recentRB &lt;- player_stats_seasonal_offense_recentRB %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentRB$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentRB$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_rbTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentRB,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Running Back Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nplotly::ggplotly(plot_rbTiers)\n\n\n\n\n\n\nFigure 21.2: Running Back Fantasy Points by Tier.\n\n\n\n\n21.1.4.4.3 Wide Receivers\n\nCodeplayer_stats_seasonal_offense_recentWR$tier &lt;- clusterModelWRs$classification\n\nplayer_stats_seasonal_offense_recentWR &lt;- player_stats_seasonal_offense_recentWR %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentWR$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentWR$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_wrTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentWR,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Wide Receiver Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nplotly::ggplotly(plot_wrTiers)\n\n\n\n\n\n\nFigure 21.3: Quarterback Fantasy Points by Tier.\n\n\n\n\n21.1.4.4.4 Tight Ends\n\nCodeplayer_stats_seasonal_offense_recentTE$tier &lt;- clusterModelTEs$classification\n\nplayer_stats_seasonal_offense_recentTE &lt;- player_stats_seasonal_offense_recentTE %&gt;%\n  mutate(\n    tier = factor(max(tier, na.rm = TRUE) + 1 - tier)\n  )\n\nplayer_stats_seasonal_offense_recentTE$position_rank &lt;- rank(\n  player_stats_seasonal_offense_recentTE$fantasyPoints * -1,\n  na.last = \"keep\",\n  ties.method = \"min\")\n\nplot_teTiers &lt;- ggplot2::ggplot(\n  data = player_stats_seasonal_offense_recentTE,\n  mapping = aes(\n    x = fantasyPoints,\n    y = position_rank,\n    color = tier\n  )) +\n  geom_point(\n    aes(\n      text = player_display_name # add player name for mouse over tooltip\n  )) +\n  scale_y_continuous(trans = \"reverse\") +\n  coord_cartesian(clip = \"off\") +\n  labs(\n    x = \"Projected Points\",\n    y = \"Position Rank\",\n    title = \"Tight End Fantasy Points by Tier\",\n    color = \"Tier\") +\n  theme_classic() +\n  theme(legend.position = \"top\")\n\nplotly::ggplotly(plot_teTiers)\n\n\n\n\n\n\nFigure 21.4: Tight End Fantasy Points by Tier.\n\n\n\n\n21.1.5 Types of Wide Receivers\n\nCode# Compute Advanced PFR Stats by Career\npfrVars &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  select(pocket_time.pass:cmp_percent.def, g, gs) %&gt;% \n  names()\n\nweightedAverageVars &lt;- c(\n  \"pocket_time.pass\",\n  \"ybc_att.rush\",\"yac_att.rush\",\n  \"ybc_r.rec\",\"yac_r.rec\",\"adot.rec\",\"rat.rec\",\n  \"yds_cmp.def\",\"yds_tgt.def\",\"dadot.def\",\"m_tkl_percent.def\",\"rat.def\"\n)\n\nrecomputeVars &lt;- c(\n  \"drop_pct.pass\", # drops.pass / pass_attempts.pass\n  \"bad_throw_pct.pass\", # bad_throws.pass / pass_attempts.pass\n  \"on_tgt_pct.pass\", # on_tgt_throws.pass / pass_attempts.pass\n  \"pressure_pct.pass\", # times_pressured.pass / pass_attempts.pass\n  \"drop_percent.rec\", # drop.rec / tgt.rec\n  \"rec_br.rec\", # rec.rec / brk_tkl.rec\n  \"cmp_percent.def\" # cmp.def / tgt.def\n)\n\nsumVars &lt;- pfrVars[pfrVars %ni% c(\n  weightedAverageVars, recomputeVars,\n  \"merge_name\", \"loaded.pass\", \"loaded.rush\", \"loaded.rec\", \"loaded.def\")]\n\nnfl_advancedStatsPFR_career &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  group_by(pfr_id, merge_name) %&gt;% \n  summarise(\n    across(all_of(weightedAverageVars), ~ weighted.mean(.x, w = g, na.rm = TRUE)),\n    across(all_of(sumVars), ~ sum(.x, na.rm = TRUE)),\n    .groups = \"drop\") %&gt;% \n  mutate(\n    drop_pct.pass = drops.pass / pass_attempts.pass,\n    bad_throw_pct.pass = bad_throws.pass / pass_attempts.pass,\n    on_tgt_pct.pass = on_tgt_throws.pass / pass_attempts.pass,\n    pressure_pct.pass = times_pressured.pass / pass_attempts.pass,\n    drop_percent.rec = drop.rec / tgt.rec,\n    rec_br.rec = drop.rec / tgt.rec,\n    cmp_percent.def = cmp.def / tgt.def\n  )\n\nuniqueCases &lt;- nfl_advancedStatsPFR_seasonal %&gt;% select(pfr_id, merge_name, gsis_id) %&gt;% unique()\n\nuniqueCases %&gt;%\n  group_by(pfr_id) %&gt;% \n  filter(n() &gt; 1)\n\n\n  \n\n\nCodenfl_advancedStatsPFR_seasonal &lt;- nfl_advancedStatsPFR_seasonal %&gt;% \n  filter(pfr_id != \"WillMa06\" | merge_name != \"MARCUSWILLIAMS\" | !is.na(gsis_id))\n\n\nnfl_advancedStatsPFR_career &lt;- left_join(\n  nfl_advancedStatsPFR_career,\n  nfl_advancedStatsPFR_seasonal %&gt;% select(pfr_id, merge_name, gsis_id) %&gt;% unique(),\n  by = c(\"pfr_id\", \"merge_name\")\n)\n\n# Compute Player Stats Per Season\nplayer_stats_seasonal_careerWRs &lt;- player_stats_seasonal %&gt;% \n  filter(position == \"WR\") %&gt;% \n  group_by(player_id) %&gt;% \n  summarise(\n    across(all_of(c(\"targets\", \"receptions\", \"receiving_air_yards\")), ~ weighted.mean(.x, w = games, na.rm = TRUE)),\n    .groups = \"drop\")\n\n# Drop players with no receiving air yards\nplayer_stats_seasonal_careerWRs &lt;- player_stats_seasonal_careerWRs %&gt;% \n  filter(receiving_air_yards != 0) %&gt;% \n  rename(\n    targets_per_season = targets,\n    receptions_per_season = receptions,\n    receiving_air_yards_per_season = receiving_air_yards\n  )\n\n# Merge\nplayerListToMerge &lt;- list(\n  nfl_players %&gt;% select(gsis_id, display_name, position, height, weight),\n  nfl_combine %&gt;% select(gsis_id, vertical, forty, ht, wt),\n  player_stats_seasonal_careerWRs %&gt;% select(player_id, targets_per_season, receptions_per_season, receiving_air_yards_per_season) %&gt;% \n    rename(gsis_id = player_id),\n  nfl_actualStats_career_player_inclPost %&gt;% select(player_id, receptions, targets, receiving_air_yards, air_yards_share, target_share) %&gt;% \n    rename(gsis_id = player_id),\n  nfl_advancedStatsPFR_career %&gt;% select(gsis_id, adot.rec, rec.rec, brk_tkl.rec, drop.rec, drop_percent.rec)\n)\n\nmerged_data &lt;- playerListToMerge %&gt;% \n  reduce(\n    full_join,\n    by = c(\"gsis_id\"),\n    na_matches = \"never\")\n\n\nAdditional processing:\n\nCodemerged_data &lt;- merged_data %&gt;% \n  mutate(\n    height_coalesced = coalesce(height, ht),\n    weight_coalesced = coalesce(weight, wt),\n    receptions_coalesced = pmax(receptions, rec.rec, na.rm = TRUE),\n    receiving_air_yards_per_rec = receiving_air_yards / receptions\n  )\n\nmerged_data$receiving_air_yards_per_rec[which(merged_data$receptions == 0)] &lt;- 0\n\nmerged_dataWRs &lt;- merged_data %&gt;% \n  filter(position == \"WR\")\n\nmerged_dataWRs_cluster &lt;- merged_dataWRs %&gt;% \n  filter(receptions_coalesced &gt;= 100) %&gt;% # keep WRs with at least 100 receptions\n  select(gsis_id, display_name, vertical, forty, height_coalesced, weight_coalesced, adot.rec, drop_percent.rec, receiving_air_yards_per_rec, brk_tkl.rec, receptions_per_season) %&gt;% #targets_per_season, receiving_air_yards_per_season, air_yards_share, target_share\n  na.omit()\n\n\n\n21.1.5.1 Identify the Number of WR Types\n\nCodewrTypes_bic &lt;- mclust::mclustBIC(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  G = 1:9\n)\n\nwrTypes_bic\n\nBayesian Information Criterion (BIC): \n        EII       VII       EEI       VEI       EVI       VVI       EEE\n1 -8521.963 -8521.963 -5185.509 -5185.509 -5185.509 -5185.509 -4992.434\n2 -8104.831 -8073.802 -5145.239 -5136.649 -5018.457 -5146.317 -5017.149\n3 -7957.960 -7920.971 -5080.707 -5085.739 -4994.395 -4997.246 -5011.675\n4 -7836.962 -7767.059 -5070.012 -5038.031 -4991.076 -4973.775 -4980.329\n5 -7743.153 -7718.246 -5064.335 -5045.676 -5010.565 -4996.813 -4958.735\n6 -7753.031 -7705.521 -5078.853 -5069.783 -5011.298 -5033.772 -5040.084\n7 -7764.288 -7721.187 -5081.158 -5045.356 -5062.471 -5060.530 -5014.736\n8 -7756.644 -7672.340 -5072.408 -5061.930 -5098.986 -5107.732 -5064.086\n9 -7774.246        NA -5077.748        NA        NA        NA -4962.765\n        VEE       EVE       VVE       EEV       VEV       EVV       VVV\n1 -4992.434 -4992.434 -4992.434 -4992.434 -4992.434 -4992.434 -4992.434\n2 -4989.172 -4756.142 -4759.198 -4808.728 -4886.250 -4886.723 -4900.238\n3 -4995.917 -4717.684 -4712.338 -4918.197 -4876.018 -4955.641 -4928.440\n4 -4912.841 -4757.765 -4762.691 -5141.402 -5159.351 -5136.086 -5168.938\n5 -4893.903 -4814.817 -4836.123 -5195.286 -5260.706 -5325.169 -5370.854\n6 -4978.477 -4853.550 -4845.930 -5345.145 -5361.930 -5494.314 -5489.821\n7 -4981.574 -4896.554 -4898.030 -5492.422 -5569.211 -5697.334 -5683.734\n8 -5011.101 -4945.459 -4947.183 -5658.904 -5695.686 -5846.447 -5867.393\n9        NA        NA        NA -5823.011        NA        NA        NA\n\nTop 3 models based on the BIC criterion: \n    VVE,3     EVE,3     EVE,2 \n-4712.338 -4717.684 -4756.142 \n\nCodesummary(wrTypes_bic)\n\nBest BIC values:\n             VVE,3        EVE,3       EVE,2\nBIC      -4712.338 -4717.684010 -4756.14230\nBIC diff     0.000    -5.345829   -43.80412\n\nCodeplot(wrTypes_bic)\n\n\n\n\n\n\nCodewrTypes_icl &lt;- mclust::mclustICL(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  G = 1:9\n)\n\nwrTypes_icl\n\nIntegrated Complete-data Likelihood (ICL) criterion: \n        EII       VII       EEI       VEI       EVI       VVI       EEE\n1 -8521.963 -8521.963 -5185.509 -5185.509 -5185.509 -5185.509 -4992.434\n2 -8110.421 -8080.036 -5164.217 -5152.012 -5032.439 -5167.841 -5025.798\n3 -7968.947 -7928.577 -5101.176 -5106.284 -5009.569 -5013.301 -5027.893\n4 -7846.600 -7775.743 -5091.038 -5055.367 -5012.572 -4993.914 -4995.932\n5 -7754.336 -7729.565 -5091.398 -5069.961 -5030.357 -5019.417 -4973.963\n6 -7770.496 -7713.518 -5105.656 -5092.076 -5029.966 -5051.763 -5057.254\n7 -7786.260 -7733.662 -5108.242 -5064.440 -5082.236 -5070.226 -5032.586\n8 -7775.389 -7684.201 -5096.702 -5080.609 -5112.700 -5116.636 -5094.738\n9 -7791.055        NA -5105.526        NA        NA        NA -4981.214\n        VEE       EVE       VVE       EEV       VEV       EVV       VVV\n1 -4992.434 -4992.434 -4992.434 -4992.434 -4992.434 -4992.434 -4992.434\n2 -5004.174 -4760.848 -4765.868 -4809.428 -4888.714 -4887.183 -4902.365\n3 -5006.245 -4735.419 -4725.314 -4919.292 -4876.541 -4963.354 -4930.929\n4 -4922.883 -4772.627 -4775.317 -5149.339 -5166.186 -5144.031 -5174.419\n5 -4904.516 -4828.564 -4852.419 -5197.371 -5263.216 -5327.647 -5374.328\n6 -4995.230 -4864.618 -4862.199 -5346.234 -5365.514 -5496.064 -5492.120\n7 -4994.700 -4907.920 -4907.062 -5493.562 -5570.041 -5698.361 -5685.074\n8 -5024.572 -4954.210 -4954.163 -5659.382 -5696.321 -5846.660 -5868.538\n9        NA        NA        NA -5823.465        NA        NA        NA\n\nTop 3 models based on the ICL criterion: \n    VVE,3     EVE,3     EVE,2 \n-4725.314 -4735.419 -4760.848 \n\nCodesummary(wrTypes_icl)\n\nBest ICL values:\n             VVE,3       EVE,3       EVE,2\nICL      -4725.314 -4735.41901 -4760.84753\nICL diff     0.000   -10.10454   -35.53306\n\nCodeplot(wrTypes_icl)\n\n\n\n\n\n\n\nBased on the cluster analyses, it appears that three clusters are the best fit to the data.\n\nCodenumTypesWR &lt;- 3\n\n\n\nCodewrTypes_boostrap &lt;- mclust::mclustBootstrapLRT(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  modelName = \"EVE\") # ellipsoidal with equal volume, variable shape, and equal orientation (for multivariate data)\n\nwrTypes_boostrap\nplot(\n  wrTypes_boostrap,\n  G = numTypesWR - 1)\n\n\n\n21.1.5.2 Fit the Cluster Model to the Optimal Number of WR Types\n\nCodeclusterModelWRtypes &lt;- mclust::Mclust(\n  data = merged_dataWRs_cluster %&gt;% select(-gsis_id, -display_name),\n  G = numTypesWR,\n)\n\nsummary(clusterModelWRtypes)\n\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVE (ellipsoidal, equal orientation) model with 3 components: \n\n log-likelihood   n df       BIC       ICL\n      -2133.336 127 92 -4712.338 -4725.314\n\nClustering table:\n 1  2  3 \n29 22 76 \n\n\n\n21.1.5.3 Plots of the Cluster Model\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"BIC\")\n\n\n\n\n\n\n\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"classification\")\n\n\n\n\n\n\n\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"uncertainty\")\n\n\n\n\n\n\n\n\nCodeplot(\n  clusterModelWRtypes,\n  what = \"density\")\n\n\n\n\n\n\n\n\n21.1.5.4 Interpreting the Clusters\n\nCodetable(clusterModelWRtypes$classification)\n\n\n 1  2  3 \n29 22 76 \n\nCodemerged_dataWRs_cluster$type &lt;- clusterModelWRtypes$classification\n\nmerged_dataWRs_cluster %&gt;% \n  group_by(type) %&gt;% \n  summarise(across(\n    where(is.numeric),\n    ~ mean(., na.rm = TRUE)\n    )) %&gt;% \n  t() %&gt;% \n  round(., 2)\n\n                              [,1]   [,2]   [,3]\ntype                          1.00   2.00   3.00\nvertical                     36.53  36.25  35.91\nforty                         4.47   4.45   4.47\nheight_coalesced             73.21  72.73  72.53\nweight_coalesced            207.21 199.95 198.24\nadot.rec                     10.18  12.30  10.35\ndrop_percent.rec              0.04   0.06   0.05\nreceiving_air_yards_per_rec  15.85  22.09  17.16\nbrk_tkl.rec                  27.14   0.64   8.39\nreceptions_per_season        78.88  37.52  45.76\n\n\nBased on this analysis (and the variables included), there appear to be three types of Wide Receivers. We examined the following variables: the player’s vertical jump in the NFL Combine,40-yard-dash time in the NFL Combine, height, weight, average depth of target, drop percentage, receiving air yards per reception, broken tackles, and receptions per season.\nType 1 Wide Receivers included the Elite WR1s who are strong possession receivers (note: not all players in a given cluster map on perfectly to the typology—i.e., not all Type 1 Wide Receivers are elite WR1s). They tended to have the lowest drop percentage, the shortest average depth of target, and the fewest receiving air yards per reception. They tended to have the most receptions per season and break the most tackles.\nType 2 Wide Receivers included the consistent contributor, WR2 types. They had fewer receptions and fewer broken tackles than Type 1 Wide Receivers. Their average depth of target was longer than Type 1, and they had more receiving air yards per reception than Type 1.\nType 3 Wide Receivers included the deep threats. They had the greatest average depth of target and the most receiving yards per reception. However, they also had the fewest receptions, the highest drop percentage, and the fewest broken tackles. Thus, they may be considered the boom-or-bust Wide Receivers.\nThe tiers were not particularly distinguishable based on their height, weight, vertical jump, or forty-yard dash time.\nType 1 (“Elite/WR1”) WRs:\n\nCodemerged_dataWRs_cluster %&gt;% \n  filter(type == 1) %&gt;% \n  select(display_name)\n\n\n  \n\n\n\nType 2 (“Consistent Contributor/WR2”) WRs:\n\nCodemerged_dataWRs_cluster %&gt;% \n  filter(type == 2) %&gt;% \n  select(display_name)\n\n\n  \n\n\n\nType 3 (“Deep Threat/Boom-or-Bust”) WRs:\n\nCodemerged_dataWRs_cluster %&gt;% \n  filter(type == 3) %&gt;% \n  select(display_name)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#sec-clusterAnalysisConclusion",
    "href": "cluster-analysis.html#sec-clusterAnalysisConclusion",
    "title": "21  Cluster Analysis",
    "section": "\n21.2 Conclusion",
    "text": "21.2 Conclusion\nThe goal of cluster analysis is to identify distinguishable subgroups of people. There are many approaches to cluster analysis, including model-based clustering, density-based clustering, centroid-based clustering, hierarchical clustering (aka connectivity-based clustering), and others. The present chapter used model-based clustering to identify tiers of players based on projected points. Using various performance metrics of Wide Receivers, we identified three subtypes of Wide Receivers: 1) Elite WR1s who are strong possession receivers; 2) Consistent Contributor/WR2s; 3) deep threats/boom-or-bust receivers. The “Elite WR1s” tended to have the lowest drop percentage, the shortest average depth of target, the fewest receiving air yards per reception, the most receptions per season, and the most broken tackles. The “Consistent Contributor/WR2s” had fewer receptions and fewer broken tackles than the Elite WR1s; their average depth of target was longer than Elite WR1s, and they had more receiving air yards per reception than Elite WR1s. The “Deep Threat/Boom-or-Bust” receivers had the greatest average depth of target and the most receiving yards per reception; however, they also had the fewest receptions, the highest drop percentage, and the fewest broken tackles. In sum, cluster analysis can be a useful way of identifying subgroups of individuals who are more similar to one another on various characteristics.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#sec-clusterAnalysisSessionInfo",
    "href": "cluster-analysis.html#sec-clusterAnalysisSessionInfo",
    "title": "21  Cluster Analysis",
    "section": "\n21.3 Session Info",
    "text": "21.3 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n [9] tidyverse_2.0.0   plotly_4.11.0     ggplot2_3.5.2     mclust_6.1.1     \n[13] nflreadr_1.4.1    petersenlab_1.2.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   psych_2.5.6        viridisLite_0.4.2  farver_2.1.2      \n [5] fastmap_1.2.0      lazyeval_0.2.2     digest_0.6.37      rpart_4.1.24      \n [9] timechange_0.3.0   lifecycle_1.0.4    cluster_2.1.8.1    magrittr_2.0.3    \n[13] compiler_4.5.1     rlang_1.1.6        Hmisc_5.2-3        tools_4.5.1       \n[17] yaml_2.3.10        data.table_1.17.8  knitr_1.50         labeling_0.4.3    \n[21] htmlwidgets_1.6.4  mnormt_2.1.1       plyr_1.8.9         RColorBrewer_1.1-3\n[25] foreign_0.8-90     withr_3.0.2        nnet_7.3-20        grid_4.5.1        \n[29] stats4_4.5.1       lavaan_0.6-19      xtable_1.8-4       colorspace_2.1-1  \n[33] scales_1.4.0       MASS_7.3-65        cli_3.6.5          mvtnorm_1.3-3     \n[37] rmarkdown_2.29     reformulas_0.4.1   generics_0.1.4     rstudioapi_0.17.1 \n[41] tzdb_0.5.0         httr_1.4.7         reshape2_1.4.4     minqa_1.2.8       \n[45] DBI_1.2.3          cachem_1.1.0       splines_4.5.1      parallel_4.5.1    \n[49] base64enc_0.1-3    mitools_2.4        vctrs_0.6.5        boot_1.3-31       \n[53] Matrix_1.7-3       jsonlite_2.0.0     hms_1.1.3          Formula_1.2-5     \n[57] htmlTable_2.4.3    crosstalk_1.2.1    glue_1.8.0         nloptr_2.2.1      \n[61] stringi_1.8.7      gtable_0.3.6       quadprog_1.5-8     lme4_1.1-37       \n[65] pillar_1.11.0      htmltools_0.5.8.1  R6_2.6.1           Rdpack_2.6.4      \n[69] mix_1.0-13         evaluate_1.0.4     pbivnorm_0.6.0     lattice_0.22-7    \n[73] rbibutils_2.3      backports_1.5.0    memoise_2.0.1      Rcpp_1.1.0        \n[77] gridExtra_2.3      nlme_3.1-168       checkmate_2.3.3    xfun_0.53         \n[81] pkgconfig_2.0.3   \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFraley, C., Raftery, A. E., & Scrucca, L. (2024). mclust: Gaussian mixture modelling for model-based clustering, classification, and density estimation. https://doi.org/10.32614/CRAN.package.mclust\n\n\nKassambara, A. (2017). Practical guide to cluster analysis in R: Unsupervised machine learning (Vol. 1). Sthda.\n\n\nScrucca, L., Fraley, C., Murphy, T. B., & Raftery, A. E. (2023). Model-based clustering, classification, and density estimation using mclust in R. Chapman; Hall/CRC. https://doi.org/10.1201/9781003277965",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Cluster Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html",
    "href": "factor-analysis.html",
    "title": "22  Factor Analysis",
    "section": "",
    "text": "22.1 Getting Started\nThis chapter provides an overview of factor analysis, which encompasses a range of latent variable modeling approaches, including exploratory and confirmatory factor analysis.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisGettingStarted",
    "href": "factor-analysis.html#sec-factorAnalysisGettingStarted",
    "title": "22  Factor Analysis",
    "section": "",
    "text": "22.1.1 Load Packages\n\nCodelibrary(\"psych\")\nlibrary(\"nFactors\")\nlibrary(\"lavaan\")\nlibrary(\"lavaanPlot\")\nlibrary(\"lavaangui\")\nlibrary(\"tidyverse\")\n\n\n\n22.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/nfl_nextGenStats_weekly.RData\")\n\n\n\n22.1.3 Prepare Data\n\n22.1.3.1 Season-Averages\n\nCodeplayer_stats_seasonal_avgPerGame &lt;- player_stats_seasonal %&gt;% \n  mutate(\n    completionsPerGame = completions / games,\n    attemptsPerGame = attempts / games,\n    passing_yardsPerGame = passing_yards / games,\n    passing_tdsPerGame = passing_tds / games,\n    passing_interceptionsPerGame = passing_interceptions / games,\n    sacks_sufferedPerGame = sacks_suffered / games,\n    sack_yards_lostPerGame = sack_yards_lost / games,\n    sack_fumblesPerGame = sack_fumbles / games,\n    sack_fumbles_lostPerGame = sack_fumbles_lost / games,\n    passing_air_yardsPerGame = passing_air_yards / games,\n    passing_yards_after_catchPerGame = passing_yards_after_catch / games,\n    passing_first_downsPerGame = passing_first_downs / games,\n    #passing_epaPerGame = passing_epa / games,\n    #passing_cpoePerGame = passing_cpoe / games,\n    passing_2pt_conversionsPerGame = passing_2pt_conversions / games,\n    #pacrPerGame = pacr / games\n    pass_40_ydsPerGame = pass_40_yds / games,\n    pass_incPerGame = pass_inc / games,\n    #pass_comp_pctPerGame = pass_comp_pct / games,\n    fumblesPerGame = fumbles / games\n  )\n\nnfl_nextGenStats_seasonal &lt;- nfl_nextGenStats_weekly %&gt;% \n  filter(season_type == \"REG\") %&gt;% \n  select(-any_of(c(\"week\",\"season_type\",\"player_display_name\",\"player_position\",\"team_abbr\",\"player_first_name\",\"player_last_name\",\"player_jersey_number\",\"player_short_name\"))) %&gt;% \n  group_by(player_gsis_id, season) %&gt;% \n  summarise(\n    across(everything(),\n      ~ mean(.x, na.rm = TRUE)),\n    .groups = \"drop\")\n\n\n\n22.1.3.2 Merge Data\n\nCodedataMerged &lt;- full_join(\n  player_stats_seasonal_avgPerGame %&gt;% select(-any_of(\"week\")),\n  nfl_nextGenStats_seasonal %&gt;% select(-any_of(c(\"player_display_name\",\"completions\",\"attempts\",\"receptions\",\"targets\"))),\n  by = c(\"player_id\" = \"player_gsis_id\",\"season\"),\n)\n\n\n\n22.1.3.3 Specify Variables\n\nCodefaVars &lt;- c(\n \"completionsPerGame\",\"attemptsPerGame\",\"passing_yardsPerGame\",\"passing_tdsPerGame\",\n \"passing_air_yardsPerGame\",\"passing_yards_after_catchPerGame\",\"passing_first_downsPerGame\",\n \"avg_completed_air_yards\",\"avg_intended_air_yards\",\"aggressiveness\",\"max_completed_air_distance\",\n \"avg_air_distance\",\"max_air_distance\",\"avg_air_yards_to_sticks\",\"passing_cpoe\",\"pass_comp_pct\",\n \"passer_rating\",\"completion_percentage_above_expectation\"\n)\n\n\n\n22.1.3.4 Subset Data\n\nCodedataMerged_subset &lt;- dataMerged %&gt;% \n  filter(!is.na(player_id)) %&gt;% \n  filter(position == \"QB\")\n\n\n\n22.1.3.5 Standardize Variables\nStandardizing variables is not strictly necessary in factor analysis; however, standardization can be helpful to prevent some variables from having considerably larger variances than others.\n\nCodedataForFA &lt;- dataMerged_subset\ndataForFA_standardized &lt;- dataForFA\n\ndataForFA_standardized[faVars] &lt;- scale(dataForFA_standardized[faVars])",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisOverview",
    "href": "factor-analysis.html#sec-factorAnalysisOverview",
    "title": "22  Factor Analysis",
    "section": "\n22.2 Overview of Factor Analysis",
    "text": "22.2 Overview of Factor Analysis\nFactor analysis involves the estimation of latent variables. Latent variables are ways of studying and operationalizing theoretical constructs that cannot be directly observed or quantified. Factor analysis is a class of latent variable models that is designated to identify the structure of a measure or set of measures, and ideally, a construct or set of constructs. It aims to identify the optimal latent structure for a group of variables. The goal of factor analysis is to identify simple, parsimonious factors that underlie the “junk” (i.e., scores filled with measurement error) that we observe.\nFactor analysis encompasses two general types: confirmatory factor analysis and exploratory factor analysis. Exploratory factor analysis (EFA) is a latent variable modeling approach that is used when the researcher has no a priori hypotheses about how a set of variables is structured. EFA seeks to identify the empirically optimal-fitting model in ways that balance accuracy (i.e., variance accounted for) and parsimony (i.e., simplicity). Confirmatory factor analysis (CFA) is a latent variable modeling approach that is used when a researcher wants to evaluate how well a hypothesized model fits, and the model can be examined in comparison to alternative models. Using a CFA approach, the researcher can pit models representing two theoretical frameworks against each other to see which better accounts for the observed data.\nFactor analysis involves observed (manifest) variables and unobserved (latent) factors. Factor analysis assumes that the latent factor influences the manifest variables, and the latent factor therefore reflects the common variance among the variables. A factor model potentially includes factor loadings, residuals (errors or disturbances), intercepts/means, covariances, and regression paths. When depicting a factor analysis model, rectangles represent variables we observe (i.e., manifest variables), and circles represent latent (i.e., unobserved) variables. A regression path indicates a hypothesis that one variable (or factor) influences another, and it is depicted using a single-headed arrow. The standardized regression coefficient (i.e., beta or \\(\\beta\\)) represents the strength of association between the variables or factors. A factor loading is a regression path from a latent factor to an observed (manifest) variable. The standardized factor loading represents the strength of association between the variable and the latent factor, where conceptually, it is intended to reflect the magnitude that the latent factor influences the observed variable. A residual is variance in a variable (or factor) that is unexplained by other variables or factors. A variable’s intercept is the expected value of the variable when the factor(s) (onto which it loads) is equal to zero. A covariance is the unstandardized index of the strength of association between between two variables (or factors), and it is depicted with a double-headed arrow.\nBecause a covariance is unstandardized, its scale depends on the scale of the variables. The covariance between two variables is the average product of their deviations from their respective means, as in Equation 10.1. The covariance of a variable with itself is equivalent to its variance, as in Equation 10.2. By contrast, a correlation is a standardized index of the strength of association between two variables. Because a correlation is standardized (fixed between [−1,1]), its scale does not depend on the scales of the variables. Because a covariance is unstandardized, its scale depends on the scale of the variables. A covariance path between two variables represents omitted shared cause(s) of the variables. For instance, if you depict a covariance path between two variables, it means that there is a shared cause of the two variables that is omitted from the model (for instance, if the common cause is not known or was not assessed).\nIn factor analysis, the relation between an indicator (\\(\\text{X}\\)) and its underlying latent factor(s) (\\(\\text{F}\\)) can be represented with a regression formula as in Equation 22.1:\n\\[\nX = \\lambda \\cdot \\text{F} + \\text{Item Intercept} + \\text{Error Term}\n\\tag{22.1}\\]\nwhere:\n\n\n\\(\\text{X}\\) is the observed value of the indicator\n\n\\(\\lambda\\) is the factor loading, indicating the strength of the association between the indicator and the latent factor(s)\n\n\\(\\text{F}\\) is the person’s value on the latent factor(s)\n\n\\(\\text{Item Intercept}\\) represents the constant term that accounts for the expected value of the indicator when the latent factor(s) are zero\n\n\\(\\text{Error Term}\\) is the residual, indicating the extent of variance in the indicator that is not explained by the latent factor(s)\n\nWhen the latent factors are uncorrelated, the (standardized) error term for an indicator is calculated as 1 minus the sum of squared standardized factor loadings for a given item (including cross-loadings). A cross-loading is when a variable loads onto more than one latent factor.\nFactor analysis is a powerful technique to help identify the factor structure that underlies a measure or construct. However, given the extensive method variance that influences scores on measure, factor analysis (and principal component analysis) tends to extract method factors. Method factors are factors that are related to the methods being assessed rather than the construct of interest. To better estimate construct factors, it is sometimes necessary to estimate both construct and method factors.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#factor-analysis-and-structural-equation-modeling",
    "href": "factor-analysis.html#factor-analysis-and-structural-equation-modeling",
    "title": "22  Factor Analysis",
    "section": "\n22.3 Factor Analysis and Structural Equation Modeling",
    "text": "22.3 Factor Analysis and Structural Equation Modeling\nFactor analysis forms the measurement model component of a structural equation model (SEM). The measurement model is what we settle on as the estimation of each construct before we add the structural component to estimate the relations among latent variables. Basically, in a structural equation model, we add the structural component onto the measurement model. For instance, our measurement model (i.e., based on factor analyis) might be to estimate, from a set of items, three latent factors: usage, aggressiveness, and performance. Our structural model then may examine what processes (e.g., sport drink consumption or sleep) influence these latent factors, how the latent factors influence each other, and what the latent factors influence (e.g., fantasy points). SEM is confirmatory factor analysis with regression paths that specify hypothesized causal relations between the latent variables (the structural component of the model). Exploratory structural equation modeling (ESEM) is a form of SEM that allows for a combination of exploratory factor analysis and confirmatory factor analysis to estimate latent variables and the relations between them.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-pathDiagrams",
    "href": "factor-analysis.html#sec-pathDiagrams",
    "title": "22  Factor Analysis",
    "section": "\n22.4 Path Diagrams",
    "text": "22.4 Path Diagrams\nA key tool when designing a factor analysis or structural equation model is a conceptual depiction of the hypothesized causal processes. A path diagram depicts the hypothesized causal processes that link two or more variables. Path diagrams are an example of a causal diagram and are similar to directed acyclic graphs discussed in Section 13.6. Karch (2025a) provides a tool to create lavaan R syntax from a path analytic diagram: https://lavaangui.org.\nIn a path analysis diagram, rectangles represent variables we observe, and circles represent latent (i.e., unobserved) variables. Single-headed arrows indicate regression paths, where conceptually, one variable is thought to influence another variable. Double-headed arrows indicate covariance paths, where conceptually, two variables are associated for some unknown reason (i.e., an omitted shared cause).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisDecisions",
    "href": "factor-analysis.html#sec-factorAnalysisDecisions",
    "title": "22  Factor Analysis",
    "section": "\n22.5 Decisions in Factor Analysis",
    "text": "22.5 Decisions in Factor Analysis\nThere are five primary decisions to make in factor analysis:\n\nwhat variables to include in the model and how to scale them\nmethod of factor extraction: whether to use exploratory or confirmatory factor analysis\nif using exploratory factor analysis, whether and how to rotate factors\nhow many factors to retain (and what variables load onto which factors)\nhow to interpret and use the factors\n\nThe answer you get can differ highly depending on the decisions you make. Below, we provide guidance for each of these decisions.\n\n22.5.1 1. Variables to Include and their Scaling\nThe first decision when conducting a factor analysis is which variables to include and the scaling of those variables. What factors you extract can differ widely depending on what variables you include in the analysis. For example, if you include many variables from the same source (e.g., self-report), it is possible that you will extract a factor that represents the common variance among the variables from that source (i.e., the self-reported variables). This would be considered a method factor, which works against the goal of estimating latent factors that represent the constructs of interest (as opposed to the measurement methods used to estimate those constructs).\nAn additional consideration is the scaling of the variables: whether to use raw variables, standardized variables, or dividing some variables by a constant to make the variables’ variances more similar. Before performing a principal component analysis (PCA), it is generally important to ensure that the variables included in the PCA are on the same scale. PCA seeks to identify components that explain variance in the data, so if the variables are not on the same scale, some variables may contribute considerably more variance than others. A common way of ensuring that variables are on the same scale is to standardize them using, for example, z scores that have a mean of zero and standard deviation of one. By contrast, factor analysis can better accommodate variables that are on different scales.\n\n22.5.2 2. Method of Factor Extraction\n\n22.5.2.1 Exploratory Factor Analysis\nExploratory factor analysis (EFA) is used if you have no a priori hypotheses about the factor structure of the model, but you would like to understand the latent variables represented by your items.\nEFA is partly induced from the data. You feed in the data and let the program build the factor model. You can set some parameters going in, including how to extract or rotate the factors. The factors are extracted from the data without specifying the number and pattern of loadings between the items and the latent factors (Bollen, 2002). All cross-loadings are freely estimated.\n\n22.5.2.2 Confirmatory Factor Analysis\nConfirmatory factor analysis (CFA) is used to (dis)confirm a priori hypotheses about the factor structure of the model. CFA is a test of the hypothesis. In CFA, you specify the model and ask how well this model represents the data. The researcher specifies the number, meaning, associations, and pattern of free parameters in the factor loading matrix (Bollen, 2002). A key advantage of CFA is the ability to directly compare alternative models (i.e., factor structures), which is valuable for theory testing (Strauss & Smith, 2009). For instance, you could use CFA to test whether the variance in several measures’ scores is best explained with one factor or two factors. In CFA, cross-loadings are not estimated unless the researcher specifies them.\n\n22.5.3 3. Factor Rotation\nWhen using EFA or principal component analysis, an important step is, possibly, to rotate the factors to make them more interpretable and simple, which is the whole goal. To interpret the results of a factor analysis, we examine the factor matrix. The columns refer to the different factors; the rows refer to the different observed variables. The cells in the table are the factor loadings—they are basically the correlation between the variable and the factor. Our goal is to achieve a model with simple structure because it is easily interpretable. Simple structure means that every variable loads perfectly on one and only one factor, as operationalized by a matrix of factor loadings with values of one and zero and nothing else. An example of a factor matrix that follows simple structure is depicted in Figure 22.1.\n\n\n\n\n\nFigure 22.1: Example of a Factor Matrix That Follows Simple Structure. From Petersen (2024) and Petersen (2025).\n\n\nAn example of a factor analysis model that follows simple structure is depicted in Figure 22.2. Each variable loads onto one and only one factor, which makes it easy to interpret the meaning of each factor, because a given factor represents the common variance among the items that load onto it.\n\n\n\n\n\nFigure 22.2: Example of a Factor Analysis Model That Follows Simple Structure. ‘INT’ = internalizing problems; ‘EXT’ = externalizing problems; ‘TD’ = thought-disordered problems. From Petersen (2024) and Petersen (2025).\n\n\nHowever, pure simple structure only occurs in simulations, not in real-life data. In reality, our unrotated factor analysis model might look like the model in Figure 22.3. In this example, the factor analysis model does not show simple structure because the items have cross-loadings—that is, the items load onto more than one factor. The cross-loadings make it difficult to interpret the factors, because all of the items load onto all of the factors, so the factors are not very distinct from each other, which makes it difficult to interpret what the factors mean.\n\n\n\n\n\nFigure 22.3: Example of a Factor Analysis Model That Does Not Follow Simple Structure. ‘INT’ = internalizing problems; ‘EXT’ = externalizing problems; ‘TD’ = thought-disordered problems. From Petersen (2024) and Petersen (2025).\n\n\nAs a result of the challenges of interpretability caused by cross-loadings, factor rotations are often performed. An example of an unrotated factor matrix is in Figure 22.4.\n\n\n\n\n\nFigure 22.4: Example of a Factor Matrix. From Petersen (2024) and Petersen (2025).\n\n\nIn the example factor matrix in Figure 22.5, the factor analysis is not very helpful—it tells us very little because it did not distinguish between the two factors. The variables have similar loadings on Factor 1 and Factor 2. An example of a unrotated factor solution is in Figure 22.5. In the figure, all of the variables are in the midst of the quadrants—they are not on the factors’ axes. Thus, the factors are not very informative.\n\n\n\n\n\nFigure 22.5: Example of an Unrotated Factor Solution. From Petersen (2024) and Petersen (2025).\n\n\nAs a result, to improve the interpretability of the factor analysis, we can do what is called rotation. Rotation leverages the idea that there are infinite solutions to the factor analysis model that fit equally well. Rotation involves changing the orientation of the factors by changing the axes so that variables end up with very high (close to one or negative one) or very low (close to zero) loadings, so that it is clear which factors include which variables. That is, rotation rescales the factors and tries to identify the ideal solution (factor) for each variable. It searches for simple structure and keeps searching until it finds a minimum. After rotation, if the rotation was successful for imposing simple structure, each factor will have loadings close to one (or negative one) for some variables and close to zero for other variables. The goal of factor rotation is to achieve simple structure, to help make it easier to interpret the meaning of the factors.\nTo perform factor rotation, orthogonal rotations are often used. Orthogonal rotations make the rotated factors uncorrelated. An example of a commonly used orthogonal rotation is varimax rotation. Varimax rotation maximizes the sum of the variance of the squared loadings (i.e., so that items have either a very high or very low loading on a factor) and yields axes with a 90-degree angle.\nAn example of a factor matrix following an orthogonal rotation is depicted in Figure 22.6. An example of a factor solution following an orthogonal rotation is depicted in Figure 22.7.\n\n\n\n\n\nFigure 22.6: Example of a Rotated Factor Matrix. From Petersen (2024) and Petersen (2025).\n\n\n\n\n\n\n\nFigure 22.7: Example of a Rotated Factor Solution. From Petersen (2024) and Petersen (2025).\n\n\nAn example of a factor matrix from SPSS following an orthogonal rotation is depicted in Figure 22.8.\n\n\n\n\n\nFigure 22.8: Example of a Rotated Factor Matrix From SPSS.\n\n\nAn example of a factor structure from an orthogonal rotation is in Figure 22.9.\n\n\n\n\n\nFigure 22.9: Example of a Factor Structure From an Orthogonal Rotation. From Petersen (2024) and Petersen (2025).\n\n\nSometimes, however, the two factors and their constituent variables may be correlated. Examples of two correlated factors may be depression and anxiety. When the two factors are correlated in reality, if we make them uncorrelated, this would result in an inaccurate model. Oblique rotation allows for factors to be correlated and yields axes with less an angle of less than 90 degrees. However, if the factors have low correlation (e.g., .2 or less), you can likely continue with orthogonal rotation. Nevertheless, just because an oblique rotation allows for correlated factors does not mean that the factors will be correlated, so oblique rotation provides greater flexibility than orthogonal rotation. An example of a factor structure from an oblique rotation is in Figure 22.10. Results from an oblique rotation are more complicated than orthogonal rotation—they provide lots of output and are more complicated to interpret. In addition, oblique rotation might not yield a smooth answer if you have a relatively small sample size.\n\n\n\n\n\nFigure 22.10: Example of a Factor Structure From an Oblique Rotation. From Petersen (2024) and Petersen (2025).\n\n\nAs an example of rotation based on interpretability, consider the Five-Factor Model of Personality (the Big Five), which goes by the acronym, OCEAN: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Although the five factors of personality are somewhat correlated, we can use rotation to ensure they are maximally independent. Upon rotation, extraversion and neuroticism are essentially uncorrelated, as depicted in Figure 22.11. The other pole of extraversion is intraversion and the other pole of neuroticism might be emotional stability or calmness.\n\n\n\n\n\nFigure 22.11: Example of a Factor Rotation of Neuroticism and Extraversion. From Petersen (2024) and Petersen (2025).\n\n\nSimple structure is achieved when each variable loads highly onto as few factors as possible (i.e., each item has only one significant or primary loading). Oftentimes this is not the case, so we choose our rotation method in order to decide if the factors can be correlated (an oblique rotation) or if the factors will be uncorrelated (an orthogonal rotation). If the factors are not correlated with each other, use an orthogonal rotation. The correlation between an item and a factor is a factor loading, which is simply a way to ask how much a variable is correlated with the underlying factor. However, its interpretation is more complicated if there are correlated factors!\nAn orthogonal rotation (e.g., varimax) can help with simplicity of interpretation because it seeks to yield simple structure without cross-loadings. Cross-loadings are instances where a variable loads onto multiple factors. My recommendation would always be to use an orthogonal rotation if you have reason to believe that finding simple structure in your data is possible; otherwise, the factors are extremely difficult to interpret—what exactly does a cross-loading even mean? However, you should always try an oblique rotation, too, to see how strongly the factors are correlated. Examples of oblique rotations include oblimin and promax.\n\n22.5.4 4. Determining the Number of Factors to Retain\nA goal of factor analysis and principal component analysis is simplification or parsimony, while still explaining as much variance as possible. The hope is that you can have fewer factors that explain the associations between the variables than the number of observed variables. It does not make sense to replace 18 variables with 18 latent factors because that would not result in any simplification. But how do you decide on the number of factors?\nThere are a number of criteria that one can use to help determine how many factors/components to keep:\n\nKaiser-Guttman criterion: factors with eigenvalues greater than zero\n\nor, for principal component analysis, components with eigenvalues greater than 1\n\n\nCattell’s scree test: the “elbow” in a scree plot minus one; sometimes operationalized with optimal coordinates (OC) or the acceleration factor (AF)\nParallel analysis: factors that explain more variance than randomly simulated data\nVery simple structure (VSS) criterion: larger is better\nVelicer’s minimum average partial (MAP) test: smaller is better\nAkaike information criterion (AIC): smaller is better\nBayesian information criterion (BIC): smaller is better\nSample size-adjusted BIC (SABIC): smaller is better\nRoot mean square error of approximation (RMSEA): smaller is better\nChi-square difference test: smaller is better; a significant test indicates that the more complex model is significantly better fitting than the less complex model\nStandardized root mean square residual (SRMR): smaller is better\nComparative Fit Index (CFI): larger is better\nTucker Lewis Index (TLI): larger is better\n\nThere is not necessarily a “correct” criterion to use in determining how many factors to keep, so it is generally recommended that researchers use multiple criteria in combination with theory and interpretability.\nA scree plot provides lots of information. A scree plot has the factor number on the x-axis and the eigenvalue on the y-axis. The eigenvalue is the variance accounted for by a factor; when using a varimax (orthogonal) rotation, an eigenvalue (or factor variance) is calculated as the sum of squared standardized factor (or component) loadings on that factor. An example of a scree plot is in Figure 22.12.\n\n\n\n\n\nFigure 22.12: Example of a Scree Plot.\n\n\nThe total variance is equal to the number of variables you have, so one eigenvalue is approximately one variable’s worth of variance. The first factor accounts for the most variance, the second factor accounts for the second-most variance, and so on. The more factors you add, the less variance is explained by the additional factor.\nOne criterion for how many factors to keep is the Kaiser-Guttman criterion. According to the Kaiser-Guttman criterion, you should keep any factors whose eigenvalue is greater than 1. That is, for the sake of simplicity, parsimony, and data reduction, you should take any factors that explain more than a single variable would explain. According to the Kaiser-Guttman criterion, we would keep three factors from Figure 22.12 that have eigenvalues greater than 1. The default in SPSS is to retain factors with eigenvalues greater than 1. However, keeping factors whose eigenvalue is greater than 1 is not the most correct rule. If you let SPSS do this, you may get many factors with eigenvalues around 1 (e.g., factors with an eigenvalue ~ 1.0001) that are not adding so much that it is worth the added complexity. The Kaiser-Guttman criterion usually results in keeping too many factors. Factors with small eigenvalues around 1 could reflect error shared across variables. For instance, factors with small eigenvalues could reflect method variance (i.e., method factor), such as a self-report factor that turns up as a factor in factor analysis, but that may be useless to you as a conceptual factor of a construct of interest.\nAnother criterion is Cattell’s scree test, which involves selecting the number of factors from looking at the scree plot. “Scree” refers to the rubble of stones at the bottom of a mountain. According to Cattell’s scree test, you should keep the factors before the last steep drop in eigenvalues—i.e., the factors before the rubble, where the slope approaches zero. The beginning of the scree (or rubble), where the slope approaches zero, is called the “elbow” of a scree plot. Using Cattell’s scree test, you retain the number of factors that explain the most variance prior to the explained variance drop-off, because, ultimately, you want to include only as many factors in which you gain substantially more by the inclusion of these factors. That is, you would keep the number of factors at the elbow of the scree plot minus one. If the last steep drop occurs from Factor 4 to Factor 5 and the elbow is at Factor 5, we would keep four factors. In Figure 22.12, the last steep drop in eigenvalues occurs from Factor 3 to Factor 4; the elbow of the scree plot occurs at Factor 4. We would keep the number of factors at the elbow minus one. Thus, using Cattell’s scree test, we would keep three factors based on Figure 22.12.\nThere are more sophisticated ways of using a scree plot, but they usually end up at a similar decision. Examples of more sophisticated tests include parallel analysis and very simple structure (VSS) plots. In a parallel analysis, you examine where the eigenvalues from observed data and random data converge, so you do not retain a factor that explains less variance than would be expected by random chance. Using the VSS criterion, the optimal number of factors to retain is the number of factors that maximizes the VSS criterion (Revelle & Rocklin, 1979). The VSS criterion is evaluated with models in which factor loadings for a given item that are less than the maximum factor loading for that item are suppressed to zero, thus forcing simple structure (i.e., no cross-loadings). The goal is finding a factor structure with interpretability so that factors are clearly distinguishable. Thus, we want to identify the number of factors with the highest VSS criterion.\nIn general, my recommendation is to use Cattell’s scree test, and then test the factor solutions with plus or minus one factor, in addition to examining model fit. You should never accept factors with eigenvalues less than zero (or components from PCA with eigenvalues less than one), because they are likely to be largely composed of error. If you are using maximum likelihood factor analysis, you can compare the fit of various models with model fit criteria to see which model fits best for its parsimony. A model will always fit better when you add additional parameters or factors, so you examine if there is significant improvement in model fit when adding the additional factor—that is, we keep adding complexity until additional complexity does not buy us much. Always try a factor solution that is one less and one more than suggested by Cattell’s scree test to buffer your final solution because the purpose of factor analysis is to explain things and to have interpretability. Even if all rules or indicators suggest to keep X number of factors, maybe \\(\\pm\\) one factor helps clarify things. Even though factor analysis is empirical, theory and interpretatability should also inform decisions.\n\n22.5.4.1 Model Fit Indices\nIn factor analysis, we fit a model to observed data, or to the variance-covariance matrix, and we evaluate the degree of model misfit. That is, fit indices evaluate how likely it is that a given causal model gave rise to the observed data. Various model fit indices can be used for evaluating how well a model fits the data and for comparing the fit of two competing models. Fit indices known as absolute fit indices compare whether the model fits better than the best-possible fitting model (i.e., a saturated model). Examples of absolute fit indices include the chi-square test, root mean square error of approximation (RMSEA), and the standardized root mean square residual (SRMR).\nThe chi-square test evaluates whether the model has a significant degree of misfit relative to the best-possible fitting model (a saturated model that fits as many parameters as possible; i.e., as many parameters as there are degrees of freedom); the null hypothesis of a chi-square test is that there is no difference between the predicted data (i.e., the data that would be observed if the model were true) and the observed data. Thus, a non-significant chi-square test indicates good model fit. However, because the null hypothesis of the chi-square test is that the model-implied covariance matrix is exactly equal to the observed covariance matrix (i.e., a model of perfect fit), this may be an unrealistic comparison. Models are simplifications of reality, and our models are virtually never expected to be a perfect description of reality. Thus, we would say a model is “useful” and partially validated if “it helps us to understand the relation between variables and does a ‘reasonable’ job of matching the data…A perfect fit may be an inappropriate standard, and a high chi-square estimate may indicate what we already know—that the hypothesized model holds approximately, not perfectly.” (Bollen, 1989, p. 268). The power of the chi-square test depends on sample size, and a large sample will likely detect small differences as significantly worse than the best-possible fitting model (Bollen, 1989).\nRMSEA is an index of absolute fit. Lower values indicate better fit.\nSRMR is an index of absolute fit with no penalty for model complexity. Lower values indicate better fit.\nThere are also various fit indices known as incremental, comparative, or relative fit indices that compare whether the model fits better than the worst-possible fitting model (i.e., a “baseline” or “null” model). Incremental fit indices include a chi-square difference test, the comparative fit index (CFI), and the Tucker-Lewis index (TLI). Unlike the chi-square test comparing the model to the best-possible fitting model, a significant chi-square test of the relative fit index indicates better fit—i.e., that the model fits better than the worst-possible fitting model.\nCFI is another relative fit index that compares the model to the worst-possible fitting model. Higher values indicate better fit.\nTLI is another relative fit index. Higher values indicate better fit.\nParsimony fit include fit indices that use information criteria fit indices, including the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). BIC penalizes model complexity more so than AIC. Lower AIC and BIC values indicate better fit.\nChi-square difference tests and CFI can be used to compare two nested models. AIC and BIC can be used to compare two non-nested models.\nCriteria for acceptable fit and good fit of SEM models are in Table 22.1. In addition, dynamic fit indexes have been proposed based on simulation to identify fit index cutoffs that are tailored to the characteristics of the specific model and data (McNeish & Wolf, 2023); dynamic fit indexes are available via the dynamic package (Wolf & McNeish, 2022) or with a webapp.\n\n\nTable 22.1: Criteria for Acceptable and Good Fit of Factor Analysis and Structural Equation Models Based on Fit Indices.\n\n\n\nSEM Fit Index\nAcceptable Fit\nGood Fit\n\n\n\nRMSEA\n\n\\(\\leq\\) .08\n\n\\(\\leq\\) .05\n\n\nCFI\n\n\\(\\geq\\) .90\n\n\\(\\geq\\) .95\n\n\nTLI\n\n\\(\\geq\\) .90\n\n\\(\\geq\\) .95\n\n\nSRMR\n\n\\(\\leq\\) .10\n\n\\(\\leq\\) .08\n\n\n\n\n\n\nHowever, good model fit does not necessarily indicate a true model.\nIn addition to global fit indices, it can also be helpful to examine evidence of local fit, such as the residual covariance matrix. The residual covariance matrix represents the difference between the observed covariance matrix and the model-implied covariance matrix (the observed covariance matrix minus the model-implied covariance matrix). These difference values are called covariance residuals. Standardizing the covariance matrix by converting each to a correlation matrix can be helpful for interpreting the magnitude of any local misfit. This is known as a residual correlation matrix, which is composed of correlation residuals. Correlation residuals greater than |.10| are possible evidence for poor local fit (Kline, 2023). If a correlation residual is positive, it suggests that the model underpredicts the observed association between the two variables (i.e., the observed covariance is greater than the model-implied covariance). If a correlation residual is negative, it suggests that the model overpredicts their observed association between the two variables (i.e., the observed covariance is smaller than the model-implied covariance). If the two variables are connected by only indirect pathways, it may be helpful to respecify the model with direct pathways between the two variables, such as a direct effect (i.e., regression path) or a covariance path. For guidance on evaluating local fit, see Kline (2024).\n\n22.5.5 5. Interpreting and Using Latent Factors\nThe next step is interpreting the model and latent factors. One data matrix can lead to many different (correct) models—you must choose one based on the factor structure and theory. Use theory to interpret the model and label the factors. In latent variable models, factors have meaning. You can use them as predictors, mediators, moderators, or outcomes. When possible, it is preferable to use the factors by examining their associations with other variables in the same model. You can extract factor scores, if necessary, for use in other analyses; however, it is preferable to examine the associations between factors and other variables in the same model if possible. And, using latent factors helps disattenuate associations for measurement error, to identify what the association is between variables when removing random measurement error.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-efaExample",
    "href": "factor-analysis.html#sec-efaExample",
    "title": "22  Factor Analysis",
    "section": "\n22.6 Example of Exploratory Factor Analysis",
    "text": "22.6 Example of Exploratory Factor Analysis\nWe generated the scree plot in Figure 22.13 using the psych::fa.parallel() function of the psych package (Revelle, 2025). The optimal coordinates and the acceleration factor attempt to operationalize the Cattell scree test: i.e., the “elbow” of the scree plot (Ruscio & Roche, 2012). The optimal coordinators factor is quantified using a series of linear equations to determine whether observed eigenvalues exceed the predicted values. The acceleration factor is quantified using the acceleration of the curve, that is, the second derivative. The Kaiser-Guttman rule states to keep principal components whose eigenvalues are greater than 1. However, for exploratory factor analysis (as opposed to PCA), the criterion is to keep the factors whose eigenvalues are greater than zero (i.e., not the factors whose eigenvalues are greater than 1) (Dinno, 2014).\nThe number of factors to keep would depend on which criteria one uses. Based on the rule to keep factors whose eigenvalues are greater than zero and based on the parallel test, we would keep five factors. However, based on the Cattell scree test (the “elbow” of the screen plot minus one), we would keep three factors. If using the optimal coordinates, we would keep eight factors; if using the acceleration factor, we would keep one factor. Therefore, interpretability of the factors would be important for deciding how many factors to keep.\n\nCodepsych::fa.parallel(\n  x = dataForFA[faVars],\n  fm = \"minres\", # errors out when using \"ml\"\n  fa = \"fa\"\n)\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n\n\n\n\n\n\nFigure 22.13: Scree Plot: With Comparisons to Simulated and Resampled Data.\n\n\n\n\nWe generated the scree plot in Figure 22.14 using the nFactors::nScree() and nFactors::plotnScree() functions of the nFactors package (Raiche & Magis, 2025).\n\nCode#screeDataEFA &lt;- nFactors::nScree(\n#  x = cor( # throws error with correlation matrix, so use covariance instead (below)\n#    dataForFA[faVars],\n#    use = \"pairwise.complete.obs\"),\n#  model = \"factors\")\n\nscreeDataEFA &lt;- nFactors::nScree(\n  x = cov(\n    dataForFA[faVars],\n    use = \"pairwise.complete.obs\"),\n  cor = FALSE,\n  model = \"factors\")\n\nnFactors::plotnScree(screeDataEFA)\n\n\n\n\n\n\nFigure 22.14: Scree Plot with Parallel Analysis.\n\n\n\n\nWe generated the very simple structure (VSS) plots in Figures 22.15 and 22.16 using the psych::vss() and psych::nfactors() functions of the psych package (Revelle, 2025). In addition to VSS plots, the output also provides additional criteria by which to determine the optimal number of factors, each for which lower values are better, including the Velicer minimum average partial (MAP) test, the Bayesian information criterion (BIC), the sample size-adjusted BIC (SABIC), and the root mean square error of approximation (RMSEA). Depending on the criterion, the optimal number of factors extracted varies between 2 and 8 factors.\n\nCodepsych::vss(\n  dataForFA[faVars],\n  rotate = \"oblimin\",\n  fm = \"minres\") # errors out when using \"mle\"\n\n\nVery Simple Structure\nCall: psych::vss(x = dataForFA[faVars], rotate = \"oblimin\", fm = \"minres\")\nVSS complexity 1 achieves a maximimum of 0.79  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.87  with  2  factors\n\nThe Velicer MAP achieves a minimum of 0.13  with  3  factors \nBIC achieves a minimum of  101984.9  with  8  factors\nSample Size adjusted BIC achieves a minimum of  102102.5  with  8  factors\n\nStatistics by number of factors \n  vss1 vss2  map dof  chisq prob sqresid  fit RMSEA    BIC  SABIC complex\n1 0.67 0.00 0.19 135 151248    0    26.4 0.67  0.75 150221 150650     1.0\n2 0.79 0.87 0.16 118 138098    0    10.5 0.87  0.76 137201 137576     1.2\n3 0.77 0.86 0.13 102    NaN  NaN     8.7 0.89    NA     NA     NA     1.2\n4 0.77 0.86 0.14  87 134790    0     8.1 0.90  0.88 134129 134406     1.3\n5 0.73 0.81 0.32  73 157011    0    10.0 0.87  1.04 156456 156687     1.3\n6 0.69 0.78 0.49  60    NaN  NaN    11.8 0.85    NA     NA     NA     1.4\n7 0.62 0.76 1.55  48    NaN  NaN    13.4 0.83    NA     NA     NA     1.6\n8 0.48 0.68  NaN  37 102266    0    17.6 0.78  1.17 101985 102102     1.6\n  eChisq  SRMR eCRMS  eBIC\n1  35972 0.242 0.258 34946\n2  10583 0.131 0.150  9686\n3   1653 0.052 0.064   877\n4    975 0.040 0.053   314\n5    492 0.028 0.041   -63\n6    238 0.020 0.032  -218\n7    116 0.014 0.025  -249\n8     39 0.008 0.016  -242\n\n\n\n\n\n\n\nFigure 22.15: Very Simple Structure Plot.\n\n\n\n\n\nCodepsych::nfactors(\n  dataForFA[faVars],\n  rotate = \"oblimin\",\n  fm = \"minres\") # errors out when using \"ml\"\n\n\nNumber of factors\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.79  with  2  factors\nVSS complexity 2 achieves a maximimum of 0.87  with  2  factors\nThe Velicer MAP achieves a minimum of 0.13  with  3  factors \nEmpirical BIC achieves a minimum of  -249.36  with  7  factors\nSample Size adjusted BIC achieves a minimum of  102102.5  with  8  factors\n\nStatistics by number of factors \n   vss1 vss2  map dof  chisq prob sqresid  fit RMSEA    BIC  SABIC complex\n1  0.67 0.00 0.19 135 151248    0    26.4 0.67  0.75 150221 150650     1.0\n2  0.79 0.87 0.16 118 138098    0    10.5 0.87  0.76 137201 137576     1.2\n3  0.77 0.86 0.13 102    NaN  NaN     8.7 0.89    NA     NA     NA     1.2\n4  0.77 0.86 0.14  87 134790    0     8.1 0.90  0.88 134129 134406     1.3\n5  0.73 0.81 0.32  73 157011    0    10.0 0.87  1.04 156456 156687     1.3\n6  0.69 0.78 0.49  60    NaN  NaN    11.8 0.85    NA     NA     NA     1.4\n7  0.62 0.76 1.55  48    NaN  NaN    13.4 0.83    NA     NA     NA     1.6\n8  0.48 0.68  NaN  37 102266    0    17.6 0.78  1.17 101985 102102     1.6\n9  0.48 0.67  NaN  27 103297    0    18.6 0.77  1.38 103092 103177     1.6\n10 0.33 0.49  NaN  18 132907    0    28.0 0.65  1.92 132770 132827     1.8\n11 0.35 0.48  NaN  10    NaN  NaN    30.1 0.62    NA     NA     NA     1.8\n12 0.41 0.50  NaN   3    NaN  NaN    28.5 0.64    NA     NA     NA     1.6\n13 0.40 0.47  NaN  -3    NaN   NA    31.3 0.61    NA     NA     NA     1.7\n14 0.41 0.45  NaN  -8    NaN   NA    30.8 0.61    NA     NA     NA     1.9\n15 0.35 0.40  NaN -12    NaN   NA    35.9 0.55    NA     NA     NA     2.0\n16 0.35 0.40  NaN -15    NaN   NA    35.9 0.55    NA     NA     NA     2.0\n17 0.35 0.40  NaN -17    NaN   NA    35.9 0.55    NA     NA     NA     2.0\n18 0.35 0.40   NA -18    NaN   NA    35.9 0.55    NA     NA     NA     2.0\n    eChisq   SRMR eCRMS  eBIC\n1  35971.9 0.2423 0.258 34946\n2  10583.1 0.1314 0.150  9686\n3   1652.9 0.0519 0.064   877\n4    975.3 0.0399 0.053   314\n5    492.1 0.0283 0.041   -63\n6    238.5 0.0197 0.032  -218\n7    115.5 0.0137 0.025  -249\n8     39.2 0.0080 0.016  -242\n9     25.1 0.0064 0.015  -180\n10    18.7 0.0055 0.016  -118\n11    14.7 0.0049 0.019   -61\n12    12.0 0.0044 0.032   -11\n13    10.2 0.0041    NA    NA\n14     9.6 0.0040    NA    NA\n15     9.5 0.0039    NA    NA\n16     9.5 0.0039    NA    NA\n17     9.5 0.0039    NA    NA\n18     9.5 0.0039    NA    NA\n\n\n\n\n\n\n\nFigure 22.16: Model Indices by Number of Factors.\n\n\n\n\nWe fit EFA models using the lavaan::efa() function of the lavaan package (Rosseel, 2012; Rosseel et al., 2024).\n\nCodeefa_fit &lt;- lavaan::efa(\n  data = dataForFA,\n  ov.names = faVars,\n  nfactors = 1:7,\n  rotation = \"geomin\",\n  missing = \"ML\",\n  estimator = \"MLR\",\n  bounds = \"standard\",\n  meanstructure = TRUE,\n  em.h1.iter.max = 2000000)\n\n\nThe model fits well according to CFI with 5 or more factors; the model fits well according to RMSEA with 6 or more factors. However, 5+ factors does not represent much of a simplification (relative to the 18 variables included). Moreover, in the model with four factors, only one variable had a significant loading on Factor 1. Thus, even with four factors, one of the factors does not seem to represent the aggregation of multiple variables. For these reasons, and because the “elbow test” for the scree plot suggested three factors, we decided to retain three factors and to see if we could achieve better fit by making additional model modifications (e.g., correlated residuals). Correlated residuals may be necessary, for example, when variables are correlated for reasons other than the latent factors.\n\nCodesummary(efa_fit)\n\nThis is lavaan 0.6-19 -- running exploratory factor analysis\n\n  Estimator                                         ML\n  Rotation method                       GEOMIN OBLIQUE\n  Geomin epsilon                                 0.001\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                          2002\n  Number of missing patterns                         4\n\nOverview models:\n                    aic      bic    sabic    chisq  df pvalue   cfi rmsea\n  nfactors = 1 125185.2 125487.7 125316.1 9230.262 135      0 0.220 0.572\n  nfactors = 2 117980.3 118378.0 118152.4 5189.697 118      0 0.662 0.406\n  nfactors = 3 114054.9 114542.3 114265.9 3191.805 102      0 0.726 0.403\n  nfactors = 4 110987.8 111559.2 111235.1 1451.682  87      0 0.896 0.287\n  nfactors = 5 109938.6 110588.5 110219.9  745.228  73      0 1.000 0.119\n  nfactors = 6 109666.4 110389.0 109979.2  624.068  60      0 1.000 0.048\n  nfactors = 7 109305.8 110095.7 109647.7  522.437  48      0 1.000 0.000\n\nEigenvalues correlation matrix:\n\n      ev1       ev2       ev3       ev4       ev5       ev6       ev7       ev8 \n  6.65346   6.50854   2.96513   0.53174   0.39828   0.33664   0.20872   0.11431 \n      ev9      ev10      ev11      ev12      ev13      ev14      ev15      ev16 \n  0.08090   0.05503   0.04064   0.03101   0.02321   0.02024   0.01553   0.00884 \n     ev17      ev18 \n  0.00578   0.00200 \n\nNumber of factors:  1 \n\nStandardized loadings: (* = significant at 1% level)\n\n                                            f1       unique.var   communalities\ncompletionsPerGame                       0.987*           0.026           0.974\nattemptsPerGame                          0.974*           0.051           0.949\npassing_yardsPerGame                     0.994*           0.011           0.989\npassing_tdsPerGame                       0.865*           0.252           0.748\npassing_air_yardsPerGame                 0.694*           0.519           0.481\npassing_yards_after_catchPerGame         0.773*           0.403           0.597\npassing_first_downsPerGame               0.992*           0.016           0.984\navg_completed_air_yards                      .*           0.919           0.081\navg_intended_air_yards                       .            0.977           0.023\naggressiveness                                            0.997           0.003\nmax_completed_air_distance               0.473*           0.776           0.224\navg_air_distance                                          1.000           0.000\nmax_air_distance                         0.368*           0.865           0.135\navg_air_yards_to_sticks                      .*           0.954           0.046\npassing_cpoe                                 .*           0.914           0.086\npass_comp_pct                            0.301*           0.910           0.090\npasser_rating                            0.629*           0.605           0.395\ncompletion_percentage_above_expectation  0.514*           0.736           0.264\n\n                           f1\nSum of squared loadings 7.068\nProportion of total     1.000\nProportion var          0.393\nCumulative var          0.393\n\nNumber of factors:  2 \n\nStandardized loadings: (* = significant at 1% level)\n\n                                            f1      f2       unique.var\ncompletionsPerGame                       0.986*                   0.029\nattemptsPerGame                          0.974*       *           0.049\npassing_yardsPerGame                     0.996*                   0.008\npassing_tdsPerGame                       0.863*                   0.254\npassing_air_yardsPerGame                 0.725*  0.693*           0.032\npassing_yards_after_catchPerGame         0.750* -0.611*           0.029\npassing_first_downsPerGame               0.990*                   0.019\navg_completed_air_yards                          0.965*           0.070\navg_intended_air_yards                        *  0.998*           0.002\naggressiveness                               .   0.785*           0.366\nmax_completed_air_distance                   .*  0.813*           0.288\navg_air_distance                              *  0.979*           0.032\nmax_air_distance                             .*  0.850*           0.260\navg_air_yards_to_sticks                          0.991*           0.017\npassing_cpoe                             0.318* -0.338*           0.777\npass_comp_pct                                .*                   0.913\npasser_rating                            0.615*      .            0.612\ncompletion_percentage_above_expectation  0.478*      .            0.730\n                                          communalities\ncompletionsPerGame                                0.971\nattemptsPerGame                                   0.951\npassing_yardsPerGame                              0.992\npassing_tdsPerGame                                0.746\npassing_air_yardsPerGame                          0.968\npassing_yards_after_catchPerGame                  0.971\npassing_first_downsPerGame                        0.981\navg_completed_air_yards                           0.930\navg_intended_air_yards                            0.998\naggressiveness                                    0.634\nmax_completed_air_distance                        0.712\navg_air_distance                                  0.968\nmax_air_distance                                  0.740\navg_air_yards_to_sticks                           0.983\npassing_cpoe                                      0.223\npass_comp_pct                                     0.087\npasser_rating                                     0.388\ncompletion_percentage_above_expectation           0.270\n\n                              f2    f1  total\nSum of sq (obliq) loadings 6.889 6.625 13.514\nProportion of total        0.510 0.490  1.000\nProportion var             0.383 0.368  0.751\nCumulative var             0.383 0.751  0.751\n\nFactor correlations: (* = significant at 1% level)\n\n       f1     f2\nf1  1.000       \nf2 -0.039  1.000\n\nNumber of factors:  3 \n\nStandardized loadings: (* = significant at 1% level)\n\n                                            f1      f2      f3       unique.var\ncompletionsPerGame                       0.978*               *           0.025\nattemptsPerGame                          0.993*       *       *           0.038\npassing_yardsPerGame                     0.986*               *           0.010\npassing_tdsPerGame                       0.833*       *      .*           0.255\npassing_air_yardsPerGame                 0.795*  0.729*                   0.022\npassing_yards_after_catchPerGame         0.696* -0.603*                   0.030\npassing_first_downsPerGame               0.978*               *           0.020\navg_completed_air_yards                          0.789*  0.331*           0.044\navg_intended_air_yards                           0.891*      .*           0.002\naggressiveness                                   0.796*                   0.350\nmax_completed_air_distance                   .*  0.666*  0.377*           0.184\navg_air_distance                              *  0.875*      .*           0.025\nmax_air_distance                             .*  0.808*      .            0.221\navg_air_yards_to_sticks                          0.866*      .*           0.012\npassing_cpoe                                             1.005*           0.020\npass_comp_pct                                   -0.449*  1.036*           0.121\npasser_rating                                            0.929*           0.092\ncompletion_percentage_above_expectation                  0.962*           0.044\n                                          communalities\ncompletionsPerGame                                0.975\nattemptsPerGame                                   0.962\npassing_yardsPerGame                              0.990\npassing_tdsPerGame                                0.745\npassing_air_yardsPerGame                          0.978\npassing_yards_after_catchPerGame                  0.970\npassing_first_downsPerGame                        0.980\navg_completed_air_yards                           0.956\navg_intended_air_yards                            0.998\naggressiveness                                    0.650\nmax_completed_air_distance                        0.816\navg_air_distance                                  0.975\nmax_air_distance                                  0.779\navg_air_yards_to_sticks                           0.988\npassing_cpoe                                      0.980\npass_comp_pct                                     0.879\npasser_rating                                     0.908\ncompletion_percentage_above_expectation           0.956\n\n                              f2    f1    f3  total\nSum of sq (obliq) loadings 6.046 5.751 4.687 16.484\nProportion of total        0.367 0.349 0.284  1.000\nProportion var             0.336 0.320 0.260  0.916\nCumulative var             0.336 0.655 0.916  0.916\n\nFactor correlations: (* = significant at 1% level)\n\n       f1     f2     f3\nf1  1.000              \nf2 -0.146  1.000       \nf3  0.171  0.429  1.000\n\nNumber of factors:  4 \n\nStandardized loadings: (* = significant at 1% level)\n\n                                            f1      f2      f3      f4 \ncompletionsPerGame                               0.970*       *       *\nattemptsPerGame                               *  1.024*                \npassing_yardsPerGame                         .*  0.902*                \npassing_tdsPerGame                       0.381*  0.669*                \npassing_air_yardsPerGame                         0.826*  0.732*        \npassing_yards_after_catchPerGame             .*  0.598* -0.606*        \npassing_first_downsPerGame                   .*  0.905*               *\navg_completed_air_yards                      .*          0.959*        \navg_intended_air_yards                                   1.024*        \naggressiveness                                *          0.814*        \nmax_completed_air_distance                   .*      .*  0.778*        \navg_air_distance                                         0.995*        \nmax_air_distance                                     .*  0.869*        \navg_air_yards_to_sticks                                  1.005*        \npassing_cpoe                                                 .*  0.923*\npass_comp_pct                                           -0.451*  1.056*\npasser_rating                                .*                  0.799*\ncompletion_percentage_above_expectation                      .*  0.887*\n                                             unique.var   communalities\ncompletionsPerGame                                0.015           0.985\nattemptsPerGame                                   0.001           0.999\npassing_yardsPerGame                              0.001           0.999\npassing_tdsPerGame                                0.204           0.796\npassing_air_yardsPerGame                          0.015           0.985\npassing_yards_after_catchPerGame                  0.023           0.977\npassing_first_downsPerGame                        0.023           0.977\navg_completed_air_yards                           0.067           0.933\navg_intended_air_yards                            0.003           0.997\naggressiveness                                    0.319           0.681\nmax_completed_air_distance                        0.295           0.705\navg_air_distance                                  0.034           0.966\nmax_air_distance                                  0.240           0.760\navg_air_yards_to_sticks                           0.018           0.982\npassing_cpoe                                      0.033           0.967\npass_comp_pct                                     0.092           0.908\npasser_rating                                     0.148           0.852\ncompletion_percentage_above_expectation           0.024           0.976\n\n                              f3    f2    f4    f1  total\nSum of sq (obliq) loadings 6.953 5.378 3.387 0.727 16.445\nProportion of total        0.423 0.327 0.206 0.044  1.000\nProportion var             0.386 0.299 0.188 0.040  0.914\nCumulative var             0.386 0.685 0.873 0.914  0.914\n\nFactor correlations: (* = significant at 1% level)\n\n       f1     f2     f3     f4\nf1  1.000                     \nf2  0.387  1.000              \nf3 -0.006 -0.175  1.000       \nf4  0.280  0.133  0.438  1.000\n\nNumber of factors:  5 \n\nStandardized loadings: (* = significant at 1% level)\n\n                                            f1      f2      f3      f4      f5 \ncompletionsPerGame                       0.952*       *       *              .*\nattemptsPerGame                          0.978*                       *        \npassing_yardsPerGame                     0.865*      .*       *       *        \npassing_tdsPerGame                       0.647*  0.439*               *       *\npassing_air_yardsPerGame                 0.805*       *  0.759*               *\npassing_yards_after_catchPerGame         0.527*  0.300* -0.611*      .*       *\npassing_first_downsPerGame               0.878*      .*                       *\navg_completed_air_yards                               *  1.007*      .*      .*\navg_intended_air_yards                                   1.004*       *        \naggressiveness                                       .*  0.791*                \nmax_completed_air_distance                   .*      .*  0.733*                \navg_air_distance                              *          0.942*      .*        \nmax_air_distance                             .*          0.660*      .*      .*\navg_air_yards_to_sticks                                  0.973*       *        \npassing_cpoe                                                 .*          0.901*\npass_comp_pct                                                .*          1.026*\npasser_rating                                    0.323*       *          0.757*\ncompletion_percentage_above_expectation                      .*          0.847*\n                                             unique.var   communalities\ncompletionsPerGame                                0.008           0.992\nattemptsPerGame                                   0.005           0.995\npassing_yardsPerGame                              0.003           0.997\npassing_tdsPerGame                                0.187           0.813\npassing_air_yardsPerGame                          0.016           0.984\npassing_yards_after_catchPerGame                  0.000           1.000\npassing_first_downsPerGame                        0.018           0.982\navg_completed_air_yards                           0.037           0.963\navg_intended_air_yards                            0.002           0.998\naggressiveness                                    0.431           0.569\nmax_completed_air_distance                        0.345           0.655\navg_air_distance                                  0.048           0.952\nmax_air_distance                                  0.300           0.700\navg_air_yards_to_sticks                           0.027           0.973\npassing_cpoe                                      0.017           0.983\npass_comp_pct                                     0.113           0.887\npasser_rating                                     0.136           0.864\ncompletion_percentage_above_expectation           0.036           0.964\n\n                              f3    f1    f5    f2    f4  total\nSum of sq (obliq) loadings 6.502 5.158 3.336 1.003 0.272 16.271\nProportion of total        0.400 0.317 0.205 0.062 0.017  1.000\nProportion var             0.361 0.287 0.185 0.056 0.015  0.904\nCumulative var             0.361 0.648 0.833 0.889 0.904  0.904\n\nFactor correlations: (* = significant at 1% level)\n\n       f1     f2     f3     f4     f5\nf1  1.000                            \nf2  0.371  1.000                     \nf3 -0.124  0.090  1.000              \nf4  0.190  0.090  0.021  1.000       \nf5  0.120  0.380  0.444  0.287  1.000\n\nNumber of factors:  6 \n\nStandardized loadings: (* = significant at 1% level)\n\n                                            f1      f2      f3      f4      f5 \ncompletionsPerGame                            *  1.022*              .*        \nattemptsPerGame                               *  1.069*              .*        \npassing_yardsPerGame                         .*  0.979*       *                \npassing_tdsPerGame                       0.313*  0.738*               *        \npassing_air_yardsPerGame                         0.918*  0.720*               *\npassing_yards_after_catchPerGame                 0.632* -0.599*       *       *\npassing_first_downsPerGame                   .*  0.969*               *       *\navg_completed_air_yards                      .*       *  0.491*          0.541*\navg_intended_air_yards                                   0.318*       *  0.716*\naggressiveness                                                       .*  0.749*\nmax_completed_air_distance                   .*      .*                  0.771*\navg_air_distance                              *              .*          0.821*\nmax_air_distance                             .*  0.303*                  0.725*\navg_air_yards_to_sticks                                      .*          0.840*\npassing_cpoe                                  *                       *       *\npass_comp_pct                                                .*         -0.602*\npasser_rating                                .*                       *        \ncompletion_percentage_above_expectation                               *      .*\n                                            f6       unique.var   communalities\ncompletionsPerGame                                        0.008           0.992\nattemptsPerGame                              .*           0.007           0.993\npassing_yardsPerGame                                      0.004           0.996\npassing_tdsPerGame                           .*           0.185           0.815\npassing_air_yardsPerGame                                  0.000           1.000\npassing_yards_after_catchPerGame             .*           0.000           1.000\npassing_first_downsPerGame                    *           0.018           0.982\navg_completed_air_yards                       *           0.031           0.969\navg_intended_air_yards                                    0.004           0.996\naggressiveness                               .*           0.429           0.571\nmax_completed_air_distance                                0.384           0.616\navg_air_distance                                          0.047           0.953\nmax_air_distance                                          0.302           0.698\navg_air_yards_to_sticks                                   0.025           0.975\npassing_cpoe                             0.971*           0.015           0.985\npass_comp_pct                            1.086*           0.046           0.954\npasser_rating                            0.934*           0.135           0.865\ncompletion_percentage_above_expectation  0.935*           0.042           0.958\n\n                              f2    f5    f6    f3    f1    f4  total\nSum of sq (obliq) loadings 5.833 4.580 3.693 1.862 0.320 0.030 16.318\nProportion of total        0.357 0.281 0.226 0.114 0.020 0.002  1.000\nProportion var             0.324 0.254 0.205 0.103 0.018 0.002  0.907\nCumulative var             0.324 0.579 0.784 0.887 0.905 0.907  0.907\n\nFactor correlations: (* = significant at 1% level)\n\n       f1     f2     f3     f4     f5     f6\nf1  1.000                                   \nf2  0.139  1.000                            \nf3 -0.187 -0.286  1.000                     \nf4 -0.140 -0.305  0.268  1.000              \nf5 -0.114 -0.058  0.838 -0.047  1.000       \nf6 -0.294  0.314  0.133 -0.208  0.380  1.000\n\nNumber of factors:  7 \n\nStandardized loadings: (* = significant at 1% level)\n\n                                            f1      f2      f3      f4      f5 \ncompletionsPerGame                               0.992*       *                \nattemptsPerGame                               *  1.013*                        \npassing_yardsPerGame                         .*  0.938*       *                \npassing_tdsPerGame                       0.313*  0.745*               *       *\npassing_air_yardsPerGame                         0.846*  0.388*  0.586*       *\npassing_yards_after_catchPerGame              *  0.603*         -0.596*       *\npassing_first_downsPerGame                    *  0.957*                       *\navg_completed_air_yards                  0.326*                  0.570*  0.416*\navg_intended_air_yards                                   0.421*  0.351*  0.474*\naggressiveness                                                       .   0.667*\nmax_completed_air_distance               0.581*                       *        \navg_air_distance                                      *  0.462*      .*  0.421*\nmax_air_distance                                     .*  0.554*       *       *\navg_air_yards_to_sticks                                  0.349*      .*  0.578*\npassing_cpoe                                          *       *       *        \npass_comp_pct                                                           -0.475*\npasser_rating                                .*                      .*        \ncompletion_percentage_above_expectation                                        \n                                            f6      f7       unique.var\ncompletionsPerGame                                   .*           0.010\nattemptsPerGame                               *                   0.003\npassing_yardsPerGame                                  *           0.002\npassing_tdsPerGame                            *       *           0.192\npassing_air_yardsPerGame                                          0.007\npassing_yards_after_catchPerGame              *      .*           0.000\npassing_first_downsPerGame                    *       *           0.017\navg_completed_air_yards                       *                   0.025\navg_intended_air_yards                                            0.001\naggressiveness                                                    0.421\nmax_completed_air_distance               0.839*                   0.000\navg_air_distance                              *                   0.047\nmax_air_distance                         0.446*                   0.165\navg_air_yards_to_sticks                                           0.027\npassing_cpoe                                     0.938*           0.019\npass_comp_pct                                    0.936*           0.042\npasser_rating                                    0.732*           0.141\ncompletion_percentage_above_expectation          0.969*           0.042\n                                          communalities\ncompletionsPerGame                                0.990\nattemptsPerGame                                   0.997\npassing_yardsPerGame                              0.998\npassing_tdsPerGame                                0.808\npassing_air_yardsPerGame                          0.993\npassing_yards_after_catchPerGame                  1.000\npassing_first_downsPerGame                        0.983\navg_completed_air_yards                           0.975\navg_intended_air_yards                            0.999\naggressiveness                                    0.579\nmax_completed_air_distance                        1.000\navg_air_distance                                  0.953\nmax_air_distance                                  0.835\navg_air_yards_to_sticks                           0.973\npassing_cpoe                                      0.981\npass_comp_pct                                     0.958\npasser_rating                                     0.859\ncompletion_percentage_above_expectation           0.958\n\n                              f2    f7    f5    f4    f3    f6    f1  total\nSum of sq (obliq) loadings 5.552 3.401 2.397 1.948 1.620 1.075 0.846 16.839\nProportion of total        0.330 0.202 0.142 0.116 0.096 0.064 0.050  1.000\nProportion var             0.308 0.189 0.133 0.108 0.090 0.060 0.047  0.935\nCumulative var             0.308 0.497 0.631 0.739 0.829 0.889 0.935  0.935\n\nFactor correlations: (* = significant at 1% level)\n\n       f1     f2     f3     f4     f5     f6     f7\nf1  1.000                                          \nf2  0.310  1.000                                   \nf3  0.533  0.104  1.000                            \nf4 -0.208 -0.367  0.200  1.000                     \nf5  0.193 -0.185  0.471  0.753  1.000              \nf6 -0.041  0.039  0.439  0.472  0.547  1.000       \nf7  0.458  0.142  0.795 -0.001  0.176  0.314  1.000\n\n\nA path diagram of the three-factor EFA model in Figure 22.17 was created using the lavaanPlot::lavaanPlot() function of the lavaanPlot package (Lishinski, 2024).\n\nCodelavaanPlot::lavaanPlot(\n  efa_fit$nf3,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nFigure 22.17: Path Diagram of the Three-Factor Exploratory Factor Analysis Model.\n\n\n\nTo make the plot interactive for editing, you can use the lavaangui::plot_lavaan() function of the lavaangui package (Karch, 2025b; Karch, 2025a):\n\nCodelavaangui::plot_lavaan(efa_fit$nf3)\n\n\nHere is the syntax for estimating a three-factor EFA using exploratory structural equation modeling (ESEM). Estimating the model in a ESEM framework allows us to make modifications to the model, such as adding correlated residuals, and adding predictors or outcomes of the latent factors. The syntax below represents the same model (with the same fit indices) as the three-factor EFA model above.\n\nCodeefa3factor_syntax &lt;- '\n # EFA Factor Loadings\n efa(\"efa1\")*f1 + \n efa(\"efa1\")*f2 + \n efa(\"efa1\")*f3 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + \n passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame + \n avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + \n avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + \n passer_rating + completion_percentage_above_expectation\n'\n\n\nTo fit the ESEM model, we use the lavaan::sem() function of the lavaan package (Rosseel, 2012; Rosseel et al., 2024).\n\nCodeefa3factor_fit &lt;- sem(\n  efa3factor_syntax,\n  data = dataForFA,\n  information = \"observed\",\n  missing = \"ML\",\n  estimator = \"MLR\",\n  rotation = \"geomin\",\n  bounds = \"standard\",\n  meanstructure = TRUE,\n  em.h1.iter.max = 2000000)\n\n\nThe fit indices suggests that the model does not fit well to the data and that additional model modifications are necessary. The fit indices are below.\n\nCodesummary(\n  efa3factor_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 343 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        93\n  Row rank of the constraints matrix                30\n\n  Rotation method                       GEOMIN OBLIQUE\n  Geomin epsilon                                 0.001\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                          2002\n  Number of missing patterns                         4\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                              5445.197    3191.805\n  Degrees of freedom                               102         102\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.706\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                             43031.563   23571.926\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.826\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.875       0.868\n  Tucker-Lewis Index (TLI)                       0.813       0.802\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.726\n  Robust Tucker-Lewis Index (TLI)                            0.590\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -56940.469  -56940.469\n  Scaling correction factor                                  1.784\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -54217.870  -54217.870\n  Scaling correction factor                                  1.742\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              114054.938  114054.938\n  Bayesian (BIC)                            114542.303  114542.303\n  Sample-size adjusted Bayesian (SABIC)     114265.900  114265.900\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.162       0.123\n  90 Percent confidence interval - lower         0.158       0.120\n  90 Percent confidence interval - upper         0.165       0.126\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.403\n  90 Percent confidence interval - lower                     0.379\n  90 Percent confidence interval - upper                     0.427\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.447       0.447\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~ efa1                                                            \n    completinsPrGm    7.869    0.092   85.428    0.000    7.869    0.978\n    attemptsPerGam   12.514    0.137   91.318    0.000   12.514    0.993\n    pssng_yrdsPrGm   91.877    1.049   87.548    0.000   91.877    0.986\n    passng_tdsPrGm    0.575    0.011   53.647    0.000    0.575    0.833\n    pssng_r_yrdsPG   96.231    1.928   49.919    0.000   96.231    0.795\n    pssng_yrds__PG   45.718    0.994   45.990    0.000   45.718    0.696\n    pssng_frst_dPG    4.457    0.053   84.519    0.000    4.457    0.978\n    avg_cmpltd_r_y    0.016    0.048    0.332    0.740    0.016    0.004\n    avg_ntndd_r_yr   -0.041    0.043   -0.958    0.338   -0.041   -0.009\n    aggressiveness   -0.276    0.383   -0.721    0.471   -0.276   -0.035\n    mx_cmpltd_r_ds    1.680    0.433    3.883    0.000    1.680    0.158\n    avg_air_distnc   -0.210    0.075   -2.782    0.005   -0.210   -0.048\n    max_air_distnc    1.695    0.436    3.885    0.000    1.695    0.185\n    avg_r_yrds_t_s    0.008    0.042    0.191    0.848    0.008    0.002\n    passing_cpoe     -0.599    0.241   -2.482    0.013   -0.599   -0.044\n    pass_comp_pct     0.001    0.001    0.927    0.354    0.001    0.004\n    passer_rating     2.610    0.977    2.671    0.008    2.610    0.081\n    cmpltn_prcnt__   -0.597    0.311   -1.917    0.055   -0.597   -0.048\n  f2 =~ efa1                                                            \n    completinsPrGm    0.028    0.030    0.958    0.338    0.028    0.004\n    attemptsPerGam    0.404    0.076    5.318    0.000    0.404    0.032\n    pssng_yrdsPrGm   -0.333    0.287   -1.158    0.247   -0.333   -0.004\n    passng_tdsPrGm   -0.032    0.009   -3.437    0.001   -0.032   -0.046\n    pssng_r_yrdsPG   88.148    1.718   51.310    0.000   88.148    0.729\n    pssng_yrds__PG  -39.560    0.941  -42.027    0.000  -39.560   -0.603\n    pssng_frst_dPG   -0.028    0.014   -2.061    0.039   -0.028   -0.006\n    avg_cmpltd_r_y    3.206    0.185   17.327    0.000    3.206    0.789\n    avg_ntndd_r_yr    4.132    0.194   21.350    0.000    4.132    0.891\n    aggressiveness    6.295    0.901    6.985    0.000    6.295    0.796\n    mx_cmpltd_r_ds    7.102    0.800    8.874    0.000    7.102    0.666\n    avg_air_distnc    3.829    0.214   17.865    0.000    3.829    0.875\n    max_air_distnc    7.382    0.747    9.888    0.000    7.382    0.808\n    avg_r_yrds_t_s    4.061    0.234   17.365    0.000    4.061    0.866\n    passing_cpoe     -0.257    0.381   -0.675    0.499   -0.257   -0.019\n    pass_comp_pct    -0.060    0.003  -18.734    0.000   -0.060   -0.449\n    passer_rating     0.531    0.626    0.849    0.396    0.531    0.016\n    cmpltn_prcnt__    0.601    0.545    1.102    0.271    0.601    0.049\n  f3 =~ efa1                                                            \n    completinsPrGm    0.394    0.050    7.844    0.000    0.394    0.049\n    attemptsPerGam   -0.671    0.080   -8.410    0.000   -0.671   -0.053\n    pssng_yrdsPrGm    3.963    0.461    8.590    0.000    3.963    0.043\n    passng_tdsPrGm    0.074    0.008    8.760    0.000    0.074    0.108\n    pssng_r_yrdsPG   -2.231    1.674   -1.332    0.183   -2.231   -0.018\n    pssng_yrds__PG    0.381    0.488    0.782    0.435    0.381    0.006\n    pssng_frst_dPG    0.249    0.026    9.576    0.000    0.249    0.055\n    avg_cmpltd_r_y    1.345    0.215    6.270    0.000    1.345    0.331\n    avg_ntndd_r_yr    0.970    0.179    5.405    0.000    0.970    0.209\n    aggressiveness    0.081    0.581    0.139    0.890    0.081    0.010\n    mx_cmpltd_r_ds    4.026    0.947    4.252    0.000    4.026    0.377\n    avg_air_distnc    0.912    0.199    4.579    0.000    0.912    0.208\n    max_air_distnc    1.374    0.797    1.725    0.085    1.374    0.150\n    avg_r_yrds_t_s    1.136    0.208    5.459    0.000    1.136    0.242\n    passing_cpoe     13.682    0.564   24.271    0.000   13.682    1.005\n    pass_comp_pct     0.138    0.005   26.711    0.000    0.138    1.036\n    passer_rating    30.032    2.160   13.907    0.000   30.032    0.929\n    cmpltn_prcnt__   11.870    0.680   17.454    0.000   11.870    0.962\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 ~~                                                                 \n    f2               -0.146       NA                     -0.146   -0.146\n    f3                0.171       NA                      0.171    0.171\n  f2 ~~                                                                 \n    f3                0.429                               0.429    0.429\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm   13.216    0.180   73.516    0.000   13.216    1.643\n   .attemptsPerGam   21.828    0.282   77.531    0.000   21.828    1.733\n   .pssng_yrdsPrGm  147.994    2.082   71.090    0.000  147.994    1.589\n   .passng_tdsPrGm    0.847    0.015   54.871    0.000    0.847    1.226\n   .pssng_r_yrdsPG  132.999    2.704   49.186    0.000  132.999    1.099\n   .pssng_yrds__PG   87.622    1.467   59.725    0.000   87.622    1.335\n   .pssng_frst_dPG    7.143    0.102   70.156    0.000    7.143    1.568\n   .avg_cmpltd_r_y    3.703    0.162   22.892    0.000    3.703    0.912\n   .avg_ntndd_r_yr    5.728    0.159   35.953    0.000    5.728    1.235\n   .aggressiveness   13.717    0.555   24.699    0.000   13.717    1.735\n   .mx_cmpltd_r_ds   33.473    0.623   53.716    0.000   33.473    3.139\n   .avg_air_distnc   19.147    0.169  113.439    0.000   19.147    4.374\n   .max_air_distnc   42.848    0.654   65.492    0.000   42.848    4.688\n   .avg_r_yrds_t_s   -3.289    0.193  -17.061    0.000   -3.289   -0.701\n   .passing_cpoe     -6.148    0.415  -14.805    0.000   -6.148   -0.451\n   .pass_comp_pct     0.590    0.003  187.208    0.000    0.590    4.417\n   .passer_rating    72.013    1.458   49.375    0.000   72.013    2.227\n   .cmpltn_prcnt__   -5.716    0.507  -11.267    0.000   -5.716   -0.463\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm    1.615    0.116   13.903    0.000    1.615    0.025\n   .attemptsPerGam    6.066    0.383   15.823    0.000    6.066    0.038\n   .pssng_yrdsPrGm   86.808    8.228   10.550    0.000   86.808    0.010\n   .passng_tdsPrGm    0.122    0.006   20.594    0.000    0.122    0.255\n   .pssng_r_yrdsPG  329.008   29.046   11.327    0.000  329.008    0.022\n   .pssng_yrds__PG  130.973    8.837   14.820    0.000  130.973    0.030\n   .pssng_frst_dPG    0.411    0.026   15.914    0.000    0.411    0.020\n   .avg_cmpltd_r_y    0.718    0.074    9.651    0.000    0.718    0.044\n   .avg_ntndd_r_yr    0.033    0.014    2.441    0.015    0.033    0.002\n   .aggressiveness   21.877       NA                     21.877    0.350\n   .mx_cmpltd_r_ds   20.943       NA                     20.943    0.184\n   .avg_air_distnc    0.470       NA                      0.470    0.025\n   .max_air_distnc   18.462       NA                     18.462    0.221\n   .avg_r_yrds_t_s    0.272       NA                      0.272    0.012\n   .passing_cpoe      3.682       NA                      3.682    0.020\n   .pass_comp_pct     0.002       NA                      0.002    0.121\n   .passer_rating    96.356       NA                     96.356    0.092\n   .cmpltn_prcnt__    6.698       NA                      6.698    0.044\n    f1                1.000                               1.000    1.000\n    f2                1.000                               1.000    1.000\n    f3                1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    completinsPrGm    0.975\n    attemptsPerGam    0.962\n    pssng_yrdsPrGm    0.990\n    passng_tdsPrGm    0.745\n    pssng_r_yrdsPG    0.978\n    pssng_yrds__PG    0.970\n    pssng_frst_dPG    0.980\n    avg_cmpltd_r_y    0.956\n    avg_ntndd_r_yr    0.998\n    aggressiveness    0.650\n    mx_cmpltd_r_ds    0.816\n    avg_air_distnc    0.975\n    max_air_distnc    0.779\n    avg_r_yrds_t_s    0.988\n    passing_cpoe      0.980\n    pass_comp_pct     0.879\n    passer_rating     0.908\n    cmpltn_prcnt__    0.956\n\n\n\nCodelavaan::fitMeasures(\n  efa3factor_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n               chisq                   df               pvalue \n            5445.197              102.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n            3191.805              102.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.706            43031.563              153.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.162                0.875 \n                 tli                 srmr         rmsea.robust \n               0.813                0.447                0.403 \n          cfi.robust           tli.robust \n               0.726                0.590 \n\n\n\nCodelavaan::residuals(\n  efa3factor_fit,\n  type = \"cor\")\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n                                        cmplPG attmPG pssng_yPG pssng_tPG\ncompletionsPerGame                       0.000                           \nattemptsPerGame                          0.020  0.000                    \npassing_yardsPerGame                    -0.004 -0.006     0.000          \npassing_tdsPerGame                      -0.022 -0.043     0.014     0.000\npassing_air_yardsPerGame                 0.003  0.007     0.001    -0.011\npassing_yards_after_catchPerGame        -0.006  0.001     0.006     0.001\npassing_first_downsPerGame               0.000 -0.006     0.003     0.019\navg_completed_air_yards                 -0.112 -0.094    -0.066    -0.026\navg_intended_air_yards                  -0.092 -0.082    -0.061    -0.029\naggressiveness                          -0.052 -0.022    -0.038    -0.027\nmax_completed_air_distance               0.077  0.099     0.123     0.148\navg_air_distance                        -0.095 -0.083    -0.064    -0.033\nmax_air_distance                         0.031  0.035     0.046     0.064\navg_air_yards_to_sticks                 -0.092 -0.081    -0.059    -0.018\npassing_cpoe                             0.016  0.015     0.019     0.017\npass_comp_pct                            0.013  0.010     0.000    -0.017\npasser_rating                            0.068  0.063     0.101     0.178\ncompletion_percentage_above_expectation -0.006 -0.009    -0.006    -0.010\n                                        pssng_r_PG p___PG pssng_f_PG avg_c__\ncompletionsPerGame                                                          \nattemptsPerGame                                                             \npassing_yardsPerGame                                                        \npassing_tdsPerGame                                                          \npassing_air_yardsPerGame                     0.000                          \npassing_yards_after_catchPerGame            -0.008  0.000                   \npassing_first_downsPerGame                  -0.003 -0.002      0.000        \navg_completed_air_yards                     -0.080 -0.047     -0.076   0.000\navg_intended_air_yards                      -0.074 -0.014     -0.067  -0.016\naggressiveness                              -0.176  0.107     -0.028  -0.113\nmax_completed_air_distance                  -0.066  0.214      0.089  -0.180\navg_air_distance                            -0.097  0.005     -0.075  -0.037\nmax_air_distance                            -0.022  0.106      0.030  -0.131\navg_air_yards_to_sticks                     -0.085 -0.002     -0.060  -0.023\npassing_cpoe                                -0.016  0.042      0.018  -0.383\npass_comp_pct                                0.003  0.000      0.001  -0.426\npasser_rating                               -0.171  0.275      0.095  -0.602\ncompletion_percentage_above_expectation      0.005 -0.016     -0.006  -0.301\n                                        avg_n__ aggrss mx_c__ avg_r_ mx_r_d\ncompletionsPerGame                                                         \nattemptsPerGame                                                            \npassing_yardsPerGame                                                       \npassing_tdsPerGame                                                         \npassing_air_yardsPerGame                                                   \npassing_yards_after_catchPerGame                                           \npassing_first_downsPerGame                                                 \navg_completed_air_yards                                                    \navg_intended_air_yards                    0.000                            \naggressiveness                           -0.123  0.000                     \nmax_completed_air_distance               -0.233 -0.287  0.000              \navg_air_distance                         -0.012 -0.127 -0.210  0.000       \nmax_air_distance                         -0.089 -0.227 -0.006 -0.080  0.000\navg_air_yards_to_sticks                  -0.007 -0.115 -0.223 -0.018 -0.099\npassing_cpoe                             -0.321 -0.408 -0.276 -0.335 -0.175\npass_comp_pct                            -0.357 -0.379 -0.248 -0.378 -0.158\npasser_rating                            -0.581 -0.598 -0.369 -0.593 -0.367\ncompletion_percentage_above_expectation  -0.248 -0.296 -0.247 -0.264 -0.130\n                                        av____ pssng_ pss_c_ pssr_r cmp___\ncompletionsPerGame                                                        \nattemptsPerGame                                                           \npassing_yardsPerGame                                                      \npassing_tdsPerGame                                                        \npassing_air_yardsPerGame                                                  \npassing_yards_after_catchPerGame                                          \npassing_first_downsPerGame                                                \navg_completed_air_yards                                                   \navg_intended_air_yards                                                    \naggressiveness                                                            \nmax_completed_air_distance                                                \navg_air_distance                                                          \nmax_air_distance                                                          \navg_air_yards_to_sticks                  0.000                            \npassing_cpoe                            -0.340  0.000                     \npass_comp_pct                           -0.385  0.018  0.000              \npasser_rating                           -0.593 -0.085  0.081  0.000       \ncompletion_percentage_above_expectation -0.271 -0.002 -0.016 -0.133  0.000\n\n$mean\n                     completionsPerGame                         attemptsPerGame \n                                  0.000                                   0.000 \n                   passing_yardsPerGame                      passing_tdsPerGame \n                                  0.000                                   0.000 \n               passing_air_yardsPerGame        passing_yards_after_catchPerGame \n                                  0.000                                   0.000 \n             passing_first_downsPerGame                 avg_completed_air_yards \n                                  0.000                                   0.419 \n                 avg_intended_air_yards                          aggressiveness \n                                  0.297                                   0.292 \n             max_completed_air_distance                        avg_air_distance \n                                  0.435                                   0.309 \n                       max_air_distance                 avg_air_yards_to_sticks \n                                  0.155                                   0.359 \n                           passing_cpoe                           pass_comp_pct \n                                  0.024                                  -0.002 \n                          passer_rating completion_percentage_above_expectation \n                                  0.243                                   0.005 \n\n\nWe can examine the model modification indices to identify parameters that, if estimated, would substantially improve model fit. For instance, the modification indices below indicate additional correlated residuals that could substantially improve model fit. However, it is generally not recommended to blindly estimate additional parameters solely based on modification indices, which can lead to data dredging and overfitting. Rather, it is generally advised to consider modification indices in light of theory. Based on the modification indices, we will add several correlated residuals to the model, to help account for why variables are associated with each other for reasons other than their underlying latent factors.\n\nCodelavaan::modificationindices(\n  efa3factor_fit,\n  sort. = TRUE)\n\n\n  \n\n\n\nBelow are factor scores from the model for the first six players:\n\nCodeefa3factor_factorScores &lt;- lavaan::lavPredict(efa3factor_fit)\n\nhead(efa3factor_factorScores)\n\n              f1          f2         f3\n[1,] -0.09277123 -1.27994978  0.3674875\n[2,]  0.22251547 -1.66512896 -0.9524800\n[3,]  0.43788991 -1.83507785 -1.1746620\n[4,]  0.07350633  0.08712282  0.7603453\n[5,]  0.89967482  1.15956276  0.4651723\n[6,] -0.02270474  0.12927767 -0.2200485\n\n\nA path diagram of the three-factor ESEM model is in Figure 22.18.\n\nCodelavaanPlot::lavaanPlot(\n  efa3factor_fit,\n  coefs = TRUE,\n  covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nFigure 22.18: Path Diagram of the Three-Factor Exploratory Structural Equation Model.\n\n\n\nTo make the plot interactive for editing, you can use the lavaangui::plot_lavaan() function of the lavaangui package (Karch, 2025b; Karch, 2025a):\n\nCodelavaangui::plot_lavaan(efa3factor_fit)\n\n\nBelow is a modification of the three-factor model with correlated residuals. For instance, it makes sense that passing completions and attempts are related to each other (even after accounting for their latent factor).\n\nCodeefa3factorModified_syntax &lt;- '\n # EFA Factor Loadings\n efa(\"efa1\")*F1 + \n efa(\"efa1\")*F2 + \n efa(\"efa1\")*F3 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + \n passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame + \n avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + \n avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + \n passer_rating + completion_percentage_above_expectation\n \n # Correlated Residuals\n completionsPerGame ~~ attemptsPerGame\n passing_yardsPerGame ~~ passing_yards_after_catchPerGame\n attemptsPerGame ~~ passing_yardsPerGame\n attemptsPerGame ~~ passing_air_yardsPerGame\n passing_air_yardsPerGame ~~ passing_yards_after_catchPerGame\n passing_yards_after_catchPerGame ~~ avg_completed_air_yards\n passing_tdsPerGame ~~ passer_rating\n completionsPerGame ~~ passing_air_yardsPerGame\n max_completed_air_distance ~~ max_air_distance\n aggressiveness ~~ completion_percentage_above_expectation\n'\n\n\n\nCodeefa3factorModified_fit &lt;- sem(\n  efa3factorModified_syntax,\n  data = dataForFA,\n  information = \"observed\",\n  missing = \"ML\",\n  estimator = \"MLR\",\n  rotation = \"geomin\",\n  bounds = \"standard\",\n  meanstructure = TRUE,\n  em.h1.iter.max = 2000000)\n\n\n\nCodesummary(\n  efa3factorModified_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 407 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                       103\n  Row rank of the constraints matrix                40\n\n  Rotation method                       GEOMIN OBLIQUE\n  Geomin epsilon                                 0.001\n  Rotation algorithm (rstarts)                GPA (30)\n  Standardized metric                             TRUE\n  Row weights                                     None\n\n  Number of observations                          2002\n  Number of missing patterns                         4\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                              1399.681     837.974\n  Degrees of freedom                                92          92\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.670\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                             43031.563   23571.926\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.826\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.970       0.968\n  Tucker-Lewis Index (TLI)                       0.949       0.947\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.948\n  Robust Tucker-Lewis Index (TLI)                            0.914\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -54917.711  -54917.711\n  Scaling correction factor                                  1.810\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -54217.870  -54217.870\n  Scaling correction factor                                  1.742\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              110029.421  110029.421\n  Bayesian (BIC)                            110572.806  110572.806\n  Sample-size adjusted Bayesian (SABIC)     110264.632  110264.632\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.084       0.064\n  90 Percent confidence interval - lower         0.080       0.061\n  90 Percent confidence interval - upper         0.088       0.067\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.965       0.000\n                                                                  \n  Robust RMSEA                                               0.189\n  90 Percent confidence interval - lower                     0.160\n  90 Percent confidence interval - upper                     0.218\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.147       0.147\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  F1 =~ efa1                                                            \n    completinsPrGm    7.817    0.092   85.037    0.000    7.817    0.972\n    attemptsPerGam   12.334    0.140   88.070    0.000   12.334    0.980\n    pssng_yrdsPrGm   92.161    1.019   90.452    0.000   92.161    0.990\n    passng_tdsPrGm    0.590    0.010   58.359    0.000    0.590    0.854\n    pssng_r_yrdsPG   90.323    1.824   49.520    0.000   90.323    0.747\n    pssng_yrds__PG   47.955    0.982   48.827    0.000   47.955    0.730\n    pssng_frst_dPG    4.477    0.051   87.545    0.000    4.477    0.983\n    avg_cmpltd_r_y   -0.001    0.076   -0.019    0.985   -0.001   -0.000\n    avg_ntndd_r_yr   -0.078    0.071   -1.092    0.275   -0.078   -0.022\n    aggressiveness    0.020    0.185    0.106    0.915    0.020    0.003\n    mx_cmpltd_r_ds    1.650    0.468    3.524    0.000    1.650    0.194\n    avg_air_distnc   -0.255    0.088   -2.893    0.004   -0.255   -0.077\n    max_air_distnc    1.441    0.433    3.332    0.001    1.441    0.194\n    avg_r_yrds_t_s   -0.011    0.075   -0.145    0.885   -0.011   -0.003\n    passing_cpoe     -1.059    0.296   -3.582    0.000   -1.059   -0.072\n    pass_comp_pct     0.001    0.002    0.405    0.686    0.001    0.005\n    passer_rating     2.624    0.924    2.840    0.005    2.624    0.096\n    cmpltn_prcnt__   -0.987    0.347   -2.841    0.004   -0.987   -0.076\n  F2 =~ efa1                                                            \n    completinsPrGm   -0.094    0.042   -2.238    0.025   -0.094   -0.012\n    attemptsPerGam    0.106    0.094    1.135    0.256    0.106    0.008\n    pssng_yrdsPrGm    0.946    0.334    2.829    0.005    0.946    0.010\n    passng_tdsPrGm   -0.002    0.007   -0.298    0.766   -0.002   -0.003\n    pssng_r_yrdsPG   84.028    2.081   40.382    0.000   84.028    0.695\n    pssng_yrds__PG  -36.824    1.056  -34.887    0.000  -36.824   -0.560\n    pssng_frst_dPG    0.001    0.016    0.066    0.948    0.001    0.000\n    avg_cmpltd_r_y    2.943    0.135   21.796    0.000    2.943    0.951\n    avg_ntndd_r_yr    3.491    0.134   26.140    0.000    3.491    1.009\n    aggressiveness    5.160    0.711    7.257    0.000    5.160    0.799\n    mx_cmpltd_r_ds    5.902    0.618    9.543    0.000    5.902    0.694\n    avg_air_distnc    3.195    0.144   22.204    0.000    3.195    0.971\n    max_air_distnc    6.180    0.599   10.311    0.000    6.180    0.834\n    avg_r_yrds_t_s    3.402    0.159   21.336    0.000    3.402    0.981\n    passing_cpoe      0.445    0.443    1.005    0.315    0.445    0.030\n    pass_comp_pct    -0.074    0.005  -16.319    0.000   -0.074   -0.559\n    passer_rating    -0.823    0.899   -0.916    0.360   -0.823   -0.030\n    cmpltn_prcnt__    0.928    0.529    1.755    0.079    0.928    0.072\n  F3 =~ efa1                                                            \n    completinsPrGm    0.459    0.070    6.546    0.000    0.459    0.057\n    attemptsPerGam   -0.593    0.117   -5.044    0.000   -0.593   -0.047\n    pssng_yrdsPrGm    3.155    0.844    3.736    0.000    3.155    0.034\n    passng_tdsPrGm    0.055    0.010    5.543    0.000    0.055    0.080\n    pssng_r_yrdsPG    0.495    1.188    0.417    0.677    0.495    0.004\n    pssng_yrds__PG   -1.413    1.336   -1.057    0.290   -1.413   -0.022\n    pssng_frst_dPG    0.218    0.044    4.962    0.000    0.218    0.048\n    avg_cmpltd_r_y    0.080    0.139    0.577    0.564    0.080    0.026\n    avg_ntndd_r_yr   -0.087    0.083   -1.040    0.298   -0.087   -0.025\n    aggressiveness   -2.489    0.720   -3.459    0.001   -2.489   -0.385\n    mx_cmpltd_r_ds    1.865    0.844    2.211    0.027    1.865    0.219\n    avg_air_distnc   -0.028    0.096   -0.290    0.772   -0.028   -0.008\n    max_air_distnc   -0.381    0.675   -0.565    0.572   -0.381   -0.051\n    avg_r_yrds_t_s    0.043    0.119    0.364    0.716    0.043    0.012\n    passing_cpoe     14.463    0.607   23.821    0.000   14.463    0.983\n    pass_comp_pct     0.146    0.005   27.318    0.000    0.146    1.096\n    passer_rating    25.152    1.873   13.428    0.000   25.152    0.922\n    cmpltn_prcnt__   12.370    0.716   17.277    0.000   12.370    0.956\n\nCovariances:\n                                       Estimate  Std.Err  z-value  P(&gt;|z|)\n .completionsPerGame ~~                                                   \n   .attemptsPerGam                        3.293    0.164   20.104    0.000\n .passing_yardsPerGame ~~                                                 \n   .pssng_yrds__PG                       92.749    7.776   11.928    0.000\n .attemptsPerGame ~~                                                      \n   .pssng_yrdsPrGm                        3.505    0.591    5.930    0.000\n   .pssng_r_yrdsPG                       53.095    2.680   19.809    0.000\n .passing_air_yardsPerGame ~~                                             \n   .pssng_yrds__PG                     -225.138   40.626   -5.542    0.000\n .passing_yards_after_catchPerGame ~~                                     \n   .avg_cmpltd_r_y                       -5.993    0.423  -14.166    0.000\n .passing_tdsPerGame ~~                                                   \n   .passer_rating                         1.762    0.146   12.085    0.000\n .completionsPerGame ~~                                                   \n   .pssng_r_yrdsPG                       13.078    1.148   11.390    0.000\n .max_completed_air_distance ~~                                           \n   .max_air_distnc                        9.132    0.882   10.356    0.000\n .aggressiveness ~~                                                       \n   .cmpltn_prcnt__                        5.806    0.731    7.947    0.000\n  F1 ~~                                                                   \n    F2                                   -0.100                           \n    F3                                    0.175                           \n  F2 ~~                                                                   \n    F3                                    0.490                           \n   Std.lv  Std.all\n                  \n    3.293    0.776\n                  \n   92.749    0.611\n                  \n    3.505    0.129\n   53.095    0.606\n                  \n -225.138   -0.460\n                  \n   -5.993   -0.435\n                  \n    1.762    0.517\n                  \n   13.078    0.314\n                  \n    9.132    0.455\n                  \n    5.806    0.529\n                  \n   -0.100   -0.100\n    0.175    0.175\n                  \n    0.490    0.490\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm   13.216    0.180   73.516    0.000   13.216    1.643\n   .attemptsPerGam   21.828    0.282   77.531    0.000   21.828    1.734\n   .pssng_yrdsPrGm  147.994    2.082   71.090    0.000  147.994    1.589\n   .passng_tdsPrGm    0.847    0.015   54.871    0.000    0.847    1.226\n   .pssng_r_yrdsPG  132.999    2.704   49.186    0.000  132.999    1.100\n   .pssng_yrds__PG   87.622    1.467   59.725    0.000   87.622    1.334\n   .pssng_frst_dPG    7.143    0.102   70.156    0.000    7.143    1.568\n   .avg_cmpltd_r_y    4.317    0.129   33.366    0.000    4.317    1.395\n   .avg_ntndd_r_yr    6.447    0.127   50.837    0.000    6.447    1.864\n   .aggressiveness   15.228    0.496   30.718    0.000   15.228    2.357\n   .mx_cmpltd_r_ds   34.773    0.502   69.299    0.000   34.773    4.087\n   .avg_air_distnc   19.820    0.128  154.559    0.000   19.820    6.022\n   .max_air_distnc   44.212    0.529   83.576    0.000   44.212    5.968\n   .avg_r_yrds_t_s   -2.561    0.143  -17.855    0.000   -2.561   -0.739\n   .passing_cpoe     -7.217    0.460  -15.687    0.000   -7.217   -0.491\n   .pass_comp_pct     0.589    0.003  185.453    0.000    0.589    4.421\n   .passer_rating    73.910    1.333   55.461    0.000   73.910    2.708\n   .cmpltn_prcnt__   -6.539    0.481  -13.599    0.000   -6.539   -0.505\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm    2.018    0.103   19.608    0.000    2.018    0.031\n   .attemptsPerGam    8.913    0.352   25.293    0.000    8.913    0.056\n   .pssng_yrdsPrGm   82.770    8.514    9.721    0.000   82.770    0.010\n   .passng_tdsPrGm    0.114    0.006   19.528    0.000    0.114    0.240\n   .pssng_r_yrdsPG  861.327   93.144    9.247    0.000  861.327    0.059\n   .pssng_yrds__PG  278.493   19.893   14.000    0.000  278.493    0.065\n   .pssng_frst_dPG    0.318    0.027   11.906    0.000    0.318    0.015\n   .avg_cmpltd_r_y    0.681    0.063   10.855    0.000    0.681    0.071\n   .avg_ntndd_r_yr    0.004    0.014    0.270    0.787    0.004    0.000\n   .aggressiveness   21.554                              21.554    0.516\n   .mx_cmpltd_r_ds   21.431                              21.431    0.296\n   .avg_air_distnc    0.481       NA                      0.481    0.044\n   .max_air_distnc   18.765       NA                     18.765    0.342\n   .avg_r_yrds_t_s    0.288                               0.288    0.024\n   .passing_cpoe      4.948       NA                      4.948    0.023\n   .pass_comp_pct     0.002                               0.002    0.085\n   .passer_rating   101.498                             101.498    0.136\n   .cmpltn_prcnt__    5.587       NA                      5.587    0.033\n    F1                1.000                               1.000    1.000\n    F2                1.000                               1.000    1.000\n    F3                1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    completinsPrGm    0.969\n    attemptsPerGam    0.944\n    pssng_yrdsPrGm    0.990\n    passng_tdsPrGm    0.760\n    pssng_r_yrdsPG    0.941\n    pssng_yrds__PG    0.935\n    pssng_frst_dPG    0.985\n    avg_cmpltd_r_y    0.929\n    avg_ntndd_r_yr    1.000\n    aggressiveness    0.484\n    mx_cmpltd_r_ds    0.704\n    avg_air_distnc    0.956\n    max_air_distnc    0.658\n    avg_r_yrds_t_s    0.976\n    passing_cpoe      0.977\n    pass_comp_pct     0.915\n    passer_rating     0.864\n    cmpltn_prcnt__    0.967\n\n\nThe model fits substantially better (though not perfectly) with the additional correlated residuals. Below are the model fit indices:\n\nCodelavaan::fitMeasures(\n  efa3factorModified_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n               chisq                   df               pvalue \n            1399.681               92.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n             837.974               92.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.670            43031.563              153.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.084                0.970 \n                 tli                 srmr         rmsea.robust \n               0.949                0.147                0.189 \n          cfi.robust           tli.robust \n               0.948                0.914 \n\n\n\nCodelavaan::residuals(\n  efa3factorModified_fit,\n  type = \"cor\")\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n                                        cmplPG attmPG pssng_yPG pssng_tPG\ncompletionsPerGame                       0.000                           \nattemptsPerGame                          0.000  0.000                    \npassing_yardsPerGame                    -0.001 -0.001     0.000          \npassing_tdsPerGame                      -0.029 -0.046     0.004     0.000\npassing_air_yardsPerGame                 0.004  0.003    -0.001    -0.036\npassing_yards_after_catchPerGame        -0.004 -0.002     0.001     0.017\npassing_first_downsPerGame               0.001  0.000     0.000     0.007\navg_completed_air_yards                 -0.058 -0.050    -0.031    -0.001\navg_intended_air_yards                  -0.052 -0.046    -0.041    -0.022\naggressiveness                          -0.031 -0.026    -0.038    -0.026\nmax_completed_air_distance               0.052  0.073     0.083     0.105\navg_air_distance                        -0.044 -0.036    -0.032    -0.016\nmax_air_distance                         0.038  0.038     0.035     0.043\navg_air_yards_to_sticks                 -0.061 -0.054    -0.047    -0.018\npassing_cpoe                             0.048  0.052     0.053     0.052\npass_comp_pct                            0.002  0.001     0.001    -0.003\npasser_rating                            0.046  0.045     0.080     0.071\ncompletion_percentage_above_expectation  0.019  0.021     0.019     0.015\n                                        pssng_r_PG p___PG pssng_f_PG avg_c__\ncompletionsPerGame                                                          \nattemptsPerGame                                                             \npassing_yardsPerGame                                                        \npassing_tdsPerGame                                                          \npassing_air_yardsPerGame                     0.000                          \npassing_yards_after_catchPerGame             0.001  0.000                   \npassing_first_downsPerGame                  -0.002  0.003      0.000        \navg_completed_air_yards                     -0.062  0.004     -0.033   0.000\navg_intended_air_yards                      -0.053 -0.011     -0.039  -0.009\naggressiveness                              -0.030 -0.033     -0.019   0.057\nmax_completed_air_distance                  -0.061  0.152      0.055  -0.075\navg_air_distance                            -0.059  0.010     -0.035  -0.019\nmax_air_distance                             0.031  0.041      0.025  -0.049\navg_air_yards_to_sticks                     -0.071 -0.004     -0.041  -0.007\npassing_cpoe                                -0.064  0.128      0.054  -0.244\npass_comp_pct                                0.003  0.000      0.000  -0.150\npasser_rating                               -0.192  0.264      0.076  -0.384\ncompletion_percentage_above_expectation     -0.032  0.050      0.022  -0.154\n                                        avg_n__ aggrss mx_c__ avg_r_ mx_r_d\ncompletionsPerGame                                                         \nattemptsPerGame                                                            \npassing_yardsPerGame                                                       \npassing_tdsPerGame                                                         \npassing_air_yardsPerGame                                                   \npassing_yards_after_catchPerGame                                           \npassing_first_downsPerGame                                                 \navg_completed_air_yards                                                    \navg_intended_air_yards                    0.000                            \naggressiveness                            0.050  0.000                     \nmax_completed_air_distance               -0.144 -0.040  0.000              \navg_air_distance                         -0.001  0.054 -0.110  0.000       \nmax_air_distance                         -0.012 -0.032 -0.027  0.009  0.000\navg_air_yards_to_sticks                  -0.001  0.067 -0.134 -0.001 -0.019\npassing_cpoe                             -0.240 -0.105 -0.198 -0.258 -0.066\npass_comp_pct                            -0.139 -0.050 -0.108 -0.171  0.022\npasser_rating                            -0.413 -0.250 -0.244 -0.426 -0.202\ncompletion_percentage_above_expectation  -0.155 -0.047 -0.160 -0.173 -0.008\n                                        av____ pssng_ pss_c_ pssr_r cmp___\ncompletionsPerGame                                                        \nattemptsPerGame                                                           \npassing_yardsPerGame                                                      \npassing_tdsPerGame                                                        \npassing_air_yardsPerGame                                                  \npassing_yards_after_catchPerGame                                          \npassing_first_downsPerGame                                                \navg_completed_air_yards                                                   \navg_intended_air_yards                                                    \naggressiveness                                                            \nmax_completed_air_distance                                                \navg_air_distance                                                          \nmax_air_distance                                                          \navg_air_yards_to_sticks                  0.000                            \npassing_cpoe                            -0.260  0.000                     \npass_comp_pct                           -0.173  0.066  0.000              \npasser_rating                           -0.429 -0.051  0.102  0.000       \ncompletion_percentage_above_expectation -0.179 -0.007  0.019 -0.103  0.000\n\n$mean\n                     completionsPerGame                         attemptsPerGame \n                                  0.000                                   0.000 \n                   passing_yardsPerGame                      passing_tdsPerGame \n                                  0.000                                   0.000 \n               passing_air_yardsPerGame        passing_yards_after_catchPerGame \n                                  0.000                                   0.000 \n             passing_first_downsPerGame                 avg_completed_air_yards \n                                  0.000                                   0.190 \n                 avg_intended_air_yards                          aggressiveness \n                                  0.088                                   0.067 \n             max_completed_air_distance                        avg_air_distance \n                                  0.247                                   0.102 \n                       max_air_distance                 avg_air_yards_to_sticks \n                                 -0.027                                   0.137 \n                           passing_cpoe                           pass_comp_pct \n                                  0.104                                  -0.001 \n                          passer_rating completion_percentage_above_expectation \n                                  0.170                                   0.073 \n\n\n\nCodelavaan::modificationindices(\n  efa3factorModified_fit,\n  sort. = TRUE)\n\n\n  \n\n\n\nBelow are factor scores from the model for the first six players:\n\nCodeefa3factorModified_factorScores &lt;- lavaan::lavPredict(efa3factorModified_fit)\n\nhead(efa3factorModified_factorScores)\n\n              F1          F2         F3\n[1,] -0.03484124 -0.91444005  0.3509666\n[2,]  0.19471699 -1.64270152 -1.0607710\n[3,]  0.43512475 -2.27576493 -1.5450498\n[4,]  0.15369026  0.29300540  0.7922653\n[5,]  0.83015427  1.09543545  0.4754419\n[6,] -0.13076090  0.07123358 -0.1724552\n\nCodedataForFA_efa &lt;- cbind(dataForFA, efa3factorModified_factorScores)\n\n\nThe path diagram of the modified ESEM model with correlated residuals is in Figure 22.19.\n\nCodelavaanPlot::lavaanPlot(\n  efa3factorModified_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nFigure 22.19: Path Diagram of the Three-Factor Exploratory Structural Equation Model With Correlated Residuals.\n\n\n\n\nCodelavaangui::plot_lavaan(efa3factorModified_fit)\n\n\nModel fit of nested models can be compared with a chi-square difference test. This allows us to evaluate whether the more complex model fits signficantly better than the less complex model. In this case, our more complex model is the three-factor model with correlated residuals; the less complex model is the three-factor model without correlated residuals. The modified model with the correlated residuals and the original model are considered “nested” models. The original model is nested within the modified model because the modified model includes all of the terms of the original model along with additional terms.\n\nCodeanova(\n  efa3factorModified_fit,\n  efa3factor_fit\n)\n\n\n  \n\n\n\nIn this case, the model with the correlated residuals fit significantly better (i.e., has a significantly smaller chi-square value) than the model without the correlated residuals.\nHere are the variables that had a standardized loading greater than 0.3 on each of the factors:\n\nCodefactor1vars &lt;- c(\n  \"completionsPerGame\",\"attemptsPerGame\",\"passing_yardsPerGame\",\"passing_tdsPerGame\",\n  \"passing_air_yardsPerGame\",\"passing_yards_after_catchPerGame\",\"passing_first_downsPerGame\")\n\nfactor2vars &lt;- c(\n  \"avg_completed_air_yards\",\"avg_intended_air_yards\",\"aggressiveness\",\"max_completed_air_distance\",\n  \"avg_air_distance\",\"max_air_distance\",\"avg_air_yards_to_sticks\")\n\nfactor3vars &lt;- c(\n  \"passing_cpoe\",\"pass_comp_pct\",\"passer_rating\",\"completion_percentage_above_expectation\")\n\n\nThe variables that loaded most strongly onto factor 1 appear to reflect Quarterback usage: completions per game, passing attempts per game, passing yards per game, passing touchdowns per game, passing air yards (total horizontal distance the ball travels on all pass attempts) per game, passing yards after the catch per game, and first downs gained per game by passing. Quarterbacks who tend to throw more tend to have higher levels on those variables. Thus, we label component 1 as “Usage”, which reflects total Quarterback involvement, regardless of efficiency or outcome.\nThe variables that loaded most strongly onto factor 2 appear to reflect Quarterback aggressiveness: average air yards on completed passes, average air yards on all attempted passes, aggressiveness (percentage of passing attempts thrown into tight windows, where there is a defender within one yard or less of the receiver at the time of the completion or incompletion), average amount of air yards ahead of or behind the first down marker on passing attempts, average air distance (the true three-dimensional distance the ball travels in the air), maximum air distance, and maximum air distance on completed passes. Quarterbacks who throw the ball farther and into tighter windows tend to have higher values on those variables. Thus, we label component 2 as “Aggressiveness”, which reflects throwing longer, more difficult passes with a tight window.\nThe variables that loaded most strongly onto factor 3 appear to reflect Quarterback performance: passing completion percentage above expectation, pass completion percentage, and passer rating. Quarterbacks who perform better tend to have higher values on those variables. Thus, we label component 3 as “Performance”.\nHere are the players and seasons that showed the highest levels of Quarterback “Usage”:\n\nCodedataForFA_efa %&gt;% \n  arrange(-F1) %&gt;% \n  select(player_display_name, season, F1, all_of(factor1vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the lowest levels of Quarterback “Usage”:\n\nCodedataForFA_efa %&gt;% \n  arrange(F1) %&gt;% \n  select(player_display_name, season, F1, all_of(factor1vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the highest levels of Quarterback “Aggressiveness”:\n\nCodedataForFA_efa %&gt;% \n  arrange(-F2) %&gt;% \n  select(player_display_name, season, F2, all_of(factor2vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the lowest levels of Quarterback “Aggressiveness”:\n\nCodedataForFA_efa %&gt;% \n  arrange(F2) %&gt;% \n  select(player_display_name, season, F2, all_of(factor2vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the highest levels of Quarterback “Performance”:\n\nCodedataForFA_efa %&gt;% \n  arrange(-F3) %&gt;% \n  select(player_display_name, season, games, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nIf we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the highest levels of Quarterback “Performance”:\n\nCodedataForFA_efa %&gt;% \n  arrange(-F3) %&gt;% \n  filter(games &gt;= 10) %&gt;% \n  select(player_display_name, season, games, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the lowest levels of Quarterback “Performance”:\n\nCodedataForFA_efa %&gt;% \n  arrange(F3) %&gt;% \n  select(player_display_name, season, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nIf we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the lowest levels of Quarterback “Performance”:\n\nCodedataForFA_efa %&gt;% \n  arrange(F3) %&gt;% \n  filter(games &gt;= 10) %&gt;% \n  select(player_display_name, season, games, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-cfaExample",
    "href": "factor-analysis.html#sec-cfaExample",
    "title": "22  Factor Analysis",
    "section": "\n22.7 Example of Confirmatory Factor Analysis",
    "text": "22.7 Example of Confirmatory Factor Analysis\nWe fit CFA models using the lavaan::cfa() function of the lavaan package (Rosseel, 2012; Rosseel et al., 2024). We compare one-, two-, and three-factor CFA models.\nBelow is the syntax for the one-factor CFA model:\n\nCodecfa1factor_syntax &lt;- '\n #Factor loadings\n F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + \n passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame + \n avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + \n avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + \n passer_rating + completion_percentage_above_expectation\n'\n\n\n\nCodecfa1factor_fit &lt;- lavaan::cfa(\n  cfa1factor_syntax,\n  data = dataForFA,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  bounds = \"standard\",\n  std.lv = TRUE,\n  cluster = \"player_id\", # account for nested data within the same player (i.e., longitudinal data)\n  em.h1.iter.max = 2000000)\n\n\n\nCodesummary(\n  cfa1factor_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 187 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        54\n  Row rank of the constraints matrix                36\n\n  Number of observations                          2002\n  Number of clusters [player_id]                   396\n  Number of missing patterns                         4\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              16641.448    8547.405\n  Degrees of freedom                                135         135\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.947\n    Yuan-Bentler correction (Mplus variant)                        \n\nModel Test Baseline Model:\n\n  Test statistic                             43031.563   21852.033\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.969\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.615       0.612\n  Tucker-Lewis Index (TLI)                       0.564       0.561\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.220\n  Robust Tucker-Lewis Index (TLI)                            0.116\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -62538.594  -62538.594\n  Scaling correction factor                                  2.590\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -54217.870  -54217.870\n  Scaling correction factor                                  2.131\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              125185.188  125185.188\n  Bayesian (BIC)                            125487.691  125487.691\n  Sample-size adjusted Bayesian (SABIC)     125316.130  125316.130\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.247       0.176\n  90 Percent confidence interval - lower         0.244       0.174\n  90 Percent confidence interval - upper         0.250       0.179\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.571\n  90 Percent confidence interval - lower                     0.553\n  90 Percent confidence interval - upper                     0.589\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.380       0.380\n\nParameter Estimates:\n\n  Standard errors                        Robust.cluster\n  Information                                  Observed\n  Observed information based on                 Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  F1 =~                                                                 \n    completinsPrGm    7.938    0.126   62.929    0.000    7.938    0.987\n    attemptsPerGam   12.274    0.178   69.044    0.000   12.274    0.974\n    pssng_yrdsPrGm   92.610    1.513   61.202    0.000   92.610    0.994\n    passng_tdsPrGm    0.597    0.019   31.445    0.000    0.597    0.865\n    pssng_r_yrdsPG   83.918    3.630   23.117    0.000   83.918    0.694\n    pssng_yrds__PG   50.712    1.537   32.996    0.000   50.712    0.773\n    pssng_frst_dPG    4.519    0.080   56.260    0.000    4.519    0.992\n    avg_cmpltd_r_y    0.411    0.105    3.912    0.000    0.411    0.285\n    avg_ntndd_r_yr    0.213    0.108    1.976    0.048    0.213    0.152\n    aggressiveness   -0.275    0.478   -0.575    0.565   -0.275   -0.053\n    mx_cmpltd_r_ds    2.806    0.424    6.615    0.000    2.806    0.473\n    avg_air_distnc    0.029    0.116    0.249    0.803    0.029    0.020\n    max_air_distnc    1.959    0.436    4.490    0.000    1.959    0.368\n    avg_r_yrds_t_s    0.317    0.114    2.783    0.005    0.317    0.214\n    passing_cpoe      3.527    0.515    6.852    0.000    3.527    0.293\n    pass_comp_pct     0.040    0.005    7.527    0.000    0.040    0.301\n    passer_rating    11.688    1.173    9.963    0.000   11.688    0.629\n    cmpltn_prcnt__    2.982    0.422    7.063    0.000    2.982    0.514\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm   13.216    0.396   33.415    0.000   13.216    1.643\n   .attemptsPerGam   21.828    0.588   37.096    0.000   21.828    1.733\n   .pssng_yrdsPrGm  147.994    4.667   31.714    0.000  147.994    1.589\n   .passng_tdsPrGm    0.847    0.036   23.649    0.000    0.847    1.226\n   .pssng_r_yrdsPG  133.000    5.791   22.966    0.000  133.000    1.099\n   .pssng_yrds__PG   87.622    3.002   29.187    0.000   87.622    1.335\n   .pssng_frst_dPG    7.143    0.230   31.073    0.000    7.143    1.568\n   .avg_cmpltd_r_y    5.553    0.100   55.435    0.000    5.553    3.848\n   .avg_ntndd_r_yr    7.933    0.107   74.074    0.000    7.933    5.668\n   .aggressiveness   16.709    0.420   39.795    0.000   16.709    3.250\n   .mx_cmpltd_r_ds   37.848    0.436   86.769    0.000   37.848    6.380\n   .avg_air_distnc   21.196    0.108  195.850    0.000   21.196   14.670\n   .max_air_distnc   46.718    0.427  109.350    0.000   46.718    8.769\n   .avg_r_yrds_t_s   -1.078    0.113   -9.534    0.000   -1.078   -0.727\n   .passing_cpoe     -2.805    0.429   -6.533    0.000   -2.805   -0.233\n   .pass_comp_pct     0.588    0.004  136.966    0.000    0.588    4.369\n   .passer_rating    79.793    1.215   65.679    0.000   79.793    4.291\n   .cmpltn_prcnt__   -2.434    0.421   -5.789    0.000   -2.434   -0.420\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm    1.697    0.123   13.754    0.000    1.697    0.026\n   .attemptsPerGam    8.032    0.458   17.551    0.000    8.032    0.051\n   .pssng_yrdsPrGm   99.645    9.143   10.899    0.000   99.645    0.011\n   .passng_tdsPrGm    0.120    0.007   17.338    0.000    0.120    0.252\n   .pssng_r_yrdsPG 7595.814  516.591   14.704    0.000 7595.814    0.519\n   .pssng_yrds__PG 1737.341  130.798   13.283    0.000 1737.341    0.403\n   .pssng_frst_dPG    0.335    0.025   13.211    0.000    0.335    0.016\n   .avg_cmpltd_r_y    1.914    0.157   12.166    0.000    1.914    0.919\n   .avg_ntndd_r_yr    1.914    0.170   11.279    0.000    1.914    0.977\n   .aggressiveness   26.364    3.307    7.972    0.000   26.364    0.997\n   .mx_cmpltd_r_ds   27.316    2.789    9.794    0.000   27.316    0.776\n   .avg_air_distnc    2.087    0.175   11.909    0.000    2.087    1.000\n   .max_air_distnc   24.549    2.486    9.875    0.000   24.549    0.865\n   .avg_r_yrds_t_s    2.102    0.210   10.025    0.000    2.102    0.954\n   .passing_cpoe    132.904   14.303    9.292    0.000  132.904    0.914\n   .pass_comp_pct     0.016    0.001   10.991    0.000    0.016    0.910\n   .passer_rating   209.149   20.026   10.444    0.000  209.149    0.605\n   .cmpltn_prcnt__   24.783    2.838    8.732    0.000   24.783    0.736\n    F1                1.000                               1.000    1.000\n\nR-Square:\n                   Estimate\n    completinsPrGm    0.974\n    attemptsPerGam    0.949\n    pssng_yrdsPrGm    0.989\n    passng_tdsPrGm    0.748\n    pssng_r_yrdsPG    0.481\n    pssng_yrds__PG    0.597\n    pssng_frst_dPG    0.984\n    avg_cmpltd_r_y    0.081\n    avg_ntndd_r_yr    0.023\n    aggressiveness    0.003\n    mx_cmpltd_r_ds    0.224\n    avg_air_distnc    0.000\n    max_air_distnc    0.135\n    avg_r_yrds_t_s    0.046\n    passing_cpoe      0.086\n    pass_comp_pct     0.090\n    passer_rating     0.395\n    cmpltn_prcnt__    0.264\n\n\nThe one-factor model did not fit well according to the fit indices:\n\nCodelavaan::fitMeasures(\n  cfa1factor_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n               chisq                   df               pvalue \n           16641.448              135.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n            8547.405              135.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.947            43031.563              153.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.247                0.615 \n                 tli                 srmr         rmsea.robust \n               0.564                0.380                0.571 \n          cfi.robust           tli.robust \n               0.220                0.116 \n\n\n\nCodelavaan::residuals(\n  cfa1factor_fit,\n  type = \"cor\")\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n                                        cmplPG attmPG pssng_yPG pssng_tPG\ncompletionsPerGame                       0.000                           \nattemptsPerGame                          0.023  0.000                    \npassing_yardsPerGame                    -0.003 -0.003     0.000          \npassing_tdsPerGame                      -0.025 -0.050     0.011     0.000\npassing_air_yardsPerGame                 0.012  0.009     0.003    -0.021\npassing_yards_after_catchPerGame        -0.008  0.014     0.009     0.005\npassing_first_downsPerGame              -0.001 -0.006     0.001     0.014\navg_completed_air_yards                 -0.411 -0.432    -0.378    -0.288\navg_intended_air_yards                  -0.311 -0.333    -0.293    -0.229\naggressiveness                          -0.126 -0.111    -0.121    -0.105\nmax_completed_air_distance              -0.231 -0.250    -0.198    -0.120\navg_air_distance                        -0.221 -0.241    -0.201    -0.150\nmax_air_distance                        -0.212 -0.232    -0.209    -0.159\navg_air_yards_to_sticks                 -0.352 -0.374    -0.331    -0.251\npassing_cpoe                            -0.095 -0.180    -0.102    -0.040\npass_comp_pct                           -0.001 -0.083    -0.019     0.020\npasser_rating                           -0.272 -0.352    -0.251    -0.084\ncompletion_percentage_above_expectation -0.357 -0.438    -0.370    -0.281\n                                        pssng_r_PG p___PG pssng_f_PG avg_c__\ncompletionsPerGame                                                          \nattemptsPerGame                                                             \npassing_yardsPerGame                                                        \npassing_tdsPerGame                                                          \npassing_air_yardsPerGame                     0.000                          \npassing_yards_after_catchPerGame            -0.428  0.000                   \npassing_first_downsPerGame                   0.000 -0.003      0.000        \navg_completed_air_yards                      0.345 -0.862     -0.381   0.000\navg_intended_air_yards                       0.442 -0.792     -0.293   0.910\naggressiveness                               0.323 -0.439     -0.108   0.652\nmax_completed_air_distance                   0.279 -0.546     -0.226   0.549\navg_air_distance                             0.474 -0.689     -0.206   0.914\nmax_air_distance                             0.403 -0.619     -0.220   0.607\navg_air_yards_to_sticks                      0.396 -0.806     -0.327   0.885\npassing_cpoe                                 0.171 -0.339     -0.093   0.192\npass_comp_pct                               -0.028 -0.052     -0.009  -0.235\npasser_rating                               -0.143 -0.282     -0.247  -0.147\ncompletion_percentage_above_expectation      0.059 -0.614     -0.358   0.246\n                                        avg_n__ aggrss mx_c__ avg_r_ mx_r_d\ncompletionsPerGame                                                         \nattemptsPerGame                                                            \npassing_yardsPerGame                                                       \npassing_tdsPerGame                                                         \npassing_air_yardsPerGame                                                   \npassing_yards_after_catchPerGame                                           \npassing_first_downsPerGame                                                 \navg_completed_air_yards                                                    \navg_intended_air_yards                    0.000                            \naggressiveness                            0.676  0.000                     \nmax_completed_air_distance                0.555  0.382  0.000              \navg_air_distance                          0.971  0.657  0.622  0.000       \nmax_air_distance                          0.717  0.467  0.597  0.757  0.000\navg_air_yards_to_sticks                   0.953  0.677  0.539  0.957  0.682\npassing_cpoe                              0.213 -0.054  0.258  0.225  0.228\npass_comp_pct                            -0.233 -0.367 -0.036 -0.225 -0.100\npasser_rating                            -0.122 -0.242 -0.003 -0.067 -0.086\ncompletion_percentage_above_expectation   0.294  0.111  0.207  0.333  0.226\n                                        av____ pssng_ pss_c_ pssr_r cmp___\ncompletionsPerGame                                                        \nattemptsPerGame                                                           \npassing_yardsPerGame                                                      \npassing_tdsPerGame                                                        \npassing_air_yardsPerGame                                                  \npassing_yards_after_catchPerGame                                          \npassing_first_downsPerGame                                                \navg_completed_air_yards                                                   \navg_intended_air_yards                                                    \naggressiveness                                                            \nmax_completed_air_distance                                                \navg_air_distance                                                          \nmax_air_distance                                                          \navg_air_yards_to_sticks                  0.000                            \npassing_cpoe                             0.199  0.000                     \npass_comp_pct                           -0.249  0.768  0.000              \npasser_rating                           -0.148  0.667  0.697  0.000       \ncompletion_percentage_above_expectation  0.261  0.813  0.630  0.466  0.000\n\n$mean\n                     completionsPerGame                         attemptsPerGame \n                                  0.000                                   0.000 \n                   passing_yardsPerGame                      passing_tdsPerGame \n                                  0.000                                   0.000 \n               passing_air_yardsPerGame        passing_yards_after_catchPerGame \n                                  0.000                                   0.000 \n             passing_first_downsPerGame                 avg_completed_air_yards \n                                  0.000                                  -0.272 \n                 avg_intended_air_yards                          aggressiveness \n                                 -0.344                                  -0.155 \n             max_completed_air_distance                        avg_air_distance \n                                 -0.197                                  -0.322 \n                       max_air_distance                 avg_air_yards_to_sticks \n                                 -0.361                                  -0.314 \n                           passing_cpoe                           pass_comp_pct \n                                 -0.226                                   0.013 \n                          passer_rating completion_percentage_above_expectation \n                                 -0.057                                  -0.266 \n\n\n\nCodelavaan::modificationindices(\n  cfa1factor_fit,\n  sort. = TRUE)\n\n\n  \n\n\n\nBelow are factor scores from the model for the first six players:\n\nCodecfa1factor_factorScores &lt;- lavaan::lavPredict(cfa1factor_fit)\n\nhead(cfa1factor_factorScores)\n\n              F1\n[1,] -0.06560882\n[2,]  0.16433996\n[3,]  0.42149699\n[4,]  0.11697062\n[5,]  0.88607540\n[6,] -0.06929236\n\n\nThe path diagram of the one-factor CFA model is in Figure 22.20.\n\nCodelavaanPlot::lavaanPlot(\n  cfa1factor_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nFigure 22.20: Path Diagram of the One-Factor Confirmatory Factor Analysis Model.\n\n\n\n\nCodelavaangui::plot_lavaan(cfa1factor_fit)\n\n\nBelow is the syntax for the two-factor CFA model:\n\nCodecfa2factor_syntax &lt;- '\n #Factor loadings\n F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + \n passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame\n \n F2 =~ avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + \n avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + \n passer_rating + completion_percentage_above_expectation\n'\n\n\n\nCodecfa2factor_fit &lt;- lavaan::cfa(\n  cfa2factor_syntax,\n  data = dataForFA,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  bounds = \"standard\",\n  std.lv = TRUE,\n  cluster = \"player_id\", # account for nested data within the same player (i.e., longitudinal data)\n  em.h1.iter.max = 2000000)\n\n\n\nCodesummary(\n  cfa2factor_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 2164 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        55\n  Row rank of the constraints matrix                37\n\n  Number of observations                          2002\n  Number of clusters [player_id]                   396\n  Number of missing patterns                         4\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              13378.048    6637.854\n  Degrees of freedom                                134         134\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   2.015\n    Yuan-Bentler correction (Mplus variant)                        \n\nModel Test Baseline Model:\n\n  Test statistic                             43031.563   21852.033\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.969\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.691       0.700\n  Tucker-Lewis Index (TLI)                       0.647       0.658\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.503\n  Robust Tucker-Lewis Index (TLI)                            0.433\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -60906.894  -60906.894\n  Scaling correction factor                                  2.411\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -54217.870  -54217.870\n  Scaling correction factor                                  2.131\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              121923.789  121923.789\n  Bayesian (BIC)                            122231.893  122231.893\n  Sample-size adjusted Bayesian (SABIC)     122057.155  122057.155\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.222       0.156\n  90 Percent confidence interval - lower         0.219       0.153\n  90 Percent confidence interval - upper         0.225       0.158\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.457\n  90 Percent confidence interval - lower                     0.439\n  90 Percent confidence interval - upper                     0.476\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.348       0.348\n\nParameter Estimates:\n\n  Standard errors                        Robust.cluster\n  Information                                  Observed\n  Observed information based on                 Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n  F1 =~                                                                  \n    completinsPrGm    7.927    0.125    63.503    0.000    7.927    0.987\n    attemptsPerGam   12.263    0.175    69.900    0.000   12.263    0.975\n    pssng_yrdsPrGm   92.397    1.500    61.596    0.000   92.397    0.994\n    passng_tdsPrGm    0.595    0.019    31.402    0.000    0.595    0.863\n    pssng_r_yrdsPG   83.777    3.595    23.302    0.000   83.777    0.693\n    pssng_yrds__PG   50.610    1.522    33.262    0.000   50.610    0.772\n    pssng_frst_dPG    4.509    0.080    56.535    0.000    4.509    0.992\n  F2 =~                                                                  \n    avg_cmpltd_r_y    2.677                                2.677    0.951\n    avg_ntndd_r_yr    3.272    0.090    36.239    0.000    3.272    0.996\n    aggressiveness    4.724    0.650     7.265    0.000    4.724    0.705\n    mx_cmpltd_r_ds    6.921    0.566    12.230    0.000    6.921    0.819\n    avg_air_distnc    3.036    0.128    23.705    0.000    3.036    0.974\n    max_air_distnc    6.366    0.638     9.975    0.000    6.366    0.820\n    avg_r_yrds_t_s    3.284       NA                       3.284    0.989\n    passing_cpoe     10.466    0.488    21.450    0.000   10.466    0.872\n    pass_comp_pct     0.111    0.005    20.441    0.000    0.111    0.844\n    passer_rating     7.212    2.721     2.651    0.008    7.212    0.397\n    cmpltn_prcnt__    2.497    0.806     3.100    0.002    2.497    0.422\n\nCovariances:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n  F1 ~~                                                                  \n    F2                0.288    0.039     7.373    0.000    0.288    0.288\n\nIntercepts:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm   13.216    0.396    33.415    0.000   13.216    1.646\n   .attemptsPerGam   21.828    0.588    37.096    0.000   21.828    1.736\n   .pssng_yrdsPrGm  147.994    4.667    31.714    0.000  147.994    1.592\n   .passng_tdsPrGm    0.847    0.036    23.649    0.000    0.847    1.228\n   .pssng_r_yrdsPG  132.999    5.791    22.966    0.000  132.999    1.100\n   .pssng_yrds__PG   87.622    3.002    29.187    0.000   87.622    1.336\n   .pssng_frst_dPG    7.143    0.230    31.073    0.000    7.143    1.571\n   .avg_cmpltd_r_y    5.102    0.110    46.242    0.000    5.102    1.813\n   .avg_ntndd_r_yr    7.237    0.135    53.525    0.000    7.237    2.202\n   .aggressiveness   15.414    0.353    43.647    0.000   15.414    2.301\n   .mx_cmpltd_r_ds   37.550    0.391    96.078    0.000   37.550    4.444\n   .avg_air_distnc   20.467    0.137   149.518    0.000   20.467    6.566\n   .max_air_distnc   46.133    0.409   112.709    0.000   46.133    5.943\n   .avg_r_yrds_t_s   -1.725    0.132   -13.112    0.000   -1.725   -0.520\n   .passing_cpoe     -3.291    0.374    -8.801    0.000   -3.291   -0.274\n   .pass_comp_pct     0.588    0.004   138.923    0.000    0.588    4.452\n   .passer_rating    83.848    1.230    68.169    0.000   83.848    4.621\n   .cmpltn_prcnt__   -1.561    0.364    -4.286    0.000   -1.561   -0.264\n\nVariances:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm    1.620    0.123    13.205    0.000    1.620    0.025\n   .attemptsPerGam    7.732    0.463    16.707    0.000    7.732    0.049\n   .pssng_yrdsPrGm  106.683    9.637    11.070    0.000  106.683    0.012\n   .passng_tdsPrGm    0.122    0.007    17.275    0.000    0.122    0.256\n   .pssng_r_yrdsPG 7592.718    0.353 21500.229    0.000 7592.718    0.520\n   .pssng_yrds__PG 1737.909  132.297    13.136    0.000 1737.909    0.404\n   .pssng_frst_dPG    0.343    0.027    12.923    0.000    0.343    0.017\n   .avg_cmpltd_r_y    0.750    0.078     9.587    0.000    0.750    0.095\n   .avg_ntndd_r_yr    0.096    0.027     3.537    0.000    0.096    0.009\n   .aggressiveness   22.565    2.996     7.533    0.000   22.565    0.503\n   .mx_cmpltd_r_ds   23.506    2.120    11.088    0.000   23.506    0.329\n   .avg_air_distnc    0.500    0.042    11.895    0.000    0.500    0.051\n   .max_air_distnc   19.728    2.181     9.045    0.000   19.728    0.327\n   .avg_r_yrds_t_s    0.230    0.039     5.859    0.000    0.230    0.021\n   .passing_cpoe     34.454    3.234    10.653    0.000   34.454    0.239\n   .pass_comp_pct     0.005    0.000    10.895    0.000    0.005    0.288\n   .passer_rating   277.206   23.560    11.766    0.000  277.206    0.842\n   .cmpltn_prcnt__   28.724    2.988     9.613    0.000   28.724    0.822\n    F1                1.000                                1.000    1.000\n    F2                1.000                                1.000    1.000\n\nR-Square:\n                   Estimate\n    completinsPrGm    0.975\n    attemptsPerGam    0.951\n    pssng_yrdsPrGm    0.988\n    passng_tdsPrGm    0.744\n    pssng_r_yrdsPG    0.480\n    pssng_yrds__PG    0.596\n    pssng_frst_dPG    0.983\n    avg_cmpltd_r_y    0.905\n    avg_ntndd_r_yr    0.991\n    aggressiveness    0.497\n    mx_cmpltd_r_ds    0.671\n    avg_air_distnc    0.949\n    max_air_distnc    0.673\n    avg_r_yrds_t_s    0.979\n    passing_cpoe      0.761\n    pass_comp_pct     0.712\n    passer_rating     0.158\n    cmpltn_prcnt__    0.178\n\n\nThe two-factor model did not fit well according to the fit indices:\n\nCodelavaan::fitMeasures(\n  cfa2factor_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n               chisq                   df               pvalue \n           13378.048              134.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n            6637.854              134.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               2.015            43031.563              153.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.222                0.691 \n                 tli                 srmr         rmsea.robust \n               0.647                0.348                0.457 \n          cfi.robust           tli.robust \n               0.503                0.433 \n\n\n\nCodelavaan::residuals(\n  cfa2factor_fit,\n  type = \"cor\")\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n                                        cmplPG attmPG pssng_yPG pssng_tPG\ncompletionsPerGame                       0.000                           \nattemptsPerGame                          0.021  0.000                    \npassing_yardsPerGame                    -0.003 -0.003     0.000          \npassing_tdsPerGame                      -0.024 -0.048     0.013     0.000\npassing_air_yardsPerGame                 0.012  0.009     0.004    -0.019\npassing_yards_after_catchPerGame        -0.008  0.014     0.010     0.007\npassing_first_downsPerGame              -0.002 -0.007     0.002     0.016\navg_completed_air_yards                 -0.400 -0.422    -0.367    -0.279\navg_intended_air_yards                  -0.444 -0.465    -0.426    -0.344\naggressiveness                          -0.379 -0.361    -0.376    -0.326\nmax_completed_air_distance               0.003 -0.019     0.038     0.085\navg_air_distance                        -0.478 -0.495    -0.460    -0.375\nmax_air_distance                        -0.083 -0.104    -0.078    -0.045\navg_air_yards_to_sticks                 -0.422 -0.444    -0.402    -0.312\npassing_cpoe                            -0.055 -0.139    -0.061    -0.004\npass_comp_pct                            0.056 -0.027     0.038     0.070\npasser_rating                            0.235  0.149     0.260     0.361\ncompletion_percentage_above_expectation  0.030 -0.056     0.020     0.059\n                                        pssng_r_PG p___PG pssng_f_PG avg_c__\ncompletionsPerGame                                                          \nattemptsPerGame                                                             \npassing_yardsPerGame                                                        \npassing_tdsPerGame                                                          \npassing_air_yardsPerGame                     0.000                          \npassing_yards_after_catchPerGame            -0.427  0.000                   \npassing_first_downsPerGame                   0.001 -0.002      0.000        \navg_completed_air_yards                      0.352 -0.854     -0.370   0.000\navg_intended_air_yards                       0.349 -0.896     -0.426   0.006\naggressiveness                               0.145 -0.637     -0.362  -0.034\nmax_completed_air_distance                   0.443 -0.362      0.010  -0.096\navg_air_distance                             0.294 -0.890     -0.465  -0.007\nmax_air_distance                             0.494 -0.518     -0.090  -0.069\navg_air_yards_to_sticks                      0.346 -0.861     -0.397   0.004\npassing_cpoe                                 0.200 -0.307     -0.052  -0.554\npass_comp_pct                                0.012 -0.007      0.048  -0.953\npasser_rating                                0.214  0.115      0.263  -0.346\ncompletion_percentage_above_expectation      0.331 -0.310      0.031  -0.010\n                                        avg_n__ aggrss mx_c__ avg_r_ mx_r_d\ncompletionsPerGame                                                         \nattemptsPerGame                                                            \npassing_yardsPerGame                                                       \npassing_tdsPerGame                                                         \npassing_air_yardsPerGame                                                   \npassing_yards_after_catchPerGame                                           \npassing_first_downsPerGame                                                 \navg_completed_air_yards                                                    \navg_intended_air_yards                    0.000                            \naggressiveness                           -0.034  0.000                     \nmax_completed_air_distance               -0.189 -0.221  0.000              \navg_air_distance                          0.005 -0.030 -0.166  0.000       \nmax_air_distance                         -0.044 -0.131  0.099 -0.034  0.000\navg_air_yards_to_sticks                   0.000 -0.032 -0.170 -0.002 -0.051\npassing_cpoe                             -0.611 -0.685 -0.318 -0.619 -0.379\npass_comp_pct                            -1.027 -0.979 -0.585 -1.041 -0.682\npasser_rating                            -0.422 -0.556 -0.031 -0.442 -0.181\ncompletion_percentage_above_expectation  -0.049 -0.214  0.104 -0.068  0.069\n                                        av____ pssng_ pss_c_ pssr_r cmp___\ncompletionsPerGame                                                        \nattemptsPerGame                                                           \npassing_yardsPerGame                                                      \npassing_tdsPerGame                                                        \npassing_air_yardsPerGame                                                  \npassing_yards_after_catchPerGame                                          \npassing_first_downsPerGame                                                \navg_completed_air_yards                                                   \navg_intended_air_yards                                                    \naggressiveness                                                            \nmax_completed_air_distance                                                \navg_air_distance                                                          \nmax_air_distance                                                          \navg_air_yards_to_sticks                  0.000                            \npassing_cpoe                            -0.602  0.000                     \npass_comp_pct                           -1.020  0.119  0.000              \npasser_rating                           -0.407  0.504  0.550  0.000       \ncompletion_percentage_above_expectation -0.047  0.595  0.428  0.622  0.000\n\n$mean\n                     completionsPerGame                         attemptsPerGame \n                                  0.000                                   0.000 \n                   passing_yardsPerGame                      passing_tdsPerGame \n                                  0.000                                   0.000 \n               passing_air_yardsPerGame        passing_yards_after_catchPerGame \n                                  0.000                                   0.000 \n             passing_first_downsPerGame                 avg_completed_air_yards \n                                  0.000                                  -0.104 \n                 avg_intended_air_yards                          aggressiveness \n                                 -0.142                                   0.039 \n             max_completed_air_distance                        avg_air_distance \n                                 -0.154                                  -0.097 \n                       max_air_distance                 avg_air_yards_to_sticks \n                                 -0.283                                  -0.117 \n                           passing_cpoe                           pass_comp_pct \n                                 -0.189                                   0.008 \n                          passer_rating completion_percentage_above_expectation \n                                 -0.213                                  -0.338 \n\n\n\nCodelavaan::modificationindices(\n  cfa2factor_fit,\n  sort. = TRUE)\n\n\n  \n\n\n\nBelow are factor scores from the model for the first six players:\n\nCodecfa2factor_factorScores &lt;- lavaan::lavPredict(cfa2factor_fit)\n\nhead(cfa2factor_factorScores)\n\n              F1         F2\n[1,] -0.07350235  0.7786304\n[2,]  0.16428295 -0.1707025\n[3,]  0.42916604 -0.2838464\n[4,]  0.11007477  0.6616575\n[5,]  0.88867369  0.1204172\n[6,] -0.06549830 -0.3928992\n\n\nThe path diagram of the two-factor CFA model is in Figure 22.21.\n\nCodelavaanPlot::lavaanPlot(\n  cfa2factor_fit,\n  coefs = TRUE,\n  covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nFigure 22.21: Path Diagram of the Two-Factor Confirmatory Factor Analysis Model.\n\n\n\n\nCodelavaangui::plot_lavaan(cfa2factor_fit)\n\n\nBecause the one-factor model is nested within the two-factor model, we can compare them using a chi-square difference test. The two factor model fit considerably better than the one-factor model in terms of a lower chi-square value:\n\nCodeanova(\n  cfa2factor_fit,\n  cfa1factor_fit\n)\n\n\n  \n\n\n\nBelow is the syntax for the three-factor model:\n\nCodecfa3factor_syntax &lt;- '\n #Factor loadings\n F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + \n passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame\n \n F2 =~ avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + \n avg_air_distance + max_air_distance + avg_air_yards_to_sticks\n \n F3 =~ passing_cpoe + pass_comp_pct + passer_rating + completion_percentage_above_expectation\n'\n\n\n\nCodecfa3factor_fit &lt;- lavaan::cfa(\n  cfa3factor_syntax,\n  data = dataForFA,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  bounds = \"standard\",\n  std.lv = TRUE,\n  cluster = \"player_id\", # account for nested data within the same player (i.e., longitudinal data)\n  em.h1.iter.max = 2000000)\n\n\n\nCodesummary(\n  cfa3factor_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 185 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        57\n  Row rank of the constraints matrix                39\n\n  Number of observations                          2002\n  Number of clusters [player_id]                   396\n  Number of missing patterns                         4\n\nModel Test User Model:\n                                               Standard      Scaled\n  Test Statistic                              10863.020    5689.900\n  Degrees of freedom                                132         132\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.909\n    Yuan-Bentler correction (Mplus variant)                        \n\nModel Test Baseline Model:\n\n  Test statistic                             43031.563   21852.033\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.969\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.750       0.744\n  Tucker-Lewis Index (TLI)                       0.710       0.703\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.714\n  Robust Tucker-Lewis Index (TLI)                            0.669\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -59649.380  -59649.380\n  Scaling correction factor                                  2.643\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -54217.870  -54217.870\n  Scaling correction factor                                  2.131\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              119412.761  119412.761\n  Bayesian (BIC)                            119732.069  119732.069\n  Sample-size adjusted Bayesian (SABIC)     119550.977  119550.977\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.202       0.145\n  90 Percent confidence interval - lower         0.198       0.143\n  90 Percent confidence interval - upper         0.205       0.147\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       1.000\n                                                                  \n  Robust RMSEA                                               0.349\n  90 Percent confidence interval - lower                     0.330\n  90 Percent confidence interval - upper                     0.369\n  P-value H_0: Robust RMSEA &lt;= 0.050                         0.000\n  P-value H_0: Robust RMSEA &gt;= 0.080                         1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.325       0.325\n\nParameter Estimates:\n\n  Standard errors                        Robust.cluster\n  Information                                  Observed\n  Observed information based on                 Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n  F1 =~                                                                  \n    completinsPrGm    7.932    0.124    63.997    0.000    7.932    0.987\n    attemptsPerGam   12.270    0.175    69.967    0.000   12.270    0.975\n    pssng_yrdsPrGm   92.456    1.492    61.983    0.000   92.456    0.994\n    passng_tdsPrGm    0.595    0.019    31.607    0.000    0.595    0.863\n    pssng_r_yrdsPG   83.828    3.587    23.367    0.000   83.828    0.693\n    pssng_yrds__PG   50.640    1.527    33.163    0.000   50.640    0.772\n    pssng_frst_dPG    4.512    0.079    56.894    0.000    4.512    0.992\n  F2 =~                                                                  \n    avg_cmpltd_r_y    1.111    0.064    17.405    0.000    1.111    0.782\n    avg_ntndd_r_yr    1.389    0.064    21.836    0.000    1.389    0.992\n    aggressiveness    2.009    0.300     6.694    0.000    2.009    0.391\n    mx_cmpltd_r_ds    2.681    0.277     9.679    0.000    2.681    0.475\n    avg_air_distnc    1.273    0.066    19.223    0.000    1.273    0.878\n    max_air_distnc    2.619    0.298     8.781    0.000    2.619    0.506\n    avg_r_yrds_t_s    1.383    0.079    17.463    0.000    1.383    0.937\n  F3 =~                                                                  \n    passing_cpoe     12.128    0.481    25.227    0.000   12.128    0.997\n    pass_comp_pct     0.121    0.005    23.672    0.000    0.121    0.914\n    passer_rating    25.942       NA                      25.942    0.921\n    cmpltn_prcnt__   10.365    0.541    19.158    0.000   10.365    0.967\n\nCovariances:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n  F1 ~~                                                                  \n    F2                0.154    0.076     2.036    0.042    0.154    0.154\n    F3                0.303    0.037     8.212    0.000    0.303    0.303\n  F2 ~~                                                                  \n    F3                0.078    0.132     0.586    0.558    0.078    0.078\n\nIntercepts:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm   13.216    0.396    33.415    0.000   13.216    1.645\n   .attemptsPerGam   21.828    0.588    37.096    0.000   21.828    1.735\n   .pssng_yrdsPrGm  147.994    4.667    31.714    0.000  147.994    1.591\n   .passng_tdsPrGm    0.847    0.036    23.649    0.000    0.847    1.228\n   .pssng_r_yrdsPG  133.000    5.791    22.966    0.000  133.000    1.100\n   .pssng_yrds__PG   87.622    3.002    29.187    0.000   87.622    1.336\n   .pssng_frst_dPG    7.143    0.230    31.073    0.000    7.143    1.570\n   .avg_cmpltd_r_y    5.671    0.095    59.795    0.000    5.671    3.987\n   .avg_ntndd_r_yr    7.930    0.108    73.374    0.000    7.930    5.663\n   .aggressiveness   16.414    0.296    55.526    0.000   16.414    3.191\n   .mx_cmpltd_r_ds   39.035    0.341   114.482    0.000   39.035    6.918\n   .avg_air_distnc   21.111    0.104   202.637    0.000   21.111   14.548\n   .max_air_distnc   47.488    0.333   142.765    0.000   47.488    9.175\n   .avg_r_yrds_t_s   -1.029    0.110    -9.332    0.000   -1.029   -0.697\n   .passing_cpoe     -3.494    0.375    -9.328    0.000   -3.494   -0.287\n   .pass_comp_pct     0.588    0.004   138.904    0.000    0.588    4.426\n   .passer_rating    80.612    0.997    80.843    0.000   80.612    2.861\n   .cmpltn_prcnt__   -2.948    0.356    -8.274    0.000   -2.948   -0.275\n\nVariances:\n                   Estimate  Std.Err  z-value   P(&gt;|z|)   Std.lv  Std.all\n   .completinsPrGm    1.625    0.123    13.221    0.000    1.625    0.025\n   .attemptsPerGam    7.757    0.462    16.775    0.000    7.757    0.049\n   .pssng_yrdsPrGm  106.237    9.617    11.047    0.000  106.237    0.012\n   .passng_tdsPrGm    0.121    0.007    17.277    0.000    0.121    0.255\n   .pssng_r_yrdsPG 7592.808    0.352 21546.125    0.000 7592.808    0.519\n   .pssng_yrds__PG 1737.972  132.291    13.138    0.000 1737.972    0.404\n   .pssng_frst_dPG    0.342    0.026    12.943    0.000    0.342    0.017\n   .avg_cmpltd_r_y    0.787    0.084     9.388    0.000    0.787    0.389\n   .avg_ntndd_r_yr    0.033    0.021     1.543    0.123    0.033    0.017\n   .aggressiveness   22.417    2.889     7.760    0.000   22.417    0.847\n   .mx_cmpltd_r_ds   24.649    2.207    11.167    0.000   24.649    0.774\n   .avg_air_distnc    0.484    0.037    13.022    0.000    0.484    0.230\n   .max_air_distnc   19.929    2.186     9.117    0.000   19.929    0.744\n   .avg_r_yrds_t_s    0.268    0.043     6.167    0.000    0.268    0.123\n   .passing_cpoe      0.877    1.122     0.782    0.434    0.877    0.006\n   .pass_comp_pct     0.003    0.000     6.555    0.000    0.003    0.165\n   .passer_rating   121.020   17.269     7.008    0.000  121.020    0.152\n   .cmpltn_prcnt__    7.407    1.173     6.314    0.000    7.407    0.065\n    F1                1.000                                1.000    1.000\n    F2                1.000                                1.000    1.000\n    F3                1.000                                1.000    1.000\n\nR-Square:\n                   Estimate\n    completinsPrGm    0.975\n    attemptsPerGam    0.951\n    pssng_yrdsPrGm    0.988\n    passng_tdsPrGm    0.745\n    pssng_r_yrdsPG    0.481\n    pssng_yrds__PG    0.596\n    pssng_frst_dPG    0.983\n    avg_cmpltd_r_y    0.611\n    avg_ntndd_r_yr    0.983\n    aggressiveness    0.153\n    mx_cmpltd_r_ds    0.226\n    avg_air_distnc    0.770\n    max_air_distnc    0.256\n    avg_r_yrds_t_s    0.877\n    passing_cpoe      0.994\n    pass_comp_pct     0.835\n    passer_rating     0.848\n    cmpltn_prcnt__    0.935\n\n\nThe three-factor model did not fit well according to fit indices:\n\nCodelavaan::fitMeasures(\n  cfa3factor_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n               chisq                   df               pvalue \n           10863.020              132.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n            5689.900              132.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.909            43031.563              153.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.202                0.750 \n                 tli                 srmr         rmsea.robust \n               0.710                0.325                0.349 \n          cfi.robust           tli.robust \n               0.714                0.669 \n\n\nHowever, we know that the three-factor model fit improved considerably when accounting for correlated residuals. So, we plan to examine modification indices to see if we can account for covariances between variables that were not explained by their underlying latent factors.\n\nCodelavaan::residuals(\n  cfa3factor_fit,\n  type = \"cor\")\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n                                        cmplPG attmPG pssng_yPG pssng_tPG\ncompletionsPerGame                       0.000                           \nattemptsPerGame                          0.022  0.000                    \npassing_yardsPerGame                    -0.003 -0.003     0.000          \npassing_tdsPerGame                      -0.024 -0.049     0.013     0.000\npassing_air_yardsPerGame                 0.012  0.009     0.004    -0.020\npassing_yards_after_catchPerGame        -0.008  0.014     0.010     0.007\npassing_first_downsPerGame              -0.002 -0.007     0.002     0.016\navg_completed_air_yards                 -0.249 -0.272    -0.215    -0.146\navg_intended_air_yards                  -0.313 -0.334    -0.294    -0.229\naggressiveness                          -0.238 -0.222    -0.234    -0.203\nmax_completed_air_distance               0.163  0.140     0.200     0.225\navg_air_distance                        -0.335 -0.354    -0.316    -0.250\nmax_air_distance                         0.073  0.050     0.079     0.091\navg_air_yards_to_sticks                 -0.284 -0.307    -0.263    -0.191\npassing_cpoe                            -0.105 -0.189    -0.112    -0.048\npass_comp_pct                            0.023 -0.060     0.004     0.041\npasser_rating                            0.072 -0.011     0.097     0.219\ncompletion_percentage_above_expectation -0.139 -0.223    -0.150    -0.089\n                                        pssng_r_PG p___PG pssng_f_PG avg_c__\ncompletionsPerGame                                                          \nattemptsPerGame                                                             \npassing_yardsPerGame                                                        \npassing_tdsPerGame                                                          \npassing_air_yardsPerGame                     0.000                          \npassing_yards_after_catchPerGame            -0.427  0.000                   \npassing_first_downsPerGame                   0.000 -0.002      0.000        \navg_completed_air_yards                      0.458 -0.736     -0.218   0.000\navg_intended_air_yards                       0.441 -0.793     -0.294   0.178\naggressiveness                               0.244 -0.527     -0.221   0.332\nmax_completed_air_distance                   0.556 -0.237      0.171   0.312\navg_air_distance                             0.394 -0.779     -0.321   0.234\nmax_air_distance                             0.603 -0.396      0.067   0.316\navg_air_yards_to_sticks                      0.444 -0.753     -0.258   0.214\npassing_cpoe                                 0.165 -0.346     -0.102   0.215\npass_comp_pct                               -0.012 -0.034      0.014  -0.205\npasser_rating                                0.100 -0.012      0.100  -0.024\ncompletion_percentage_above_expectation      0.212 -0.443     -0.139   0.333\n                                        avg_n__ aggrss mx_c__ avg_r_ mx_r_d\ncompletionsPerGame                                                         \nattemptsPerGame                                                            \npassing_yardsPerGame                                                       \npassing_tdsPerGame                                                         \npassing_air_yardsPerGame                                                   \npassing_yards_after_catchPerGame                                           \npassing_first_downsPerGame                                                 \navg_completed_air_yards                                                    \navg_intended_air_yards                    0.000                            \naggressiveness                            0.281  0.000                     \nmax_completed_air_distance                0.156  0.171  0.000              \navg_air_distance                          0.104  0.314  0.215  0.000       \nmax_air_distance                          0.271  0.250  0.530  0.320  0.000\navg_air_yards_to_sticks                   0.057  0.300  0.195  0.140  0.287\npassing_cpoe                              0.181 -0.100  0.360  0.163  0.297\npass_comp_pct                            -0.257 -0.411  0.072 -0.281 -0.026\npasser_rating                            -0.097 -0.304  0.261 -0.117  0.109\ncompletion_percentage_above_expectation   0.297  0.054  0.415  0.278  0.377\n                                        av____ pssng_ pss_c_ pssr_r cmp___\ncompletionsPerGame                                                        \nattemptsPerGame                                                           \npassing_yardsPerGame                                                      \npassing_tdsPerGame                                                        \npassing_air_yardsPerGame                                                  \npassing_yards_after_catchPerGame                                          \npassing_first_downsPerGame                                                \navg_completed_air_yards                                                   \navg_intended_air_yards                                                    \naggressiveness                                                            \nmax_completed_air_distance                                                \navg_air_distance                                                          \nmax_air_distance                                                          \navg_air_yards_to_sticks                  0.000                            \npassing_cpoe                             0.189  0.000                     \npass_comp_pct                           -0.251 -0.056  0.000              \npasser_rating                           -0.080 -0.067  0.044  0.000       \ncompletion_percentage_above_expectation  0.300 -0.001 -0.099 -0.101  0.000\n\n$mean\n                     completionsPerGame                         attemptsPerGame \n                                  0.000                                   0.000 \n                   passing_yardsPerGame                      passing_tdsPerGame \n                                  0.000                                   0.000 \n               passing_air_yardsPerGame        passing_yards_after_catchPerGame \n                                  0.000                                   0.000 \n             passing_first_downsPerGame                 avg_completed_air_yards \n                                  0.000                                  -0.316 \n                 avg_intended_air_yards                          aggressiveness \n                                 -0.344                                  -0.111 \n             max_completed_air_distance                        avg_air_distance \n                                 -0.368                                  -0.295 \n                       max_air_distance                 avg_air_yards_to_sticks \n                                 -0.463                                  -0.329 \n                           passing_cpoe                           pass_comp_pct \n                                 -0.174                                   0.011 \n                          passer_rating completion_percentage_above_expectation \n                                 -0.089                                  -0.223 \n\n\nBelow are the modification indices, suggesting potential model modifications that would improve model fit such as correlated residuals and cross-loadings.\n\nCodelavaan::modificationindices(\n  cfa3factor_fit,\n  sort. = TRUE)\n\n\n  \n\n\n\nBelow are factor scores from the model for the first six players:\n\nCodecfa3factor_factorScores &lt;- lavaan::lavPredict(cfa3factor_fit)\n\nhead(cfa3factor_factorScores)\n\n              F1          F2         F3\n[1,] -0.07259821  0.01834072  0.8517998\n[2,]  0.16408224  0.01722349 -0.1903004\n[3,]  0.42835856  0.05079569 -0.3243843\n[4,]  0.11043343  0.03759429  0.6407341\n[5,]  0.88829206  0.13630769  0.2430781\n[6,] -0.06581186 -0.02435272 -0.4394584\n\n\nThe path diagram of the three-factor CFA model is in Figure 22.22.\n\nCodelavaanPlot::lavaanPlot(\n  cfa3factor_fit,\n  coefs = TRUE,\n  covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nFigure 22.22: Path Diagram of the Three-Factor Confirmatory Factor Analysis Model.\n\n\n\n\nCodelavaangui::plot_lavaan(cfa3factor_fit)\n\n\nBecause the two-factor model is nested within the three-factor model, we can compare them using a chi-square difference test. The three factor model fit considerably better than the two-factor model in terms of a lower chi-square value:\n\nCodeanova(\n  cfa3factor_fit,\n  cfa2factor_fit\n)\n\n\n  \n\n\n\nBased on the model modifications suggested by the modification indices for the three-factor model, we modify the model to account for correlated residuals, using the syntax below:\n\nCodecfa3factorModified_syntax &lt;- '\n # Factor loadings\n F1 =~ NA*completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + \n passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame\n \n F2 =~ NA*avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + \n avg_air_distance + max_air_distance + avg_air_yards_to_sticks\n \n F3 =~ NA*passing_cpoe + pass_comp_pct + passer_rating + completion_percentage_above_expectation\n \n # Cross loadings\n #F3 =~ attemptsPerGame\n\n # Correlated residuals\n passing_air_yardsPerGame ~~ passing_yards_after_catchPerGame\n completionsPerGame ~~ attemptsPerGame\n attemptsPerGame ~~ passing_yards_after_catchPerGame\n passing_yards_after_catchPerGame ~~ passing_first_downsPerGame\n completionsPerGame ~~ passing_first_downsPerGame\n attemptsPerGame ~~ passing_tdsPerGame\n completionsPerGame ~~ passing_tdsPerGame\n passing_tdsPerGame ~~ passing_air_yardsPerGame\n \n max_completed_air_distance ~~ max_air_distance\n avg_completed_air_yards ~~ max_completed_air_distance\n avg_completed_air_yards ~~ avg_air_distance\n avg_completed_air_yards ~~ max_air_distance\n avg_intended_air_yards ~~ max_completed_air_distance\n \n completionsPerGame ~~ pass_comp_pct\n passing_yardsPerGame ~~ avg_completed_air_yards\n passing_yardsPerGame ~~ max_completed_air_distance\n passing_tdsPerGame ~~ passer_rating\n aggressiveness ~~ completion_percentage_above_expectation\n \n # Variances\n F1 ~~ 1*F1\n F2 ~~ 1*F2\n'\n\n\n\nCodecfa3factorModified_fit &lt;- lavaan::cfa(\n  cfa3factorModified_syntax,\n  data = dataForFA,\n  missing = \"ML\",\n  estimator = \"MLR\",\n  bounds = \"standard\",\n  #std.lv = TRUE,\n  cluster = \"player_id\", # account for nested data within the same player (i.e., longitudinal data)\n  em.h1.iter.max = 2000000)\n\n\n\nCodesummary(\n  cfa3factorModified_fit,\n  fit.measures = TRUE,\n  standardized = TRUE,\n  rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1347 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        76\n  Row rank of the constraints matrix                40\n\n  Number of observations                          2002\n  Number of clusters [player_id]                   396\n  Number of missing patterns                         4\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                              2192.914    1205.054\n  Degrees of freedom                               113         113\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.820\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                             43031.563   21852.033\n  Degrees of freedom                               153         153\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.969\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.951       0.950\n  Tucker-Lewis Index (TLI)                       0.934       0.932\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -55314.327  -55314.327\n  Scaling correction factor                                  2.593\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -54217.870  -54217.870\n  Scaling correction factor                                  2.131\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                              110780.655  110780.655\n  Bayesian (BIC)                            111206.399  111206.399\n  Sample-size adjusted Bayesian (SABIC)     110964.943  110964.943\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.096       0.069\n  90 Percent confidence interval - lower         0.092       0.067\n  90 Percent confidence interval - upper         0.099       0.072\n  P-value H_0: RMSEA &lt;= 0.050                    0.000       0.000\n  P-value H_0: RMSEA &gt;= 0.080                    1.000       0.000\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                        NA\n  90 Percent confidence interval - upper                        NA\n  P-value H_0: Robust RMSEA &lt;= 0.050                            NA\n  P-value H_0: Robust RMSEA &gt;= 0.080                            NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.324       0.324\n\nParameter Estimates:\n\n  Standard errors                        Robust.cluster\n  Information                                  Observed\n  Observed information based on                 Hessian\n\nLatent Variables:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)   Std.lv   Std.all\n  F1 =~                                                                   \n    completinsPrGm     7.880    0.126   62.745    0.000     7.880    0.982\n    attemptsPerGam    12.208    0.179   68.234    0.000    12.208    0.968\n    pssng_yrdsPrGm    92.521    1.492   62.020    0.000    92.521    0.996\n    passng_tdsPrGm     0.596    0.018   32.317    0.000     0.596    0.873\n    pssng_r_yrdsPG    84.416    3.660   23.064    0.000    84.416    0.696\n    pssng_yrds__PG    51.194    1.547   33.092    0.000    51.194    0.779\n    pssng_frst_dPG     4.514    0.080   56.080    0.000     4.514    0.991\n  F2 =~                                                                   \n    avg_cmpltd_r_y     1.105    0.061   18.086    0.000     1.105    0.809\n    avg_ntndd_r_yr     1.389    0.065   21.387    0.000     1.389    0.986\n    aggressiveness     1.726    0.299    5.774    0.000     1.726    0.342\n    mx_cmpltd_r_ds     2.863    0.280   10.235    0.000     2.863    0.506\n    avg_air_distnc     1.284    0.067   19.227    0.000     1.284    0.881\n    max_air_distnc     2.648    0.302    8.775    0.000     2.648    0.511\n    avg_r_yrds_t_s     1.399    0.080   17.480    0.000     1.399    0.943\n  F3 =~                                                                   \n    passing_cpoe       4.414    0.547    8.071    0.000    12.228    0.994\n    pass_comp_pct      0.042    0.005    8.364    0.000     0.117    0.906\n    passer_rating      9.934    1.359    7.312    0.000    27.524    0.935\n    cmpltn_prcnt__     4.006    0.520    7.697    0.000    11.099    0.973\n\nCovariances:\n                                      Estimate   Std.Err  z-value  P(&gt;|z|)\n .passing_air_yardsPerGame ~~                                             \n   .pssng_yrds__PG                    -3491.729  256.243  -13.627    0.000\n .completionsPerGame ~~                                                   \n   .attemptsPerGam                        3.496    0.187   18.726    0.000\n .attemptsPerGame ~~                                                      \n   .pssng_yrds__PG                       15.610    1.055   14.801    0.000\n .passing_yards_after_catchPerGame ~~                                     \n   .pssng_frst_dPG                       -3.403    0.280  -12.173    0.000\n .completionsPerGame ~~                                                   \n   .pssng_frst_dPG                        0.129    0.027    4.784    0.000\n .attemptsPerGame ~~                                                      \n   .passng_tdsPrGm                       -0.438    0.036  -12.334    0.000\n .completionsPerGame ~~                                                   \n   .passng_tdsPrGm                       -0.153    0.015  -10.311    0.000\n .passing_tdsPerGame ~~                                                   \n   .pssng_r_yrdsPG                       -2.829    0.304   -9.296    0.000\n .max_completed_air_distance ~~                                           \n   .max_air_distnc                       11.329    1.521    7.450    0.000\n .avg_completed_air_yards ~~                                              \n   .mx_cmpltd_r_ds                        1.260    0.233    5.407    0.000\n   .avg_air_distnc                       -0.112    0.024   -4.772    0.000\n   .max_air_distnc                       -0.401    0.149   -2.686    0.007\n .avg_intended_air_yards ~~                                               \n   .mx_cmpltd_r_ds                       -0.460    0.135   -3.408    0.001\n .completionsPerGame ~~                                                   \n   .pass_comp_pct                         0.022    0.002   13.658    0.000\n .passing_yardsPerGame ~~                                                 \n   .avg_cmpltd_r_y                        4.794    0.319   15.037    0.000\n   .mx_cmpltd_r_ds                       17.131    1.451   11.807    0.000\n .passing_tdsPerGame ~~                                                   \n   .passer_rating                         1.302    0.135    9.654    0.000\n .aggressiveness ~~                                                       \n   .cmpltn_prcnt__                        5.594    0.987    5.667    0.000\n  F1 ~~                                                                   \n    F2                                    0.224    0.073    3.086    0.002\n    F3                                    0.865    0.139    6.240    0.000\n  F2 ~~                                                                   \n    F3                                    0.266    0.362    0.737    0.461\n   Std.lv   Std.all\n                   \n -3491.729   -0.973\n                   \n     3.496    0.725\n                   \n    15.610    0.120\n                   \n    -3.403   -0.135\n                   \n     0.129    0.138\n                   \n    -0.438   -0.415\n                   \n    -0.153   -0.300\n                   \n    -2.829   -0.097\n                   \n    11.329    0.521\n                   \n     1.260    0.322\n    -0.112   -0.203\n    -0.401   -0.112\n                   \n    -0.460   -0.403\n                   \n     0.022    0.265\n                   \n     4.794    0.690\n    17.131    0.406\n                   \n     1.302    0.374\n                   \n     5.594    0.448\n                   \n     0.224    0.224\n     0.312    0.312\n                   \n     0.096    0.096\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)   Std.lv   Std.all\n   .completinsPrGm    13.216    0.396   33.416    0.000    13.216    1.647\n   .attemptsPerGam    21.828    0.588   37.097    0.000    21.828    1.731\n   .pssng_yrdsPrGm   147.994    4.667   31.714    0.000   147.994    1.593\n   .passng_tdsPrGm     0.847    0.036   23.649    0.000     0.847    1.239\n   .pssng_r_yrdsPG   132.999    5.791   22.966    0.000   132.999    1.097\n   .pssng_yrds__PG    87.622    3.002   29.188    0.000    87.622    1.333\n   .pssng_frst_dPG     7.143    0.230   31.074    0.000     7.143    1.568\n   .avg_cmpltd_r_y     5.648    0.090   63.065    0.000     5.648    4.135\n   .avg_ntndd_r_yr     7.884    0.105   75.362    0.000     7.884    5.598\n   .aggressiveness    16.380    0.292   56.166    0.000    16.380    3.244\n   .mx_cmpltd_r_ds    38.973    0.342  113.903    0.000    38.973    6.892\n   .avg_air_distnc    21.068    0.101  208.210    0.000    21.068   14.451\n   .max_air_distnc    47.399    0.331  142.982    0.000    47.399    9.144\n   .avg_r_yrds_t_s    -1.076    0.108  -10.001    0.000    -1.076   -0.725\n   .passing_cpoe      -3.288    0.375   -8.768    0.000    -3.288   -0.267\n   .pass_comp_pct      0.588    0.004  138.832    0.000     0.588    4.556\n   .passer_rating     80.907    1.126   71.837    0.000    80.907    2.748\n   .cmpltn_prcnt__    -2.888    0.370   -7.805    0.000    -2.888   -0.253\n\nVariances:\n                   Estimate   Std.Err  z-value  P(&gt;|z|)   Std.lv   Std.all\n    F1                 1.000                                1.000    1.000\n    F2                 1.000                                1.000    1.000\n   .completinsPrGm     2.326    0.111   20.869    0.000     2.326    0.036\n   .attemptsPerGam     9.989    0.452   22.091    0.000     9.989    0.063\n   .pssng_yrdsPrGm    74.862    6.377   11.739    0.000    74.862    0.009\n   .passng_tdsPrGm     0.111    0.007   17.008    0.000     0.111    0.238\n   .pssng_r_yrdsPG  7576.573  522.372   14.504    0.000  7576.573    0.515\n   .pssng_yrds__PG  1698.105  126.909   13.381    0.000  1698.105    0.393\n   .pssng_frst_dPG     0.373    0.029   12.742    0.000     0.373    0.018\n   .avg_cmpltd_r_y     0.645    0.063   10.202    0.000     0.645    0.346\n   .avg_ntndd_r_yr     0.055    0.020    2.702    0.007     0.055    0.028\n   .aggressiveness    22.510    2.923    7.701    0.000    22.510    0.883\n   .mx_cmpltd_r_ds    23.778    1.952   12.179    0.000    23.778    0.744\n   .avg_air_distnc     0.476    0.037   12.791    0.000     0.476    0.224\n   .max_air_distnc    19.857    2.179    9.112    0.000    19.857    0.739\n   .avg_r_yrds_t_s     0.246    0.038    6.552    0.000     0.246    0.112\n   .passing_cpoe       1.745    0.841    2.075    0.038     1.745    0.012\n   .pass_comp_pct      0.003    0.000    6.550    0.000     0.003    0.179\n   .passer_rating    108.999   16.145    6.751    0.000   108.999    0.126\n   .cmpltn_prcnt__     6.916    1.038    6.663    0.000     6.916    0.053\n    F3                 7.676    1.904    4.032    0.000     1.000    1.000\n\nR-Square:\n                   Estimate \n    completinsPrGm     0.964\n    attemptsPerGam     0.937\n    pssng_yrdsPrGm     0.991\n    passng_tdsPrGm     0.762\n    pssng_r_yrdsPG     0.485\n    pssng_yrds__PG     0.607\n    pssng_frst_dPG     0.982\n    avg_cmpltd_r_y     0.654\n    avg_ntndd_r_yr     0.972\n    aggressiveness     0.117\n    mx_cmpltd_r_ds     0.256\n    avg_air_distnc     0.776\n    max_air_distnc     0.261\n    avg_r_yrds_t_s     0.888\n    passing_cpoe       0.988\n    pass_comp_pct      0.821\n    passer_rating      0.874\n    cmpltn_prcnt__     0.947\n\n\nThe three-factor model with correlated residuals fit was acceptable according to CFI and TLI. The model fit was subpar for RMSEA and SRMR.\n\nCodelavaan::fitMeasures(\n  cfa3factorModified_fit,\n  fit.measures = c(\n    \"chisq\", \"df\", \"pvalue\",\n    \"chisq.scaled\", \"df.scaled\", \"pvalue.scaled\",\n    \"chisq.scaling.factor\",\n    \"baseline.chisq\",\"baseline.df\",\"baseline.pvalue\",\n    \"rmsea\", \"cfi\", \"tli\", \"srmr\",\n    \"rmsea.robust\", \"cfi.robust\", \"tli.robust\"))\n\n               chisq                   df               pvalue \n            2192.914              113.000                0.000 \n        chisq.scaled            df.scaled        pvalue.scaled \n            1205.054              113.000                0.000 \nchisq.scaling.factor       baseline.chisq          baseline.df \n               1.820            43031.563              153.000 \n     baseline.pvalue                rmsea                  cfi \n               0.000                0.096                0.951 \n                 tli                 srmr         rmsea.robust \n               0.934                0.324                   NA \n          cfi.robust           tli.robust \n                  NA                   NA \n\n\n\nCodelavaan::residuals(\n  cfa3factorModified_fit,\n  type = \"cor\")\n\n$type\n[1] \"cor.bollen\"\n\n$cov\n                                        cmplPG attmPG pssng_yPG pssng_tPG\ncompletionsPerGame                       0.000                           \nattemptsPerGame                         -0.001  0.000                    \npassing_yardsPerGame                     0.001  0.002     0.000          \npassing_tdsPerGame                      -0.001 -0.001     0.001     0.000\npassing_air_yardsPerGame                 0.013  0.011     0.000     0.005\npassing_yards_after_catchPerGame        -0.011 -0.007     0.002    -0.007\npassing_first_downsPerGame               0.001  0.001     0.001     0.007\navg_completed_air_yards                 -0.308 -0.330    -0.313    -0.200\navg_intended_air_yards                  -0.378 -0.399    -0.362    -0.290\naggressiveness                          -0.254 -0.237    -0.251    -0.218\nmax_completed_air_distance               0.124  0.101     0.127     0.190\navg_air_distance                        -0.395 -0.413    -0.378    -0.305\nmax_air_distance                         0.038  0.015     0.043     0.059\navg_air_yards_to_sticks                 -0.348 -0.371    -0.329    -0.251\npassing_cpoe                            -0.112 -0.195    -0.121    -0.058\npass_comp_pct                           -0.003 -0.064    -0.002     0.033\npasser_rating                            0.061 -0.022     0.083     0.140\ncompletion_percentage_above_expectation -0.148 -0.232    -0.161    -0.101\n                                        pssng_r_PG p___PG pssng_f_PG avg_c__\ncompletionsPerGame                                                          \nattemptsPerGame                                                             \npassing_yardsPerGame                                                        \npassing_tdsPerGame                                                          \npassing_air_yardsPerGame                     0.000                          \npassing_yards_after_catchPerGame             0.004  0.000                   \npassing_first_downsPerGame                  -0.002  0.003      0.000        \navg_completed_air_yards                      0.416 -0.784     -0.278   0.000\navg_intended_air_yards                       0.394 -0.847     -0.361   0.156\naggressiveness                               0.233 -0.540     -0.237   0.360\nmax_completed_air_distance                   0.528 -0.269      0.131   0.111\navg_air_distance                             0.350 -0.828     -0.382   0.263\nmax_air_distance                             0.578 -0.425      0.031   0.355\navg_air_yards_to_sticks                      0.397 -0.806     -0.324   0.183\npassing_cpoe                                 0.158 -0.355     -0.111   0.198\npass_comp_pct                               -0.017 -0.040      0.008  -0.220\npasser_rating                                0.090 -0.024      0.087  -0.041\ncompletion_percentage_above_expectation      0.204 -0.453     -0.150   0.316\n                                        avg_n__ aggrss mx_c__ avg_r_ mx_r_d\ncompletionsPerGame                                                         \nattemptsPerGame                                                            \npassing_yardsPerGame                                                       \npassing_tdsPerGame                                                         \npassing_air_yardsPerGame                                                   \npassing_yards_after_catchPerGame                                           \npassing_first_downsPerGame                                                 \navg_completed_air_yards                                                    \navg_intended_air_yards                    0.000                            \naggressiveness                            0.331  0.000                     \nmax_completed_air_distance                0.185  0.183  0.000              \navg_air_distance                          0.106  0.355  0.186  0.000       \nmax_air_distance                          0.269  0.273  0.125  0.314  0.000\navg_air_yards_to_sticks                   0.056  0.343  0.163  0.131  0.279\npassing_cpoe                              0.163 -0.102  0.348  0.147  0.287\npass_comp_pct                            -0.273 -0.413  0.062 -0.296 -0.034\npasser_rating                            -0.115 -0.306  0.249 -0.134  0.099\ncompletion_percentage_above_expectation   0.280 -0.046  0.403  0.261  0.367\n                                        av____ pssng_ pss_c_ pssr_r cmp___\ncompletionsPerGame                                                        \nattemptsPerGame                                                           \npassing_yardsPerGame                                                      \npassing_tdsPerGame                                                        \npassing_air_yardsPerGame                                                  \npassing_yards_after_catchPerGame                                          \npassing_first_downsPerGame                                                \navg_completed_air_yards                                                   \navg_intended_air_yards                                                    \naggressiveness                                                            \nmax_completed_air_distance                                                \navg_air_distance                                                          \nmax_air_distance                                                          \navg_air_yards_to_sticks                  0.000                            \npassing_cpoe                             0.171  0.000                     \npass_comp_pct                           -0.267 -0.046  0.000              \npasser_rating                           -0.098 -0.079  0.038  0.000       \ncompletion_percentage_above_expectation  0.283 -0.004 -0.097 -0.120  0.000\n\n$mean\n                     completionsPerGame                         attemptsPerGame \n                                  0.000                                   0.000 \n                   passing_yardsPerGame                      passing_tdsPerGame \n                                  0.000                                   0.000 \n               passing_air_yardsPerGame        passing_yards_after_catchPerGame \n                                  0.000                                   0.000 \n             passing_first_downsPerGame                 avg_completed_air_yards \n                                  0.000                                  -0.308 \n                 avg_intended_air_yards                          aggressiveness \n                                 -0.330                                  -0.106 \n             max_completed_air_distance                        avg_air_distance \n                                 -0.359                                  -0.282 \n                       max_air_distance                 avg_air_yards_to_sticks \n                                 -0.451                                  -0.315 \n                           passing_cpoe                           pass_comp_pct \n                                 -0.190                                   0.009 \n                          passer_rating completion_percentage_above_expectation \n                                 -0.100                                  -0.228 \n\n\n\nCodelavaan::modificationindices(\n  cfa3factorModified_fit,\n  sort. = TRUE)\n\nError: lavaan-&gt;modificationindices():  \n   could not compute modification indices; information matrix is singular\n\n\nBelow are factor scores from the model for the first six players:\n\nCodecfa3factorModified_factorScores &lt;- lavaan::lavPredict(cfa3factorModified_fit)\n\nhead(cfa3factorModified_factorScores)\n\n               F1          F2         F3\n[1,]  0.155936734  0.06008852  2.5399976\n[2,]  0.161137983  0.03162709 -0.2902111\n[3,]  0.243284575  0.04885075 -0.3328656\n[4,]  0.109019463  0.04151196  1.7281787\n[5,]  0.974356926  0.21606794  0.6206635\n[6,] -0.009807153 -0.01506287 -1.2393687\n\nCodedataForFA_cfa &lt;- cbind(dataForFA, cfa3factorModified_factorScores)\n\n\nThe path diagram of the modified three-factor CFA model with correlated residuals is in Figure 22.23.\n\nCodelavaanPlot::lavaanPlot(\n  cfa3factorModified_fit,\n  coefs = TRUE,\n  #covs = TRUE,\n  stand = TRUE)\n\n\n\n\n\n\nFigure 22.23: Path Diagram of the Modified Three-Factor Confirmatory Factor Analysis Model With Correlated Residuals.\n\n\n\n\nCodelavaangui::plot_lavaan(cfa3factorModified_fit)\n\n\n\nCodeanova(\n  cfa3factorModified_fit,\n  cfa3factor_fit\n)\n\n\n  \n\n\n\nHere are the variables that loaded onto each of the factors:\n\nCodefactor1vars &lt;- c(\n  \"completionsPerGame\",\"attemptsPerGame\",\"passing_yardsPerGame\",\"passing_tdsPerGame\",\n  \"passing_air_yardsPerGame\",\"passing_yards_after_catchPerGame\",\"passing_first_downsPerGame\")\n\nfactor2vars &lt;- c(\n  \"avg_completed_air_yards\",\"avg_intended_air_yards\",\"aggressiveness\",\"max_completed_air_distance\",\n  \"avg_air_distance\",\"max_air_distance\",\"avg_air_yards_to_sticks\")\n\nfactor3vars &lt;- c(\n  \"passing_cpoe\",\"pass_comp_pct\",\"passer_rating\",\"completion_percentage_above_expectation\")\n\n\nThe variables that loaded most strongly onto factor 1 appear to reflect Quarterback usage: completions per game, passing attempts per game, passing yards per game, passing touchdowns per game, passing air yards (total horizontal distance the ball travels on all pass attempts) per game, passing yards after the catch per game, and first downs gained per game by passing. Quarterbacks who tend to throw more tend to have higher levels on those variables. Thus, we label component 1 as “Usage”, which reflects total Quarterback involvement, regardless of efficiency or outcome.\nThe variables that loaded most strongly onto factor 2 appear to reflect Quarterback aggressiveness: average air yards on completed passes, average air yards on all attempted passes, aggressiveness (percentage of passing attempts thrown into tight windows, where there is a defender within one yard or less of the receiver at the time of the completion or incompletion), average amount of air yards ahead of or behind the first down marker on passing attempts, average air distance (the true three-dimensional distance the ball travels in the air), maximum air distance, and maximum air distance on completed passes. Quarterbacks who throw the ball farther and into tighter windows tend to have higher values on those variables. Thus, we label component 2 as “Aggressiveness”, which reflects throwing longer, more difficult passes with a tight window.\nThe variables that loaded most strongly onto factor 3 appear to reflect Quarterback performance: passing completion percentage above expectation, pass completion percentage, and passer rating. Quarterbacks who perform better tend to have higher values on those variables. Thus, we label component 3 as “Performance”.\nHere are the players and seasons that showed the highest levels of Quarterback “Usage”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(-F1) %&gt;% \n  select(player_display_name, season, F1, all_of(factor1vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the lowest levels of Quarterback “Usage”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(F1) %&gt;% \n  select(player_display_name, season, F1, all_of(factor1vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the highest levels of Quarterback “Aggressiveness”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(-F2) %&gt;% \n  select(player_display_name, season, F2, all_of(factor2vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the lowest levels of Quarterback “Aggressiveness”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(F2) %&gt;% \n  select(player_display_name, season, F2, all_of(factor2vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the highest levels of Quarterback “Performance”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(-F3) %&gt;% \n  select(player_display_name, season, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nIf we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the highest levels of Quarterback “Performance”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(-F3) %&gt;% \n  filter(games &gt;= 10) %&gt;% \n  select(player_display_name, season, games, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and seasons that showed the lowest levels of Quarterback “Performance”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(F3) %&gt;% \n  select(player_display_name, season, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nIf we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the lowest levels of Quarterback “Performance”:\n\nCodedataForFA_cfa %&gt;% \n  arrange(F3) %&gt;% \n  filter(games &gt;= 10) %&gt;% \n  select(player_display_name, season, games, F3, all_of(factor3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisConclusion",
    "href": "factor-analysis.html#sec-factorAnalysisConclusion",
    "title": "22  Factor Analysis",
    "section": "\n22.8 Conclusion",
    "text": "22.8 Conclusion\nFactor analysis is a class of latent variable models that aims to identify the optimal, parsimonious latent structure for a group of variables. Latent variables are ways of studying and operationalizing theoretical constructs that cannot be directly observed or quantified. Factor analysis encompasses two general types: confirmatory factor analysis and exploratory factor analysis. Exploratory factor analysis (EFA) is used when the researcher has no a priori hypotheses about how a set of variables is structured. Confirmatory factor analysis (CFA) is used when a researcher wants to evaluate how well a hypothesized model fits. A goal of factor analysis is to balance accuracy (i.e., variance accounted for) and parsimony (i.e., simplicity). Factor analysis estimates the latent factors as the common variance among the variables that load onto that factor and discards the remaining variance as “error”. There are many decisions to make in factor analysis. These decisions can have important impacts on the resulting solution. Thus, it can be helpful for theory and interpretability to help guide decision-making when conducting factor analysis. Using both exploratory and confirmatory factor analysis, we were able to identify three latent factors that accounted for considerable variance in the variables we examined, pertaining to Quarterbacks: 1) usage; 2) aggressiveness; 3) performance. We were then able to determine which players were highest and lowest on each of these factors.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "factor-analysis.html#sec-factorAnalysisSessionInfo",
    "href": "factor-analysis.html#sec-factorAnalysisSessionInfo",
    "title": "22  Factor Analysis",
    "section": "\n22.9 Session Info",
    "text": "22.9 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4  forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4     \n [5] purrr_1.1.0      readr_2.1.5      tidyr_1.3.1      tibble_3.3.0    \n [9] ggplot2_3.5.2    tidyverse_2.0.0  lavaanPlot_0.8.1 lavaan_0.6-19   \n[13] nFactors_2.4.1.2 psych_2.5.6     \n\nloaded via a namespace (and not attached):\n [1] GPArotation_2025.3-1 DiagrammeR_1.0.11    generics_0.1.4      \n [4] stringi_1.8.7        lattice_0.22-7       hms_1.1.3           \n [7] digest_0.6.37        magrittr_2.0.3       evaluate_1.0.4      \n[10] grid_4.5.1           timechange_0.3.0     RColorBrewer_1.1-3  \n[13] fastmap_1.2.0        jsonlite_2.0.0       scales_1.4.0        \n[16] pbivnorm_0.6.0       numDeriv_2016.8-1.1  mnormt_2.1.1        \n[19] cli_3.6.5            rlang_1.1.6          visNetwork_2.1.2    \n[22] withr_3.0.2          yaml_2.3.10          tools_4.5.1         \n[25] parallel_4.5.1       tzdb_0.5.0           vctrs_0.6.5         \n[28] R6_2.6.1             stats4_4.5.1         lifecycle_1.0.4     \n[31] htmlwidgets_1.6.4    MASS_7.3-65          pkgconfig_2.0.3     \n[34] pillar_1.11.0        gtable_0.3.6         glue_1.8.0          \n[37] xfun_0.53            tidyselect_1.2.1     rstudioapi_0.17.1   \n[40] knitr_1.50           farver_2.1.2         htmltools_0.5.8.1   \n[43] nlme_3.1-168         rmarkdown_2.29       compiler_4.5.1      \n[46] quadprog_1.5-8      \n\n\n\n\n\n\nBollen, K. A. (1989). Structural equations with latent variables. John Wiley & Sons.\n\n\nBollen, K. A. (2002). Latent variables in psychology and the social sciences. Annual Review of Psychology, 53(1), 605–634. https://doi.org/10.1146/annurev.psych.53.100901.135239\n\n\nBox, G. E. P. (1979). Robustness in the strategy of scientific model building. In R. L. Launer & G. N. Wilkinson (Eds.), Robustness in statistics. Academic Press.\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDinno, A. (2014). Gently clarifying the application of Horn’s parallel analysis to principal component analysis versus factor analysis. http://archives.pdx.edu/ds/psu/10527\n\n\nKarch, J. D. (2025a). lavaangui: A web-based graphical interface for specifying lavaan models by drawing path diagrams. Structural Equation Modeling: A Multidisciplinary Journal, 1–12. https://doi.org/10.1080/10705511.2024.2420678\n\n\nKarch, J. D. (2025b). lavaangui: Graphical user interface with integrated diagrammer for lavaan. https://doi.org/10.32614/CRAN.package.lavaangui\n\n\nKline, R. B. (2023). Principles and practice of structural equation modeling (5th ed.). Guilford Publications.\n\n\nKline, R. B. (2024). How to evaluate local fit (residuals) in large structural equation models. International Journal of Psychology, 59(6), 1293–1306. https://doi.org/10.1002/ijop.13252\n\n\nLishinski, A. (2024). lavaanPlot: Path diagrams for lavaan models via DiagrammeR. https://doi.org/10.32614/CRAN.package.lavaanPlot\n\n\nMcNeish, D., & Wolf, M. G. (2023). Dynamic fit index cutoffs for confirmatory factor analysis models. Psychological Methods, 28(1), 61–88. https://doi.org/10.1037/met0000425\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRaiche, G., & Magis, D. (2025). nFactors: Parallel analysis and other non graphical solutions to the Cattell scree test. https://doi.org/10.32614/CRAN.package.nFactors\n\n\nRevelle, W. (2025). psych: Procedures for psychological, psychometric, and personality research. https://doi.org/10.32614/CRAN.package.psych\n\n\nRevelle, W., & Rocklin, T. (1979). Very simple structure: An alternative procedure for estimating the optimal number of interpretable factors. Multivariate Behavioral Research, 14(4), 403–414. https://doi.org/10.1207/s15327906mbr1404_2\n\n\nRosseel, Y. (2012). lavaan: An R package for structural equation modeling. Journal of Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02\n\n\nRosseel, Y., Jorgensen, T. D., & De Wilde, L. (2024). lavaan: Latent variable analysis. https://doi.org/10.32614/CRAN.package.lavaan\n\n\nRuscio, J., & Roche, B. (2012). Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure. Psychological Assessment, 24(2), 282–292. https://doi.org/10.1037/a0025697\n\n\nStrauss, M. E., & Smith, G. T. (2009). Construct validity: Advances in theory and methodology. Annual Review of Clinical Psychology, 5(1), 1–25. https://doi.org/10.1146/annurev.clinpsy.032408.153639\n\n\nWolf, M. G., & McNeish, D. (2022). dynamic: DFI cutoffs for latent variable models. https://doi.org/10.32614/CRAN.package.dynamic",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factor Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "",
    "text": "23.1 Getting Started\nThis chapter provides an overview of principal component analysis as a useful technique for data reduction.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaGettingStarted",
    "href": "pca.html#sec-pcaGettingStarted",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "",
    "text": "23.1.1 Load Packages\n\nCodelibrary(\"psych\")\nlibrary(\"nFactors\")\nlibrary(\"tidyverse\")\n\n\n\n23.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/nfl_nextGenStats_weekly.RData\")\n\n\n\n23.1.3 Prepare Data\n\n23.1.3.1 Merge Data\n\nCodedataMerged &lt;- full_join(\n  player_stats_weekly,\n  nfl_nextGenStats_weekly %&gt;% select(-any_of(c(\"player_display_name\",\"completions\",\"attempts\",\"receptions\",\"targets\"))),\n  by = c(\"player_id\" = \"player_gsis_id\",\"season\",\"season_type\",\"week\"),\n)\n\n\n\n23.1.3.2 Specify Variables\n\nCodepcaVars &lt;- c(\n  \"completions\",\"attempts\",\"passing_yards\",\"passing_tds\",\"passing_interceptions\",\n  \"sacks_suffered\",\"sack_yards_lost\",\"sack_fumbles\",\"sack_fumbles_lost\",\n  \"passing_air_yards\",\"passing_yards_after_catch\",\"passing_first_downs\",\n  \"passing_epa\",\"passing_cpoe\",\"passing_2pt_conversions\",\"pacr\",\"pass_40_yds\",\n  \"pass_inc\",\"pass_comp_pct\",\"fumbles\",\"two_pts\",\n  \"avg_time_to_throw\",\"avg_completed_air_yards\",\"avg_intended_air_yards\",\n  \"avg_air_yards_differential\",\"aggressiveness\",\"max_completed_air_distance\",\n  \"avg_air_yards_to_sticks\",\"passer_rating\", #,\"completion_percentage\"\n  \"expected_completion_percentage\",\"completion_percentage_above_expectation\",\n  \"avg_air_distance\",\"max_air_distance\")\n\n\n\n23.1.3.3 Standardize Variables\n\nCodedataForPCA &lt;- dataMerged\ndataForPCA[pcaVars] &lt;- scale(dataForPCA[pcaVars])",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaOverview",
    "href": "pca.html#sec-pcaOverview",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "\n23.2 Overview of Principal Component Analysis",
    "text": "23.2 Overview of Principal Component Analysis\nPrincipal component analysis (PCA) is used if you want to reduce your data matrix. PCA composites represent the variances of an observed measure in as economical a fashion as possible, with no latent underlying variables. The goal of PCA is to identify a smaller number of components that explain as much variance in a set of variables as possible. It is an atheoretical way to decompose a matrix. PCA involves decomposition of a data matrix into a set of eigenvectors, which are transformations of the old variables.\nThe eigenvectors attempt to simplify the data in the matrix. PCA takes the data matrix and identifies the weighted sum of all variables that does the best job at explaining variance: these are the principal components, also called eigenvectors. Principal components reflect optimally weighted sums.\nPCA decomposes the data matrix into any number of components—as many as the number of variables, which will always account for all variance. After the PCA is performed, you can look at the results and discard the components which likely reflect error variance. Judgments about which components to retain are based on empirical criteria in conjunction with theory to select a parsimonious number of components that account for the majority of variance.\nThe eigenvalue reflects the amount of variance explained by the component (eigenvector). When using a varimax (orthogonal) rotation, an eigenvalue for a component is calculated as the sum of squared standardized component loadings on that component. When using oblique rotation, however, the items explain more variance than is attributable to their factor loadings because the factors are correlated.\nPCA pulls the first principal component out (i.e., the eigenvector that explains the most variance) and makes a new data matrix: i.e., new correlation matrix. Then the PCA pulls out the component that explains the next most variance—i.e., the eigenvector with the next largest eigenvalue, and it does this for all components, equal to the same number of variables. For instance, if there are six variables, it will iteratively extract an additional component up to six components. You can extract as many eigenvectors as there are variables. If you extract all six components, the data matrix left over will be the same as the correlation matrix in Figure 23.1. That is, the remaining variables (as part of the leftover data matrix) will be entirely uncorrelated with each other, because six components explain 100% of the variance from six variables. In other words, you can explain (6) variables with (6) new things!\n\n\n\n\n\nFigure 23.1: Example Correlation Matrix 2. From Petersen (2024) and Petersen (2025).\n\n\nHowever, it does no good if you have to use all (6) components because there is no data reduction from the original number of variables. When the goal is data reduction (as in PCA), the hope is that the first few components will explain most of the variance, so we can explain the variability in the data with fewer components than there are variables.\nThe sum of all eigenvalues is equal to the number of variables in the analysis. PCA does not have the same assumptions as factor analysis, which assumes that measures are partly from common variance and error. But if you estimate (6) eigenvectors and only keep (2), the model is a two-component model and whatever left becomes error. Therefore, PCA does not have the same assumptions as factor analysis, but it often ends up in the same place.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaDecisions",
    "href": "pca.html#sec-pcaDecisions",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "\n23.3 Decisions in Principal Component Analysis",
    "text": "23.3 Decisions in Principal Component Analysis\nThere are four primary decisions to make in PCA:\n\nwhat variables to include in the model and how to scale them\nwhether and how to rotate components\nhow many components to retain\nhow to interpret and use the components\n\nAs in factor analysis, the answer you get can differ highly depending on the decisions you make. We provide guidance on each of these decisions below and in Section 22.5.\n\n23.3.1 1. Variables to Include and their Scaling\nAs in factor analysis. The first decision when conducting a factor analysis is which variables to include and the scaling of those variables. Guidance on which variables to include is in Section 22.5.1.\nIn addition, before performing a PCA, it is important to ensure that the variables included in the PCA are on the same scale. PCA seeks to identify components that explain variance in the data, so if the variables are not on the same scale, some variables may contribute considerably more variance than others. A common way of ensuring that variables are on the same scale is to standardize them using, for example, z scores.\n\n23.3.2 2. Component Rotation\nSimilar considerations as in factor analysis can be used to determine whether and how to rotate components in PCA. The considerations for determining whether and how to rotate factors in factor analysis are described in Section 22.5.3.\n\n23.3.3 3. Determining the Number of Components to Retain\nSimilar criteria as in factor analysis can be used to determine the number of components to retain in PCA. The criteria for determining the number of factors to retain in factor analysis are described in Section 22.5.4.\n\n23.3.4 4. Interpreting and Using PCA Components\nThe next step is interpreting the PCA components. Use theory to interpret and label the components.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaVsFactorAnalysis",
    "href": "pca.html#sec-pcaVsFactorAnalysis",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "\n23.4 PCA Versus Factor Analysis",
    "text": "23.4 PCA Versus Factor Analysis\nBoth factor analysis and PCA can be used for data reduction. The key distinction between factor analysis and PCA is depicted in Figure 23.2.\n\n\n\n\n\nFigure 23.2: Distinction Between Factor Analysis and Principal Component Analysis. From Petersen (2025).\n\n\nThere are several differences between factor analysis and PCA. Factor analysis has greater sophistication than PCA, but greater sophistication often results in greater assumptions. Factor analysis does not always work; the data may not always fit to a factor analysis model. However, PCA can decompose any data matrix; it always works. PCA is okay if you are not interested in the factor structure. PCA uses all variance of variables and assumes variables have no error, so it does not account for measurement error. PCA is good if you just want to form a linear composite and perform data reduction. However, if you are interested in the factor structure, use factor analysis, which estimates a latent variable that accounts for the common variance and discards error variance. Factor analysis better handles error than PCA—factor analysis assumes that what is in the variable is the combination of common construct variance and error. By contrast, PCA assumes that the measures have no measurement error. Factor analysis is useful for the identification of latent constructs—i.e., underlying dimensions or factors that explain (cause) observed scores.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaExample",
    "href": "pca.html#sec-pcaExample",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "\n23.5 Example of Principal Component Analysis",
    "text": "23.5 Example of Principal Component Analysis\nWe generated the scree plot in Figure 23.3 using the psych::fa.parallel() function of the psych package (Revelle, 2025).\nThe number of components to keep would depend on which criteria one uses. Based on the rule to keep factors whose eigenvalues are greater than one and based on the parallel test, we would keep nine components. However, based on the Cattell scree test (the “elbow” of the screen plot minus one), we would keep three components. If using the optimal coordinates, we would keep four components; if using the acceleration factor, we would keep one component. Therefore, interpretability of the components would be important for deciding how many components to keep.\n\nCodepsych::fa.parallel(\n  x = dataForPCA[pcaVars],\n  fa = \"pc\"\n)\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  9 \n\n\n\n\n\n\n\nFigure 23.3: Scree Plot: With Comparisons to Simulated and Resampled Data.\n\n\n\n\nWe generated the scree plot in Figure 23.4 using the nFactors::nScree() and nFactors::plotnScree() functions of the nFactors package (Raiche & Magis, 2025).\n\nCodescreeDataPCA &lt;- nFactors::nScree(\n  x = cor(\n    dataForPCA[pcaVars],\n    use = \"pairwise.complete.obs\"),\n  model = \"components\")\n\nnFactors::plotnScree(screeDataPCA)\n\n\n\n\n\n\nFigure 23.4: Scree Plot with Parallel Analysis.\n\n\n\n\nWe generated the very simple structure (VSS) plots in Figures 23.5 and 23.6 using the psych::vss() and psych::nfactors() functions of the psych package (Revelle, 2025). The optimal number of components based on the VSS criterion is three components.\n\nCodepsych::vss(\n  dataForPCA[pcaVars],\n  rotate = \"oblimin\",\n  fm = \"pc\")\n\n\nVery Simple Structure\nCall: psych::vss(x = dataForPCA[pcaVars], rotate = \"oblimin\", fm = \"pc\")\nVSS complexity 1 achieves a maximimum of 0.76  with  3  factors\nVSS complexity 2 achieves a maximimum of 0.9  with  3  factors\n\nThe Velicer MAP achieves a minimum of \n\n\nInf  with    factors \n\n\n\nBIC achieves a minimum of  Inf  with    factors\n\n\nSample Size adjusted BIC achieves a minimum of  Inf  with    factors\n\nStatistics by number of factors \n  vss1 vss2 map dof chisq prob sqresid  fit RMSEA BIC SABIC complex eChisq SRMR\n1 0.56 0.00 NaN   0    NA   NA    77.3 0.56    NA  NA    NA      NA     NA   NA\n2 0.70 0.76 NaN   0    NA   NA    42.5 0.76    NA  NA    NA      NA     NA   NA\n3 0.76 0.89 NaN   0    NA   NA    16.5 0.90    NA  NA    NA      NA     NA   NA\n4 0.73 0.89 NaN   0    NA   NA    13.3 0.92    NA  NA    NA      NA     NA   NA\n5 0.75 0.89 NaN   0    NA   NA    10.9 0.94    NA  NA    NA      NA     NA   NA\n6 0.75 0.90 NaN   0    NA   NA     8.6 0.95    NA  NA    NA      NA     NA   NA\n7 0.72 0.88 NaN   0    NA   NA     7.0 0.96    NA  NA    NA      NA     NA   NA\n8 0.74 0.88 NaN   0    NA   NA     5.7 0.97    NA  NA    NA      NA     NA   NA\n  eCRMS eBIC\n1    NA   NA\n2    NA   NA\n3    NA   NA\n4    NA   NA\n5    NA   NA\n6    NA   NA\n7    NA   NA\n8    NA   NA\n\n\n\n\n\n\n\nFigure 23.5: Very Simple Structure Plot.\n\n\n\n\n\nCodepsych::nfactors(\n  dataForPCA[pcaVars],\n  rotate = \"oblimin\",\n  fm = \"pc\")\n\nError in plot.window(...): need finite 'ylim' values\n\n\n\n\n\n\n\nFigure 23.6: Model Indices by Number of Components.\n\n\n\n\n\nCodepca1ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 1,\n  rotate = \"oblimin\")\n\npca2ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 2,\n  rotate = \"oblimin\")\n\npca3ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 3,\n  rotate = \"oblimin\")\n\npca4ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 4,\n  rotate = \"oblimin\")\n\npca5ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 5,\n  rotate = \"oblimin\")\n\npca6ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 6,\n  rotate = \"oblimin\")\n\npca7ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 7,\n  rotate = \"oblimin\")\n\npca8ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 8,\n  rotate = \"oblimin\")\n\npca9ComponentOblique &lt;- psych::principal(\n  dataForPCA[pcaVars],\n  nfactors = 9,\n  rotate = \"oblimin\")\n\n\n\nCodepca1ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 1, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          PC1      h2      u2 com\ncompletions                              0.98 0.95693  0.0431   1\nattempts                                 0.96 0.92856  0.0714   1\npassing_yards                            1.00 1.00562 -0.0056   1\npassing_tds                              0.83 0.69384  0.3062   1\npassing_interceptions                    0.65 0.42148  0.5785   1\nsacks_suffered                           0.81 0.64819  0.3518   1\nsack_yards_lost                         -0.78 0.60956  0.3904   1\nsack_fumbles                             0.57 0.32564  0.6744   1\nsack_fumbles_lost                        0.46 0.21414  0.7859   1\npassing_air_yards                        0.84 0.70339  0.2966   1\npassing_yards_after_catch                0.90 0.80549  0.1945   1\npassing_first_downs                      0.98 0.96525  0.0348   1\npassing_epa                              0.15 0.02383  0.9762   1\npassing_cpoe                             0.17 0.02992  0.9701   1\npassing_2pt_conversions                  0.26 0.06876  0.9312   1\npacr                                     0.09 0.00786  0.9921   1\npass_40_yds                              0.30 0.09241  0.9076   1\npass_inc                                 0.89 0.79671  0.2033   1\npass_comp_pct                            0.23 0.05423  0.9458   1\nfumbles                                  0.50 0.25041  0.7496   1\ntwo_pts                                  0.20 0.03958  0.9604   1\navg_time_to_throw                       -0.06 0.00332  0.9967   1\navg_completed_air_yards                  0.11 0.01190  0.9881   1\navg_intended_air_yards                  -0.20 0.04121  0.9588   1\navg_air_yards_differential               0.08 0.00645  0.9936   1\naggressiveness                          -0.02 0.00060  0.9994   1\nmax_completed_air_distance               0.20 0.03903  0.9610   1\navg_air_yards_to_sticks                  0.05 0.00287  0.9971   1\npasser_rating                            0.16 0.02504  0.9750   1\nexpected_completion_percentage           0.03 0.00067  0.9993   1\ncompletion_percentage_above_expectation  0.16 0.02598  0.9740   1\navg_air_distance                         0.03 0.00067  0.9993   1\nmax_air_distance                         0.15 0.02139  0.9786   1\n\n                PC1\nSS loadings    9.82\nProportion Var 0.30\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.23 \n with the empirical chi square  24992582  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.6\n\nCodepca2ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 2, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC2    h2      u2 com\ncompletions                              0.96  0.23 0.992  0.0082 1.1\nattempts                                 0.97 -0.02 0.932  0.0676 1.0\npassing_yards                            0.99  0.14 1.014 -0.0136 1.0\npassing_tds                              0.82  0.18 0.714  0.2861 1.1\npassing_interceptions                    0.67 -0.23 0.490  0.5096 1.2\nsacks_suffered                           0.81 -0.10 0.667  0.3333 1.0\nsack_yards_lost                         -0.79  0.10 0.627  0.3729 1.0\nsack_fumbles                             0.58 -0.08 0.336  0.6641 1.0\nsack_fumbles_lost                        0.47 -0.07 0.222  0.7780 1.0\npassing_air_yards                        0.87 -0.40 0.900  0.1001 1.4\npassing_yards_after_catch                0.87  0.30 0.872  0.1277 1.2\npassing_first_downs                      0.97  0.17 0.980  0.0197 1.1\npassing_epa                              0.12  0.40 0.178  0.8221 1.2\npassing_cpoe                             0.13  0.55 0.328  0.6724 1.1\npassing_2pt_conversions                  0.26 -0.02 0.070  0.9300 1.0\npacr                                     0.04  0.66 0.445  0.5554 1.0\npass_40_yds                              0.30  0.01 0.092  0.9076 1.0\npass_inc                                 0.92 -0.37 0.969  0.0311 1.3\npass_comp_pct                            0.18  0.70 0.531  0.4685 1.1\nfumbles                                  0.51 -0.07 0.258  0.7418 1.0\ntwo_pts                                  0.20 -0.03 0.041  0.9590 1.0\navg_time_to_throw                       -0.03 -0.36 0.134  0.8661 1.0\navg_completed_air_yards                  0.14 -0.36 0.144  0.8564 1.3\navg_intended_air_yards                  -0.15 -0.69 0.503  0.4968 1.1\navg_air_yards_differential               0.03  0.61 0.380  0.6199 1.0\naggressiveness                           0.01 -0.47 0.216  0.7839 1.0\nmax_completed_air_distance               0.21 -0.12 0.054  0.9455 1.6\navg_air_yards_to_sticks                  0.11 -0.71 0.517  0.4830 1.0\npasser_rating                            0.11  0.65 0.443  0.5566 1.1\nexpected_completion_percentage          -0.03  0.75 0.563  0.4371 1.0\ncompletion_percentage_above_expectation  0.12  0.51 0.277  0.7229 1.1\navg_air_distance                         0.09 -0.76 0.576  0.4238 1.0\nmax_air_distance                         0.18 -0.47 0.253  0.7471 1.3\n\n                       TC1  TC2\nSS loadings           9.81 5.91\nProportion Var        0.30 0.18\nCumulative Var        0.30 0.48\nProportion Explained  0.62 0.38\nCumulative Proportion 0.62 1.00\n\n With component correlations of \n     TC1  TC2\nTC1 1.00 0.03\nTC2 0.03 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.17 \n with the empirical chi square  13393660  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.78\n\nCodepca3ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 3, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC2   TC3    h2      u2 com\ncompletions                              0.95 -0.16  0.19 0.993  0.0068 1.1\nattempts                                 0.98 -0.04 -0.04 0.953  0.0467 1.0\npassing_yards                            0.94  0.10  0.36 1.076 -0.0763 1.3\npassing_tds                              0.75  0.12  0.45 0.834  0.1660 1.7\npassing_interceptions                    0.72 -0.02 -0.36 0.612  0.3885 1.5\nsacks_suffered                           0.86 -0.10 -0.25 0.775  0.2252 1.2\nsack_yards_lost                         -0.84  0.11  0.25 0.735  0.2651 1.2\nsack_fumbles                             0.62 -0.10 -0.22 0.418  0.5824 1.3\nsack_fumbles_lost                        0.50 -0.08 -0.18 0.275  0.7248 1.3\npassing_air_yards                        0.84  0.46 -0.04 0.931  0.0687 1.5\npassing_yards_after_catch                0.87 -0.25  0.19 0.879  0.1210 1.3\npassing_first_downs                      0.93  0.01  0.30 1.003 -0.0027 1.2\npassing_epa                             -0.01  0.17  0.83 0.700  0.3002 1.1\npassing_cpoe                             0.03 -0.07  0.77 0.608  0.3921 1.0\npassing_2pt_conversions                  0.26  0.02  0.00 0.070  0.9299 1.0\npacr                                     0.04 -0.58  0.33 0.457  0.5428 1.6\npass_40_yds                              0.22  0.30  0.40 0.307  0.6933 2.5\npass_inc                                 0.97  0.13 -0.39 1.058 -0.0581 1.4\npass_comp_pct                            0.10 -0.32  0.71 0.636  0.3637 1.4\nfumbles                                  0.54 -0.08 -0.18 0.311  0.6892 1.3\ntwo_pts                                  0.20  0.04  0.01 0.041  0.9589 1.1\navg_time_to_throw                       -0.04  0.37 -0.12 0.153  0.8467 1.2\navg_completed_air_yards                  0.02  0.76  0.37 0.689  0.3108 1.5\navg_intended_air_yards                  -0.23  0.89  0.01 0.830  0.1698 1.1\navg_air_yards_differential              -0.02 -0.32  0.56 0.432  0.5685 1.6\naggressiveness                           0.01  0.42 -0.21 0.227  0.7732 1.5\nmax_completed_air_distance               0.11  0.49  0.42 0.425  0.5754 2.1\navg_air_yards_to_sticks                  0.03  0.93  0.03 0.872  0.1278 1.0\npasser_rating                           -0.03 -0.02  0.99 0.973  0.0272 1.0\nexpected_completion_percentage          -0.01 -0.74  0.27 0.628  0.3725 1.3\ncompletion_percentage_above_expectation  0.00  0.03  0.83 0.688  0.3115 1.0\navg_air_distance                         0.02  0.91 -0.06 0.844  0.1565 1.0\nmax_air_distance                         0.13  0.61  0.01 0.387  0.6129 1.1\n\n                       TC1  TC2  TC3\nSS loadings           9.73 5.58 5.51\nProportion Var        0.29 0.17 0.17\nCumulative Var        0.29 0.46 0.63\nProportion Explained  0.47 0.27 0.26\nCumulative Proportion 0.47 0.74 1.00\n\n With component correlations of \n     TC1   TC2   TC3\nTC1 1.00  0.02  0.08\nTC2 0.02  1.00 -0.04\nTC3 0.08 -0.04  1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n with the empirical chi square  4035534  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.93\n\nCodepca4ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 4, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC2   TC3   TC4   h2      u2\ncompletions                              0.95 -0.18  0.12  0.06 1.00 -0.0044\nattempts                                 0.97 -0.07 -0.13  0.07 0.98  0.0205\npassing_yards                            0.96  0.08  0.27  0.02 1.08 -0.0818\npassing_tds                              0.83  0.09  0.32 -0.10 0.85  0.1510\npassing_interceptions                    0.65 -0.04 -0.37  0.17 0.62  0.3791\nsacks_suffered                           0.63 -0.06 -0.11  0.46 0.81  0.1910\nsack_yards_lost                         -0.61  0.06  0.10 -0.47 0.77  0.2276\nsack_fumbles                             0.21  0.02  0.13  0.78 0.73  0.2699\nsack_fumbles_lost                        0.11  0.05  0.17  0.75 0.60  0.4026\npassing_air_yards                        0.83  0.44 -0.11  0.08 0.94  0.0628\npassing_yards_after_catch                0.91 -0.28  0.08 -0.02 0.91  0.0885\npassing_first_downs                      0.94 -0.01  0.22  0.04 1.01 -0.0089\npassing_epa                              0.31  0.10  0.54 -0.56 0.79  0.2058\npassing_cpoe                            -0.03  0.00  0.88  0.14 0.72  0.2796\npassing_2pt_conversions                  0.41 -0.04 -0.17 -0.25 0.16  0.8405\npacr                                     0.03 -0.56  0.35  0.02 0.46  0.5411\npass_40_yds                              0.39  0.25  0.22 -0.29 0.34  0.6565\npass_inc                                 0.94  0.08 -0.47  0.09 1.10 -0.1040\npass_comp_pct                            0.08 -0.27  0.76  0.07 0.68  0.3206\nfumbles                                  0.20  0.03  0.11  0.65 0.52  0.4793\ntwo_pts                                  0.34 -0.03 -0.16 -0.25 0.12  0.8782\navg_time_to_throw                       -0.14  0.40 -0.03  0.17 0.18  0.8153\navg_completed_air_yards                  0.04  0.78  0.37 -0.01 0.71  0.2870\navg_intended_air_yards                  -0.24  0.91  0.04  0.03 0.86  0.1439\navg_air_yards_differential               0.00 -0.29  0.57 -0.03 0.44  0.5581\naggressiveness                          -0.06  0.44 -0.15  0.12 0.24  0.7642\nmax_completed_air_distance               0.23  0.47  0.31 -0.20 0.43  0.5716\navg_air_yards_to_sticks                  0.07  0.92 -0.02 -0.07 0.87  0.1274\npasser_rating                            0.10 -0.01  0.91 -0.20 0.98  0.0216\nexpected_completion_percentage           0.08 -0.76  0.19 -0.15 0.65  0.3508\ncompletion_percentage_above_expectation -0.04  0.10  0.92  0.11 0.80  0.2019\navg_air_distance                         0.02  0.92 -0.06  0.01 0.85  0.1506\nmax_air_distance                         0.22  0.57 -0.10 -0.16 0.40  0.5981\n                                        com\ncompletions                             1.1\nattempts                                1.1\npassing_yards                           1.2\npassing_tds                             1.3\npassing_interceptions                   1.8\nsacks_suffered                          1.9\nsack_yards_lost                         2.0\nsack_fumbles                            1.2\nsack_fumbles_lost                       1.2\npassing_air_yards                       1.6\npassing_yards_after_catch               1.2\npassing_first_downs                     1.1\npassing_epa                             2.6\npassing_cpoe                            1.1\npassing_2pt_conversions                 2.1\npacr                                    1.7\npass_40_yds                             3.3\npass_inc                                1.5\npass_comp_pct                           1.3\nfumbles                                 1.3\ntwo_pts                                 2.3\navg_time_to_throw                       1.6\navg_completed_air_yards                 1.4\navg_intended_air_yards                  1.1\navg_air_yards_differential              1.5\naggressiveness                          1.4\nmax_completed_air_distance              2.7\navg_air_yards_to_sticks                 1.0\npasser_rating                           1.1\nexpected_completion_percentage          1.2\ncompletion_percentage_above_expectation 1.1\navg_air_distance                        1.0\nmax_air_distance                        1.5\n\n                       TC1  TC2  TC3  TC4\nSS loadings           9.04 5.50 4.99 3.09\nProportion Var        0.27 0.17 0.15 0.09\nCumulative Var        0.27 0.44 0.59 0.69\nProportion Explained  0.40 0.24 0.22 0.14\nCumulative Proportion 0.40 0.64 0.86 1.00\n\n With component correlations of \n     TC1   TC2   TC3   TC4\nTC1 1.00  0.03  0.12  0.29\nTC2 0.03  1.00 -0.06 -0.07\nTC3 0.12 -0.06  1.00 -0.23\nTC4 0.29 -0.07 -0.23  1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 4 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.08 \n with the empirical chi square  3333582  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.95\n\nCodepca5ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 5, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC2   TC3   TC4   TC5   h2\ncompletions                              0.94 -0.18  0.12  0.07  0.05 1.01\nattempts                                 0.96 -0.08 -0.13  0.07  0.04 0.99\npassing_yards                            0.97  0.08  0.26  0.02  0.01 1.09\npassing_tds                              0.82  0.09  0.32 -0.09  0.06 0.85\npassing_interceptions                    0.66 -0.04 -0.38  0.16 -0.01 0.63\nsacks_suffered                           0.62 -0.06 -0.11  0.47  0.02 0.81\nsack_yards_lost                         -0.60  0.06  0.10 -0.47 -0.02 0.77\nsack_fumbles                             0.20  0.03  0.14  0.80  0.01 0.75\nsack_fumbles_lost                        0.09  0.06  0.18  0.77  0.01 0.62\npassing_air_yards                        0.82  0.43 -0.11  0.08  0.05 0.94\npassing_yards_after_catch                0.91 -0.28  0.08 -0.02  0.03 0.92\npassing_first_downs                      0.93 -0.01  0.22  0.05  0.05 1.01\npassing_epa                              0.29  0.09  0.55 -0.55  0.07 0.79\npassing_cpoe                            -0.04  0.00  0.89  0.16  0.01 0.73\npassing_2pt_conversions                  0.03  0.00  0.00 -0.02  0.89 0.80\npacr                                     0.06 -0.56  0.34  0.00 -0.06 0.46\npass_40_yds                              0.45  0.24  0.19 -0.33 -0.11 0.38\npass_inc                                 0.94  0.08 -0.48  0.09  0.04 1.11\npass_comp_pct                            0.07 -0.26  0.77  0.09  0.03 0.69\nfumbles                                  0.19  0.03  0.12  0.67  0.02 0.53\ntwo_pts                                 -0.05  0.02  0.02 -0.02  0.91 0.80\navg_time_to_throw                       -0.17  0.41 -0.01  0.19  0.06 0.20\navg_completed_air_yards                  0.06  0.77  0.36 -0.02 -0.03 0.71\navg_intended_air_yards                  -0.24  0.91  0.04  0.03 -0.01 0.86\navg_air_yards_differential               0.03 -0.30  0.56 -0.04 -0.07 0.44\naggressiveness                          -0.06  0.44 -0.15  0.12 -0.01 0.24\nmax_completed_air_distance               0.30  0.46  0.28 -0.24 -0.13 0.45\navg_air_yards_to_sticks                  0.06  0.92 -0.01 -0.07  0.04 0.87\npasser_rating                            0.10 -0.01  0.92 -0.20  0.01 0.98\nexpected_completion_percentage           0.08 -0.76  0.19 -0.16 -0.01 0.65\ncompletion_percentage_above_expectation -0.04  0.10  0.92  0.11 -0.01 0.81\navg_air_distance                         0.01  0.92 -0.06  0.01  0.01 0.85\nmax_air_distance                         0.25  0.56 -0.12 -0.18 -0.05 0.41\n                                            u2 com\ncompletions                             -0.007 1.1\nattempts                                 0.014 1.1\npassing_yards                           -0.088 1.2\npassing_tds                              0.150 1.4\npassing_interceptions                    0.370 1.8\nsacks_suffered                           0.191 2.0\nsack_yards_lost                          0.227 2.0\nsack_fumbles                             0.254 1.2\nsack_fumbles_lost                        0.383 1.2\npassing_air_yards                        0.061 1.6\npassing_yards_after_catch                0.081 1.2\npassing_first_downs                     -0.010 1.1\npassing_epa                              0.206 2.6\npassing_cpoe                             0.265 1.1\npassing_2pt_conversions                  0.198 1.0\npacr                                     0.539 1.7\npass_40_yds                              0.622 3.0\npass_inc                                -0.114 1.5\npass_comp_pct                            0.312 1.3\nfumbles                                  0.466 1.2\ntwo_pts                                  0.195 1.0\navg_time_to_throw                        0.803 1.8\navg_completed_air_yards                  0.287 1.4\navg_intended_air_yards                   0.142 1.1\navg_air_yards_differential               0.557 1.6\naggressiveness                           0.764 1.4\nmax_completed_air_distance               0.547 3.3\navg_air_yards_to_sticks                  0.126 1.0\npasser_rating                            0.020 1.1\nexpected_completion_percentage           0.349 1.2\ncompletion_percentage_above_expectation  0.193 1.1\navg_air_distance                         0.150 1.0\nmax_air_distance                         0.588 1.7\n\n                       TC1  TC2  TC3  TC4  TC5\nSS loadings           8.81 5.49 4.98 3.15 1.74\nProportion Var        0.27 0.17 0.15 0.10 0.05\nCumulative Var        0.27 0.43 0.58 0.68 0.73\nProportion Explained  0.36 0.23 0.21 0.13 0.07\nCumulative Proportion 0.36 0.59 0.80 0.93 1.00\n\n With component correlations of \n     TC1   TC2   TC3   TC4  TC5\nTC1 1.00  0.03  0.12  0.29 0.21\nTC2 0.03  1.00 -0.06 -0.08 0.01\nTC3 0.12 -0.06  1.00 -0.22 0.00\nTC4 0.29 -0.08 -0.22  1.00 0.07\nTC5 0.21  0.01  0.00  0.07 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 5 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.08 \n with the empirical chi square  2975002  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.95\n\nCodepca6ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 6, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC2   TC3   TC4   TC6   TC5\ncompletions                              0.95 -0.17  0.19  0.04 -0.07  0.03\nattempts                                 0.98 -0.07 -0.06  0.05 -0.09  0.03\npassing_yards                            0.93  0.07  0.14  0.05  0.26  0.03\npassing_tds                              0.79  0.09  0.22 -0.07  0.22  0.08\npassing_interceptions                    0.67 -0.05 -0.31  0.15 -0.13 -0.02\nsacks_suffered                           0.63 -0.05 -0.05  0.45 -0.10  0.01\nsack_yards_lost                         -0.61  0.05  0.04 -0.46  0.11 -0.01\nsack_fumbles                             0.17  0.03  0.08  0.82  0.07  0.02\nsack_fumbles_lost                        0.06  0.06  0.10  0.79  0.10  0.02\npassing_air_yards                        0.84  0.44 -0.01  0.05 -0.13  0.03\npassing_yards_after_catch                0.90 -0.28  0.07 -0.02  0.07  0.03\npassing_first_downs                      0.92 -0.01  0.20  0.05  0.09  0.06\npassing_epa                              0.26  0.09  0.41 -0.53  0.30  0.09\npassing_cpoe                            -0.02  0.05  0.94  0.09 -0.03 -0.01\npassing_2pt_conversions                  0.02  0.00 -0.04 -0.01 -0.03  0.90\npacr                                    -0.03 -0.59  0.01  0.11  0.54  0.00\npass_40_yds                              0.37  0.21 -0.12 -0.22  0.56 -0.05\npass_inc                                 0.95  0.07 -0.42  0.09 -0.10  0.03\npass_comp_pct                            0.11 -0.22  0.93 -0.01 -0.19 -0.01\nfumbles                                  0.16  0.03  0.05  0.69  0.09  0.03\ntwo_pts                                 -0.05  0.01 -0.03  0.00 -0.02  0.91\navg_time_to_throw                       -0.19  0.40 -0.08  0.22  0.08  0.07\navg_completed_air_yards                 -0.03  0.75  0.05  0.08  0.53  0.03\navg_intended_air_yards                  -0.24  0.91  0.06  0.02 -0.02 -0.01\navg_air_yards_differential              -0.09 -0.34  0.09  0.10  0.79  0.01\naggressiveness                          -0.04  0.44 -0.09  0.10 -0.12 -0.02\nmax_completed_air_distance               0.22  0.43 -0.01 -0.15  0.52 -0.08\navg_air_yards_to_sticks                  0.07  0.93  0.02 -0.08 -0.04  0.03\npasser_rating                            0.05  0.00  0.72 -0.17  0.40  0.04\nexpected_completion_percentage           0.08 -0.76  0.16 -0.16  0.08 -0.01\ncompletion_percentage_above_expectation -0.03  0.14  0.96  0.05  0.02 -0.03\navg_air_distance                         0.02  0.92 -0.02  0.00 -0.07  0.00\nmax_air_distance                         0.28  0.57 -0.01 -0.21 -0.13 -0.07\n                                          h2      u2 com\ncompletions                             1.03 -0.0330 1.2\nattempts                                1.00  0.0047 1.0\npassing_yards                           1.10 -0.0969 1.2\npassing_tds                             0.85  0.1492 1.4\npassing_interceptions                   0.63  0.3703 1.6\nsacks_suffered                          0.81  0.1901 1.9\nsack_yards_lost                         0.77  0.2267 2.0\nsack_fumbles                            0.77  0.2313 1.1\nsack_fumbles_lost                       0.65  0.3549 1.1\npassing_air_yards                       0.96  0.0399 1.6\npassing_yards_after_catch               0.92  0.0802 1.2\npassing_first_downs                     1.01 -0.0123 1.1\npassing_epa                             0.79  0.2054 3.2\npassing_cpoe                            0.84  0.1610 1.0\npassing_2pt_conversions                 0.81  0.1895 1.0\npacr                                    0.64  0.3579 2.1\npass_40_yds                             0.52  0.4794 2.6\npass_inc                                1.11 -0.1148 1.4\npass_comp_pct                           0.89  0.1090 1.2\nfumbles                                 0.56  0.4420 1.2\ntwo_pts                                 0.81  0.1852 1.0\navg_time_to_throw                       0.21  0.7852 2.3\navg_completed_air_yards                 0.85  0.1482 1.9\navg_intended_air_yards                  0.86  0.1404 1.1\navg_air_yards_differential              0.78  0.2207 1.5\naggressiveness                          0.24  0.7617 1.4\nmax_completed_air_distance              0.56  0.4447 2.6\navg_air_yards_to_sticks                 0.88  0.1210 1.0\npasser_rating                           0.98  0.0190 1.7\nexpected_completion_percentage          0.65  0.3485 1.2\ncompletion_percentage_above_expectation 0.90  0.1028 1.1\navg_air_distance                        0.86  0.1448 1.0\nmax_air_distance                        0.44  0.5601 1.9\n\n                       TC1  TC2  TC3  TC4  TC6  TC5\nSS loadings           8.70 5.48 4.11 3.09 2.57 1.74\nProportion Var        0.26 0.17 0.12 0.09 0.08 0.05\nCumulative Var        0.26 0.43 0.55 0.65 0.73 0.78\nProportion Explained  0.34 0.21 0.16 0.12 0.10 0.07\nCumulative Proportion 0.34 0.55 0.71 0.83 0.93 1.00\n\n With component correlations of \n     TC1   TC2   TC3   TC4   TC6  TC5\nTC1 1.00  0.03  0.11  0.31  0.08 0.21\nTC2 0.03  1.00 -0.10 -0.08  0.02 0.01\nTC3 0.11 -0.10  1.00 -0.16  0.35 0.04\nTC4 0.31 -0.08 -0.16  1.00 -0.14 0.06\nTC6 0.08  0.02  0.35 -0.14  1.00 0.04\nTC5 0.21  0.01  0.04  0.06  0.04 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 6 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.07 \n with the empirical chi square  2403218  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.96\n\nCodepca7ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 7, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC2   TC3   TC4   TC6   TC7\ncompletions                              0.94 -0.19  0.20  0.05 -0.05  0.04\nattempts                                 0.98 -0.10 -0.04  0.05 -0.07 -0.01\npassing_yards                            0.91  0.12  0.14  0.06  0.20  0.15\npassing_tds                              0.78  0.13  0.22 -0.06  0.16  0.13\npassing_interceptions                    0.71 -0.06 -0.28  0.10 -0.05 -0.18\nsacks_suffered                           0.63 -0.06 -0.04  0.43 -0.04 -0.13\nsack_yards_lost                         -0.61  0.07  0.03 -0.44  0.05  0.12\nsack_fumbles                             0.11  0.05  0.05  0.86  0.05  0.05\nsack_fumbles_lost                        0.00  0.08  0.07  0.85  0.05  0.08\npassing_air_yards                        0.85  0.39  0.01  0.03 -0.18 -0.05\npassing_yards_after_catch                0.86 -0.29  0.06  0.04  0.02  0.21\npassing_first_downs                      0.93  0.02  0.21  0.03  0.10  0.01\npassing_epa                              0.25  0.14  0.40 -0.50  0.18  0.27\npassing_cpoe                            -0.03  0.03  0.93  0.09 -0.02  0.00\npassing_2pt_conversions                  0.03  0.00 -0.04  0.00 -0.01 -0.03\npacr                                     0.01 -0.41  0.01  0.06  0.66  0.01\npass_40_yds                              0.28  0.30 -0.16 -0.10  0.29  0.53\npass_inc                                 0.97  0.05 -0.39  0.05 -0.07 -0.12\npass_comp_pct                            0.08 -0.27  0.91  0.02 -0.16  0.05\nfumbles                                  0.11  0.06  0.03  0.72  0.07  0.05\ntwo_pts                                 -0.05  0.02 -0.03  0.01 -0.01 -0.02\navg_time_to_throw                       -0.21  0.41 -0.09  0.24 -0.01  0.05\navg_completed_air_yards                  0.00  0.88  0.05  0.03  0.41  0.05\navg_intended_air_yards                  -0.27  0.86  0.04  0.06 -0.23  0.12\navg_air_yards_differential              -0.04 -0.09  0.09  0.03  0.87  0.03\naggressiveness                           0.12  0.47 -0.03 -0.12  0.09 -0.61\nmax_completed_air_distance               0.12  0.50 -0.05 -0.03  0.21  0.53\navg_air_yards_to_sticks                  0.07  0.88  0.02 -0.08 -0.21  0.02\npasser_rating                            0.01  0.08  0.69 -0.12  0.30  0.30\nexpected_completion_percentage          -0.05 -0.77  0.11  0.02 -0.01  0.48\ncompletion_percentage_above_expectation  0.03  0.17  0.97 -0.03  0.11 -0.19\navg_air_distance                         0.01  0.86 -0.02  0.01 -0.25  0.03\nmax_air_distance                         0.15  0.45 -0.05 -0.04 -0.45  0.44\n                                          TC5   h2     u2 com\ncompletions                              0.03 1.03 -0.033 1.2\nattempts                                 0.03 1.00  0.003 1.0\npassing_yards                            0.03 1.10 -0.098 1.3\npassing_tds                              0.08 0.85  0.147 1.4\npassing_interceptions                   -0.03 0.65  0.352 1.5\nsacks_suffered                           0.01 0.81  0.189 1.9\nsack_yards_lost                          0.00 0.77  0.226 2.0\nsack_fumbles                             0.02 0.81  0.195 1.1\nsack_fumbles_lost                        0.02 0.69  0.305 1.1\npassing_air_yards                        0.03 0.97  0.035 1.5\npassing_yards_after_catch                0.04 0.93  0.067 1.4\npassing_first_downs                      0.05 1.02 -0.023 1.1\npassing_epa                              0.10 0.79  0.205 3.8\npassing_cpoe                            -0.01 0.84  0.161 1.0\npassing_2pt_conversions                  0.90 0.81  0.189 1.0\npacr                                    -0.01 0.67  0.327 1.7\npass_40_yds                             -0.04 0.58  0.420 3.2\npass_inc                                 0.03 1.13 -0.129 1.4\npass_comp_pct                            0.00 0.90  0.104 1.3\nfumbles                                  0.03 0.58  0.417 1.1\ntwo_pts                                  0.91 0.82  0.184 1.0\navg_time_to_throw                        0.07 0.22  0.778 2.4\navg_completed_air_yards                  0.02 0.88  0.119 1.4\navg_intended_air_yards                   0.00 0.88  0.123 1.4\navg_air_yards_differential               0.00 0.85  0.149 1.1\naggressiveness                          -0.05 0.57  0.425 2.1\nmax_completed_air_distance              -0.06 0.63  0.370 2.5\navg_air_yards_to_sticks                  0.03 0.88  0.121 1.1\npasser_rating                            0.04 0.98  0.015 1.9\nexpected_completion_percentage           0.02 0.84  0.159 1.7\ncompletion_percentage_above_expectation -0.04 0.96  0.035 1.2\navg_air_distance                         0.01 0.86  0.143 1.2\nmax_air_distance                        -0.04 0.63  0.366 3.3\n\n                       TC1  TC2  TC3  TC4  TC6  TC7  TC5\nSS loadings           8.56 5.27 4.00 3.07 2.39 1.93 1.74\nProportion Var        0.26 0.16 0.12 0.09 0.07 0.06 0.05\nCumulative Var        0.26 0.42 0.54 0.63 0.71 0.76 0.82\nProportion Explained  0.32 0.20 0.15 0.11 0.09 0.07 0.06\nCumulative Proportion 0.32 0.51 0.66 0.78 0.86 0.94 1.00\n\n With component correlations of \n     TC1   TC2   TC3   TC4   TC6   TC7  TC5\nTC1 1.00  0.05  0.10  0.35  0.05  0.10 0.21\nTC2 0.05  1.00 -0.04 -0.10 -0.10  0.04 0.01\nTC3 0.10 -0.04  1.00 -0.13  0.31  0.27 0.05\nTC4 0.35 -0.10 -0.13  1.00 -0.08 -0.12 0.07\nTC6 0.05 -0.10  0.31 -0.08  1.00  0.17 0.03\nTC7 0.10  0.04  0.27 -0.12  0.17  1.00 0.05\nTC5 0.21  0.01  0.05  0.07  0.03  0.05 1.00\n\nMean item complexity =  1.6\nTest of the hypothesis that 7 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.07 \n with the empirical chi square  2111929  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.97\n\nCodepca8ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 8, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC3   TC2   TC7   TC6   TC4\ncompletions                              0.92  0.17 -0.10 -0.13 -0.02  0.08\nattempts                                 0.97 -0.02 -0.05 -0.04 -0.05  0.05\npassing_yards                            0.87  0.12  0.23  0.01  0.15  0.11\npassing_tds                              0.72  0.09  0.09  0.07  0.05  0.09\npassing_interceptions                    0.77 -0.06  0.04  0.03  0.08 -0.08\nsacks_suffered                           0.65  0.05 -0.06  0.02  0.02  0.34\nsack_yards_lost                         -0.62 -0.06  0.06 -0.01 -0.02 -0.35\nsack_fumbles                             0.07 -0.03 -0.02 -0.01 -0.01  0.89\nsack_fumbles_lost                       -0.05 -0.02  0.02 -0.02 -0.02  0.88\npassing_air_yards                        0.84  0.09  0.21  0.23 -0.23  0.00\npassing_yards_after_catch                0.82 -0.01 -0.05 -0.28  0.03  0.12\npassing_first_downs                      0.89  0.16  0.00  0.06  0.07  0.09\npassing_epa                              0.15  0.12  0.07  0.03 -0.01 -0.21\npassing_cpoe                            -0.02  0.98  0.10 -0.06  0.04 -0.01\npassing_2pt_conversions                  0.02 -0.02 -0.05 -0.02  0.01 -0.01\npacr                                     0.04  0.12  0.07 -0.17  0.82 -0.07\npass_40_yds                              0.23 -0.20  0.62 -0.15  0.16 -0.02\npass_inc                                 0.98 -0.31 -0.01  0.12 -0.05  0.03\npass_comp_pct                            0.09  0.88 -0.14 -0.26 -0.08 -0.01\nfumbles                                  0.07 -0.05 -0.01  0.00  0.01  0.75\ntwo_pts                                 -0.05 -0.01 -0.03 -0.03  0.01 -0.01\navg_time_to_throw                       -0.19  0.05  0.38  0.12 -0.03  0.12\navg_completed_air_yards                 -0.04  0.07  0.63  0.54  0.22  0.05\navg_intended_air_yards                  -0.31  0.03  0.49  0.36 -0.44  0.09\navg_air_yards_differential              -0.06  0.07  0.17  0.09  0.89  0.04\naggressiveness                           0.10 -0.09 -0.29  0.82 -0.02 -0.04\nmax_completed_air_distance               0.10  0.03  0.84 -0.11  0.11 -0.06\navg_air_yards_to_sticks                  0.03  0.02  0.44  0.46 -0.41 -0.03\npasser_rating                           -0.07  0.46  0.14 -0.07  0.16  0.07\nexpected_completion_percentage          -0.04  0.07 -0.06 -0.82  0.12  0.01\ncompletion_percentage_above_expectation  0.02  0.92 -0.05  0.25  0.10 -0.04\navg_air_distance                        -0.01  0.03  0.48  0.41 -0.43  0.01\nmax_air_distance                         0.15  0.03  0.58 -0.20 -0.54 -0.08\n                                          TC8   TC5   h2     u2 com\ncompletions                              0.11  0.03 1.03 -0.034 1.2\nattempts                                 0.00  0.03 1.00  0.003 1.0\npassing_yards                            0.18  0.04 1.10 -0.098 1.4\npassing_tds                              0.44  0.06 0.90  0.100 1.8\npassing_interceptions                   -0.51  0.02 0.78  0.218 1.8\nsacks_suffered                          -0.30  0.02 0.82  0.176 2.0\nsack_yards_lost                          0.30 -0.02 0.79  0.213 2.1\nsack_fumbles                            -0.03  0.00 0.85  0.147 1.0\nsack_fumbles_lost                       -0.01  0.00 0.74  0.258 1.0\npassing_air_yards                       -0.12  0.05 0.98  0.020 1.5\npassing_yards_after_catch                0.24  0.02 0.95  0.052 1.5\npassing_first_downs                      0.20  0.05 1.03 -0.028 1.2\npassing_epa                              0.87  0.05 0.97  0.030 1.3\npassing_cpoe                            -0.11  0.01 0.95  0.045 1.1\npassing_2pt_conversions                  0.01  0.90 0.81  0.187 1.0\npacr                                    -0.20  0.01 0.79  0.214 1.3\npass_40_yds                              0.32 -0.04 0.58  0.420 2.4\npass_inc                                -0.16  0.04 1.13 -0.130 1.3\npass_comp_pct                            0.05  0.00 0.92  0.078 1.3\nfumbles                                  0.00  0.01 0.62  0.378 1.0\ntwo_pts                                  0.00  0.92 0.82  0.180 1.0\navg_time_to_throw                       -0.32  0.11 0.29  0.709 3.2\navg_completed_air_yards                  0.10  0.05 0.89  0.110 2.3\navg_intended_air_yards                   0.06  0.01 0.88  0.122 3.7\navg_air_yards_differential               0.14  0.00 0.86  0.140 1.2\naggressiveness                           0.08 -0.05 0.63  0.370 1.3\nmax_completed_air_distance               0.04 -0.03 0.70  0.297 1.1\navg_air_yards_to_sticks                  0.09  0.05 0.88  0.120 3.1\npasser_rating                            0.66  0.01 1.04 -0.041 2.1\nexpected_completion_percentage           0.12  0.00 0.84  0.158 1.1\ncompletion_percentage_above_expectation  0.11 -0.04 0.98  0.020 1.2\navg_air_distance                        -0.05  0.03 0.86  0.140 3.0\nmax_air_distance                        -0.06 -0.02 0.66  0.338 2.4\n\n                       TC1  TC3  TC2  TC7  TC6  TC4  TC8  TC5\nSS loadings           8.33 3.40 3.28 2.94 2.84 2.89 2.66 1.75\nProportion Var        0.25 0.10 0.10 0.09 0.09 0.09 0.08 0.05\nCumulative Var        0.25 0.36 0.45 0.54 0.63 0.72 0.80 0.85\nProportion Explained  0.30 0.12 0.12 0.10 0.10 0.10 0.09 0.06\nCumulative Proportion 0.30 0.42 0.53 0.64 0.74 0.84 0.94 1.00\n\n With component correlations of \n      TC1   TC3   TC2   TC7   TC6   TC4   TC8   TC5\nTC1  1.00  0.08  0.09 -0.03  0.03  0.41  0.03  0.21\nTC3  0.08  1.00  0.12 -0.11  0.24  0.01  0.38  0.02\nTC2  0.09  0.12  1.00  0.31 -0.15 -0.02  0.19  0.06\nTC7 -0.03 -0.11  0.31  1.00 -0.28 -0.01 -0.04  0.04\nTC6  0.03  0.24 -0.15 -0.28  1.00  0.03  0.18 -0.02\nTC4  0.41  0.01 -0.02 -0.01  0.03  1.00 -0.10  0.11\nTC8  0.03  0.38  0.19 -0.04  0.18 -0.10  1.00  0.03\nTC5  0.21  0.02  0.06  0.04 -0.02  0.11  0.03  1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 8 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.06 \n with the empirical chi square  1779393  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.97\n\nCodepca9ComponentOblique\n\nPrincipal Components Analysis\nCall: psych::principal(r = dataForPCA[pcaVars], nfactors = 9, rotate = \"oblimin\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                                          TC1   TC3   TC4   TC2   TC6   TC8\ncompletions                              0.90  0.18  0.09 -0.03 -0.01  0.10\nattempts                                 0.95 -0.01  0.06 -0.05  0.02 -0.01\npassing_yards                            0.86  0.11  0.11  0.15  0.21  0.19\npassing_tds                              0.73  0.06  0.06  0.05  0.01  0.50\npassing_interceptions                    0.79 -0.05 -0.09  0.09  0.02 -0.50\nsacks_suffered                           0.70  0.04  0.31  0.01 -0.14 -0.24\nsack_yards_lost                         -0.67 -0.04 -0.31 -0.01  0.14  0.25\nsack_fumbles                             0.06 -0.02  0.90 -0.01  0.00 -0.04\nsack_fumbles_lost                       -0.06 -0.01  0.89 -0.02  0.04 -0.02\npassing_air_yards                        0.85  0.09 -0.01 -0.21  0.17 -0.09\npassing_yards_after_catch                0.81 -0.02  0.12  0.02  0.02  0.23\npassing_first_downs                      0.90  0.15  0.08  0.06 -0.02  0.23\npassing_epa                              0.14  0.10 -0.23 -0.01  0.04  0.90\npassing_cpoe                            -0.01  0.98 -0.02  0.04  0.06 -0.10\npassing_2pt_conversions                  0.01 -0.01  0.00  0.01 -0.02  0.00\npacr                                     0.03  0.13 -0.05  0.81  0.08 -0.23\npass_40_yds                              0.16 -0.19  0.03  0.19  0.68  0.26\npass_inc                                 0.99 -0.31  0.02 -0.05 -0.01 -0.14\npass_comp_pct                            0.07  0.89  0.00 -0.09 -0.06  0.03\nfumbles                                  0.06 -0.04  0.76  0.01  0.00  0.00\ntwo_pts                                 -0.06  0.00  0.00  0.01  0.00 -0.01\navg_time_to_throw                       -0.01 -0.05 -0.02 -0.02 -0.14 -0.07\navg_completed_air_yards                  0.00  0.04  0.01  0.25  0.35  0.19\navg_intended_air_yards                  -0.28  0.01  0.08 -0.40  0.32  0.11\navg_air_yards_differential              -0.06  0.07  0.04  0.88  0.09  0.16\naggressiveness                           0.02 -0.02  0.03 -0.01 -0.06 -0.04\nmax_completed_air_distance               0.01  0.06  0.00  0.16  0.91 -0.06\navg_air_yards_to_sticks                  0.06  0.00 -0.06 -0.38  0.26  0.16\npasser_rating                           -0.08  0.44  0.06  0.16  0.09  0.69\nexpected_completion_percentage          -0.06  0.06  0.02  0.10  0.03  0.08\ncompletion_percentage_above_expectation  0.00  0.93 -0.02  0.10 -0.03  0.10\navg_air_distance                         0.02  0.01 -0.02 -0.39  0.30  0.02\nmax_air_distance                         0.07  0.05 -0.01 -0.50  0.72 -0.15\n                                          TC7   TC9   TC5   h2      u2 com\ncompletions                             -0.08 -0.17  0.04 1.03 -0.0347 1.2\nattempts                                 0.00 -0.13  0.04 1.00  0.0015 1.1\npassing_yards                            0.00  0.04  0.03 1.10 -0.0978 1.4\npassing_tds                              0.00  0.10  0.04 0.93  0.0704 1.9\npassing_interceptions                    0.02  0.06  0.02 0.78  0.2176 1.8\nsacks_suffered                          -0.04  0.14  0.00 0.84  0.1636 1.9\nsack_yards_lost                          0.05 -0.13  0.00 0.80  0.2022 2.0\nsack_fumbles                             0.02 -0.02  0.00 0.86  0.1376 1.0\nsack_fumbles_lost                        0.01 -0.02  0.01 0.76  0.2445 1.0\npassing_air_yards                        0.18  0.16  0.05 0.98  0.0201 1.4\npassing_yards_after_catch               -0.24 -0.18  0.02 0.95  0.0503 1.5\npassing_first_downs                      0.02  0.01  0.04 1.03 -0.0338 1.2\npassing_epa                             -0.03 -0.01  0.04 1.00  0.0014 1.2\npassing_cpoe                            -0.07  0.09  0.01 0.96  0.0438 1.1\npassing_2pt_conversions                  0.00 -0.01  0.90 0.82  0.1802 1.0\npacr                                    -0.14 -0.09  0.02 0.79  0.2074 1.3\npass_40_yds                             -0.09 -0.05 -0.02 0.61  0.3861 1.8\npass_inc                                 0.11  0.02  0.04 1.13 -0.1305 1.3\npass_comp_pct                           -0.21 -0.16  0.01 0.93  0.0744 1.2\nfumbles                                  0.02  0.01  0.02 0.63  0.3737 1.0\ntwo_pts                                  0.00  0.00  0.92 0.83  0.1720 1.0\navg_time_to_throw                       -0.25  0.89  0.01 0.71  0.2868 1.2\navg_completed_air_yards                  0.35  0.57  0.01 0.91  0.0942 3.2\navg_intended_air_yards                   0.24  0.43 -0.01 0.88  0.1185 4.6\navg_air_yards_differential               0.05  0.06 -0.01 0.86  0.1402 1.1\naggressiveness                           0.95 -0.26  0.00 0.82  0.1831 1.2\nmax_completed_air_distance              -0.01  0.00  0.00 0.83  0.1733 1.1\navg_air_yards_to_sticks                  0.32  0.44  0.02 0.89  0.1107 4.0\npasser_rating                           -0.11  0.03  0.00 1.05 -0.0520 2.0\nexpected_completion_percentage          -0.74 -0.29  0.00 0.84  0.1554 1.4\ncompletion_percentage_above_expectation  0.26  0.00 -0.02 0.99  0.0069 1.2\navg_air_distance                         0.29  0.44  0.01 0.86  0.1356 3.6\nmax_air_distance                        -0.07 -0.08  0.02 0.76  0.2353 2.0\n\n                       TC1  TC3  TC4  TC2  TC6  TC8  TC7  TC9  TC5\nSS loadings           8.31 3.37 2.85 2.70 2.68 2.68 2.46 2.38 1.74\nProportion Var        0.25 0.10 0.09 0.08 0.08 0.08 0.07 0.07 0.05\nCumulative Var        0.25 0.35 0.44 0.52 0.60 0.68 0.76 0.83 0.88\nProportion Explained  0.28 0.12 0.10 0.09 0.09 0.09 0.08 0.08 0.06\nCumulative Proportion 0.28 0.40 0.50 0.59 0.68 0.77 0.86 0.94 1.00\n\n With component correlations of \n      TC1   TC3   TC4   TC2   TC6   TC8   TC7   TC9   TC5\nTC1  1.00  0.07  0.43  0.02  0.13  0.04 -0.02 -0.02  0.22\nTC3  0.07  1.00  0.01  0.24  0.13  0.37 -0.13 -0.05  0.01\nTC4  0.43  0.01  1.00  0.04 -0.04 -0.09 -0.03  0.00  0.11\nTC2  0.02  0.24  0.04  1.00 -0.10  0.17 -0.25 -0.19 -0.01\nTC6  0.13  0.13 -0.04 -0.10  1.00  0.24  0.23  0.28  0.05\nTC8  0.04  0.37 -0.09  0.17  0.24  1.00 -0.02  0.02  0.04\nTC7 -0.02 -0.13 -0.03 -0.25  0.23 -0.02  1.00  0.31  0.00\nTC9 -0.02 -0.05  0.00 -0.19  0.28  0.02  0.31  1.00  0.02\nTC5  0.22  0.01  0.11 -0.01  0.05  0.04  0.00  0.02  1.00\n\nMean item complexity =  1.7\nTest of the hypothesis that 9 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.06 \n with the empirical chi square  1584360  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.97\n\n\nBased on the component solutions, the three-component solution maps onto our three-factor solution from factor analysis. The three-component solution explains more than half of the variance in the variables. The fourth component in a four-component solution is not particularly interpretable, explains relatively little variance, and seems to be related to sacks: sacks suffered, sack yards lost, sack fumbles, and sack fumbles lost. However, two of the sack-related variables (sacks suffered and sack yards lost) load more strongly onto the first component, suggesting that these sack-related variables are better captured by another component. Moreover, sacks might be considered a scoring methodological component rather than a particular concept of interest. For these resources we prefer the three-component solution and choose to retain three components.\n\nCodedataMergedWithPCA &lt;- cbind(dataMerged, pca3ComponentOblique$scores)\n\n\nHere are the variables that had a standardized component loading of 0.4 or greater on each component:\n\nCodecomponent1vars &lt;- c(\n  \"completions\",\"attempts\",\"passing_yards\",\"passing_tds\",\"passing_interceptions\",\n  \"sacks_suffered\",\"sack_yards_lost\",\"sack_fumbles\",\"sack_fumbles_lost\",\n  \"passing_air_yards\",\"passing_yards_after_catch\",\"passing_first_downs\",\n  #\"passing_epa\",\"passing_cpoe\",\"passing_2pt_conversions\",\"pacr\",\"pass_40_yds\",\n  \"pass_inc\",\"fumbles\")#,\"two_pts\",\"pass_comp_pct\",\n  #\"avg_time_to_throw\",\"avg_completed_air_yards\",\"avg_intended_air_yards\",\n  #\"avg_air_yards_differential\",\"aggressiveness\",\"max_completed_air_distance\",\n  #\"avg_air_yards_to_sticks\",\"passer_rating\", #,\"completion_percentage\"\n  #\"expected_completion_percentage\",\"completion_percentage_above_expectation\",\n  #\"avg_air_distance\",\"max_air_distance\")\n\ncomponent2vars &lt;- c(\n  #\"completions\",\"attempts\",\"passing_yards\",\"passing_tds\",\"passing_interceptions\",\n  #\"sacks_suffered\",\"sack_yards_lost\",\"sack_fumbles\",\"sack_fumbles_lost\",\n  \"passing_air_yards\",#\"passing_yards_after_catch\",\"passing_first_downs\",\n  \"pacr\",#\"passing_epa\",\"passing_cpoe\",\"passing_2pt_conversions\",\"pass_40_yds\",\n  #\"pass_inc\",\"pass_comp_pct\",\"fumbles\",\"two_pts\",\n  \"avg_completed_air_yards\",\"avg_intended_air_yards\",#\"avg_time_to_throw\",\n  \"aggressiveness\",\"max_completed_air_distance\",#\"avg_air_yards_differential\",\n  \"avg_air_yards_to_sticks\",#\"passer_rating\", #,\"completion_percentage\"\n  \"expected_completion_percentage\",#\"completion_percentage_above_expectation\",\n  \"avg_air_distance\",\"max_air_distance\")\n\ncomponent3vars &lt;- c(\n  \"passing_tds\",#\"completions\",\"attempts\",\"passing_yards\",\"passing_interceptions\",\n  #\"sacks_suffered\",\"sack_yards_lost\",\"sack_fumbles\",\"sack_fumbles_lost\",\n  #\"passing_air_yards\",\"passing_yards_after_catch\",\"passing_first_downs\",\n  \"passing_epa\",\"passing_cpoe\",\"pass_40_yds\",#\"passing_2pt_conversions\",\"pacr\",\n  \"pass_comp_pct\",#\"fumbles\",\"two_pts\",\"pass_inc\",\n  \"avg_air_yards_differential\",\"max_completed_air_distance\",\n  \"avg_time_to_throw\",\"avg_completed_air_yards\",\"avg_intended_air_yards\",\n  #\"aggressiveness\",\n  \"passer_rating\",#\"avg_air_yards_to_sticks\", #,\"completion_percentage\"\n  \"completion_percentage_above_expectation\")#,\"expected_completion_percentage\",\n  #\"avg_air_distance\",\"max_air_distance\")\n\n\nThe variables that loaded most strongly onto component 1 appear to reflect Quarterback usage: completions, incompletions, passing attempts, passing yards, passing touchdowns, interceptions thrown, fumbles, times sacked, sack yards lost (“reversed”—i.e., negatively associated with the component), sack fumbles, sack fumbles lost, passing air yards (total horizontal distance the ball travels on all pass attempts), passing yards after the catch, and first downs gained by passing. Quarterbacks who tend to throw more tend to have higher levels on those variables. Thus, we label component 1 as “Usage”, which reflects total Quarterback involvement, regardless of efficiency or outcome.\nThe variables that loaded most strongly onto component 2 appear to reflect Quarterback aggressiveness: passing air yards, passing air conversion ratio (reversed; ratio of passing yards to passing air yards), average air yards on completed passes, average air yards on all attempted passes, aggressiveness (percentage of passing attempts thrown into tight windows, where there is a defender within one yard or less of the receiver at the time of the completion or incompletion), expected completion percentage (reversed; based on air distance, receiver separation, Quarterback/Wide Receiver movement, pass location, whether there was pressure on the Quarterback when throwing, the throw angle and trajectory, receiver and defender positioning at the catch point, and defensive coverage scheme), average amount of air yards ahead of or behind the first down marker on passing attempts, average air distance (the true three-dimensional distance the ball travels in the air), maximum air distance, and maximum air distance on completed passes. Quarterbacks who throw the ball farther and into tighter windows tend to have higher values on those variables. Thus, we label component 2 as “Aggressiveness”, which reflects throwing longer, more difficult passes with a tight window.\nThe variables that loaded most strongly onto component 3 appear to reflect Quarterback performance: passing touchdowns, passing expected points added, passing completion percentage above expectation, passes completed of 40 yards or more, pass completion percentage, air yards differential (intended air yards \\(-\\) completed air yards; attempting deeper passes than he on average completes), maximum completed air distance, average time to throw, average completed air yards, average intended air yards, and passer rating. Quarterbacks who perform better tend to have higher values on those variables. Thus, we label component 3 as “Performance”.\nBelow are component scores from the PCA for the first six players:\n\nCodepca3ComponentOblique$scores %&gt;% \n  na.omit() %&gt;% \n  head()\n\n          TC1        TC2        TC3\n[1,] 5.623830  0.1212148 -1.3399587\n[2,] 4.009919  1.9548287  0.8406664\n[3,] 8.418577 -0.3938434 -2.8524482\n[4,] 4.189431  1.0714847  1.7692927\n[5,] 5.804268  1.3782986 -0.5757511\n[6,] 6.157528 -0.3560422  0.3905272\n\n\nHere are the players and weeks that showed the highest levels of Quarterback “Usage”:\n\nCodedataMergedWithPCA %&gt;% \n  arrange(-TC1) %&gt;% \n  select(player_display_name, season, week, TC1, all_of(component1vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and weeks that showed the lowest levels of Quarterback “Usage”:\n\nCodedataMergedWithPCA %&gt;% \n  arrange(TC1) %&gt;% \n  select(player_display_name, season, week, TC1, all_of(component1vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and weeks that showed the highest levels of Quarterback “Aggressiveness”:\n\nCodedataMergedWithPCA %&gt;% \n  arrange(-TC2) %&gt;% \n  select(player_display_name, season, week, TC2, all_of(component2vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and weeks that showed the lowest levels of Quarterback “Aggressiveness”:\n\nCodedataMergedWithPCA %&gt;% \n  arrange(TC2) %&gt;% \n  select(player_display_name, season, week, TC2, all_of(component2vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and weeks that showed the highest levels of Quarterback “Performance”:\n\nCodedataMergedWithPCA %&gt;% \n  arrange(-TC3) %&gt;% \n  select(player_display_name, season, week, TC3, all_of(component3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()\n\n\n  \n\n\n\nHere are the players and weeks that showed the lowest levels of Quarterback “Performance”:\n\nCodedataMergedWithPCA %&gt;% \n  arrange(TC3) %&gt;% \n  head %&gt;% \n  select(player_display_name, season, week, TC3, all_of(component3vars)) %&gt;% \n  na.omit() %&gt;% \n  head()",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaConclusion",
    "href": "pca.html#sec-pcaConclusion",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "\n23.6 Conclusion",
    "text": "23.6 Conclusion\nPrincipal component analysis (PCA) is a technique used for data reduction—reducing a large set of a variables down to a smaller set of components that capture most of the variance in the larger set. There are many decisions to make in factor analysis. These decisions can have important impacts on the resulting solution. Thus, it can be helpful for theory and interpretability to help guide decision-making when conducting factor analysis. There are several differences between factor analysis and PCA. Unlike factor analysis, which estimates the latent factors as the common variance among the variables that load onto that factor and discards the remaining variance as “error”, PCA uses all variance of variables and assumes variables have no error. Thus, PCA does not account for measurement error. Using PCA, we were able to identify three PCA components that accounted for considerable variance in the variables we examined, pertaining to Quarterbacks: 1) usage; 2) aggressiveness; 3) performance. We were then able to determine which players were highest and lowest on each of these components.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#sec-pcaSessionInfo",
    "href": "pca.html#sec-pcaSessionInfo",
    "title": "23  Data Reduction: Principal Component Analysis",
    "section": "\n23.7 Session Info",
    "text": "23.7 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4  forcats_1.0.0    stringr_1.5.1    dplyr_1.1.4     \n [5] purrr_1.1.0      readr_2.1.5      tidyr_1.3.1      tibble_3.3.0    \n [9] ggplot2_3.5.2    tidyverse_2.0.0  nFactors_2.4.1.2 psych_2.5.6     \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6         jsonlite_2.0.0       compiler_4.5.1      \n [4] tidyselect_1.2.1     parallel_4.5.1       scales_1.4.0        \n [7] yaml_2.3.10          fastmap_1.2.0        lattice_0.22-7      \n[10] R6_2.6.1             generics_0.1.4       knitr_1.50          \n[13] htmlwidgets_1.6.4    pillar_1.11.0        RColorBrewer_1.1-3  \n[16] tzdb_0.5.0           rlang_1.1.6          stringi_1.8.7       \n[19] xfun_0.53            timechange_0.3.0     cli_3.6.5           \n[22] withr_3.0.2          magrittr_2.0.3       digest_0.6.37       \n[25] grid_4.5.1           hms_1.1.3            lifecycle_1.0.4     \n[28] nlme_3.1-168         vctrs_0.6.5          mnormt_2.1.1        \n[31] evaluate_1.0.4       glue_1.8.0           GPArotation_2025.3-1\n[34] farver_2.1.2         rmarkdown_2.29       tools_4.5.1         \n[37] pkgconfig_2.0.3      htmltools_0.5.8.1   \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nRaiche, G., & Magis, D. (2025). nFactors: Parallel analysis and other non graphical solutions to the Cattell scree test. https://doi.org/10.32614/CRAN.package.nFactors\n\n\nRevelle, W. (2025). psych: Procedures for psychological, psychometric, and personality research. https://doi.org/10.32614/CRAN.package.psych",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Data Reduction: Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "24  Simulation: Bootstrapping and the Monte Carlo Method",
    "section": "",
    "text": "24.1 Getting Started\nThis chapter provides an overview of various approaches to simulation, including bootstrapping and the Monte Carlo method.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Simulation: Bootstrapping and the Monte Carlo Method</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-simulationGettingStarted",
    "href": "simulation.html#sec-simulationGettingStarted",
    "title": "24  Simulation: Bootstrapping and the Monte Carlo Method",
    "section": "",
    "text": "24.1.1 Load Packages\n\nCodelibrary(\"ffanalytics\")\nlibrary(\"data.table\")\nlibrary(\"future\")\nlibrary(\"future.apply\")\nlibrary(\"progressr\")\nlibrary(\"SimDesign\")\nlibrary(\"fitdistrplus\")\nlibrary(\"sn\")\nlibrary(\"tidyverse\")\n\n\n\n24.1.2 Load Data\n\nCodeload(file = \"./data/players_projectedPoints_seasonal.RData\")\nload(file = \"./data/nfl_playerIDs.RData\")",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Simulation: Bootstrapping and the Monte Carlo Method</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-simulationOverview",
    "href": "simulation.html#sec-simulationOverview",
    "title": "24  Simulation: Bootstrapping and the Monte Carlo Method",
    "section": "\n24.2 Overview",
    "text": "24.2 Overview\nA simulation is an “imitative representation” of a phenomenon that could exist the real world. In statistics, simulations are computer-driven investigations to better understand a phenomenon by studying its behavior under different conditions. For instance, we might want to determine the likely range of outcomes for a player, in terms of the range of fantasy points that a player might score over the course of a season. Simulations can be conducted in various ways. Two common ways of conducting simulations are via bootstrapping and via Monte Carlo simulation.\n\n24.2.1 Bootstrapping\nBootstrapping involves repeated resampling (with replacement) from observed data. For instance, if we have 100 sources provide projections for a player, we could estimate the most likely range of fantasy points for the player by repeatedly sampling from the 100 projections.\n\n24.2.2 Monte Carlo Simulation\nMonte Carlo simulation involves repeated random sampling from a known distribution (Sigal & Chalmers, 2016). For instance, if we know the population distribution for the likely outcomes for a player—e.g., a normal distribution with a known mean (e.g., 150 points) and standard deviation (e.g., 20 points)—we can repeatedly sample randomly from this distribution. The distribution could be, as examples, a normal distribution, a log-normal distribution, a binomial distribution, a chi-square distribution, etc. (Sigal & Chalmers, 2016). The distribution provides a probability density function, which indicates the probability that any particular value would be observed if the data arose from that distribution.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Simulation: Bootstrapping and the Monte Carlo Method</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-simulationProjectedPoints",
    "href": "simulation.html#sec-simulationProjectedPoints",
    "title": "24  Simulation: Bootstrapping and the Monte Carlo Method",
    "section": "\n24.3 Simulation of Projected Statistics and Points",
    "text": "24.3 Simulation of Projected Statistics and Points\nBelow, we perform bootstrapping and Monte Carlo simulations of projected statistics and points. However, it is worth noting that—as for any simulation—the quality of the results depend on the quality of the inputs. In this case, the quality of the simulation depends on the quality of the projections. If the projections are no good, the simulation results will not be trustworthy. Garbage in, garbage out. As we evaluated in Section 17.12, projections tend to show moderate accuracy for fantasy performance, but they are not highly accurate. Thus, we should treat simulation results arising from fantasy projections with a good dose of skepticism.\n\n24.3.1 Bootstrapping\n\n24.3.1.1 Prepare Data\n\nCodeall_proj &lt;- dplyr::bind_rows(players_projectedPoints_seasonal)\n\n\n\nCodevars_by_pos &lt;- list(\n  QB = c(\n    \"games\",\n    \"pass_att\", \"pass_comp\", \"pass_inc\", \"pass_yds\", \"pass_tds\", \"pass_int\",\n    \"rush_att\", \"rush_yds\", \"rush_tds\",\n    \"fumbles_lost\", \"fumbles_total\", \"two_pts\",\n    \"sacks\",\n    \"pass_09_tds\", \"pass_1019_tds\", \"pass_2029_tds\", \"pass_3039_tds\", \"pass_4049_tds\", \"pass_50_tds\",\n    \"pass_40_yds\", \"pass_250_yds\", \"pass_300_yds\", \"pass_350_yds\", \"pass_400_yds\",\n    \"rush_40_yds\", \"rush_50_yds\", \"rush_100_yds\", \"rush_150_yds\", \"rush_200_yds\"\n    ),\n  RB = c(\n    \"games\",\n    \"rush_att\", \"rush_yds\", \"rush_tds\",\n    \"rec_tgt\", \"rec\", \"rec_yds\", \"rec_tds\", \"rec_rz_tgt\",\n    \"fumbles_lost\", \"fumbles_total\", \"two_pts\",\n    \"return_yds\", \"return_tds\",\n    \"rush_09_tds\", \"rush_1019_tds\", \"rush_2029_tds\", \"rush_3039_tds\", \"rush_4049_tds\", \"rush_50_tds\",\n    \"rush_40_yds\", \"rush_50_yds\", \"rush_100_yds\", \"rush_150_yds\", \"rush_200_yds\",\n    \"rec_40_yds\", \"rec_50_yds\", \"rec_100_yds\", \"rec_150_yds\", \"rec_200_yds\"\n  ),\n  WR = c(\n    \"games\",\n    \"pass_att\", \"pass_comp\", \"pass_inc\", \"pass_yds\", \"pass_tds\", \"pass_int\",\n    \"rush_att\", \"rush_yds\", \"rush_tds\",\n    \"rec_tgt\", \"rec\", \"rec_yds\", \"rec_tds\", \"rec_rz_tgt\",\n    \"fumbles_lost\", \"fumbles_total\", \"two_pts\",\n    \"return_yds\", \"return_tds\",\n    \"rush_09_tds\", \"rush_1019_tds\", \"rush_2029_tds\", \"rush_3039_tds\", \"rush_4049_tds\", \"rush_50_tds\",\n    \"rush_40_yds\", \"rush_50_yds\", \"rush_100_yds\", \"rush_150_yds\", \"rush_200_yds\",\n    \"rec_40_yds\", \"rec_50_yds\", \"rec_100_yds\", \"rec_150_yds\", \"rec_200_yds\"\n  ),\n  TE = c(\n    \"games\",\n    \"pass_att\", \"pass_comp\", \"pass_inc\", \"pass_yds\", \"pass_tds\", \"pass_int\",\n    \"rush_att\", \"rush_yds\", \"rush_tds\",\n    \"rec_tgt\", \"rec\", \"rec_yds\", \"rec_tds\", \"rec_rz_tgt\",\n    \"fumbles_lost\", \"fumbles_total\", \"two_pts\",\n    \"return_yds\", \"return_tds\",\n    \"rush_09_tds\", \"rush_1019_tds\", \"rush_2029_tds\", \"rush_3039_tds\", \"rush_4049_tds\", \"rush_50_tds\",\n    \"rush_40_yds\", \"rush_50_yds\", \"rush_100_yds\", \"rush_150_yds\", \"rush_200_yds\",\n    \"rec_40_yds\", \"rec_50_yds\", \"rec_100_yds\", \"rec_150_yds\", \"rec_200_yds\"\n  ),\n  K = c(\n    \"fg_0019\", \"fg_2029\", \"fg_3039\", \"fg_4049\", \"fg_50\", \"fg_50_att\",\n    \"fg_39\", \"fg_att_39\", \"fg_49\", \"fg_49_att\",\n    \"fg\", \"fg_att\", \"fg_miss\", \"xp\", \"xp_att\"\n  ),\n  D = c(\n    \"idp_solo\", \"idp_asst\", \"idp_sack\", \"idp_int\", \"idp_fum_force\", \"idp_fum_rec\", \"idp_pd\", \"idp_td\", \"idp_safety\"\n  ),\n  DL = c(\n    \"idp_solo\", \"idp_asst\", \"idp_sack\", \"idp_int\", \"idp_fum_force\", \"idp_fum_rec\", \"idp_pd\", \"idp_td\", \"idp_safety\"\n  ),\n  LB = c(\n    \"idp_solo\", \"idp_asst\", \"idp_sack\", \"idp_int\", \"idp_fum_force\", \"idp_fum_rec\", \"idp_pd\", \"idp_td\", \"idp_safety\"\n  ),\n  DB = c(\n    \"idp_solo\", \"idp_asst\", \"idp_sack\", \"idp_int\", \"idp_fum_force\", \"idp_fum_rec\", \"idp_pd\", \"idp_td\", \"idp_safety\"\n  ),\n  DST = c(\n    \"dst_fum_recvr\", \"dst_fum_rec\", \"dst_int\", \"dst_safety\", \"dst_sacks\", \"dst_td\", \"dst_blk\",\n    \"dst_fumbles\", \"dst_tackles\", \"dst_yds_against\", \"dst_pts_against\", \"dst_pts_allowed\", \"dst_ret_yds\"\n  )\n)\n\n\n\n24.3.1.2 Bootstrapping Function\nFor performing the bootstrapping, we leverage the data.table (Barrett et al., 2025) (data.table::as.data.table(); data.table::data.table(); data.table::rbindlist()), future (Bengtsson, 2025a) (future::plan(); future::multisession()), and future.apply (Bengtsson, 2025b) (future.apply::future_lapply()) packages for speed (by using parallel processing) and memory efficiency. We use the progressr (Bengtsson, 2024) package (progressr::handlers(); progressr::with_progress(); progressr::progressor()) to create a progress bar.\n\nCodebootstrapSimulation &lt;- function(\n    projectedStats,\n    vars_by_pos,\n    n_iter = 10000,\n    seed = NULL,\n    progress = TRUE) {\n  \n  dt &lt;- data.table::as.data.table(projectedStats) # use data.table for speed\n  all_ids &lt;- unique(dt$id)\n  \n  if (!is.null(seed)) set.seed(seed)\n  \n  future::plan(future::multisession) # parallelize tasks across multiple background R sessions using multiple cores to speed up simulation\n  \n  if (progress) progressr::handlers(\"txtprogressbar\") # specify progress-bar style\n  \n  results &lt;- progressr::with_progress({ # wrap in with_progress for progress bar\n    p &lt;- if (progress) progressr::progressor(along = all_ids) else NULL # create progressor for progress bar\n    \n    future.apply::future_lapply(\n      all_ids, # apply the function below to each player using a parallelized loop\n      function(player_id) {\n        if (!is.null(p)) p() # advance progress bar\n        \n        player_data &lt;- dt[id == player_id]\n        player_pos  &lt;- unique(player_data$pos)\n        \n        if (length(player_pos) != 1 || !player_pos %in% names(vars_by_pos)) return(NULL)\n        \n        stat_vars &lt;- vars_by_pos[[player_pos]] # pull the relevant stat variables to simulate for this player's position\n        out &lt;- data.table(iteration = seq_len(n_iter), id = player_id, pos = player_pos)\n        \n        for (var in stat_vars) { # loop over each stat variable that should be simulated for the player's position\n          if (var %in% names(player_data)) {\n            non_na_values &lt;- player_data[[var]][!is.na(player_data[[var]])] # pull non-missing values of the stat for the player (from all projection sources)\n            \n            if (length(non_na_values) &gt; 0) {\n              out[[var]] &lt;- sample(non_na_values, n_iter, replace = TRUE) # if there are valid values, sample with replacement to simulate n_iter values\n            } else {\n              out[[var]] &lt;- NA_real_ # specify a numeric missing value (if all values were missing)\n            }\n          } else {\n            out[[var]] &lt;- NA_real_ # specify a numeric missing value (if the stat variable doesn't exist)\n          }\n        }\n        \n        return(out)\n      },\n      future.seed = TRUE # ensures that each parallel process gets a reproducible random seed\n    )\n  })\n  \n  data.table::rbindlist(results, use.names = TRUE, fill = TRUE) # combines all the individual player results into one large data table, aligning columns by name; fill = TRUE ensures that missing columns are filled with NA where necessary\n}\n\n\n\n24.3.1.3 Run the Bootstrapping Simulation\n\nCodebootstappedStats &lt;- bootstrapSimulation(\n  projectedStats = all_proj,\n  vars_by_pos = vars_by_pos,\n  n_iter = 5000,\n  seed = 52242)\n\n\n\n24.3.1.4 Score Fantasy Points from the Simulation\ndata.table::setnames()\n\nCodedata.table::setnames(bootstappedStats, \"iteration\", \"data_src\") # data.table equivalent to: bootstappedStats$data_src &lt;- bootstappedStats$iteration\n\nbootstappedStatsByPosition &lt;- split(\n  bootstappedStats,\n  by = \"pos\",\n  keep.by = TRUE)\n\n\nbase::lapply()\n\nCodebootstappedStatsByPosition &lt;- lapply(\n  bootstappedStatsByPosition,\n  setDF)\n\nattr(bootstappedStatsByPosition, \"season\") &lt;- 2024\nattr(bootstappedStatsByPosition, \"week\") &lt;- 0\n\n\n\nCodebootstrappedFantasyPoints &lt;- ffanalytics:::source_points(\n  data_result = bootstappedStatsByPosition,\n  scoring_rules = ffanalytics::scoring)\n\n\n\nCodebootstrappedFantasyPoints$iteration &lt;- bootstrappedFantasyPoints$data_src\nbootstrappedFantasyPoints$data_src &lt;- NULL\n\nbootstrappedFantasyPoints &lt;- bootstrappedFantasyPoints %&gt;% \n  left_join(\n    nfl_playerIDs[,c(\"mfl_id\",\"name\",\"merge_name\",\"team\")],\n    by = c(\"id\" = \"mfl_id\")\n  )\n\nbootstrappedFantasyPoints &lt;- bootstrappedFantasyPoints %&gt;% \n  rename(projectedPoints = raw_points)\n\n\n\n24.3.1.5 Summarize Players’ Distribution of Projected Fantasy Points\n\nCodebootstrappedFantasyPoints_summary &lt;- bootstrappedFantasyPoints %&gt;% \n  group_by(id) %&gt;% \n  summarise(\n    mean = mean(projectedPoints, na.rm = TRUE),\n    SD = sd(projectedPoints, na.rm = TRUE),\n    min = min(projectedPoints, na.rm = TRUE),\n    max = max(projectedPoints, na.rm = TRUE),\n    q10 = quantile(projectedPoints, .10, na.rm = TRUE), # 10th quantile\n    q90 = quantile(projectedPoints, .90, na.rm = TRUE), # 90th quantile\n    range = max(projectedPoints, na.rm = TRUE) - min(projectedPoints, na.rm = TRUE),\n    IQR = IQR(projectedPoints, na.rm = TRUE),\n    MAD = mad(projectedPoints, na.rm = TRUE),\n    CV = SD/mean,\n    median = median(projectedPoints, na.rm = TRUE),\n    pseudomedian = DescTools::HodgesLehmann(projectedPoints, na.rm = TRUE),\n    mode = petersenlab::Mode(projectedPoints, multipleModes = \"mean\"),\n    skewness = psych::skew(projectedPoints, na.rm = TRUE),\n    kurtosis = psych::kurtosi(projectedPoints, na.rm = TRUE)\n  )\n\n\n\n24.3.1.6 View Players’ Distribution of Projected Fantasy Points\n\nCodebootstrappedFantasyPoints_summary &lt;- bootstrappedFantasyPoints_summary %&gt;% \n  left_join(\n    nfl_playerIDs[,c(\"mfl_id\",\"name\",\"merge_name\",\"team\",\"position\")],\n    by = c(\"id\" = \"mfl_id\")\n  ) %&gt;% \n  select(name, team, position, mean:kurtosis, everything()) %&gt;% \n  arrange(-mean)\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position == \"QB\") %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position == \"RB\") %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position == \"WR\") %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position == \"TE\") %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position == c(\"K\",\"PK\")) %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position %in% c(\"DL\",\"DT\",\"DE\")) %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position %in% c(\"LB\",\"MLB\",\"OLB\")) %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\n\nCodebootstrappedFantasyPoints_summary %&gt;% \n  filter(position %in% c(\"DB\",\"S\",\"CB\")) %&gt;% \n  mutate(\n    across(\n      where(is.numeric),\n      \\(x) round(x, digits = 2)))\n\n\n  \n\n\n\nAn example distribution of projected fantasy points is in Figure 24.1.\n\nCodeggplot2::ggplot(\n  data = bootstrappedFantasyPoints %&gt;%\n    filter(pos == \"QB\" & name == \"Patrick Mahomes\"),\n  mapping = aes(\n    x = projectedPoints)\n) +\n  geom_histogram(\n    aes(y = after_stat(density)),\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  geom_density(\n    color = \"#000000\",\n    fill = \"#F85700\",\n    alpha = 0.6 # add transparency\n  ) +\n  geom_rug() +\n  #coord_cartesian(\n  #  xlim = c(0,400)) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Distribution of Projected Fantasy Points for Patrick Mahomes\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 24.1: Distribution of Projected Fantasy Points for Patrick Mahomes from Bootstrapping.\n\n\n\n\nProjections of two players—one with relatively narrow uncertainty and one with relatively wide uncertainty—are depicted in Figure 24.2.\n\nCodeggplot2::ggplot(\n  data = bootstrappedFantasyPoints %&gt;%\n    filter(pos == \"QB\" & (name %in% c(\"Dak Prescott\", \"Drake Maye\"))),\n  mapping = aes(\n    x = projectedPoints,\n    group = name,\n    #color = name,\n    fill = name)\n) +\n  geom_histogram(\n    aes(y = after_stat(density))\n  ) +\n  geom_density(\n    alpha = 0.6 # add transparency\n  ) +\n  coord_cartesian(\n    xlim = c(0,NA),\n    expand = FALSE) +\n  #geom_rug() +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    fill = \"\",\n    color = \"\",\n    title = \"Distribution of Projected Fantasy Points\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 24.2: Distribution of Projected Fantasy Points for Two Players from Bootstrapping. There is relatively narrow uncertainty around projected fantasy points for Dak Prescott, whereas there is relatively wide uncertainty around the projected fantasy points for Drake Maye.\n\n\n\n\n\n24.3.2 Monte Carlo Simulation\n\n24.3.2.1 SimDesign Package\nYou can generate a template for Monte Carlo simulations in the SimDesign (Chalmers, 2025; Chalmers & Adkins, 2020) package using the following code:\n\nCodeSimDesign::SimFunctions()\n\n#-------------------------------------------------------------------\n\nlibrary(SimDesign)\n\nDesign &lt;- createDesign(factor1 = NA,\n                       factor2 = NA)\n\n#-------------------------------------------------------------------\n\nGenerate &lt;- function(condition, fixed_objects) {\n    dat &lt;- data.frame()\n    dat\n}\n\nAnalyse &lt;- function(condition, dat, fixed_objects) {\n    ret &lt;- nc(stat1 = NaN, stat2 = NaN)\n    ret\n}\n\nSummarise &lt;- function(condition, results, fixed_objects) {\n    ret &lt;- c(bias = NaN, RMSE = NaN)\n    ret\n}\n\n#-------------------------------------------------------------------\n\nres &lt;- runSimulation(design=Design, replications=2, generate=Generate, \n                     analyse=Analyse, summarise=Summarise)\nres\n\n\n\n24.3.2.2 Prepare Data\n\nCodeall_proj &lt;- all_proj %&gt;% \n  rename(projectedPoints = raw_points)\n\n\n\n24.3.2.3 Optimal Distribution for Each Player\nFor each player, we identify the optimal distribution as either a normal distribution , or as a skew-normal distribution. The normal distribution was fit using the fitdistrplus::fitdist() function of the fitdistrplus package (Delignette-Muller & Dutang, 2015; Delignette-Muller et al., 2025). The skew-normal distribution was fit using the sn::selm() function of the sn package (A. Azzalini, 2023; A. A. Azzalini, 2023).\n\nCode# Function to identify the \"best\" distribution (Normal vs Skew‑Normal) for every player (tries both families and picks by AIC; uses empirical distribution if fewer than 2 unique scores)\nfit_best &lt;- function(x) {\n  # Basic checks\n  if (length(unique(x)) &lt; 2 || all(is.na(x))) { # Use empirical distribution if there are fewer than 2 unique scores\n    return(list(type = \"empirical\", empirical = x))\n  }\n\n  # Try Normal Distribution\n  fit_norm &lt;- tryCatch(\n    fitdistrplus::fitdist(x, distr = \"norm\"),\n    error = function(e) NULL\n  )\n\n  # Try Skew-Normal Distribution\n  fit_skew &lt;- tryCatch(\n    sn::selm(x ~ 1),\n    error = function(e) NULL\n  )\n\n  # Handle bad fits: sd = NA, etc.\n  if (!is.null(fit_norm) && any(is.na(fit_norm$estimate))) {\n    fit_norm &lt;- NULL\n  }\n\n  if (!is.null(fit_skew)) {\n    pars &lt;- tryCatch(sn::coef(fit_skew, param.type = \"dp\"), error = function(e) NULL)\n    if (is.null(pars) || any(is.na(pars))) {\n      fit_skew &lt;- NULL\n    }\n  }\n\n  # Choose best available\n  if (!is.null(fit_norm) && !is.null(fit_skew)) {\n    aic_norm &lt;- AIC(fit_norm)\n    aic_skew &lt;- AIC(fit_skew)\n    if (aic_skew + 2 &lt; aic_norm) { # skew-normal is more complex (has more parameters) than normal distribution, so only select a skew-normal distribution if it fits substantially better than a normal distribution\n      pars &lt;- sn::coef(fit_skew, param.type = \"dp\")\n      return(list(\n        type  = \"skewnorm\",\n        xi    = pars[\"dp.location\"],\n        omega = pars[\"dp.scale\"],\n        alpha = pars[\"dp.shape\"]))\n    } else {\n      return(list(\n        type = \"norm\",\n        mean = fit_norm$estimate[\"mean\"],\n        sd   = fit_norm$estimate[\"sd\"]))\n    }\n  } else if (!is.null(fit_norm)) {\n    return(list(\n      type = \"norm\",\n      mean = fit_norm$estimate[\"mean\"],\n      sd   = fit_norm$estimate[\"sd\"]))\n  } else {\n    return(list(type = \"empirical\", empirical = x))\n  }\n}\n\n\n\nCodeproj_dists_tbl &lt;- all_proj %&gt;%\n  filter(!is.na(id) & id != \"\") %&gt;%\n  group_by(id) %&gt;%\n  summarise(\n    dist_info = list(fit_best(projectedPoints)),\n    n_proj = n(), # record of how many sources they have\n    .groups = \"drop\"\n  )\n\nproj_dists &lt;- proj_dists_tbl %&gt;%\n  filter(!is.na(id) & id != \"\") %&gt;%\n  distinct(id, .keep_all = TRUE) %&gt;%\n  (\\(x) setNames(x$dist_info, x$id))()\n\n\n\nCodeproj_dists_tbl %&gt;%\n  dplyr::mutate(\n    dist_type = purrr::map_chr(dist_info, ~ .x$type)\n  ) %&gt;%\n  dplyr::count(dist_type)\n\n\n  \n\n\n\n\n24.3.2.4 SimDesign Step 1: Design Grid\nNow we build the SimDesign design grid based on the number of projections that each player had.\n\nCodeDesign &lt;- proj_dists_tbl %&gt;%\n  dplyr::mutate(\n    id,\n    n_sources = n_proj,\n    .keep = \"none\"\n  )\n\n\n\nCodemissing_ids &lt;- setdiff(Design$id, names(proj_dists))\nlength(missing_ids) # should be 0\n\nany(is.na(proj_dists_tbl$id)) # should be FALSE\nany(is.na(Design$id))         # should be FALSE\nany(is.na(names(proj_dists))) # should be FALSE\n\n\n\n24.3.2.5 SimDesign Step 2: Generate\n\nCodeGenerate &lt;- function(condition, fixed_objects = NULL) {\n\n  dist_info &lt;- fixed_objects$proj_dists[[as.character(condition$id)]]\n  n_sources &lt;- condition$n_sources\n\n  sim_points &lt;- switch(\n    dist_info$type,\n    empirical = sample(\n      dist_info$empirical,\n      n_sources,\n      replace = TRUE),\n    \n    norm = rnorm(\n      n_sources,\n      mean = dist_info$mean,\n      sd = dist_info$sd),\n    \n    skewnorm = sn::rsn(\n      n_sources,\n      xi = dist_info$xi,\n      omega = dist_info$omega,\n      alpha = dist_info$alpha),\n    \n    stop(\"Unknown distribution type: \", dist_info$type)\n  )\n\n  data.frame(\n    id = condition$id,\n    sim_points = sim_points)\n}\n\n\n\n24.3.2.6 SimDesign Step 3: Analyze\n\nCodeAnalyse &lt;- function(condition, dat, fixed_objects = NULL) {\n  tibble::tibble(\n    id        = condition$id,\n    mean_pts  = mean(dat$sim_points, na.rm = TRUE),\n    sd_pts    = sd(dat$sim_points, na.rm = TRUE),\n    q10       = quantile(dat$sim_points, 0.10, na.rm = TRUE),\n    q90       = quantile(dat$sim_points, 0.90, na.rm = TRUE),\n    p100      = mean(dat$sim_points &gt;= 100, na.rm = TRUE),\n    p150      = mean(dat$sim_points &gt;= 150, na.rm = TRUE),\n    p200      = mean(dat$sim_points &gt;= 200, na.rm = TRUE),\n    p250      = mean(dat$sim_points &gt;= 250, na.rm = TRUE),\n    p300      = mean(dat$sim_points &gt;= 300, na.rm = TRUE),\n    p350      = mean(dat$sim_points &gt;= 350, na.rm = TRUE)\n  )\n}\n\n\n\n24.3.2.7 SimDesign Step 4: Summarize\n\nCodeSummarise &lt;- function(condition, results, fixed_objects = NULL) {\n  dplyr::summarise(\n    results,\n    across(\n      where(is.numeric),\n      list(\n        mean = ~mean(.x, na.rm = TRUE),\n        sd   = ~sd(.x,  na.rm = TRUE)),\n      .names = \"{.col}_{.fn}\"\n    ),\n    .groups = \"drop\"\n  )\n}\n\n\n\n24.3.2.8 SimDesign Step 5: Run the Simulation\nNow, we can run the model using the SimDesign::runSimulation() function.\n\n\n\n\n\n\nNote 24.1: Monte Carlo Simulation\n\n\n\nNote: the following code that runs the simulation takes a while. If you just want to save time and load the results object instead of running the simulation, you can load the results object of the simulation (which has already been run) using this code:\n\nCodeload(url(\"https://osf.io/download/ues7n/\"))\n\n\n\n\n\nCodemonteCarloSim_results &lt;- SimDesign::runSimulation(\n  design = Design,\n  replications = 1000,\n  generate = Generate,\n  analyse = Analyse,\n  summarise = Summarise,\n  fixed_objects = list(proj_dists = proj_dists),\n  seed = genSeeds(Design, iseed = 52242), # for reproducibility\n  parallel = TRUE # for faster (parallel) processing\n)\n\n\n\n24.3.2.9 Simulation Results\n\nCodemonteCarloSim_results &lt;- monteCarloSim_results %&gt;% \n  left_join(\n    nfl_playerIDs[,c(\"mfl_id\",\"name\",\"merge_name\",\"position\",\"team\")],\n    by = c(\"id\" = \"mfl_id\")\n  ) %&gt;% \n  select(name, team, position, everything()) %&gt;% \n  arrange(-mean_pts_mean)\n\n\nThe pX variable represent the probability that a player scoring more than X number of points. For example, the p300 variable represents the probability that each player scores more than 300 points. However, it is important to note that this is based on the distribution of projected points.\n\nCodemonteCarloSim_results\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position == \"QB\")\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position == \"RB\")\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position == \"WR\")\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position == \"TE\")\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position %in% c(\"K\",\"PK\"))\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position %in% c(\"DL\",\"DT\",\"DE\"))\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position %in% c(\"LB\",\"MLB\",\"OLB\"))\n\n\n  \n\n\n\n\nCodemonteCarloSim_results %&gt;% \n  filter(position %in% c(\"DB\",\"S\",\"CB\"))",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Simulation: Bootstrapping and the Monte Carlo Method</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-simulationConclusion",
    "href": "simulation.html#sec-simulationConclusion",
    "title": "24  Simulation: Bootstrapping and the Monte Carlo Method",
    "section": "\n24.4 Conclusion",
    "text": "24.4 Conclusion\nA simulation is an “imitative representation” of a phenomenon that could exist the real world. In statistics, simulations are computer-driven investigations to better understand a phenomenon by studying its behavior under different conditions. Statistical simulations can be conducted in various ways. Two common types of simulations are bootstrapping and Monte Carlo simulation. Bootstrapping involves repeated resampling (with replacement) from observed data. Monte Carlo simulation involves repeated random sampling from a known distribution. We demonstrated bootstrapping and Monte Carlo approaches to simulating the most likely range of outcomes for a player in terms of fantasy points.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Simulation: Bootstrapping and the Monte Carlo Method</span>"
    ]
  },
  {
    "objectID": "simulation.html#sec-simulationSessionInfo",
    "href": "simulation.html#sec-simulationSessionInfo",
    "title": "24  Simulation: Bootstrapping and the Monte Carlo Method",
    "section": "\n24.5 Session Info",
    "text": "24.5 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] lubridate_1.9.4         forcats_1.0.0           stringr_1.5.1          \n [4] dplyr_1.1.4             purrr_1.1.0             readr_2.1.5            \n [7] tidyr_1.3.1             tibble_3.3.0            ggplot2_3.5.2          \n[10] tidyverse_2.0.0         sn_2.1.1                fitdistrplus_1.2-4     \n[13] survival_3.8-3          MASS_7.3-65             SimDesign_2.20.0       \n[16] progressr_0.15.1        future.apply_1.20.0     future_1.67.0          \n[19] data.table_1.17.8       ffanalytics_3.1.10.0000\n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3  rstudioapi_0.17.1   audio_0.1-11       \n  [4] jsonlite_2.0.0      magrittr_2.0.3      farver_2.1.2       \n  [7] nloptr_2.2.1        rmarkdown_2.29      fs_1.6.6           \n [10] vctrs_0.6.5         minqa_1.2.8         base64enc_0.1-3    \n [13] htmltools_0.5.8.1   haven_2.5.5         cellranger_1.1.0   \n [16] Formula_1.2-5       parallelly_1.45.1   htmlwidgets_1.6.4  \n [19] plyr_1.8.9          testthat_3.2.3      httr2_1.2.1        \n [22] rootSolve_1.8.2.4   lifecycle_1.0.4     pkgconfig_2.0.3    \n [25] Matrix_1.7-3        R6_2.6.1            fastmap_1.2.0      \n [28] rbibutils_2.3       digest_0.6.37       Exact_3.3          \n [31] numDeriv_2016.8-1.1 colorspace_2.1-1    ps_1.9.1           \n [34] Hmisc_5.2-3         labeling_0.4.3      timechange_0.3.0   \n [37] httr_1.4.7          compiler_4.5.1      proxy_0.4-27       \n [40] withr_3.0.2         htmlTable_2.4.3     backports_1.5.0    \n [43] DBI_1.2.3           psych_2.5.6         R.utils_2.13.0     \n [46] rappdirs_0.3.3      sessioninfo_1.2.3   petersenlab_1.2.0  \n [49] gld_2.6.7           tools_4.5.1         chromote_0.5.1     \n [52] pbivnorm_0.6.0      foreign_0.8-90      nnet_7.3-20        \n [55] R.oo_1.27.1         glue_1.8.0          quadprog_1.5-8     \n [58] nlme_3.1-168        promises_1.3.3      grid_4.5.1         \n [61] checkmate_2.3.3     cluster_2.1.8.1     reshape2_1.4.4     \n [64] generics_0.1.4      gtable_0.3.6        tzdb_0.5.0         \n [67] R.methodsS3_1.8.2   class_7.3-23        websocket_1.4.4    \n [70] lmom_3.2            hms_1.1.3           xml2_1.4.0         \n [73] pillar_1.11.0       later_1.4.3         mitools_2.4        \n [76] splines_4.5.1       lattice_0.22-7      tidyselect_1.2.1   \n [79] pbapply_1.7-4       mix_1.0-13          knitr_1.50         \n [82] reformulas_0.4.1    gridExtra_2.3       xfun_0.53          \n [85] expm_1.0-0          brio_1.1.5          stringi_1.8.7      \n [88] yaml_2.3.10         boot_1.3-31         evaluate_1.0.4     \n [91] codetools_0.2-20    beepr_2.0           cli_3.6.5          \n [94] rpart_4.1.24        xtable_1.8-4        DescTools_0.99.60  \n [97] Rdpack_2.6.4        processx_3.8.6      lavaan_0.6-19      \n[100] Rcpp_1.1.0          readxl_1.4.5        globals_0.18.0     \n[103] parallel_4.5.1      lme4_1.1-37         listenv_0.9.1      \n[106] viridisLite_0.4.2   mvtnorm_1.3-3       scales_1.4.0       \n[109] e1071_1.7-16        rrapply_1.2.7       rlang_1.1.6        \n[112] rvest_1.0.4         mnormt_2.1.1       \n\n\n\n\n\n\nAzzalini, A. (2023). sn: The skew-normal and related distributions such as the Skew-t and the SUN. https://doi.org/10.32614/CRAN.package.sn\n\n\nAzzalini, A. A. (2023). The R package sn: The skew-normal and related distributions such as the skew-$t$ and the SUN (version 2.1.1). https://cran.r-project.org/package=sn\n\n\nBarrett, T., Dowle, M., Srinivasan, A., Gorecki, J., Chirico, M., Hocking, T., Schwendinger, B., & Krylov, I. (2025). data.table: Extension of ‘data.frame‘. https://doi.org/10.32614/CRAN.package.data.table\n\n\nBengtsson, H. (2024). progressr: An inclusive, unifying API for progress updates. https://doi.org/10.32614/CRAN.package.progressr\n\n\nBengtsson, H. (2025a). ‘future‘: Unified parallel and distributed processing in ‘R‘ for everyone. https://doi.org/10.32614/CRAN.package.future\n\n\nBengtsson, H. (2025b). ‘future.apply‘: Apply function to elements in parallel using futures. https://doi.org/10.32614/CRAN.package.future.apply\n\n\nChalmers, R. P. (2025). SimDesign: Structure for organizing Monte Carlo simulation designs. https://doi.org/10.32614/CRAN.package.SimDesign\n\n\nChalmers, R. P., & Adkins, M. C. (2020). Writing effective and reliable Monte Carlo simulations with the SimDesign package. The Quantitative Methods for Psychology, 16(4), 248–280. https://doi.org/10.20982/tqmp.16.4.p248\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nDelignette-Muller, M. L., & Dutang, C. (2015). fitdistrplus: An R package for fitting distributions. Journal of Statistical Software, 64(4), 1–34. https://doi.org/10.18637/jss.v064.i04\n\n\nDelignette-Muller, M.-L., Dutang, C., & Siberchicot, A. (2025). fitdistrplus: Help to fit of a parametric distribution to non-censored or censored data. https://doi.org/10.32614/CRAN.package.fitdistrplus\n\n\nSigal, M. J., & Chalmers, R. P. (2016). Play it again: Teaching statistics with Monte Carlo simulation. Journal of Statistics Education, 24(3), 136–156. https://doi.org/10.1080/10691898.2016.1246953",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Simulation: Bootstrapping and the Monte Carlo Method</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html",
    "href": "time-series-analysis.html",
    "title": "25  Time Series Analysis",
    "section": "",
    "text": "25.1 Getting Started\nThis chapter provides an overview of time series analysis.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesGettingStarted",
    "href": "time-series-analysis.html#sec-timeSeriesGettingStarted",
    "title": "25  Time Series Analysis",
    "section": "",
    "text": "25.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"xts\")\nlibrary(\"zoo\")\nlibrary(\"forecast\")\nlibrary(\"brms\")\nlibrary(\"rstan\")\nlibrary(\"plotly\")\nlibrary(\"tidyverse\")\n\n\n\n25.1.2 Load Data\n\nCodeload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nThe following code loads the Bayesian model object that was fit in Section 12.4.5.\n\nCodepetersenlab::robust_load(\"https://osf.io/download/q6rjf/\") #load(url(\"https://osf.io/download/q6rjf/\")) # Bayesian model object",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesOverview",
    "href": "time-series-analysis.html#sec-timeSeriesOverview",
    "title": "25  Time Series Analysis",
    "section": "\n25.2 Overview of Time Series Analysis",
    "text": "25.2 Overview of Time Series Analysis\nTime series analysis is useful when trying to generate forecasts from longitudinal data. That is, time series analysis seeks to evaluate change over time to predict future values.\nThere many different types of time series analyses. For simplicity, in this chapter, we use autoregressive integrated moving average (ARIMA) models to demonstrate one approach to time series analysis. We also leverage Bayesian mixed models to generate forecasts of future performance and plots of individuals model-implied performance by age and position.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesARIMA",
    "href": "time-series-analysis.html#sec-timeSeriesARIMA",
    "title": "25  Time Series Analysis",
    "section": "\n25.3 Autoregressive Integrated Moving Average (ARIMA) Models",
    "text": "25.3 Autoregressive Integrated Moving Average (ARIMA) Models\nHyndman & Athanasopoulos (2021) provide a nice overview of ARIMA models. As noted by Hyndman & Athanasopoulos (2021), ARIMA models aim to describe how a variable is correlated with itself over time (autocorrelation)—i.e., how earlier levels of a variable are correlated with later levels of the same variable. ARIMA models perform best when there is a clear pattern where later values are influenced by earlier values. ARIMA models incorporate autoregression effects, moving average effects, and differencing.\nARIMA models can have various numbers of terms and model complexity. They are specified in the following form: \\(\\text{ARIMA}(p,d,q)\\), where:\n\n\n\\(p =\\) the number of autoregressive terms\n\n\\(d =\\) the number of differences between consecutive scores (to make the time series stationary by reducing trends and seasonality)\n\n\\(q =\\) the number of moving average terms\n\nARIMA models assume that the data are stationary (i.e., there are no long-term trends), are non-seasonal (i.e., there is no consistency of the timing of the peaks or troughs in the line), and that earlier values influence later values. This may not strongly be the case in fantasy football, so ARIMA models may not be particularly useful in forecasting fantasy football performance. Other approaches, such as exponential smoothing, may be useful for data that show longer-term trends and seasonality (Hyndman & Athanasopoulos, 2021). Nevertheless, ARIMA models are widely used in forecasting financial markets and economic indicators. Thus, it is a useful technique to learn.\nAdapted from: https://rc2e.com/timeseriesanalysis [Long & Teetor (2019); archived at https://perma.cc/U5P6-2VWC].\n\n25.3.1 Create the Time Series Objects\n\nCodeweeklyFantasyPoints_tomBrady &lt;- player_stats_weekly %&gt;% \n  filter(\n    player_id == \"00-0019596\" | player_display_name == \"Tom Brady\")\n\nweeklyFantasyPoints_peytonManning &lt;- player_stats_weekly %&gt;% \n  filter(\n    player_id == \"00-0010346\" | player_display_name == \"Peyton Manning\")\n\nts_tomBrady &lt;- xts::xts(\n  x = weeklyFantasyPoints_tomBrady[\"fantasyPoints\"],\n  order.by = weeklyFantasyPoints_tomBrady$gameday)\n\nts_peytonManning &lt;- xts::xts(\n  x = weeklyFantasyPoints_peytonManning[\"fantasyPoints\"],\n  order.by = weeklyFantasyPoints_peytonManning$gameday)\n\nts_tomBrady\n\n           fantasyPoints\n2000-11-23          0.24\n2001-09-23          2.74\n2001-09-30          6.92\n2001-10-07          4.34\n2001-10-14         22.56\n2001-10-21         19.88\n2001-10-28         10.02\n2001-11-04         22.00\n2001-11-11          8.18\n2001-11-18          9.00\n       ...              \n2022-10-27         17.10\n2022-11-06         15.20\n2022-11-13         17.02\n2022-11-27         18.04\n2022-12-05         17.14\n2022-12-11         10.12\n2022-12-18         20.58\n2022-12-25         11.34\n2023-01-01         37.68\n2023-01-08          7.36\n\nCodets_peytonManning\n\n           fantasyPoints\n1999-09-12         15.06\n1999-09-19         17.22\n1999-09-26         29.56\n1999-10-10         20.66\n1999-10-17         10.10\n1999-10-24         17.86\n1999-10-31         18.52\n1999-11-07         20.60\n1999-11-14         15.18\n1999-11-21         22.80\n       ...              \n2015-09-13          4.90\n2015-09-17         20.24\n2015-09-27         18.86\n2015-10-04          8.32\n2015-10-11          6.64\n2015-10-18          9.60\n2015-11-01         11.60\n2015-11-08         15.24\n2015-11-15         -6.60\n2016-01-03          2.56\n\nCodets_combined &lt;- merge(\n  ts_tomBrady,\n  ts_peytonManning\n)\n\nnames(ts_combined) &lt;- c(\"Tom Brady\",\"Peyton Manning\")\n\n\n\n25.3.2 Plot the Time Series\n\nCodeplot(\n  ts_tomBrady,\n  main = \"Tom Brady's Fantasy Points by Game\")\n\n\n\n\n\n\nFigure 25.1: Tom Brady’s Historical Fantasy Points by Game.\n\n\n\n\n\nCodeplot(\n  ts_combined,\n  legend,\n  legend.loc = \"topright\",\n  main = \"Fantasy Points by Game\")\n\n\n\n\n\n\nFigure 25.2: Historical Fantasy Points by Game for Tom Brady and Peyton Manning.\n\n\n\n\n\n25.3.3 Rolling Mean/Median\n\nCodezoo::rollmean(\n  x = ts_tomBrady,\n  k = 5)\n\n           fantasyPoints\n2001-09-30         7.360\n2001-10-07        11.288\n2001-10-14        12.744\n2001-10-21        15.760\n2001-10-28        16.528\n2001-11-04        13.816\n2001-11-11        15.384\n2001-11-18        15.084\n2001-11-25        11.568\n2001-12-02        11.888\n       ...              \n2022-10-16        18.332\n2022-10-23        15.892\n2022-10-27        15.348\n2022-11-06        16.212\n2022-11-13        16.900\n2022-11-27        15.504\n2022-12-05        16.580\n2022-12-11        15.444\n2022-12-18        19.372\n2022-12-25        17.416\n\nCodezoo::rollmedian(\n  x = ts_tomBrady,\n  k = 5)\n\n           fantasyPoints\n2001-09-30          4.34\n2001-10-07          6.92\n2001-10-14         10.02\n2001-10-21         19.88\n2001-10-28         19.88\n2001-11-04         10.02\n2001-11-11         10.02\n2001-11-18          9.00\n2001-11-25          8.52\n2001-12-02          9.00\n       ...              \n2022-10-16         17.10\n2022-10-23         15.20\n2022-10-27         15.20\n2022-11-06         17.02\n2022-11-13         17.10\n2022-11-27         17.02\n2022-12-05         17.14\n2022-12-11         17.14\n2022-12-18         17.14\n2022-12-25         11.34\n\n\n\n25.3.4 Autocorrelation\nThe autocorrelation function (ACF) plot depicts the autocorrelation of scores as a function of the length of the lag. We can generate an ACF plot using the stats::acf() function. Significant autocorrelation is detected when the autocorrelation exceeds the dashed blue lines, as is depicted in Figure 25.3.\n\nCodeacf(ts_tomBrady)\n\n\n\n\n\n\nFigure 25.3: Autocorrelation Function (ACF) Plot of Tom Brady’s Historical Fantasy Points by Game.\n\n\n\n\n\nCodeBox.test(ts_tomBrady)\n\n\n    Box-Pierce test\n\ndata:  ts_tomBrady\nX-squared = 4.6744, df = 1, p-value = 0.03062\n\n\n\n25.3.5 Fit an Autoregressive Integrated Moving Average Model\nWe can fit an ARIMA model using the forecast::auto.arima() function of the forecast package (Hyndman et al., 2024; Hyndman & Khandakar, 2008):\n\nCodeforecast::auto.arima(ts_tomBrady)\n\nSeries: ts_tomBrady \nARIMA(2,1,2) \n\nCoefficients:\n         ar1     ar2      ma1     ma2\n      0.5827  0.0703  -1.5041  0.5161\ns.e.  0.2615  0.0658   0.2577  0.2496\n\nsigma^2 = 62.76:  log likelihood = -1164.4\nAIC=2338.8   AICc=2338.99   BIC=2357.86\n\nCodeforecast::auto.arima(ts_peytonManning)\n\nSeries: ts_peytonManning \nARIMA(1,0,1) with non-zero mean \n\nCoefficients:\n         ar1      ma1     mean\n      0.8659  -0.7311  18.1529\ns.e.  0.0973   0.1287   0.9575\n\nsigma^2 = 57.94:  log likelihood = -860.72\nAIC=1729.43   AICc=1729.59   BIC=1743.52\n\n\nWe can generate a plot of an ARIMA model using the stats::arima() function.\n\nCodearima_tomBrady &lt;- arima(\n  ts_tomBrady,\n  order = c(5, 1, 4))\n\nsummary(arima_tomBrady)\n\n\nCall:\narima(x = ts_tomBrady, order = c(5, 1, 4))\n\nCoefficients:\n\n\n         ar1      ar2     ar3      ar4     ar5      ma1     ma2     ma3\n      0.3416  -0.0655  -0.205  -0.0959  0.1961  -1.2442  0.3689  0.2403\ns.e.     NaN   0.2256     NaN   0.0475  0.0571      NaN     NaN     NaN\n          ma4\n      -0.3361\ns.e.      NaN\n\nsigma^2 estimated as 59.78:  log likelihood = -1158.36,  aic = 2336.72\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE     MASE\nTraining set 0.6879023 7.720194 6.223572 -21.21763 50.46479 0.732154\n                      ACF1\nTraining set -0.0007899074\n\nCodeconfint(arima_tomBrady)\n\n          2.5 %       97.5 %\nar1         NaN          NaN\nar2 -0.50779428  0.376718457\nar3         NaN          NaN\nar4 -0.18900813 -0.002859993\nar5  0.08427103  0.307929276\nma1         NaN          NaN\nma2         NaN          NaN\nma3         NaN          NaN\nma4         NaN          NaN\n\nCodeforecast::checkresiduals(arima_tomBrady)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(5,1,4)\nQ* = 4.3247, df = 3, p-value = 0.2285\n\nModel df: 9.   Total lags used: 12\n\n\n\n\n\n\n\nFigure 25.4: Model Summary of Autoregressive Integrated Moving Average Model fit to Tom Brady’s Historical Performance by Game.\n\n\n\n\n\nCodearima_tomBrady_removeNonSigTerms &lt;- arima(\n  ts_tomBrady,\n  order = c(5, 1, 4),\n  fixed = c(NA, NA, 0, NA, NA, NA, NA, NA, NA))\n\nsummary(arima_tomBrady_removeNonSigTerms)\n\n\nCall:\narima(x = ts_tomBrady, order = c(5, 1, 4), fixed = c(NA, NA, 0, NA, NA, NA, \n    NA, NA, NA))\n\nCoefficients:\n          ar1      ar2  ar3     ar4     ar5     ma1      ma2      ma3      ma4\n      -0.9051  -0.7933    0  0.0987  0.1551  0.0137  -0.0185  -0.6539  -0.2529\ns.e.   0.3144   0.1769    0  0.1013  0.0929  0.3217   0.2181   0.1528   0.1079\n\nsigma^2 estimated as 59.38:  log likelihood = -1157.29,  aic = 2332.58\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.6609654 7.694434 6.168512 -20.93448 50.25172 0.7256766\n                     ACF1\nTraining set -0.006363576\n\nCodeconfint(arima_tomBrady_removeNonSigTerms)\n\n          2.5 %      97.5 %\nar1 -1.52137219 -0.28875955\nar2 -1.13999746 -0.44652063\nar3          NA          NA\nar4 -0.09983263  0.29728144\nar5 -0.02703935  0.33727487\nma1 -0.61686843  0.64426701\nma2 -0.44596328  0.40905685\nma3 -0.95344054 -0.35438261\nma4 -0.46435785 -0.04145554\n\nCodeforecast::checkresiduals(arima_tomBrady_removeNonSigTerms)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(5,1,4)\nQ* = 3.3999, df = 3, p-value = 0.334\n\nModel df: 9.   Total lags used: 12\n\n\n\n\n\n\n\nFigure 25.5: Model Summary of modified Autoregressive Integrated Moving Average Model fit to Tom Brady’s Historical Performance by Game.\n\n\n\n\n\nCodearima_peytonManning &lt;- arima(\n  ts_peytonManning,\n  order = c(1, 0, 1))\n\nsummary(arima_peytonManning)\n\n\nCall:\narima(x = ts_peytonManning, order = c(1, 0, 1))\n\nCoefficients:\n         ar1      ma1  intercept\n      0.8659  -0.7311    18.1529\ns.e.  0.0973   0.1287     0.9575\n\nsigma^2 estimated as 57.24:  log likelihood = -860.72,  aic = 1729.43\n\nTraining set error measures:\n                      ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.002008698 7.565751 5.932631 -101.5589 126.2153 0.7578856\n                   ACF1\nTraining set 0.02467125\n\nCodeconfint(arima_peytonManning)\n\n               2.5 %     97.5 %\nar1        0.6752778  1.0565343\nma1       -0.9833206 -0.4789367\nintercept 16.2763161 20.0295532\n\nCodeforecast::checkresiduals(arima_peytonManning)\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(1,0,1) with non-zero mean\nQ* = 9.7829, df = 8, p-value = 0.2806\n\nModel df: 2.   Total lags used: 10\n\n\n\n\n\n\n\nFigure 25.6: Model Summary of Autoregressive Integrated Moving Average Model fit to Peyton Manning’s Historical Performance by Game.\n\n\n\n\n\n25.3.6 Generate the Model Forecasts\nWe can generate model forecasts from the ARIMA model using the forecast::forecast() function.\n\nCodeforecast_tomBrady &lt;- forecast::forecast(\n  arima_tomBrady,\n  level = c(80, 95)) # 80% and 95% confidence intervals\n\nforecast_peytonManning &lt;- forecast::forecast(\n  arima_peytonManning,\n  level = c(80, 95)) # 80% and 95% confidence intervals\n\nforecast_tomBrady\n\n    Point Forecast     Lo 80    Hi 80      Lo 95    Hi 95\n336       16.89941  6.990778 26.80803  1.7454680 32.05334\n337       21.84148 11.885926 31.79704  6.6157730 37.06719\n338       14.62678  4.629175 24.62438 -0.6632356 29.91679\n339       22.63793 12.473318 32.80254  7.0924968 38.18336\n340       17.97384  7.804932 28.14275  2.4218376 33.52584\n341       18.73072  8.416107 29.04533  2.9558825 34.50555\n342       19.31420  8.980909 29.64748  3.5107975 35.11759\n343       18.23659  7.893548 28.57964  2.4182709 34.05491\n344       19.69349  9.341389 30.04559  3.8613185 35.52566\n345       19.15502  8.802780 29.50726  3.3226357 34.98740\n\nCodeforecast_peytonManning\n\n    Point Forecast    Lo 80    Hi 80       Lo 95    Hi 95\n251       12.03144 2.335537 21.72734 -2.79716238 26.86004\n252       12.85229 3.068726 22.63586 -2.11038095 27.81497\n253       13.56308 3.714290 23.41186 -1.49934216 28.62550\n254       14.17855 4.281143 24.07595 -0.95822712 29.31533\n255       14.71149 4.777786 24.64519 -0.48079979 29.90378\n256       15.17297 5.212133 25.13380 -0.06081402 30.40675\n257       15.57256 5.591435 25.55369  0.30774583 30.83738\n258       15.91857 5.922259 25.91489  0.63052902 31.20662\n259       16.21819 6.210500 26.22588  0.91274926 31.52363\n260       16.47763 6.461418 26.49383  1.15915808 31.79610\n\n\n\n25.3.7 Plot the Model Forecasts\nWe can plot the model forecasts using the forecast::autoplot() function.\n\nCodeforecast::autoplot(forecast_tomBrady) + \n  labs(\n    x = \"Game Number\",\n    y = \"Fantasy Points\",\n    title = \"Tom Brady's Historical and Projected Fantasy Points by Game\",\n    subtitle = \"(if he were to have continued playing additional seasons)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 25.7: Tom Brady’s Historical and Projected Fantasy Points by Game.\n\n\n\n\n\nCodeforecast::autoplot(forecast_peytonManning) + \n  labs(\n    x = \"Game Number\",\n    y = \"Fantasy Points\",\n    title = \"Peyton Manning's Historical and Projected Fantasy Points by Game\",\n    subtitle = \"(if he were to have continued playing additional seasons)\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure 25.8: Peyton Manning’s Historical and Projected Fantasy Points by Game.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesBayesian",
    "href": "time-series-analysis.html#sec-timeSeriesBayesian",
    "title": "25  Time Series Analysis",
    "section": "\n25.4 Bayesian Mixed Models",
    "text": "25.4 Bayesian Mixed Models\nThe Bayesian longitudinal mixed models were estimated in Section 12.4.5.\n\n25.4.1 Prepare New Data Object\n\nCodeplayer_stats_seasonal_offense_subset &lt;- player_stats_seasonal %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\") | position %in% c(\"K\"))\n\nplayer_stats_seasonal_offense_subset$position[which(player_stats_seasonal_offense_subset$position == \"HB\")] &lt;- \"RB\"\n\nplayer_stats_seasonal_offense_subset$player_idFactor &lt;- factor(player_stats_seasonal_offense_subset$player_id)\nplayer_stats_seasonal_offense_subset$positionFactor &lt;- factor(player_stats_seasonal_offense_subset$position)\n\n\n\nCodeplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subset %&gt;%\n  filter(\n    !is.na(player_idFactor),\n    !is.na(fantasyPoints),\n    !is.na(positionFactor),\n    !is.na(ageCentered20),\n    !is.na(ageCentered20Quadratic),\n    !is.na(years_of_experience))\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;% \n  filter(player_id %in% bayesianMixedModelFit$data$player_idFactor) %&gt;% \n  mutate(positionFactor = droplevels(positionFactor))\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;%\n  group_by(player_id) %&gt;% \n  group_modify(~ add_row(.x, season = max(player_stats_seasonal_offense_subsetCC$season) + 1)) %&gt;% \n  fill(player_display_name, player_idFactor, position, position_group, positionFactor, team, .direction = \"downup\") %&gt;% \n  ungroup\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;% \n  left_join(\n    player_stats_seasonal_offense_subsetCC %&gt;% \n      filter(season == max(player_stats_seasonal_offense_subsetCC$season) - 1) %&gt;% \n      select(player_id, age_lastYear = age, years_of_experience_lastYear = years_of_experience),\n    by = \"player_id\") %&gt;%\n  mutate(\n    age = if_else(season == max(player_stats_seasonal_offense_subsetCC$season), age_lastYear + 1, age), # increment age by 1\n    ageCentered20 = age - 20,\n    years_of_experience = if_else(season == max(player_stats_seasonal_offense_subsetCC$season), years_of_experience_lastYear + 1, years_of_experience)) # increment experience by 1\n\nactivePlayers &lt;- unique(player_stats_seasonal_offense_subsetCC[c(\"player_id\",\"season\")]) %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season) - 1) %&gt;% \n  select(player_id) %&gt;% \n  pull()\n\ninactivePlayers &lt;- player_stats_seasonal_offense_subsetCC$player_id[which(player_stats_seasonal_offense_subsetCC$player_id %ni% activePlayers)]\n\nplayer_stats_seasonal_offense_subsetCC &lt;- player_stats_seasonal_offense_subsetCC %&gt;% \n  filter(player_id %in% activePlayers | (player_id %in% inactivePlayers & season &lt; max(player_stats_seasonal_offense_subsetCC$season) - 1)) %&gt;% \n  mutate(\n    player_idFactor = droplevels(player_idFactor) \n  )\n\n\n\n25.4.2 Generate Predictions\n\nCodeplayer_stats_seasonal_offense_subsetCC$fantasyPoints_bayesian &lt;- predict(\n  bayesianMixedModelFit,\n  newdata = player_stats_seasonal_offense_subsetCC\n)[,\"Estimate\"]\n\n\n\n25.4.3 Table of Next Season Predictions\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"QB\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"RB\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"WR\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\nCodeplayer_stats_seasonal_offense_subsetCC %&gt;% \n  filter(season == max(player_stats_seasonal_offense_subsetCC$season), position == \"TE\") %&gt;%\n  arrange(-fantasyPoints_bayesian) %&gt;% \n  select(player_display_name, fantasyPoints_bayesian)\n\n\n  \n\n\n\n\n25.4.4 Plot of Individuals’ Model-Implied Predictions\n\n25.4.4.1 Quarterbacks\n\nCodeplot_individualFantasyPointsByAgeQB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"QB\"),\n  mapping = aes(\n    x = round(age, 2),\n    y = round(fantasyPoints_bayesian, 2),\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Quarterbacks\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeQB,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 25.9: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Quarterbacks.\n\n\n\n\n25.4.4.2 Running Backs\n\nCodeplot_individualFantasyPointsByAgeRB &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"RB\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_bayesian,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Running Backs\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeRB,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 25.10: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Running Backs.\n\n\n\n\n25.4.4.3 Wide Receivers\n\nCodeplot_individualFantasyPointsByAgeWR &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"WR\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_bayesian,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Wide Receivers\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeWR,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 25.11: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Wide Receivers.\n\n\n\n\n25.4.4.4 Tight Ends\n\nCodeplot_individualFantasyPointsByAgeTE &lt;- ggplot(\n  data = player_stats_seasonal_offense_subsetCC %&gt;% filter(position == \"TE\"),\n  mapping = aes(\n    x = age,\n    y = fantasyPoints_bayesian,\n    group = player_id)) +\n  geom_smooth(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    se = FALSE,\n    linewidth = 0.5,\n    color = \"black\") +\n  geom_point(\n    aes(\n      x = age,\n      y = fantasyPoints_bayesian,\n      text = player_display_name, # add player name for mouse over tooltip\n      label = season # add season for mouse over tooltip\n    ),\n    size = 1,\n    color = \"transparent\" # make points invisible but keep tooltips\n  ) +\n  labs(\n    x = \"Player Age (years)\",\n    y = \"Fantasy Points (Season)\",\n    title = \"Fantasy Points (Season) by Player Age: Tight Ends\"\n  ) +\n  theme_classic()\n\nplotly::ggplotly(\n  plot_individualFantasyPointsByAgeTE,\n  tooltip = c(\"age\",\"fantasyPoints_bayesian\",\"text\",\"label\")\n)\n\n\n\n\n\n\nFigure 25.12: Plot of Individuals’ Implied Trajectories of Fantasy Points by Age, from a Bayesian Generalized Additive Model, for Tight Ends.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesConclusion",
    "href": "time-series-analysis.html#sec-timeSeriesConclusion",
    "title": "25  Time Series Analysis",
    "section": "\n25.5 Conclusion",
    "text": "25.5 Conclusion\nThat is, time series analysis seeks to evaluate change over time to predict future values. There many different types of time series analyses. We demonstrated use of autoregressive integrated moving average (ARIMA) models to predict future fantasy points. ARIMA models aim to describe how how earlier levels of a variable are correlated with later levels of the same variable. ARIMA models perform best when there is a clear pattern where later values are influenced by earlier values. ARIMA models assume that the data are stationary (i.e., there are no long-term trends), are non-seasonal (i.e., there is no consistency of the timing of the peaks or troughs in the line), and that earlier values influence later values. This may not strongly be the case in fantasy football, so ARIMA models may not be particularly useful in forecasting fantasy football performance. We also used Bayesian mixed models to generate forecasts of future performance and plots of individuals model-implied performance by age and position.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "time-series-analysis.html#sec-timeSeriesSessionInfo",
    "href": "time-series-analysis.html#sec-timeSeriesSessionInfo",
    "title": "25  Time Series Analysis",
    "section": "\n25.6 Session Info",
    "text": "25.6 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4     forcats_1.0.0       stringr_1.5.1      \n [4] dplyr_1.1.4         purrr_1.1.0         readr_2.1.5        \n [7] tidyr_1.3.1         tibble_3.3.0        tidyverse_2.0.0    \n[10] plotly_4.11.0       ggplot2_3.5.2       rstan_2.32.7       \n[13] StanHeaders_2.32.10 brms_2.22.0         Rcpp_1.1.0         \n[16] forecast_8.24.0     xts_0.14.1          zoo_1.8-14         \n[19] petersenlab_1.2.0  \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3   tensorA_0.36.2.1     rstudioapi_0.17.1   \n  [4] jsonlite_2.0.0       magrittr_2.0.3       TH.data_1.1-3       \n  [7] estimability_1.5.1   farver_2.1.2         nloptr_2.2.1        \n [10] rmarkdown_2.29       vctrs_0.6.5          minqa_1.2.8         \n [13] base64enc_0.1-3      htmltools_0.5.8.1    distributional_0.5.0\n [16] curl_7.0.0           Formula_1.2-5        TTR_0.24.4          \n [19] htmlwidgets_1.6.4    plyr_1.8.9           sandwich_3.1-1      \n [22] emmeans_1.11.2       lifecycle_1.0.4      pkgconfig_2.0.3     \n [25] Matrix_1.7-3         R6_2.6.1             fastmap_1.2.0       \n [28] rbibutils_2.3        digest_0.6.37        colorspace_2.1-1    \n [31] ps_1.9.1             crosstalk_1.2.1      Hmisc_5.2-3         \n [34] labeling_0.4.3       timechange_0.3.0     mgcv_1.9-3          \n [37] httr_1.4.7           abind_1.4-8          compiler_4.5.1      \n [40] withr_3.0.2          htmlTable_2.4.3      backports_1.5.0     \n [43] tseries_0.10-58      inline_0.3.21        DBI_1.2.3           \n [46] psych_2.5.6          QuickJSR_1.8.0       pkgbuild_1.4.8      \n [49] MASS_7.3-65          loo_2.8.0            tools_4.5.1         \n [52] pbivnorm_0.6.0       foreign_0.8-90       lmtest_0.9-40       \n [55] quantmod_0.4.28      nnet_7.3-20          glue_1.8.0          \n [58] quadprog_1.5-8       nlme_3.1-168         grid_4.5.1          \n [61] cmdstanr_0.9.0.9000  checkmate_2.3.3      cluster_2.1.8.1     \n [64] reshape2_1.4.4       generics_0.1.4       gtable_0.3.6        \n [67] tzdb_0.5.0           data.table_1.17.8    hms_1.1.3           \n [70] pillar_1.11.0        posterior_1.6.1      mitools_2.4         \n [73] splines_4.5.1        lattice_0.22-7       survival_3.8-3      \n [76] tidyselect_1.2.1     mix_1.0-13           knitr_1.50          \n [79] reformulas_0.4.1     gridExtra_2.3        V8_6.0.6            \n [82] urca_1.3-4           stats4_4.5.1         xfun_0.53           \n [85] bridgesampling_1.1-2 timeDate_4041.110    matrixStats_1.5.0   \n [88] stringi_1.8.7        lazyeval_0.2.2       yaml_2.3.10         \n [91] boot_1.3-31          evaluate_1.0.4       codetools_0.2-20    \n [94] cli_3.6.5            RcppParallel_5.1.10  rpart_4.1.24        \n [97] xtable_1.8-4         Rdpack_2.6.4         processx_3.8.6      \n[100] lavaan_0.6-19        coda_0.19-4.1        parallel_4.5.1      \n[103] rstantools_2.4.0     fracdiff_1.5-3       bayesplot_1.13.0    \n[106] Brobdingnag_1.2-9    lme4_1.1-37          viridisLite_0.4.2   \n[109] mvtnorm_1.3-3        scales_1.4.0         rlang_1.1.6         \n[112] multcomp_1.4-28      mnormt_2.1.1        \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nHyndman, R. J., Athanasopoulos, G., Bergmeir, C., Caceres, G., Chhay, L., Kuroptev, K., O’Hara-Wild, M., Petropoulos, F., Razbash, S., Wang, E., & Yasmeen, F. (2024). forecast: Forecasting functions for time series and linear models. https://doi.org/10.32614/CRAN.package.forecast\n\n\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(3), 1–22. https://doi.org/10.18637/jss.v027.i03\n\n\nLong, J. D., & Teetor, P. (2019). R cookbook: Proven recipes for data analysis, statistics, and graphics (2nd ed.). O’Reilly Media. https://rc2e.com",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "decision-making.html",
    "href": "decision-making.html",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "",
    "text": "26.1 Getting Started\nThis chapter provides an overview of various considerations in judgment and decision making in the context of uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingGettingStarted",
    "href": "decision-making.html#sec-decisionMakingGettingStarted",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "",
    "text": "26.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"tidyverse\")\n\n\n\n26.1.2 Load Data\n\nCodeload(file = \"./data/projectionsWithActuals_seasonal.RData\")\nload(file = \"./data/projectionsWithActuals_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\n\n\nWe created the projectionsWithActuals_seasonal.RData object in Section 17.1.4.1; we created the projectionsWithActuals_weekly.RData object in Section 17.1.4.2.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingOverview",
    "href": "decision-making.html#sec-decisionMakingOverview",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.2 Overview of Decision Making in Uncertainty",
    "text": "26.2 Overview of Decision Making in Uncertainty",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-wisdomOfCrowd",
    "href": "decision-making.html#sec-wisdomOfCrowd",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.3 Wisdom of the Crowd",
    "text": "26.3 Wisdom of the Crowd\nIn many domains, the average of forecasters’ predictions is more accurate than the accuracy of the constituent individuals. In some domains, the average of non-expert forecasts is more accurate than the forecasts by individual experts. This phenomenon is called “collective intelligence”, the “wisdom of the crowd”, or the “wisdom of crowds” (Larrick et al., 2024; Rader et al., 2017; Simoiu et al., 2019; Surowiecki, 2005; Wagner & Vinaimont, 2010).\nAggregation of predictions from multiple people leverages several important features, including cognitive diversity and error cancellation. Cognitive diversity refers to the representation of individuals with different perspectives because of their “differences in knowledge, training, experience, or thinking styles” (Rader et al., 2017, p. 8). Cognitive diversity is important because judgments from a cognitively homogeneous group will tend to err systematically. That is, they tend to err in the same direction—either consistently above or below the truth; thus, their errors are correlated. By contrast, a cognitively diverse group will not tend to err systematically. Individuals of a cognitively diverse group will bring different areas of expertise (e.g., determination of player skill, player opportunity, matchup strength, etc.) to bear in making their predictions and will thus make different mistakes (Larrick et al., 2024). Consequently, for the people comprising a cognitively diverse group, their judgments will tend to err randomly—where some people’s predictions fall above the truth and some people’s predictions fall below the truth—i.e., individual judgments “bracket” the truth (Mannes et al., 2014). Thus, judgments from a cognitively diverse group tend to have uncorrelated errors.\nError cancellation deals with the idea that, when individuals’ judgments bracket the truth and show random rather than systematic error, the average of the predictions will “cancel out” some of the errors so that the predictions average out to more closely approximate the truth. However, when individuals’ judgments do not bracket the truth, the average of the predictions will not cancel out the errors.\nAveraging projections from individuals tends to yield predictions that are more accurate than the accuracy of most forecasters in the group (Mannes et al., 2014). Indeed, when at least some of the projections bracket the truth, averaged predictions must be more accurate than the average individual forecaster—in terms of mean absolute error (MAE)—and averaged predictions are often much more accurate (Larrick et al., 2024). When referring to the accuracy of the “average individual forecaster”, we are referring to accuracy in terms of mean absolute error (MAE)—not to the accuracy of the forecaster at the 50th percentile. If none of the projections bracket the truth—e.g., all projections overestimate the truth—averaged predictions will be as accurate as the average individual forecaster in terms of mean absolute error (Larrick et al., 2024). In sum, “Averaging the answers of a crowd, therefore, ensures a level of accuracy no worse than the average member of the crowd and, in some cases, a level better than nearly all members” (Larrick et al., 2024, p. 126). Moreover, averaged projections tend to be more accurate than consensus-based judgments from groups of people that interact and discuss, due to cognitive biases associated with the social interaction among groups, such as herding in which people align their behavior with others (Mannes et al., 2014; Simoiu et al., 2019), though discussion can be helpful in some contexts (Larrick et al., 2024).\nThere are well-known prediction markets, in which people bet money to make predictions for various events, which allows determining the crowd-averaged prediction for events:\n\nhttps://www.predictit.org\nhttps://polymarket.com\n\nThere are also betting markets for football:\n\nhttps://www.rotowire.com/betting/nfl/player-futures.php\nhttps://vegasprojections.com\nhttps://www.actionnetwork.com/nfl/props\nhttps://tools.32beatwriters.com\nhttps://the-odds-api.com\nhttps://www.evsharps.com/ranks?format=std\n\nCrowd-averaged projections tend to be most accurate when:\n\nthe crowd consists of individuals who hold expertise in the domain such that they will make predictions that fall close to the truth\nthere is relatively low variability in the expertise of the individual forecasters in terms of their ability to make accurate forecasts\nthere is cognitive diversity among the forecasters\nthe projections are made independently—i.e., the forecasters are not aware of others’ forecasts and do not discuss or interact with the other forecasters\nthe bracketing rate—i.e., the frequency with which any two forecasters’ predictions fall on opposite sides of the truth—is high\nthere are at least 5–10 sources of projections\n\nHowever, the crowd is not more accurate than the expert or best forecaster in all situations or domains. For instance, the crowd tends to be less accurate than the (prospectively identified) best forecaster when there is great variability in forecasters’ expertise (in terms of the forecasters’ ability to forecast accurately) and when the bracketing rate is low (Mannes et al., 2014). Some forecasters may provide terrible projections; thus, including them in an average may make the average projections substantially less accurate. Thus, it may be necessary to examine the average of a “select crowd”, by aggregating the projections of the most consistently accurate forecasters (Mannes et al., 2014). Incorporating at least 5–10 forecasters leverages most of the benefits of the crowd; adding additional forecasters tends to result in diminishing returns (Larrick et al., 2024). However, to the extent that those who are most accurate in a given period reflects luck, you are better off averaging the predictions of all forecasters than selecting the forecasters who were most accurate in the most recent period (Larrick et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-accuracyOfCrowd",
    "href": "decision-making.html#sec-accuracyOfCrowd",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.4 Accuracy of Fantasy Football Crowd Projections",
    "text": "26.4 Accuracy of Fantasy Football Crowd Projections\nEven though the crowd tends to be more accurate than individual forecasters [Kartes (2024); archived at https://perma.cc/69F7-LLTN; Petersen (2017); archived at https://perma.cc/BG2W-ANUF], crowd-averaged projections (at least among experts) are not necessarily highly accurate, as described in Section 17.12.3. In fantasy football, crowd-averaged seasonal projections explain ~60–75% of the variance in fantasy points among offensive players; however, the percent of variance explained drops to ~30% when considering only those players with high projected or actual points—who are the players that are the most important to distinguish, because those are the players that you are most likely trying to decide between when drafting. Nevertheless, individual sources tend to be even less accurate. Individual projections sources tend to explain ~50–65% of the variance in fantasy points among offensive players; however, the percent of variance explained also drops considerably when considering only those players with high projected or actual points.\nThe petersenlab package (Petersen, 2025) has the petersenlab::wisdomOfCrowd() function that computes the overall accuracy of the crowd-averaged projections, including the bracketing rate of the individual projections.\n\n26.4.1 Examples\nBelow are examples where the predictions a) are centered on the truth, b) bracket the truth, and c) do not bracket the truth. They are illustrated in Figure 26.1.\n\n\n\n\n\nFigure 26.1: Three distributions of individual judgments in which judgments bracket and are centered on the truth (top left panel), bracket but are not centered on the truth (top right panel), or do not bracket the truth (i.e., are biased below the truth; bottom panel). The distributions depict the predictions. The dashed lines represent the true value. (Figure and caption adapted from Larrick et al. (2024); Larrick, R. P., Mannes, A. E., & Soll, J. B. (2024). The social psychology of the wisdom of crowds (with a new section on recent advances). In F. M. Federspiel, G. Montibeller, & M. Seifert (Eds.), Behavioral decision analysis (pp. 121–143). Springer. https://doi.org/10.1007/978-3-031-44424-1_7).\n\n\nHere is a code example of when the individual predictions (from which the crowd-averaged predictions are derived) are centered on the truth for a player. The distribution of projected points versus truth is depicted in Figure 26.2.\n\nCodepredictedValues &lt;- c(10,20,30,40,60,70,80,90)\nactualValue &lt;- 50\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -40 -30 -20 -10  10  20  30  40\n\nCodemean(predictedValues)\n\n[1] 50\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodebracketingCentered &lt;- petersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\nbracketingCentered_bracketingRate &lt;- bracketingCentered[which(row.names(bracketingCentered) == \"crowdAveraged\"), \"bracketingRate\"] * 100\n\nmaeBracketingCentered_individual &lt;- bracketingCentered[which(row.names(bracketingCentered) == \"individual\"), \"MAE\"]\nmaeBracketingCentered_crowd &lt;- bracketingCentered[which(row.names(bracketingCentered) == \"crowdAveraged\"), \"MAE\"]\n\nmdaeBracketingCentered_individual &lt;- bracketingCentered[which(row.names(bracketingCentered) == \"individual\"), \"MdAE\"]\nmeBracketingCentered_crowd &lt;- bracketingCentered[which(row.names(bracketingCentered) == \"crowdAveraged\"), \"ME\"]\n\nggplot2::ggplot(,\n  mapping = aes(\n    x = predictedValues)\n) +\n  geom_density(\n    fill = \"lightgray\"\n  ) +\n  geom_vline(\n    xintercept = mean(predictedValues),\n    color = \"darkgray\",\n    linewidth = 1.5) +\n    geom_vline(\n    xintercept = actualValue,\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  #annotate(\n  #  \"segment\",\n  #  x = mean(predictedValues),\n  #  xend = actualValue,\n  #  y = 0,\n  #  yend = 0,\n  #  linewidth = 1.5,\n  #  arrow = arrow(\n  #    angle = 20,\n  #    ends = \"both\",\n  #    type = \"closed\")\n  #)\n  annotate(\n    \"text\",\n    x = mean(predictedValues) + (maeBracketingCentered_crowd / 2) + 1,\n    y = 0.0003,\n    label = paste(\"Mean Error: \", meBracketingCentered_crowd, sep = \"\"),\n    hjust = 0 # left-justify\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Predicted Fantasy Points for a Player\",\n    subtitle = \"Crowd-Averaged Projection (Vertical Gray Line) Vs.\\nTruth, i.e., Actual Fantasy Points (Dashed Line)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 26.2: Example Distribution of Projected Fantasy Points that Brackets and is Centered on the Truth.\n\n\n\n\nAs demonstrated above, when the individual predictions are centered on the truth, the bracketing rate is very high (in this case, 57.14%), and the crowd-averaged predictions are perfectly accurate and are more accurate (MAE/MdAE = 0) than the accuracy of the typical individual forecaster (MAE = 25; MdAE = 25) in terms of mean absolute error and median absolute error.\nHere is a code example of when the individual predictions bracket but are not centered on the truth for a player, with a high bracketing rate and no strong outliers. The distribution of projected points versus truth is depicted in Figure 26.3.\n\nCodepredictedValues &lt;- c(10,29,29,40,60,70,80,90)\nactualValue &lt;- 71\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -61 -42 -42 -31 -11  -1   9  19\n\nCodemean(predictedValues)\n\n[1] 51\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodebracketingNotCentered1 &lt;- petersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\nbracketingNotCentered1_bracketingRate &lt;- bracketingNotCentered1[which(row.names(bracketingNotCentered1) == \"crowdAveraged\"), \"bracketingRate\"] * 100\n\nmaeBracketingNotCentered1_individual &lt;- bracketingNotCentered1[which(row.names(bracketingNotCentered1) == \"individual\"), \"MAE\"]\nmaeBracketingNotCentered1_crowd &lt;- bracketingNotCentered1[which(row.names(bracketingNotCentered1) == \"crowdAveraged\"), \"MAE\"]\n\nmdaeBracketingNotCentered1_individual &lt;- bracketingNotCentered1[which(row.names(bracketingNotCentered1) == \"individual\"), \"MdAE\"]\nmeBracketingNotCentered1_crowd &lt;- bracketingNotCentered1[which(row.names(bracketingNotCentered1) == \"crowdAveraged\"), \"ME\"]\n\nggplot2::ggplot(,\n  mapping = aes(\n    x = predictedValues)\n) +\n  geom_density(\n    fill = \"lightgray\"\n  ) +\n  geom_vline(\n    xintercept = mean(predictedValues),\n    color = \"darkgray\",\n    linewidth = 1.5) +\n    geom_vline(\n    xintercept = actualValue,\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  annotate(\n    \"segment\",\n    x = mean(predictedValues),\n    xend = actualValue,\n    y = 0,\n    yend = 0,\n    linewidth = 1.5,\n    arrow = arrow(\n      angle = 20,\n      ends = \"both\",\n      type = \"closed\")\n  ) +\n  annotate(\n    \"text\",\n    x = actualValue + (meBracketingNotCentered1_crowd / 2),\n    y = 0.0007,\n    label = paste(\"Mean Error: \", meBracketingNotCentered1_crowd, sep = \"\"),\n    hjust = 0.5 # center-justify\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Predicted Fantasy Points for a Player\",\n    subtitle = \"Crowd-Averaged Projection (Vertical Gray Line) Vs.\\nTruth, i.e., Actual Fantasy Points (Dashed Line)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 26.3: Example Distribution of Projected Fantasy Points that Brackets But is Not Centered on the Truth.\n\n\n\n\nAs demonstrated above, when the individual predictions bracket but are not centered on the truth, and the bracketing rate is high (in this case, 42.86%) with no strong outliers, the crowd-averaged predictions (MAE/MdAE = 20) are more accurate than the accuracy of the typical individual forecaster (MAE = 27; MdAE = 25) in terms of mean absolute error and median absolute error.\nHere is a code example of when the individual predictions bracket but are not centered on the truth for a player, with a high bracketing rate but with an outlier projection that is far away from the truth. The distribution of projected points versus truth is depicted in Figure 26.4.\n\nCodepredictedValues &lt;- c(40,60,70,80,90,100,110,250)\nactualValue &lt;- 74\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -34 -14  -4   6  16  26  36 176\n\nCodemean(predictedValues)\n\n[1] 100\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodebracketingnotCentered2 &lt;- petersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\nbracketingnotCentered2_bracketingRate &lt;- bracketingnotCentered2[which(row.names(bracketingnotCentered2) == \"crowdAveraged\"), \"bracketingRate\"] * 100\n\nmaeBracketingnotCentered2_individual &lt;- bracketingnotCentered2[which(row.names(bracketingnotCentered2) == \"individual\"), \"MAE\"]\nmaeBracketingnotCentered2_crowd &lt;- bracketingnotCentered2[which(row.names(bracketingnotCentered2) == \"crowdAveraged\"), \"MAE\"]\n\nmdaeBracketingnotCentered2_individual &lt;- bracketingnotCentered2[which(row.names(bracketingnotCentered2) == \"individual\"), \"MdAE\"]\nmeBracketingnotCentered2_crowd &lt;- bracketingnotCentered2[which(row.names(bracketingnotCentered2) == \"crowdAveraged\"), \"ME\"]\n\nggplot2::ggplot(,\n  mapping = aes(\n    x = predictedValues)\n) +\n  geom_density(\n    fill = \"lightgray\"\n  ) +\n  geom_vline(\n    xintercept = mean(predictedValues),\n    color = \"darkgray\",\n    linewidth = 1.5) +\n    geom_vline(\n    xintercept = actualValue,\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  annotate(\n    \"segment\",\n    x = mean(predictedValues),\n    xend = actualValue,\n    y = 0,\n    yend = 0,\n    linewidth = 1.5,\n    arrow = arrow(\n      angle = 20,\n      ends = \"both\",\n      type = \"closed\")\n  ) +\n  annotate(\n    \"text\",\n    x = actualValue + (meBracketingnotCentered2_crowd / 2),\n    y = 0.0007,\n    label = paste(\"Mean Error: \", meBracketingnotCentered2_crowd, sep = \"\"),\n    hjust = 0.5 # center-justify\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Predicted Fantasy Points for a Player\",\n    subtitle = \"Crowd-Averaged Projection (Vertical Gray Line) Vs.\\nTruth, i.e., Actual Fantasy Points (Dashed Line)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 26.4: Example Distribution of Projected Fantasy Points that Brackets But is Not Centered on the Truth.\n\n\n\n\nAs demonstrated above, when the individual predictions bracket but are not centered on the truth, and the bracketing rate is high (in this case, 53.57%) but with an outlier projection that is far away from the truth, the crowd-averaged predictions (MAE/MdAE = 26) are more accurate than the accuracy of the typical individual forecaster (MAE = 39) in terms of mean absolute error, but they can still be less accurate—in terms of median absolute error—than the individual forecaster who is at the 50th percentile in accuracy (MdAE = 21).\nHere is another code example of when the individual predictions bracket but are not centered on the truth for a player, with a low bracketing rate. The distribution of projected points versus truth is depicted in Figure 26.5.\n\nCodepredictedValues &lt;- c(10,29,29,40,60,70,80,90)\nactualValue &lt;- 14\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -4 15 15 26 46 56 66 76\n\nCodemean(predictedValues)\n\n[1] 51\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodebracketingnotCentered3 &lt;- petersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\nbracketingnotCentered3_bracketingRate &lt;- bracketingnotCentered3[which(row.names(bracketingnotCentered3) == \"crowdAveraged\"), \"bracketingRate\"] * 100\n\nmaeBracketingnotCentered3_individual &lt;- bracketingnotCentered3[which(row.names(bracketingnotCentered3) == \"individual\"), \"MAE\"]\nmaeBracketingnotCentered3_crowd &lt;- bracketingnotCentered3[which(row.names(bracketingnotCentered3) == \"crowdAveraged\"), \"MAE\"]\n\nmdaeBracketingnotCentered3_individual &lt;- bracketingnotCentered3[which(row.names(bracketingnotCentered3) == \"individual\"), \"MdAE\"]\nmeBracketingnotCentered3_crowd &lt;- bracketingnotCentered3[which(row.names(bracketingnotCentered3) == \"crowdAveraged\"), \"ME\"]\n\nggplot2::ggplot(,\n  mapping = aes(\n    x = predictedValues)\n) +\n  geom_density(\n    fill = \"lightgray\"\n  ) +\n  geom_vline(\n    xintercept = mean(predictedValues),\n    color = \"darkgray\",\n    linewidth = 1.5) +\n    geom_vline(\n    xintercept = actualValue,\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  annotate(\n    \"segment\",\n    x = mean(predictedValues),\n    xend = actualValue,\n    y = 0,\n    yend = 0,\n    linewidth = 1.5,\n    arrow = arrow(\n      angle = 20,\n      ends = \"both\",\n      type = \"closed\")\n  ) +\n  annotate(\n    \"text\",\n    x = actualValue + (meBracketingnotCentered3_crowd / 2),\n    y = 0.0003,\n    label = paste(\"Mean Error: \", meBracketingnotCentered3_crowd, sep = \"\"),\n    hjust = 0.5 # center-justify\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Predicted Fantasy Points for a Player\",\n    subtitle = \"Crowd-Averaged Projection (Vertical Gray Line) Vs.\\nTruth, i.e., Actual Fantasy Points (Dashed Line)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 26.5: Example Distribution of Projected Fantasy Points that Brackets But is Not Centered on the Truth.\n\n\n\n\nAs demonstrated above, when the individual predictions bracket but are not centered on the truth and the bracketing rate is low (in this case, 25.00%), the crowd-averaged predictions (MAE/MdAE = 37) are more accurate than the accuracy of the typical individual forecaster (MAE = 38) in terms of mean absolute error, but they can still be less accurate—in terms of median absolute error—than the individual forecaster who is at the 50th percentile in accuracy (MdAE = 36).\nHere is a code example of when the individual predictions do not bracket the truth for a player, and there are no strong outliers. The distribution of projected points versus truth is depicted in Figure 26.6.\n\nCodepredictedValues &lt;- c(10,20,30,40,60,70,80,90)\nactualValue &lt;- 100\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -90 -80 -70 -60 -40 -30 -20 -10\n\nCodemean(predictedValues)\n\n[1] 50\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodenoBracketing1 &lt;- petersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\nmaeNoBracketing1_individual &lt;- noBracketing1[which(row.names(noBracketing1) == \"individual\"), \"MAE\"]\nmaeNoBracketing1_crowd &lt;- noBracketing1[which(row.names(noBracketing1) == \"crowdAveraged\"), \"MAE\"]\n\nmdaeNoBracketing1_individual &lt;- noBracketing1[which(row.names(noBracketing1) == \"individual\"), \"MdAE\"]\nmeNoBracketing1_crowd &lt;- noBracketing1[which(row.names(noBracketing1) == \"crowdAveraged\"), \"ME\"]\n\nggplot2::ggplot(,\n  mapping = aes(\n    x = predictedValues)\n) +\n  geom_density(\n    fill = \"lightgray\"\n  ) +\n  geom_vline(\n    xintercept = mean(predictedValues),\n    color = \"darkgray\",\n    linewidth = 1.5) +\n    geom_vline(\n    xintercept = actualValue,\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  annotate(\n    \"segment\",\n    x = mean(predictedValues),\n    xend = actualValue,\n    y = 0,\n    yend = 0,\n    linewidth = 1.5,\n    arrow = arrow(\n      angle = 20,\n      ends = \"both\",\n      type = \"closed\")\n  ) +\n  annotate(\n    \"text\",\n    x = actualValue + (meNoBracketing1_crowd / 2),\n    y = 0.0003,\n    label = paste(\"Mean Error: \", meNoBracketing1_crowd, sep = \"\"),\n    hjust = 0.5 # center-justify\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Predicted Fantasy Points for a Player\",\n    subtitle = \"Crowd-Averaged Projection (Vertical Gray Line) Vs.\\nTruth, i.e., Actual Fantasy Points (Dashed Line)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 26.6: Example Distribution of Projected Fantasy Points that Does Not Bracket the Truth.\n\n\n\n\nAs demonstrated above, when the individual predictions do not bracket the truth (i.e., the bracketing rate is 0%) and there are no strong outliers, the crowd-averaged predictions (MAE/MdAE = 50) are as accurate as the accuracy of the typical individual forecaster (MAE = 50; MdAE = 50) in terms of mean absolute error and median absolute error.\nHere is another code example of when the crowd-averaged predictions do not bracket the truth for a player and there is an outlier prediction that is far away from the truth. The distribution of projected points versus truth is depicted in Figure 26.7.\n\nCodepredictedValues &lt;- c(0,65,67,70,80,85,90,95)\nactualValue &lt;- 100\nerrors &lt;- predictedValues - actualValue\n\nerrors\n\n[1] -100  -35  -33  -30  -20  -15  -10   -5\n\nCodemean(predictedValues)\n\n[1] 69\n\nCodepetersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\n\n  \n\n\n\n\nCodenoBracketing2 &lt;- petersenlab::wisdomOfCrowd(\n  predicted = predictedValues,\n  actual = actualValue\n)\n\nmaeNoBracketing2_individual &lt;- noBracketing2[which(row.names(noBracketing2) == \"individual\"), \"MAE\"]\nmaeNoBracketing2_crowd &lt;- noBracketing2[which(row.names(noBracketing2) == \"crowdAveraged\"), \"MAE\"]\n\nmdaeNoBracketing2_individual &lt;- noBracketing2[which(row.names(noBracketing2) == \"individual\"), \"MdAE\"]\nmeNoBracketing2_crowd &lt;- noBracketing2[which(row.names(noBracketing2) == \"crowdAveraged\"), \"ME\"]\n\nggplot2::ggplot(,\n  mapping = aes(\n    x = predictedValues)\n) +\n  geom_density(\n    fill = \"lightgray\"\n  ) +\n  geom_vline(\n    xintercept = mean(predictedValues),\n    color = \"darkgray\",\n    linewidth = 1.5) +\n    geom_vline(\n    xintercept = actualValue,\n    linetype = \"dashed\",\n    linewidth = 1.5\n  ) +\n  annotate(\n    \"segment\",\n    x = mean(predictedValues),\n    xend = actualValue,\n    y = 0,\n    yend = 0,\n    linewidth = 1.5,\n    arrow = arrow(\n      angle = 20,\n      ends = \"both\",\n      type = \"closed\")\n  ) +\n  annotate(\n    \"text\",\n    x = actualValue + (meNoBracketing2_crowd / 2),\n    y = 0.0015,\n    label = paste(\"Mean Error: \", meNoBracketing2_crowd, sep = \"\"),\n    hjust = 0.5 # center-justify\n  ) +\n  labs(\n    x = \"Fantasy Points\",\n    y = \"Density\",\n    title = \"Density Plot of Predicted Fantasy Points for a Player\",\n    subtitle = \"Crowd-Averaged Projection (Vertical Gray Line) Vs.\\nTruth, i.e., Actual Fantasy Points (Dashed Line)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title\n\n\n\n\n\n\nFigure 26.7: Example Distribution of Projected Fantasy Points that Does Not Bracket the Truth.\n\n\n\n\nAs demonstrated above, when the crowd-averaged predictions do not bracket the truth and there are one or more predictions that are far away from the truth, the crowd-averaged predictions (MAE/MdAE = 31) are as accurate as the accuracy of the typical individual forecaster (MAE = 31) in terms of mean absolute error, but they can be less accurate—in terms of median absolute error—than the individual forecaster who is at the 50th percentile in accuracy (MdAE = 25).\nIn conclusion, as demonstrated from these examples, when the crowd-averaged predictions are centered on the truth, the crowd-averaged predictions are perfectly accurate and are more accurate than the accuracy of the typical individual forecaster in terms of mean absolute error and of most individual forecasters in terms of median absolute error. When the crowd-averaged predictions bracket but are not centered on the truth, the crowd-averaged predictions are more accurate than the accuracy of the typical individual forecaster in terms of mean absolute error. If the bracketing rate is high and there are no strong outliers, the crowd-averaged predictions will also be more accurate—in terms of median absolute error—than most individual forecasters; however, if the bracketing rate is low or there are strong outliers, the crowd-averaged predictions can be less accurate than than most individual forecasters. Outliers could make the crowd-averaged prediction more or less accurate, depending on whether the outlying prediction is closer to or farther away from the truth, compared to the other predictions. When the crowd-averaged predictions do not bracket the truth, the crowd-averaged predictions are as accurate as the accuracy of the typical individual forecaster in terms of mean absolute error. When the crowd-averaged predictions do not bracket the truth and there are outliers, the crowd-averaged projections can be less accurate—in terms of median absolute error—than most individual forecasters (again, depending on whether the outlying prediction is closer to or farther away from the truth, compared to the other predictions).\nIn sum, going with crowd-averaged projections will always yield predictions that are as or more accurate (in terms of mean absolute error) than the (collective) accuracy of the individuals’ projections. However, the crowd-averaged projections may be less accurate than any individual projection, including the projection of the forecaster who is at the 50th percentile in accuracy. That is, if there is a low bracketing rate or inaccurate outliers, the crowd-averaged projections can be less accurate than most individual projections. In general, the higher the bracketing rate and the more closely that the projections are centered on the truth, the more accurate the crowd-averaged projections will be.\nThere may be multiple ways of handling outlier predictions. For instance, instead of computing a simple average of the predictions, one could calculate a weighted average that weights each prediction according to the historical accuracy of the prediction source. Alternatively, one could calculate the crowd-averaged prediction using an index of the center of a distribution that is less sensitive to outliers, such as a median or robust average such as the Hodges-Lehmann statistic (aka pseudomedian).\nEven though some sources are more accurate than the average in a given year, they are not consistently more accurate than the average. Prediction involves a combination of luck and skill. In some years, a prediction will invariably do better than others, in part, based on luck. However, luck is unlikely to continue systematically into future years, so a source that got lucky in a given year is likely to regress to the mean in subsequent years. That is, determining the most accurate source in a given year, after the fact, is not necessarily the same as identifying the most skilled forecaster. It is easy to identify the most accurate source after the fact, but it is challenging to predict, in advance, who the best forecaster will be (Larrick et al., 2024). It requires a large sample of predictions to determine whether a given forecaster is reliably (i.e., consistently) more accurate than other forecasters and to identify the most accurate forecaster (Larrick et al., 2024). Thus, it can be challenging to know, in advance, who the most accurate forecasters will be. Because average projections are as or more accurate than the average forecaster’s prediction, averaging projections across all forecasters is superior to choosing individual forecasters when the forecasters are roughly similar in forecasting ability or when it is hard to distinguish their ability in advance (Larrick et al., 2024).\nThe relatively modest accuracy of the projections by so-called fantasy “experts” and of their average of their projections could occur for a number of reasons. One possibility is that the level of expertise of the “expert” forecasters in terms of being able to provide accurate forecasts is not strong. That is, because football performance and injuries are so challenging to predict, individual forecasters’ projections may not be particularly close to the truth.\nA second possibility is that the bracketing rate of the predictions is not particularly high (Mannes et al., 2014). Even if the individual forecasters’ projections are not close to the truth, if ~50% of them overestimate the truth and the other 50% of the underestimate the truth, the average will more closely approximate the truth. However, if all forecasters overestimate the truth for a given player, averaging the projections will not necessarily lead to more accurate projections.\nA third possibility is that the forecasts of the different experts are not independent.\nEach of these possibilities is likely true to some degree. First, individuals’ predictions are unlikely to be highly accurate consistently. Second, there are many players who are systematically overpredicted (e.g., due to their injury) or underpredicted (e.g., due to their becoming the starter after a teammate becomes injured, is traded, etc.)—an example of overextremity miscalibration. In general, it is likely for players who are projected to score more points to be overpredicted and for players who are projected to score fewer points to be underpredicted, as described in Section Section 17.12. Third, the experts may interact and discuss with one another. Interaction and discussion among experts may lead them to follow the herd and conform their projections to what each other predict. This has been terms informational influence and may reflect the anchoring and adjustment heuristic (Larrick et al., 2024). In any case, they are able to see each other’s projections and make change their projections, accordingly.\n\n26.4.2 Seasonal Projections\n\nCodeseasonalIndividualSources &lt;- projectionsWithActuals_seasonal %&gt;%\n  filter(!is.na(raw_points)) %&gt;% \n  filter(!is.na(fantasyPoints)) %&gt;% \n  group_by(player_id, season) %&gt;%\n  filter(n() &gt;= 2) %&gt;% # at least 2 projections\n  summarise(\n    numProjections = n(),\n    type = \"individual\",\n    petersenlab::wisdomOfCrowd(\n      predicted = raw_points,\n      actual = unique(fantasyPoints),\n      dropUndefined = TRUE\n      )[1,],\n    .groups = \"drop\"\n  )\n\nseasonalCrowd &lt;- projectionsWithActuals_seasonal %&gt;%\n  filter(!is.na(raw_points)) %&gt;% \n  filter(!is.na(fantasyPoints)) %&gt;% \n  group_by(player_id, season) %&gt;%\n  filter(n() &gt;= 2) %&gt;% # at least 2 projections\n  summarise(\n    numProjections = n(),\n    type = \"crowd\",\n    petersenlab::wisdomOfCrowd(\n      predicted = raw_points,\n      actual = unique(fantasyPoints),\n      dropUndefined = TRUE\n      )[2,],\n    .groups = \"drop\"\n  )\n\nseasonalIndividualSourcesAndCrowd &lt;- bind_rows(\n  seasonalIndividualSources,\n  seasonalCrowd\n)\n\nseasonalIndividualSources &lt;- seasonalIndividualSources %&gt;% \n  left_join(\n    player_stats_seasonal,\n    by = c(\"player_id\", \"season\")\n  )\n\nseasonalCrowd &lt;- seasonalCrowd %&gt;% \n  left_join(\n    player_stats_seasonal,\n    by = c(\"player_id\", \"season\")\n  )\n\nseasonalIndividualSourcesAndCrowd &lt;- seasonalIndividualSourcesAndCrowd %&gt;% \n  left_join(\n    player_stats_seasonal,\n    by = c(\"player_id\", \"season\")\n  )\n\n\n\nCodeseasonalIndividualSources %&gt;% \n  select(player_display_name, position, season, numProjections, bracketingRate, ME:RMSLE)\n\n\n  \n\n\n\n\nCodeggplot2::ggplot(\n  data = seasonalIndividualSources %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = bracketingRate)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  labs(\n    x = \"Bracketing Rate\",\n    y = \"Count\",\n    title = \"Histogram of Bracketing Rate (Seasonal Projections)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5))\n\n\n\n\n\n\nFigure 26.8: Distribution of Bracketing Rate Across Players For Seasonal Projections.\n\n\n\n\nBelow is the proportion of players whose bracketing rate was zero:\n\nCodeseasonalIndividualSources %&gt;% \n  filter(!is.na(bracketingRate)) %&gt;% \n  mutate(isZeroBracketingRate = ifelse(bracketingRate == 0, 1, 0)) %&gt;% \n  summarise(proportionZeroBracketingRate = sum(isZeroBracketingRate) / n())\n\n\n  \n\n\n\nA high proportion (68%) of players’ forecasts did not bracket the truth.\n\n26.4.3 Weekly Projections\n\nCodeweeklyIndividualSources &lt;- projectionsWithActuals_weekly %&gt;%\n  filter(!is.na(raw_points)) %&gt;% \n  filter(!is.na(fantasyPoints)) %&gt;% \n  group_by(player_id, season, week) %&gt;%\n  filter(n() &gt;= 2) %&gt;% # at least 2 projections\n  summarise(\n    numProjections = n(),\n    type = \"individual\",\n    petersenlab::wisdomOfCrowd(\n      predicted = raw_points,\n      actual = unique(fantasyPoints),\n      dropUndefined = TRUE\n      )[1,],\n    .groups = \"drop\"\n  )\n\nweeklyCrowd &lt;- projectionsWithActuals_weekly %&gt;%\n  filter(!is.na(raw_points)) %&gt;% \n  filter(!is.na(fantasyPoints)) %&gt;% \n  group_by(player_id, season, week) %&gt;%\n  filter(n() &gt;= 2) %&gt;% # at least 2 projections\n  summarise(\n    numProjections = n(),\n    type = \"crowd\",\n    petersenlab::wisdomOfCrowd(\n      predicted = raw_points,\n      actual = unique(fantasyPoints), # [1] error w/o [1] because have more than one unique value for a given player-season-week\n      dropUndefined = TRUE\n      )[2,],\n    .groups = \"drop\"\n  )\n\nweeklyIndividualSourcesAndCrowd &lt;- bind_rows(\n  weeklyIndividualSources,\n  weeklyCrowd\n)\n\nweeklyIndividualSources &lt;- weeklyIndividualSources %&gt;% \n  left_join(\n    player_stats_weekly,\n    by = c(\"player_id\", \"season\", \"week\")\n  )\n\nweeklyCrowd &lt;- weeklyCrowd %&gt;% \n  left_join(\n    player_stats_weekly,\n    by = c(\"player_id\", \"season\", \"week\")\n  )\n\nweeklyIndividualSourcesAndCrowd &lt;- weeklyIndividualSourcesAndCrowd %&gt;% \n  left_join(\n    player_stats_weekly,\n    by = c(\"player_id\", \"season\", \"week\")\n  )\n\n\n\nCodeweeklyIndividualSources %&gt;% \n  select(player_display_name, position, season, week, numProjections, bracketingRate, ME:RMSLE)\n\n\n  \n\n\n\n\nCodeggplot2::ggplot(\n  data = weeklyIndividualSources %&gt;%\n    filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\")),\n  mapping = aes(\n    x = bracketingRate)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    fill = \"#0099F8\"\n  ) +\n  labs(\n    x = \"Bracketing Rate\",\n    y = \"Count\",\n    title = \"Histogram of Bracketing Rate (Weekly Projections)\"\n  ) +\n  theme_classic() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5))\n\n\n\n\n\n\nFigure 26.9: Distribution of Bracketing Rate Across Players For Weekly Projections.\n\n\n\n\nBelow is the proportion of players whose bracketing rate was zero:\n\nCodeweeklyIndividualSources %&gt;% \n  filter(!is.na(bracketingRate)) %&gt;% \n  mutate(isZeroBracketingRate = ifelse(bracketingRate == 0, 1, 0)) %&gt;% \n  summarise(proportionZeroBracketingRate = sum(isZeroBracketingRate) / n())\n\n\n  \n\n\n\nA high proportion (79%) of players’ forecasts did not bracket the truth.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-incoporateAdvice",
    "href": "decision-making.html#sec-incoporateAdvice",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.5 How Well Do People Incorporate Advice From Others?",
    "text": "26.5 How Well Do People Incorporate Advice From Others?\nAn important question is how well people incorporate advice from others. In the context of fantasy football, advice might be sources of projections. In general, evidence from social psychology suggests that people tend to underweight how much weight they give to advice from others relative to their own opinions, a phenomenon called egocentric discounting (Larrick et al., 2024; Rader et al., 2017). People tend to weight others’ advice around 30% in terms of the proportion of a shift a person makes toward another person’s perspective, though this depends on the perceived accuracy of the advisor. Moreover, people frequently ignores others’ advice entirely. In general, people make use of crowds too little—they put too much weight on their own prediction and not enough weight on others’ predictions whose diversity can be leveraged for error cancellation (Larrick et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-managingRiskUncertainty",
    "href": "decision-making.html#sec-managingRiskUncertainty",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.6 Strategies for Managing Risk and Uncertainty",
    "text": "26.6 Strategies for Managing Risk and Uncertainty\nSports analyst Tanney (2021) encourages people to consider framing your decisions as bets, to use a probabilistic mindset (rather than a determininistic mindset). Even if something is perceived as likely to occur with 100% certainty, it is not guaranteed that such an event will occur. Other things can and do happen, and overconfidence can get in the way of effective decision-making.\nA key goal is to identify decisions that provide a net advantage in expected value. Although there is considerable variability and the outcome is not guaranteed, small advantages can yield considerable gains in the long run (Tanney, 2021); however, one must be able to survive the costs of losses to obtain enough long-term trials. So, do not bet more than you are willing to lose.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-riskManagementCognitivePsych",
    "href": "decision-making.html#sec-riskManagementCognitivePsych",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.7 Risk Management Principles from Cognitive Psychology",
    "text": "26.7 Risk Management Principles from Cognitive Psychology",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-sportsBetting",
    "href": "decision-making.html#sec-sportsBetting",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.8 Sports Betting/Gambling",
    "text": "26.8 Sports Betting/Gambling\nThe National Football League (NFL) used to be adamantly against sports gambling. They wanted to protect the integrity of the game. They felt that sports gambling might lead to match fixing, point shaving, and other forms of corruption that would undermine the legitimacy of competition in the game. Match fixing occurs when players, referees, coaches, or others conspire to manipulate the outcome of the game. Point shaving involves manipulating the score—commonly by a player intentionally underperforming (even if it does not lead to “throwing the game” and causing the team to lose). Match fixing and point shaving can occur for a variety of reasons, but a common reason is to ensure that particular gamblers win their bet. Another reason the NFL was against sports betting was that, prior to the U.S. Supreme Court overturning the Professional and Amateur Sports Protection Act (PASPA) in 2018, there were legal restrictions on gambling that prevented sports betting in many parts of the country.\nHowever, after the legalization of sports betting following the overturning of PASPA in 2018, the NFL has changed their tune and are now strongly in favor of sports betting. The rise of legal sports betting has created massive revenue streams for the NFL through partnerships, sponsorships, and advertising deals with sportsbooks.\nThere is substantial overconfidence (in particular overestimation of one’s actual performance) in sports betting/gambling. A study (archived at https://perma.cc/X2AW-SUBZ) of frequent sports bettors found that they tended to predict that they would gain 0.3 cents for every dollar wagered, but in fact lost 7.5 cents for every dollar wagered (Brown et al., 2025). Overconfidence was greatest among those who frequently wagered multi-leg bets (parlays), losing ~25 cents for every dollar wagered (Brown et al., 2025).\nPeople may make a sports bet because they believe strongly in their predicted outcome and that they “cannot be wrong”. However, the initial Vegas lines represent the aggregation of a massive amount of information (by professional oddsmakers), including team power rankings, injuries, weather, home/away, coaching, matchups, historical betting behavior, advanced statistics, and proprietary algorithms. Leading up to the game, the Vegas lines are adjusted to balance the total money on each side of the bet (so that the sportsbook can be ensured it makes money). In this way, the Vegas lines are based on the bets of many, many people and many dollars and reflect market wisdom. That is, by making a sports bet, you are implicitly claiming that you have information, insight, or a model that makes you better than the market consensus at predicting the outcome—even though half of the dollars bet will lose. Betting markets, similar to the stock market (as described in Section 20.3.1), are often highly efficient and beating them consistently is extremely challenging. Moreover, people tend to show confirmation bias, such that they tend to remember their predictive successes and to forget their failures. So, when they they get a bet correct, they may be more likely to bet again in the future, especially because sports betting involves intermittent reinforcement (in particular, variable ratio reinforcement), which can make it highly addictive.\nHowever, not all people are equally prone to gambling addiction. One of the most robust risk factors of gambling addiction is a steep delay discounting curve (Amlung et al., 2017; Weinsztok et al., 2021). The delay discounting curve can be generated from asking respondents a series of hypothetical choices, such as: 1) Would you prefer $10 now or $15 in 1 hour? 2) Would you prefer $10 now or $20 tomorrow? 3) Would you prefer $10 now or $50 in one month? Based on these questions, one can estimate how much a respondent’s valuation of a reward (in this case, money) decreases with the passage of time. An example of delay discounting curves of two people is in Figure 26.10. Some individuals have a shallow delay discounting curve and will prefer more money even if they have to wait, a form of delayed gratification. Other individuals have a steep delay discounting curve and will prefer obtaining the money now, even if that means gaining less money in the long run, a form of impulsivity. In particular, individuals who have a steep delay discounting curve are most likely to develop gambling addictions, possibly because they are more sensitive to immediate rewards and are less driven by long-term consequences.\n\n\n\n\n\nFigure 26.10: Prototypic hyperbolic delayed reward discounting curves reflecting the discounted subjective value of $100 delayed from 1 day to 1 year The curves reflect the points at which the smaller immediate reward is equal in value to the $100 delayed reward. For example, at a delay of 100 days, $100 has lost ∼50% of its nominal value for the low impulsivity profile and ∼90% of its nominal value for the high impulsivity profile. (Figure and caption adapted from Gray & MacKillop (2015); Gray, J. C., & MacKillop, J. (2015). Impulsive delayed reward discounting as a genetically-influenced target for drug abuse prevention: A critical evaluation. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.01104).\n\n\n\n26.8.1 Reinforcement\nReinforcement involves a (typically appetitive) stimulus that increases the frequency of a behavior. For instance, reinforcers could include things like food, praise, attention, money, etc. Thus, sports gambling can provide reinforcement through the receipt of money.\n\n26.8.1.1 Continuous Reinforcement\nIn general, when you want to train a new behavior, continuous reinforcement is the fastest way to do so. Continuous reinforcement means rewarding the behavior each time that it occurs. For instance, a parent might use continuous reinforcement to train their child to put their toys away—the parent may praise them or give them physical affection each time they put their toys away. However, continuous reinforcement is susceptible to extinction. Extinction means cessation of the target behavior (putting the toys away, in this example). That is, for a previously continusouly reinforced behavior, if the behavior stops being rewarded, the person is less likely to continue the behavior. Thus, to make a behavior less susceptible to extinction, intermittent reinforcement may be used.\n\n26.8.1.2 Intermittent Reinforcement\nIntermittent reinforcement (also called partial reinforcement) means rewarding the behavior sometimes but not everytime. There are various approaches to intermittent reinforcement, called schedules of reinforcement. The four primary reinforcement schedules are: fixed interval, fixed ratio, variable interval, and variable ratio. The reinforcement schedules are depicted in Figure 26.11, as adapted from Spielman et al. (2020).\n\n\n\n\n\nFigure 26.11: Reinforcement Schedules. The four reinforcement schedules yield different response patterns. The variable ratio schedule is unpredictable and yields high and steady response rates, with little if any pause after reinforcement (e.g., gambler who plays the slot machines). A fixed ratio schedule is predictable and produces a high response rate, with a short pause after reinforcement (e.g., salesperson paid on commission). The variable interval schedule is unpredictable and produces a moderate, steady response rate (e.g., restaurant manager who is paid a bonus if the food inspector, who comes at unpredictable times, gives the restaurant a good rating). The fixed interval schedule yields a scallop-shaped response pattern, reflecting a significant pause after reinforcement (e.g., surgery patient who can receive a painkiller after pressing a button, but only up to once per hour). (Figure and caption adapted from Spielman et al. (2020); https://openstax.org/books/psychology-2e/pages/6-3-operant-conditioning; Spielman, R. M., Jenkins, W. J., & Lovett, M. D. (2020). Psychology. OpenStax. https://openstax.org/details/books/psychology-2e; archived at https://perma.cc/9DZ8-34X2).\n\n\nA fixed interval reinforcement schedule occurs when the person is rewarded after a set amount of time. For instance, if the reward becomes available every 30 minutes, the person receives the reward the first time they engage in the behavior after the reward becomes available. As an example, consider a surgery patient who can receive a painkiller after pressing a button, but only up to once per hour.\nA variable interval reinforcement schedule occurs when the person is rewarded after a varying (and unpredictable) amount of time. As an example, consider a restaurant manager who is paid a bonus if the food inspector, who comes at unpredictable times, gives the restaurant a good rating.\nA fixed ratio reinforcement schedule occurs when the person is rewarded after a set number of responses. As an example, consider a coffee shop that offers a free coffee after every 10 purchases that a person makes.\nA variable ratio reinforcement schedule occurs when the person is rewarded after a varying (and unpredictable) number of responses. As an example, consider a slot machine that provides a large monetary reward (paired with the machine lighting up and bells going off) after a varying number of attempts. Many forms of gambling involve variable ratio reinforcement.\nThe variable ratio reinforcement schedule is the least susceptible to extinction. People can go without reinforcement many times and may still continue to gamble out of the hope for a large reward on the next attempt. The low susceptibility of variable ratio reinforcement to extinction is, in part, why gambling (which often involves variable ratio reinforcement) can be highly addictive despite people’s tendency to lose money gambling.\n\n26.8.2 Is Fantasy Football a Game of Luck or Skill?\nThe question of whether fantasy football is a game of chance or skill is important because such considerations help determine whether betting on one’s performance is considered gambling, which might be illegal or regulated in many jurisdictions. Fantasy football (and sports betting, more generally) is not 100% luck. Skill can be involved in identifying undervalued assets—whether in terms of stocks in the stock market or professional football players, where inside information can be especially valuable (and thus illegal to profit from). To evaluate the percentage of variability in fantasy football that is attributable to luck versus skill, Getty et al. (2018) evaluated the extent to which one’s performance in the first half of the season was correlated with one’s performance in the second half of the season, under the assumption that underlying skill would lead to persistence of performance across time. The authors examined FanDuel daily fantasy sports (DFS) leagues, which allow the user to set an entirely new lineup each week from all players, based on salary constraints (so a user cannot just select the best player at every position). The authors estimated that ~55% of the variability in fantasy football performance across time was due to skill and 45% was due to luck (Getty et al., 2018). Thus, performance in fantasy football is around half and half luck versus skill.\nBecause fantasy football is not 100% luck, it is not the same as a slot machine. However, there are still unpredictable elements (e.g., injuries), and there is considerable luck involved. As a result, sports betting and gambling on fantasy football still involve components of variable ratio reinforcement that lend them to being potentially highly addictive despite people’s tendency to lose money.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingSuggestions",
    "href": "decision-making.html#sec-decisionMakingSuggestions",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.9 Suggestions",
    "text": "26.9 Suggestions\nBased on the above discussion, here are some suggestions for decision making in the context of uncertainty:\n\nSeek advice from diverse perspectives and incorporate it into your decision making.\nGet opinions from others before you state your perspective and before the various sources of advice discuss with each other, to ensure independence of advice.\n\nAs noted by Kahneman (2011), “before an issue is discussed, all members of the committee should be asked to write a very brief summary of their position… The standard practice of open discussion gives too much weight to the opinions of those who speak early and assertively, causing others to line up behind them.” (p. 85).\n\n\nIf some sources of advice (or projections) are clearly more skilled and accurate than others, you can average this “select” crowd of projections or give them greater weight in a weighted average.\nIf it is unclear whether or which sources are reliably more accurate than others, using a simple average across all sources (i.e., crowd-averaged projections) can be a useful approach that is as accurate as—if not more accurate than—the average individual forecaster.\nIncorporate at least 5–10 sources of projections. Use a weighted or robust average to account for outlier projections.\nDo not bet on fantasy football (or anything else for that matter) unless you are willing to lose the money. Sports bettors tend to be overconfident; on average, they lose 7.5 cents for every dollar wagered (Brown et al., 2025). If you are going to bet, only bet a small portion of your money and never more than you are willing to lose. And do not make multi-leg bets (parlays); people lose on average 25 cents for every dollar wagered on parlays (Brown et al., 2025).",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingConclusion",
    "href": "decision-making.html#sec-decisionMakingConclusion",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.10 Conclusion",
    "text": "26.10 Conclusion\nThe wisdom of the crowd is the idea that the average of the predictions of many people is often more accurate than the prediction of individual experts. When at least some of the projections bracket—i.e., fall on opposite sides of—the truth, averaged predictions are more accurate than the average individual forecaster—in terms of mean absolute error (MAE). Crowd-averaged projections tend to be most accurate when the crowd consists of individuals who hold expertise in the domain such that they will make predictions that fall close to the truth, there is relatively low variability in the expertise of the individual forecasters in terms of their ability to make accurate forecasts, there is cognitive diversity among the forecasters, the projections are made independently—i.e., the forecasters are not aware of others’ forecasts and do not discuss or interact with the other forecasters, the bracketing rate is high, and there are at least 5–10 sources of projections. In the case of fantasy football, however, a high proportion of players’ forecasts do not bracket the truth, suggesting that the so-called experts are not accurate in predicting players’ future fantasy performance. Nevertheless, going with the crowd-averaged projections tends to be more accurate than relying on an individual projection source. Performance in fantasy football is around half and half luck versus skill. Sports betting and gambling on fantasy football involve components of variable ratio reinforcement that lend them to being potentially highly addictive.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "decision-making.html#sec-decisionMakingSessionInfo",
    "href": "decision-making.html#sec-decisionMakingSessionInfo",
    "title": "26  Decision Making in the Context of Uncertainty",
    "section": "\n26.11 Session Info",
    "text": "26.11 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.1.0       readr_2.1.5       tidyr_1.3.1       tibble_3.3.0     \n [9] ggplot2_3.5.2     tidyverse_2.0.0   petersenlab_1.2.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   psych_2.5.6        viridisLite_0.4.2  farver_2.1.2      \n [5] fastmap_1.2.0      digest_0.6.37      rpart_4.1.24       timechange_0.3.0  \n [9] lifecycle_1.0.4    cluster_2.1.8.1    magrittr_2.0.3     compiler_4.5.1    \n[13] rlang_1.1.6        Hmisc_5.2-3        tools_4.5.1        yaml_2.3.10       \n[17] data.table_1.17.8  knitr_1.50         labeling_0.4.3     htmlwidgets_1.6.4 \n[21] mnormt_2.1.1       plyr_1.8.9         RColorBrewer_1.1-3 foreign_0.8-90    \n[25] withr_3.0.2        nnet_7.3-20        grid_4.5.1         stats4_4.5.1      \n[29] lavaan_0.6-19      xtable_1.8-4       colorspace_2.1-1   scales_1.4.0      \n[33] MASS_7.3-65        cli_3.6.5          mvtnorm_1.3-3      rmarkdown_2.29    \n[37] reformulas_0.4.1   generics_0.1.4     rstudioapi_0.17.1  reshape2_1.4.4    \n[41] tzdb_0.5.0         minqa_1.2.8        DBI_1.2.3          splines_4.5.1     \n[45] parallel_4.5.1     base64enc_0.1-3    mitools_2.4        vctrs_0.6.5       \n[49] boot_1.3-31        Matrix_1.7-3       jsonlite_2.0.0     hms_1.1.3         \n[53] Formula_1.2-5      htmlTable_2.4.3    glue_1.8.0         nloptr_2.2.1      \n[57] stringi_1.8.7      gtable_0.3.6       quadprog_1.5-8     lme4_1.1-37       \n[61] pillar_1.11.0      htmltools_0.5.8.1  R6_2.6.1           Rdpack_2.6.4      \n[65] mix_1.0-13         evaluate_1.0.4     pbivnorm_0.6.0     lattice_0.22-7    \n[69] rbibutils_2.3      backports_1.5.0    Rcpp_1.1.0         gridExtra_2.3     \n[73] nlme_3.1-168       checkmate_2.3.3    xfun_0.53          pkgconfig_2.0.3   \n\n\n\n\n\n\nAmlung, M., Vedelago, L., Acker, J., Balodis, I., & MacKillop, J. (2017). Steep delay discounting and addictive behavior: A meta-analysis of continuous associations. Addiction, 112(1), 51–62. https://doi.org/10.1111/add.13535\n\n\nBrown, M., Grasley, N., & Guido, M. (2025). Do sports bettors need consumer protection? Evidence from a field experiment. https://mattbrownecon.github.io/assets/papers/jmp/sportsbetting.pdf\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nGetty, D., Li, H., Yano, M., Gao, C., & Hosoi, A. E. (2018). Luck and the law: Quantifying chance in fantasy sports and other contests. SIAM Review, 60(4), 869–887. https://doi.org/10.1137/16m1102094\n\n\nGray, J. C., & MacKillop, J. (2015). Impulsive delayed reward discounting as a genetically-influenced target for drug abuse prevention: A critical evaluation. Frontiers in Psychology, 6. https://doi.org/10.3389/fpsyg.2015.01104\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nKartes, J. (2024). Which fantasy football projections are most accurate? https://fantasyfootballanalytics.net/2024/12/which-fantasy-football-projections-are-most-accurate.html\n\n\nLarrick, R. P., Mannes, A. E., & Soll, J. B. (2024). The social psychology of the wisdom of crowds (with a new section on recent advances). In F. M. Federspiel, G. Montibeller, & M. Seifert (Eds.), Behavioral decision analysis (pp. 121–143). Springer. https://doi.org/10.1007/978-3-031-44424-1_7\n\n\nMannes, A. E., Soll, J. B., & Larrick, R. P. (2014). The wisdom of select crowds. Journal of Personality and Social Psychology, 107(2), 276–299. https://doi.org/10.1037/a0036677\n\n\nPetersen, I. T. (2017). Who has the best fantasy football projections? 2017 update. https://fantasyfootballanalytics.net/2017/03/best-fantasy-football-projections-2017.html\n\n\nPetersen, I. T. (2025). petersenlab: A collection of R functions by the Petersen Lab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nRader, C. A., Larrick, R. P., & Soll, J. B. (2017). Advice as a form of social influence: Informational motives and the consequences for accuracy. Social and Personality Psychology Compass, 11(8), e12329. https://doi.org/10.1111/spc3.12329\n\n\nSimoiu, C., Sumanth, C., Mysore, A., & Goel, S. (2019). Studying the \"wisdom of crowds\" at scale. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 7(1), 171–179. https://doi.org/10.1609/hcomp.v7i1.5271\n\n\nSpielman, R. M., Jenkins, W. J., & Lovett, M. D. (2020). Psychology (2nd ed.). OpenStax. https://openstax.org/details/books/psychology-2e\n\n\nSurowiecki, J. (2005). The wisdom of crowds. Anchor Books.\n\n\nTanney, M. (2021). R in sports analytics. https://www.youtube.com/watch?v=1zCDWtNEucI\n\n\nWagner, C., & Vinaimont, T. (2010). Evaluating the wisdom of crowds. Issues in Information Systems, 11(1), 724–732. http://iacis.org/iis/2010/724-732_LV2010_1546.pdf\n\n\nWeinsztok, S., Brassard, S., Balodis, I., Martin, L. E., & Amlung, M. (2021). Delay discounting in established and proposed behavioral addictions: A systematic review and meta-analysis. Frontiers in Behavioral Neuroscience, 15. https://doi.org/10.3389/fnbeh.2021.786358",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Decision Making in the Context of Uncertainty</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html",
    "href": "sports-cognitive-psychology.html",
    "title": "27  Sports and Cognitive Psychology",
    "section": "",
    "text": "27.1 Getting Started\nThis chapter provides an overview of some of the roles of cognitive psychology and analytics in sports.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyGettingStarted",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyGettingStarted",
    "title": "27  Sports and Cognitive Psychology",
    "section": "",
    "text": "27.1.1 Load Packages\n\nCodelibrary(\"nflreadr\")\nlibrary(\"tidyverse\")\nlibrary(\"ggtext\")\n\n\n\n27.1.2 Download Football Data\n\nCodeload(file = \"./data/nfl_pbp.RData\")\nload(file = \"./data/nfl_4thdown.RData\")",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyOverview",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyOverview",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.2 Overview",
    "text": "27.2 Overview\nThere are a number of ways in which analytics have changed sports [Underwood (2019); archived at https://perma.cc/PQ5R-TWFA]. Here is a summary of some of the key ways that analytics has led to changes.\nMuch of the history of analytics in sports traces back to Bill James. James was a member of the Society for American Baseball Research (SABR). He published a series of Baseball Abstracts, which included players’ statistics. James published the first Baseball Abstract in 1977 (https://www.pbs.org/thinktank/transcript1197.html; archived at https://perma.cc/Y7J7-GB9V). He coined the term Sabermetrics (which was originally named SABRmetrics, based on the name of the society) to refer to advanced metrics and statistical/empirical analysis of baseball. James believed that traditional statistics like batting average, runs batted in (RBIs), and pitchers’ wins did not reflect a player’s true value—i.e., their contribution to the team’s success. Instead, he advocated for the use of more advanced metrics that could provide a deeper understanding of a player’s performance and value such as on-base percentage (OBP), slugging percentage (SLG), wins above replacement (WAR), and fielding independent pitching (FIP). He also developed advanced metrics such as runs created (RC) and defense efficiency rating (DER).\nJames’ ideas were slow to catch on among those in baseball. Nevertheless, some people eventually caught on to his ideas—and to good success. Billy Beane, a general manager, used Sabermetrics to help the Oakland Athletics, a small market team with a limited budget, better compete with teams with larger budgets. He used statistics such as on-base percentage to identify player value more accurately, especially for identifying undervalued players. The story was described in Michael Lewis’ book, Moneyball, which was turned into a movie. Following publication of Moneyball, Theo Epstein, who was president of the Boston Red Sox and then of the Chicago Cubs, used sabermetrics to help each win the World Series.\nIn addition to teams using Sabermetrics to evaluate player talent, teams also began to frequently use statistical analysis to inform decision making during games, which led to key changes in the style of play. For instance, defensive shifts—where defensive players moved to locations on the field where particular hitters were most likely to hit the ball—became more common, attempts to steal bases became less common, there were fewer bunts, batters took more pitches (i.e., watched more pitches without swinging), there were more frequent pitching changes (for particular pitcher–batter matchups; such as to have a right-handed pitcher face a left-handed batter or vice versa), and a greater focus on velocity and spin rate among pitchers. Some of these analytics-driven changes in play style eventually led Major League Baseball (MLB) to make rule changes in an attempt to make the game more exciting to watch, including banning defensive shifts, reducing the number of pitching changes allowed, and making the bases larger and easier to steal.\nAlthough baseball was one of the first major sports to embrace analytics, other sports have been transformed by analytics, as well. For instance, in basketball, there has been a greater focus on three points and buckets close to the rim (e.g., dunks and layups), with way fewer midrange shots (Partnow, 2021). Moreover, star players rest more games (“load management”). Here is an article on how Shane Battier, who was not the quickest, fastest, or most athletic player, used analytics successfully to guard players who were quicker or more athletic than him: https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html [Lewis (2009); archived at https://web.archive.org/web/20250219225627/https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html].\nFootball has also seen greater use of analytics, although its uptake has been somewhat slower than in many other sports (Carroll et al., 2023). When asked about how he makes the decision about whether to go for a two-point conversion after a touchdown, Steelers Head Coach Mike Tomlin stated:\n\n“We work a menu of plays in that area over the course of the week. We rank them at the latter part of the week, and then we get into the stadium and we play it by ear. A lot of it has to do with the feel or the flow of the game—maybe what personnel group we think they’re going to match our personnel group with. As we start to play and work the ball down the field on the drives that produce the touchdowns before the point after, we have a little inclination of what their personality might be at least in terms of matching our personnel. All of those things weigh into the decision. It legitimately is a feel thing. I think that’s why you play the game. You can take analytics to baseball and things like that, but football is always going to be football. I got a lot of respect for analytics and numbers, but I’m not going to make judgments based on those numbers. The game is the game. It’s an emotional one played by emotional and driven men. That’s an element of the game you can’t measure. Oftentimes, decisions such as that weigh heavily into the equation.”\n— Steelers Head Coach Mike Tomlin, 2015 (archived at https://perma.cc/7CHJ-BTWX)\n\nThat is, Mike Tomlin, suggested that he does not take into account analytics when making decisions. Perhaps that is why the Pittsburgh Steelers were voted one of the least analytically advanced teams in the NFL in 2020 [Walder (2020); archived at https://perma.cc/R7FA-HGGB]. Nevertheless, the Steelers did eventually hire a person focused on analytics [Fowler (2015); archived at https://perma.cc/W7VE-JZQX].\nA player talent evaluator for the NFL noted that many people around the league believe that the use of analytics is better suited for baseball than football because baseball involves more games, players, and one-on-one matchups [Reed (2016); archived at https://web.archive.org/web/20200803205803/https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html]. By contrast, player performance in football may be more dependent on teammates and play calls. The player talent evaluator noted, “In football they don’t know how to use all the numbers yet…They have the data, they have all these different stats but the people I have talked to aren’t completely sure how to use it.” [Reed (2016); archived at https://web.archive.org/web/20200803205803/https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html]. Some dismiss analytics entirely.\nAs an example of people dismissing analytics, Figure 27.1 is a 2015 video clip of a football commentator, Mike Patrick, making fun of people who compile statistics for informing decision-making in football, calling the people who compile the numbers “guys who wear socks with flip-flops” (ABC, 2015a). His characterization suggests he believes that people who compile analytics to make recommendations in football are out of touch with the realities of football and do not have the same on-the-field understanding as the coaches and players. The full game is available to watch here (ABC, 2015b).\n\n\n\n\n\nFigure 27.1\n\n\nNevertheless, more and more people around the league use analytics—likely because it works; for examples of articles, see here [Clark (2018); archived at https://perma.cc/7BKP-A4GJ] and here [Awbrey (2020); archived at https://perma.cc/WXE3-53E6]. Head Coach Doug Peterson heavily relied on analytics with the Philadelphia Eagles to help them win the 2017 Super Bowl; for more information, see here [Fox (2021); archived at https://perma.cc/49KQ-R785] and here [Rosenthal (2018); archived at https://perma.cc/2GRF-8Z5K].\nExamples of changes in football due to the use of analytics include more often going for it on fourth down, a greater emphasis on the passing game, drafting Running Backs later in the draft (and, more generally, valuing Running Backs less), and trading down in the draft to obtain more low picks (rather than having fewer high picks) because top picks are frequently overvalued (Massey & Thaler, 2013).",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-coachingRiskAversion",
    "href": "sports-cognitive-psychology.html#sec-coachingRiskAversion",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.3 Coaching and Loss/Risk Aversion",
    "text": "27.3 Coaching and Loss/Risk Aversion\n\n27.3.1 Going for It on Fourth Down\nIt had been known for a long time that going for it on fourth down would frequently increase a team’s chances of winning. Despite that, historically, teams rarely went for it on fourth down and elected to punt or kick a field goal instead. It is curious that there was such a discrepancy between the decisions that would maximize teams’ winning percentage and the decisions coaches actually made. In many fourth down situations, by punting the ball, coaches actively and systematically made decisions that reduced their team’s chances of winning. One potential explanation for the discrepancy is because of coaches’ risk aversion. As noted in Section 14.4.10, when it is possible to experience either a gain or a loss from a decision, loss aversion bias tends to lead people to make risk-averse decisions (Kahneman, 2011). According to this idea, in the case of failing to successfully convert on fourth down, coaches do not want to have to defend their decision to go for it to the media or the owner or general manager. That is, they may often play not to lose, rather than to win, in order to keep their job. However, in many cases, this is the wrong choice (Moskowitz & Wertheim, 2011).\nRates of going for it on fourth down in the NFL were low until 2017. After the 2017 season, rates of going for it on fourth down increased dramatically, as depicted in Figure 27.2.\n\nCodenfl_pbp4thDown &lt;- nfl_pbp %&gt;% \n  filter(down == 4) %&gt;% \n  filter(!(play_type %in% c(\"no_play\",\"qb_kneel\")))\n\nnfl_pbp4thDown$goForIt &lt;- NA\nnfl_pbp4thDown$goForIt[which(nfl_pbp4thDown$play_type %in% c(\"field_goal\",\"punt\"))] &lt;- 0\nnfl_pbp4thDown$goForIt[which(nfl_pbp4thDown$play_type %in% c(\"pass\",\"run\"))] &lt;- 1\n\nnfl_pbp4thDownPlotData &lt;- nfl_pbp4thDown %&gt;% \n  filter(!is.na(goForIt)) %&gt;% \n  group_by(season) %&gt;% \n  summarise(\n    goForItPct = mean(goForIt, na.rm = TRUE),\n    n = n(),\n    sd = sd(goForIt),\n    se = sd / n\n  )\n\nggplot2::ggplot(\n  data = nfl_pbp4thDownPlotData,\n  ggplot2::aes(\n    x = season,\n    y = goForItPct)) +\n  geom_point() +\n  geom_line() +\n  geom_ribbon(\n    aes(\n      y = goForItPct,\n      ymin = goForItPct - qnorm(0.975)*se,\n      ymax = goForItPct + qnorm(0.975)*se),\n    alpha = 0.2) +\n  scale_y_continuous(\n    limits = c(0, NA)\n  ) +\n  scale_x_continuous(\n    minor_breaks = seq(0, 3000, 1),\n    breaks = seq(0, 3000, 5)\n  ) +\n  labs(\n    x = \"Season\",\n    y = \"Proportion of 4th Down Plays that\\nare Attempts (to Get the First Down)\",\n    title = \"4th Down Attempts (Proportion) by Season\",\n  ) +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) + # horizontal y-axis title\n  annotate(\n    \"segment\",\n    x = 2015,\n    xend = 2017,\n    y = 0.08,\n    yend = 0.123,\n    color = \"blue\",\n    linewidth = 1.5,\n    alpha = 0.6,\n    arrow = arrow()) +\n  annotate(\n    \"text\",\n    x = 2015,\n    y = 0.07,\n    label = \"Rapid increases in 4th down\\nattempts after the 2017 season\",\n    hjust = 0.5) # center\n\n\n\n\n\n\nFigure 27.2: The Proportion of Fourth Downs That are Attempts to Go for it (Rather Than Punts or Field Goals).\n\n\n\n\nAs depicted in the Figure 27.3 [which was adapted from a figure by Ben Baldwin; Baldwin (2023); archived at https://perma.cc/S5D8-3NCU], teams went for it on fourth down nearly twice as often in 2021 compared to 2017, when when their win probability was at least 20% (given the current situation at the start of the given play) and they would increase their win probably by going for it. We use the ggtext package (Wilke & Wiernik, 2022) to annotate text:\n\nCode# labels on the plot\ntext_df &lt;- tibble(\n  label = c(\n    \"NFL coaches&lt;br&gt;in &lt;span style='color:#00BFC4'&gt;**2021**&lt;/span&gt;\",\n    \"NFL coaches&lt;br&gt;in &lt;span style='color:#F8766D'&gt;**2017**&lt;/span&gt;\"),\n  x = c(5, 7),\n  y = c(87.5, 22.5),\n  angle = c(10, 10),\n  color = c(\"black\", \"black\")\n)\n\nnfl_4thdown %&gt;%\n  filter(\n    vegas_wp &gt; .2,\n    between(go_boost, -10, 10),\n    season %in% c(2017, 2021)) %&gt;%\n  ggplot(\n    aes(\n      x = go_boost,\n      y = go,\n      color = as.factor(season))) + \n  geom_vline(xintercept = 0) +\n  stat_smooth(\n    method = \"gam\",\n    method.args = list(gamma = 1),\n    formula = y ~ s(x, bs = \"cs\", k = 10),\n    show.legend = FALSE,\n    se = FALSE,\n    linewidth = 4) +\n  # this is just to get the plot to draw the full 0 to 100 range\n  geom_hline(\n    yintercept = 100,\n    alpha = 0) +\n  geom_hline(\n    yintercept = 0,\n    alpha = 0) +\n  ggtext::geom_richtext(\n    data = text_df,\n    aes(\n      x,\n      y,\n      label = label,\n      angle = angle),\n    color = \"black\",\n    fill = NA,\n    label.color = NA,\n    size = 5) + \n  theme_classic() +\n  labs(\n    x = \"Gain in win probability by going for it\",\n    y = \"Go-for-it percentage\",\n    subtitle = \"4th down decisions in 2021 versus 2017, win prob. &gt; 20%\",\n    title = glue::glue(\"How &lt;span style='color:red'&gt;math&lt;/span&gt; is changing football\")) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_markdown(\n      size = 22,\n      hjust = 0.5),\n    plot.subtitle = element_markdown(\n      size = 13.5,\n      hjust = 0.5),\n    axis.title.x = element_text(\n      size = 14,\n      face = \"bold\"),\n    axis.title.y = element_text(\n      size = 14,\n      face = \"bold\",\n      angle = 0, # horizontal y-axis title\n      vjust = 0.5)\n  ) +\n  scale_y_continuous(\n    breaks = scales::pretty_breaks(n = 4),\n    limits = c(0, 100),\n    expand = c(0, 0)) +\n  scale_x_continuous(\n    breaks = scales::pretty_breaks(n = 10),\n    limits = c(-10, 10),\n    expand = c(0, 0)) +\n  annotate(\n    \"text\",\n    x = -1.5,\n    y = 70,\n    label = \"Should\\nkick\",\n    color = \"black\",\n    size = 5) +\n  annotate(\n    \"text\",\n    x = 1.75,\n    y = 70,\n    label = \"Should\\ngo for it\",\n    color = \"black\",\n    size = 5) +\n  annotate(\n    \"segment\",\n    x = -.1,\n    y = 80,\n    xend = -2,\n    yend = 80,\n    arrow = arrow(length = unit(0.05, \"npc\")),\n    color = \"black\",\n    linewidth = 2) +\n  annotate(\n    \"segment\",\n    x = .1,\n    y = 80,\n    xend = 2,\n    yend = 80,\n    arrow = arrow(length = unit(0.05, \"npc\")),\n    color = \"black\",\n    linewidth = 2)\n\n\n\n\n\n\nFigure 27.3: Go-For-It Percentage on Fourth Downs in 2021 Versus 2017 as a Function of the Change in Win Probability for Going for it. To remove desperation fourth down attempts, the figure focuses on just plays in which the estimated win probability for the offensive team (given the current situation at the start of the given play and incorporating the pre-game Vegas line) was greater than 20%.\n\n\n\n\nNFL teams were later to implement analytics into their decision making than even teams at lower levels of the sport. Some high school teams, such as the Pulaski Academy Bruins (Little Rock, AR), coached by Kevin Kelley, consistently went for it on fourth down well before 2017 (Moskowitz & Wertheim, 2011).\n\n[Bruins Head Football Coach Kevin] Kelley believes that the “quant jocks” don’t go far enough to validate the no-punting worldview and, more generally, the virtues of risk-taking. “The math guys, the astrophysicist guys, they just do the raw numbers and they don’t figure emotion into it—and that’s the biggest thing of all,” he says. “The built-in emotion involved in football is unbelievable, and that’s where the benefits really pay off.” What he means is this: A defense that stops an opponent on third down is usually ecstatic. They’ve done their job. The punting unit comes on, and the offense takes over. When that defense instead gives up a fourth-down conversion, it has a hugely deflating effect. At Pulaski’s games, you can see the shoulders of the opposing defensive players slump and their eyes look down when they fail to stop the Bruins on fourth down.\n— (Moskowitz & Wertheim, 2011, p. 37)\n\nBased on Romer’s (2006) analysis of third down plays (because few teams went for it on fourth down), focusing on plays in the first quarter (to remove desperation plays), he identified several general conclusions about fourth-down situations:\n\nInside the opponent’s 45-yard line, a team is better off going for it than punting with 6 (or less) yards to go (for a first down).\nInside the opponent’s 33-yard line, a team is better off going for it with 10 (or less) yards to go (unless little time remains and a field goal would decide the game).\nOnce reaching the opponent’s 5-yard line, a team is better off going for it.\nRegardless of field position, a team is always better off going for it with 3 (or less) yards to go.\n\nNevertheless, out of the fourth down plays Romer (2006) identified between 1998–2000 in which it would have been advantageous to go for it, the team made the suboptimal decision (punting or kicking) 90% of the time. “Inasmuch as an academic paper can become a cult hit, Romer’s made the rounds in NFL executive offices, but most NFL coaches seemed to dismiss his findings as the handiwork of an egghead, polluting art with science.” (Moskowitz & Wertheim, 2011, p. 39).\nHere is an analysis by the New York Times on when to go for it on fourth down: https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html [NYT 4th Down Bot (2014); archived at https://perma.cc/KA9Y-BRUD]. Here is a fourth down calculator: https://rbsdm.com/stats/fourth_calculator/.\n\n\n\n\n\nFigure 27.4: Analysis of When to Go for it on Fourth Down Versus What Coaches Actually Do on 4th Down (From 2014). Retrieved from https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html (archived at https://perma.cc/KA9Y-BRUD).\n\n\nIn addition to punting on fourth down, loss aversion bias may lead coaches to make more conservative play-calling, in general, such as running the ball when passing would tend to be more advantageous.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#challengingCall",
    "href": "sports-cognitive-psychology.html#challengingCall",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.4 Challenging a Call",
    "text": "27.4 Challenging a Call\nA coach can challenge a call (and have it reviewed) by throwing a red flag onto the field. If, after the review, the play is reversed, the team does not lose a timeout; however, if the play stands as originally called, the team loses a timeout. One task is for coaches to determine when it is optimal to challenge a call. NFL.com provides an opportunity for you to determine whether you should challenge a call: https://operations.nfl.com/learn-the-game/making-the-call/you-make-the-call/.\nTanney (2021) provides a framework for determining whether a coach should challenge a call, based on the expected value of the change in winning probability of a reversal versus a non-reversal:\n\\[\n\\begin{aligned}\n  \\text{expected value} = \\; &\\text{Sum}(\\text{Probabilities} \\times \\text{Payouts}) \\\\\n  = \\; &(\\text{Reversal \\%})(\\text{Positive change in Winning Probability of a Reversal}) + \\\\\n  &(1 - \\text{Reversal \\%})(\\text{Negative change in Winning Probability of Losing a Timeout})\n\\end{aligned}\n\\tag{27.1}\\]",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-beliefsInSports",
    "href": "sports-cognitive-psychology.html#sec-beliefsInSports",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.5 Other Beliefs in Sports",
    "text": "27.5 Other Beliefs in Sports\n\n27.5.1 Icing the Kicker\nIn a 2008 game between the Jets and Raiders, Jets Kicker Jay Feely missed a game-tying field goal to send the game to overtime. However, moments before the snap, the Raiders called a timeout in an attempt to “ice the kicker.” The idea behind icing the kicker is making the kicker think about the pressure of upcoming kick so he gets “cold feet” (metaphorically speaking) and misses the kick. After the brief timeout, Feely got a second chance and made the field goal to send the game into overtime.\nAfter the game, Feely noted, “I heard the whistle before I started, which is an advantage to the kicker…If you’re going to do that, do that before he kicks. I can kick it down the middle, see what the wind does and adjust. It helps the kicker tremendously.” [The Associated Press (2008); archived at https://perma.cc/U7SV-UE2E].\nHowever, does icing the kicker work? There is mixed evidence on icing the kicker. Some studies find some evidence for lower percentage of iced kicks made compared to non-iced kicks (Goldschmied et al., 2025), whereas other studies do not (e.g., Berry & Wood, 2004; Gonzalez Sanchez et al., 2024; Moskowitz & Wertheim, 2011). This suggests that, if there is such an effect, the effect size is likely very small, and it may better suit the team to use your timeouts in other situations.\n\n27.5.2 Hot Hand\nThe belief in the “hot hand” is described in Section 14.5.3.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-psychologicalFactorsPerformance",
    "href": "sports-cognitive-psychology.html#sec-psychologicalFactorsPerformance",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.6 Psychological Factors in Player Performance",
    "text": "27.6 Psychological Factors in Player Performance\nMeta-analyses have demonstrated that there are a number of psychological factors that influence player performance in sports. For instance, a meta analysis by Lochbaum et al. (2022) found that psychological processes such as confidence and mindfulness were associated with better performance in sports.\nAthletes sometimes describe periods of optimal performance where they are “in the zone.” These periods of mental state are called “flow”, and are characterized by the following subjective characteristics (Nakamura & Csikszentmihalyi, 2021):\n\nintense and focused concentration on the present moment\nmerging of action and awareness\nloss of reflective self-consciousness (i.e., loss of awareness of oneself as a social actor)\na sense that one can control one’s actions and deal with the situation because one knows how to respond to whatever happens next\ndistorted sense of time (often, a sense that time has passed faster than usual)\nexperiencing the activity as intrinsically rewarding\n\nBecause these periods often coincide with strong performance, athletes commonly strive to find ways to achieve flow.\nOne process that may have a nonlinear association with performance is arousal or anxiety. The Yerkes–Dodson law describes the association between arousal and performance, as depicted in Figure 27.5. For simple tasks, performance tends to increase with arousal. For difficult tasks, there is an inverted-U-shaped association between arousal and performance, as depicted in Figure 27.6. For difficult tasks, too little arousal is bad because it leads to insufficient attention on the task at hand; however, too much arousal is also bad because anxiety can impair performance through its negative effects on attention (e.g., “tunnel vision”), memory, decision making, problem-solving, and multitasking. That is, there is an optimal range of arousal for complex and challenging tasks, such as those in sports, which often require divided attention and multitasking.\n\n\n\n\n\nFigure 27.5: Yerkes–Dodson law. For simple tasks, performance increases with arousal. For difficult tasks, there is an inverted-U-shaped association between arousal and performance such that too little and too much arousal impairs performance. (Figure from https://en.wikipedia.org/wiki/File:OriginalYerkesDodson.svg.)\n\n\n\n\n\n\n\nFigure 27.6: Yerkes–Dodson law for difficult tasks. For difficult tasks, there is an inverted-U-shaped association between arousal and performance. Too little arousal is bad because it leads to insufficient attention on the task at hand; by contrast, too much arousal is also bad because anxiety can impair performance through its negative effects on attention (e.g., “tunnel vision”), memory, decision making, problem-solving, and multitasking. (Figure from https://en.wikipedia.org/wiki/File:HebbianYerkesDodson.svg.)",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-expertPerformanceAutomaticity",
    "href": "sports-cognitive-psychology.html#sec-expertPerformanceAutomaticity",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.7 Expert Performance and Automaticity",
    "text": "27.7 Expert Performance and Automaticity\nThis section was adapted from course materials from Motz (2018).\nAutomaticity involves fast and effortless processing that requires little or no focused attention. The more automatic a process is, the less likely a person is to be consciously aware of it. For instance, breathing is a fairly automatic process—people can breathe even when they are not concentrating on breathing. Chewing food can also be automatic. Automaticity helps free up resources for more demanding tasks. Football players and others often face situations requiring multitasking—i.e., performing two (or more) tasks simultaneously. For instance, a Quarterback must look for open receivers, monitor approaching defenders, move to open space when necessary, and make an accurate, well-timed throw.\nMultitasking is common in everyday life, but some tasks may suffer more from divided attention than others. Driving, for example, is a complex task that may be impacted by cognitive load from secondary tasks like phone conversations. It is an important question of how well people are able to multitask. For instance, we could ask whether talking on a cell phone and driving simultaneously drains cognitive resources. If so, does talking on a phone while driving impair driving performance or conversation skills? Both tasks (conversing or driving) are highly practiced, so we might expect there to be little cost to dividing attention across these two tasks. This provides a nice opportunity to examine automaticity.\nMany countries have banned hand-held cell phone use while driving. Some countries have banned all use of cell phones, including hands-free, while driving. However, states in the United States differ in their cell phone laws.\nAlmost all cell phone bans restrict the use of handheld cell phones while driving but do not restrict the use of hands-free devices. This is implicitly based on the assumption, from the peripheral-interference hypothesis, that any impairment to driving while talking on a cell phone is due to the use of a handheld device. However, an alternative possibility, the attentional hypothesis, posits that that driving impairment with cell phone use is due to dividing one’s attention. If this is the case, it would impact driving with both handheld and hands-free devices.\nA study by Redelmeier & Tibshirani (1997) found that 25% of a sample of 699 accidents involved drives that had spoken on a cell phone within 10 minutes prior to the accident. The use of handheld and hands-free phone were equally associated with accidents suggesting divided attention is the problem, not the use of a peripheral device. However, this association was observed from a correlational design; thus, we cannot conclude that cell phone use directly caused the accidents. It may be, for instance, that risk-taking individuals or individuals in a strongly emotional state may be more likely to both a) have car accidents and b) to use cell phones while driving, which might account for such an association. Thus, we need studies to examine the effects of multitasking using experimental designs to establish causality.\nStrayer & Johnston (2001) conducted an experimental study on multitasking. For the first task, the tracking task, participants tracked the position of a moving target on a computer screen. Every 10–20 seconds, the moving target flashed either red or green to simulate a traffic signal. If the target turned red (such as one might see when a traffic light turns red), the participant was supposed to push a “brake button”, which might simulate the act of needing to brake while driving. If the target turned green (such as one might see when a traffic light turns green), the participant was not supposed to do anything. During a dual-task phase, the participant was supposed to perform a second task while completing the tracking task. For the second task, participants were randomly assigned to one of four conditions: 1) conversing with an experimenter using a hand-held mobile device, 2) conversing with an experimenter using a hands-free device, 3) listening to a radio program of their choice, or 4) listening to a book on tape. The researchers found that the probability of missing a color change (i.e., a simulated traffic signal) more than doubled for the cell phone group when performing both the tracking and conversation tasks simultaneously (relative to performing only the tracking task). Reaction time also increased for the cell phone group when performing the dual tasks compared to when performing the single tracking task. However, there was no difference—in terms of the probability of missing a color change or reaction time—between the dual tasks and single task for the radio or book-on-tape control groups, suggesting that listening, by itself, does not interfere with the tracking task—rather, it appears that actively having a conversation (that includes talking and listening) interferes with tracking changes. Moreover, there was no difference between handheld or hands-free phones in terms of the probability of missing a color change or reaction time, suggesting that it was not the use of a hands-occupying device that caused their weaker performance in the tracking task—it was the act of dividing one’s attention across tasks (i.e., conversation task and tracking task) that led to weaker performance in tracking changes. Thus, the findings were consistent with the attentional hypothesis, and suggest that divided attention can interfere with performance—in addition to any deficits incurred due to peripheral factors such as handling the phone while dialing.\nMany people talk on the phone while driving. What does it mean that people are (somehow) able to perform these two tasks simultaneously? It suggests that those who use the cell phone while driving are treating one of the tasks—driving—as an automatic process and not giving it focused attention. Evidence suggests that talking on the phone while driving increases the risk of accident to levels comparable to driving drunk [i.e., a blood alcohol level above the legal limit; Redelmeier & Tibshirani (1997)]. The cognitive task of talking on the phone uses attentional resources that would otherwise be used for driving a car.\nStrayer & Drew (2004) also replicated their findings using a driving simulator, which is a more ecologically valid assessment of driving performance than the tracking task. In the study, using the driving simulator, participants drove on a highway in both single-task (i.e., driving only) and dual-task (i.e., driving while talking on a cell phone) conditions. Participants drove while following a car that braked at various times. The authors found that use of a cell phone led drivers to show a 18% slower brake onset time, 12% greater following distance, 17% longer half-recovery time (the time to recover 50% of the speed that was lost during braking), and twice as many rear-end collisions—all despite no general differences in speed. Although older drivers were poorer overall in driving performance, the effects of cell phone use did not differ for younger (~20 years of age) and older (~70 years of age) drivers, suggesting that the driving performance for both younger and older adults is impaired by cell phone use. Indeed, the reaction time of younger drivers who were talking on a cell phone was equivalent to the reaction time of older drivers who were not using a cell phone.\nThe findings described above might suggest that more attention leads to better performance. Based on that assumption, we should tell a Quarterback who has thrown multiple interceptions during a game to pay more attention to his throwing mechanics when throwing the ball, right? However, it is not always the case that devoting more attention to a task leads to better performance. There are two primary competing theories for “choking” in pressure situations in sports. “Choking” refers to underperforming relative to one’s ability (Beilock & Carr, 2001).\nDistraction theories posit that the pressure situation creates a distracting environment that shifts one’s attention to cues that are irrelevant to the task, such as one’s worries about the situation and its consequences (Beilock & Carr, 2001).\nAn alternative possibility, based on explicit monitoring theory, is that pressure situations raise one’s self-awareness and anxiety about performing well, which increases one’s attention to the step-by-step procedures involved in skill processes, which may disrupt well-learned or proceduralized performance (Beilock & Carr, 2001). Beilock & Carr (2001) found that, among automatic tasks (e.g. putting in golf among highly trained individuals), focusing too much on the task can actually hinder performance, consistent with explicit monitoring theory. For highly practiced experts, automatic skill tasks might include tasks like hitting/putting a golf ball, pitching a baseball, throwing/kicking a football, shooting free throws in basketball, shooting arrows in archery, serving in tennis, etc. Sports players often refer to highly practiced tasks as involving “muscle memory.” When a task is highly practiced or automatic, excessive attention can hurt performance. That is, more attention can lead to worse performance. This is called “choking under pressure” and is based on explicit monitoring theory.\nBy contrast, for tasks that require conscious control, decision-making, and real-time adjustments, more focused attention tends to lead to better performance. Tasks that require real-time adjustment include tasks such as avoiding unexpected hazards while driving or trying to avoid rushers while operating as a Quarterback. Unpredictable events occur when driving and when operating as a Quarterback; when unpredictable things happen and one’s attentions is diverted, one’s performance is likely to be impaired (Strayer & Johnston, 2001). In sum, devoting explicit attention to a task appears to improve performance when the task requires deliberate cognitive processing but can hurt performance when the task is highly automatic.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyConclusion",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologyConclusion",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.8 Conclusion",
    "text": "27.8 Conclusion\nFootball has recently seen greater use of analytics; however, its uptake has been slower compared to other sports like baseball and basketball. Examples of changes in football due to the use of analytics include more often going for it on fourth down, a greater emphasis on the passing game, drafting Running Backs later in the draft (and, more generally, valuing Running Backs less), and trading down in the draft to obtain more low picks (rather than having fewer high picks) because top picks are frequently overvalued. Historically, despite the decision hurting their chances of winning, teams rarely went for it on fourth down and elected to punt or kick a field goal instead, likely due to loss aversion bias. Loss aversion bias may lead coaches to make more conservative play-calling, in general, such as running the ball when passing would tend to be more advantageous. Rates of going for it on fourth down in the NFL were low until 2017. After the 2017 season, rates of going for it on fourth down increased dramatically. In addition to poor decisions relating to going for it on fourth down, there is not strong evidence that icing the kicker works, suggesting that it may better suit your team to use timeouts in other situations.\nThere are many psychological factors that influence sports performance. According to the Yerkes–Dodson law that describes the association between arousal and performance. For simple tasks, performance tends to increase with arousal. There is an optimal range of arousal for complex and challenging tasks, such as those in sports, which often require divided attention and multitasking. Too little arousal is bad because it leads to insufficient attention on the task at hand; however, too much arousal is also bad because anxiety can impair performance through its negative effects on attention (e.g., “tunnel vision”), memory, decision making, problem-solving, and multitasking. When a task is highly practiced or automatic, excessive attention can hurt performance. By contrast, for tasks that require conscious control, decision-making, and real-time adjustments, more focused attention tends to lead to better performance.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologySessionInfo",
    "href": "sports-cognitive-psychology.html#sec-sportsCognitivePsychologySessionInfo",
    "title": "27  Sports and Cognitive Psychology",
    "section": "\n27.9 Session Info",
    "text": "27.9 Session Info\n\nCodesessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 24.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggtext_0.1.2    lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1  \n [5] dplyr_1.1.4     purrr_1.1.0     readr_2.1.5     tidyr_1.3.1    \n [9] tibble_3.3.0    ggplot2_3.5.2   tidyverse_2.0.0 nflreadr_1.4.1 \n\nloaded via a namespace (and not attached):\n [1] generics_0.1.4     xml2_1.4.0         stringi_1.8.7      lattice_0.22-7    \n [5] hms_1.1.3          digest_0.6.37      magrittr_2.0.3     evaluate_1.0.4    \n [9] grid_4.5.1         timechange_0.3.0   RColorBrewer_1.1-3 fastmap_1.2.0     \n[13] jsonlite_2.0.0     Matrix_1.7-3       mgcv_1.9-3         scales_1.4.0      \n[17] cli_3.6.5          rlang_1.1.6        litedown_0.7       commonmark_2.0.0  \n[21] splines_4.5.1      withr_3.0.2        cachem_1.1.0       yaml_2.3.10       \n[25] tools_4.5.1        tzdb_0.5.0         memoise_2.0.1      vctrs_0.6.5       \n[29] R6_2.6.1           lifecycle_1.0.4    htmlwidgets_1.6.4  pkgconfig_2.0.3   \n[33] pillar_1.11.0      gtable_0.3.6       data.table_1.17.8  glue_1.8.0        \n[37] Rcpp_1.1.0         xfun_0.53          tidyselect_1.2.1   knitr_1.50        \n[41] farver_2.1.2       htmltools_0.5.8.1  nlme_3.1-168       rmarkdown_2.29    \n[45] labeling_0.4.3     compiler_4.5.1     markdown_2.0       gridtext_0.1.5    \n\n\n\n\n\n\nABC. (2015a). Clip from football game between Ohio State University and Northern Illinois University. https://streamable.com/1hw8\n\n\nABC. (2015b). Northern Illinois Huskies @ Ohio State Buckeyes - Sept. 19, 2015 - Shawun Lurry, Drew Hare. https://www.youtube.com/watch?v=1L9D2rp0mk8\n\n\nAwbrey, J. (2020). The future of NFL data analytics. https://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics\n\n\nBaldwin, B. (2023). nfl4th: Functions to calculate optimal fourth down decisions in the National Football League. https://doi.org/10.32614/CRAN.package.nfl4th\n\n\nBeilock, S. L., & Carr, T. H. (2001). On the fragility of skilled performance: What governs choking under pressure? Journal of Experimental Psychology: General, 130(4), 701–725. https://doi.org/10.1037/0096-3445.130.4.701\n\n\nBerry, S., & Wood, C. (2004). The cold-foot effect. CHANCE, 17(4), 47–51. https://doi.org/10.1080/09332480.2004.10554926\n\n\nCarroll, B., Palmer, P., & Thorn, J. (2023). The hidden game of football: A revolutionary approach to the game and its statistics. University of Chicago Press.\n\n\nClark, K. (2018). The NFL’s analytics revolution has arrived. https://www.theringer.com/2018/12/19/nfl/nfl-analytics-revolution\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell.\n\n\nFowler, J. (2015). Why the Steelers hired a Carnegie Mellon professor for advanced analytics. https://www.espn.com/blog/pittsburgh-steelers/post/_/id/14521/why-the-steelers-hired-a-carnegie-mellon-professor-for-advanced-analytics\n\n\nFox, L. (2021). How the NFL uses analytics, according to the lead analyst of a Super Bowl champion. https://www.forbes.com/sites/liamfox/2021/08/12/how-the-nfl-uses-analytics-according-to-the-lead-analyst-of-a-super-bowl-champion\n\n\nGoldschmied, N., Ratkovich, T., & Raphaeli, M. (2025). Brief report: Exploring the icing the kicker strategy in the NFL. Journal of Applied Sport Psychology, 37(3), 389–396. https://doi.org/10.1080/10413200.2024.2437166\n\n\nGonzalez Sanchez, A., Martinez, S., Yurko, R., Elmore, R., & Macdonald, B. (2024). Beyond the box score: Does icing the field goal kicker work in the NFL? CHANCE, 37(3), 41–48. https://doi.org/10.1080/09332480.2024.2415841\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar, Straus, and Giroux.\n\n\nLewis, M. (2009). The no-stats all-star. https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html\n\n\nLochbaum, M., Stoner, E., Hefner, T., Cooper, S., Lane, A. M., & Terry, P. C. (2022). Sport psychology and performance meta-analyses: A systematic review of the literature. PLOS ONE, 17(2), e0263408. https://doi.org/10.1371/journal.pone.0263408\n\n\nMassey, C., & Thaler, R. H. (2013). The loser’s curse: Decision making and market efficiency in the National Football League draft. Management Science, 59(7), 1479–1495. https://doi.org/10.1287/mnsc.1120.1657\n\n\nMoskowitz, T. J., & Wertheim, L. J. (2011). Scorecasting: The hidden influences behind how sports are played and games are won. Three Rivers Press.\n\n\nMotz, B. (2018). GitHub repository for COLL C-105: Prediction, Probability, and Pigskin. https://github.com/bmotz/prediction-probability-and-pigskin\n\n\nNakamura, J., & Csikszentmihalyi, M. (2021). The experience of flow theory and research. In C. R. Snyder, S. J. Lopez, L. M. Edwards, & S. C. Marques (Eds.), The Oxford handbook of positive psychology (pp. 279–296). Oxford University Press.\n\n\nNYT 4th Down Bot. (2014). 4th down: When to go for it and why. https://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html\n\n\nPartnow, S. (2021). The midrange theory: Basketball’s evolution in the age of analytics. Triumph Books.\n\n\nRedelmeier, D. A., & Tibshirani, R. J. (1997). Association between cellular-telephone calls and motor vehicle collisions. New England Journal of Medicine, 336(7), 453–458. https://doi.org/10.1056/NEJM199702133360701\n\n\nReed, T. (2016). In an NFL divided over analytics, Cleveland Browns look to make numbers add up in their favor. https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html\n\n\nRomer, D. (2006). Do firms maximize? Evidence from professional football. Journal of Political Economy, 114(2), 340–365. https://doi.org/10.1086/501171\n\n\nRosenthal, G. (2018). Super Bowl LII: How the 2017 Philadelphia Eagles were built. https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753\n\n\nStrayer, D. L., & Drew, F. A. (2004). Profiles in driver distraction: Effects of cell phone conversations on younger and older drivers. Human Factors, 46(4), 640–649. https://doi.org/10.1518/hfes.46.4.640.56806\n\n\nStrayer, D. L., & Johnston, W. A. (2001). Driven to distraction: Dual-task studies of simulated driving and conversing on a cellular telephone. Psychological Science, 12(6), 462–466. https://doi.org/10.1111/1467-9280.00386\n\n\nTanney, M. (2021). R in sports analytics. https://www.youtube.com/watch?v=1zCDWtNEucI\n\n\nThe Associated Press. (2008). Janikowski gives Raiders win over Jets in overtime. https://www.nfl.com/news/janikowski-gives-raiders-win-over-jets-in-overtime-09000d5d80bc3910\n\n\nUnderwood, A. (2019). 15 ways analytics has changed sports. https://stacker.com/stories/sports/15-ways-analytics-has-changed-sports\n\n\nWalder, S. (2020). 2020 NFL analytics survey: Which teams are most, least analytically inclined? https://www.espn.com/nfl/story/_/id/29939438/2020-nfl-analytics-survey-which-teams-most-least-analytically-inclined\n\n\nWilke, C. O., & Wiernik, B. M. (2022). ggtext: Improved text rendering support for ggplot2. https://doi.org/10.32614/CRAN.package.ggtext",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Sports and Cognitive Psychology</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "I need your help!\n    \n    I want your feedback to make the book better for you and other readers. If you find typos, errors, or places where the text may be improved, please let me know.\n    The best ways to provide feedback are by GitHub or hypothes.is annotations.\n    \n\n    \n      \n      You can leave a comment at the bottom of the page/chapter, or open an issue or submit a pull request on GitHub: https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook\n    \n    \n      \n      Alternatively, you can leave an annotation using hypothes.is.\n      To add an annotation, select some text and then click the\n      \n      \n    \n    \n      symbol on the pop-up menu.\n      To see the annotations of others, click the\n      \n\n      symbol in the upper right-hand corner of the page.\n    \n  \n\n\n\n\n\n\n\n4for4 Staff. (2018). The definitive guide to stacking on\nDraftKings. https://www.4for4.com/2018/preseason/definitive-guide-stacking-draftkings\n\n\nABC. (2015a). Clip from football game between Ohio State\nUniversity and Northern Illinois University. https://streamable.com/1hw8\n\n\nABC. (2015b). Northern Illinois Huskies @ Ohio\nState Buckeyes - Sept. 19, 2015 - Shawun\nLurry, Drew Hare. https://www.youtube.com/watch?v=1L9D2rp0mk8\n\n\nAden-Buie, G., Schloerke, B., Allaire, J., & Rossell Hayes, A.\n(2023). learnr: Interactive tutorials\nfor R. https://doi.org/10.32614/CRAN.package.learnr\n\n\nAdobe Express. (2020). 8 basic design principles to help you make\nawesome graphics. https://www.adobe.com/express/learn/blog/8-basic-design-principles-to-help-you-create-better-graphics\n\n\nÆgisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S.,\nAnderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K.,\nWalker, B. S., Cohen, G., & Rush, J. D. (2006). The meta-analysis of\nclinical judgment project: Fifty-six years of accumulated research on\nclinical versus statistical prediction. The Counseling\nPsychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875\n\n\nAIDSVu. (2022). Understanding the current HIV epidemic\nin the United States. https://map.aidsvu.org/profiles/nation/usa/overview\n\n\nAkinshin, A. (2023). Weighted quantile estimators. arXiv. https://doi.org/10.48550/arXiv.2304.07265\n\n\nAltarejos, J., & Hayward, R. (2025). Likelihood ratio\nnomogram. Centre for Health Evidence. https://jamaevidence.mhmedical.com/data/calculators/LR_nomogram.html\n\n\nAmlung, M., Vedelago, L., Acker, J., Balodis, I., & MacKillop, J.\n(2017). Steep delay discounting and addictive behavior: A meta-analysis\nof continuous associations. Addiction, 112(1), 51–62.\nhttps://doi.org/10.1111/add.13535\n\n\nAtanasov, P., Witkowski, J., Ungar, L., Mellers, B., & Tetlock, P.\n(2020). Small steps to accuracy: Incremental belief updaters are better\nforecasters. Organizational Behavior and Human Decision\nProcesses, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001\n\n\nAtaneka, A., Kelcey, B., Dong, N., Bulus, M., & Bai, F. (2023).\nPowerUp R Shiny app (v. 0.9) manual. https://www.causalevaluation.org/uploads/7/3/3/6/73366257/r_shinnyapp_manual_0.9.pdf\n\n\nAttali, D., & Baker, C. (2023). ggExtra: Add marginal histograms to ggplot2, and more ggplot2 enhancements. https://doi.org/10.32614/CRAN.package.ggExtra\n\n\nAustin, P. C., & Steyerberg, E. W. (2014). Graphical assessment of\ninternal and external calibration of logistic regression models by using\nloess smoothers. Statistics in Medicine, 33(3),\n517–535. https://doi.org/10.1002/sim.5941\n\n\nAvugos, S., Köppen, J., Czienskowski, U., Raab, M., & Bar-Eli, M.\n(2013). The “hot hand” reconsidered: A meta-analytic\napproach. Psychology of Sport and Exercise, 14(1),\n21–27. https://doi.org/10.1016/j.psychsport.2012.07.005\n\n\nAwbrey, J. (2020). The future of NFL data\nanalytics. https://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics\n\n\nAzzalini, A. (2023). sn: The skew-normal\nand related distributions such as the Skew-t\nand the SUN. https://doi.org/10.32614/CRAN.package.sn\n\n\nAzzalini, A. A. (2023). The R package sn:\nThe skew-normal and related distributions such as the skew-$t$ and the\nSUN (version 2.1.1). https://cran.r-project.org/package=sn\n\n\nBaird, C., & Wagner, D. (2000). The relative validity of actuarial-\nand consensus-based risk assessment systems. Children and Youth\nServices Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5\n\n\nBaldwin, B. (2023). nfl4th: Functions to\ncalculate optimal fourth down decisions in the National Football\nLeague. https://doi.org/10.32614/CRAN.package.nfl4th\n\n\nBales, J. (2012). 2012 contract year players and the myth of\nincreased production. https://www.4for4.com/2012/preseason/2012-contract-year-players-and-myth-increased-production\n\n\nBar-Eli, M., Avugos, S., & Raab, M. (2006). Twenty years of\n“hot hand” research: Review and critique. Psychology of\nSport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001\n\n\nBarrett, M. (2024). ggdag: Analyze and\ncreate elegant directed acyclic graphs. https://doi.org/10.32614/CRAN.package.ggdag\n\n\nBarrett, T., Dowle, M., Srinivasan, A., Gorecki, J., Chirico, M.,\nHocking, T., Schwendinger, B., & Krylov, I. (2025). data.table: Extension of ‘data.frame‘. https://doi.org/10.32614/CRAN.package.data.table\n\n\nBartoń, K. (2024). MuMIn: Multi-model inference.\nhttps://doi.org/10.32614/CRAN.package.MuMIn\n\n\nBates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting\nlinear mixed-effects models using lme4.\nJournal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01\n\n\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2025). lme4: Linear mixed-effects models using\nEigen and S4. https://doi.org/10.32614/CRAN.package.lme4\n\n\nBecker, M., & Klößner, S. (2025). PearsonDS:\nPearson distribution system. https://doi.org/10.32614/CRAN.package.PearsonDS\n\n\nBeilock, S. L., & Carr, T. H. (2001). On the fragility of skilled\nperformance: What governs choking under pressure? Journal of\nExperimental Psychology: General, 130(4), 701–725. https://doi.org/10.1037/0096-3445.130.4.701\n\n\nBengtsson, H. (2024). progressr: An\ninclusive, unifying API for progress updates. https://doi.org/10.32614/CRAN.package.progressr\n\n\nBengtsson, H. (2025a). ‘future‘: Unified\nparallel and distributed processing in ‘R‘ for\neveryone. https://doi.org/10.32614/CRAN.package.future\n\n\nBengtsson, H. (2025b). ‘future.apply‘:\nApply function to elements in parallel using futures. https://doi.org/10.32614/CRAN.package.future.apply\n\n\nBengtsson, H. (2025c). parallelly:\nEnhancing the parallel package. https://doi.org/10.32614/CRAN.package.parallelly\n\n\nBen-Shachar, M. S., Lüdecke, D., & Makowski, D. (2020). effectsize: Estimation of effect size indices and\nstandardized parameters. Journal of Open Source Software,\n5(56), 2815. https://doi.org/10.21105/joss.02815\n\n\nBen-Shachar, M. S., Makowski, D., Lüdecke, D., Patil, I., Wiernik, B.\nM., Thériault, R., & Waggoner, P. (2025). effectsize: Indices of effect size. https://doi.org/10.32614/CRAN.package.effectsize\n\n\nBerry, S., & Wood, C. (2004). The cold-foot effect. CHANCE,\n17(4), 47–51. https://doi.org/10.1080/09332480.2004.10554926\n\n\nBocskocsky, A., Ezekowitz, J., & Stein, C. (2014). The hot hand: A\nnew approach to an old “fallacy.” MIT Sloan Sports\nAnalytics Conference. https://www.sloansportsconference.com/research-papers/the-hot-hand-a-new-approach-to-an-old-fallacy\n\n\nBolger, F., & Önkal-Atay, D. (2004). The effects of feedback on\njudgmental interval predictions. International Journal of\nForecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8\n\n\nBolker, B., & R Development Core Team. (2023). bbmle: Tools for general maximum likelihood\nestimation. https://doi.org/10.32614/CRAN.package.bbmle\n\n\nBolker, B., & Robinson, D. (2024). broom.mixed: Tidying methods for mixed\nmodels. https://doi.org/10.32614/CRAN.package.broom.mixed\n\n\nBollen, K. A. (1989). Structural equations with latent\nvariables. John Wiley & Sons.\n\n\nBollen, K. A. (2002). Latent variables in psychology and the social\nsciences. Annual Review of Psychology, 53(1), 605–634.\nhttps://doi.org/10.1146/annurev.psych.53.100901.135239\n\n\nBox, G. E. P. (1979). Robustness in the strategy of scientific model\nbuilding. In R. L. Launer & G. N. Wilkinson (Eds.), Robustness\nin statistics. Academic Press.\n\n\nBrauer, M., & Curtin, J. J. (2018). Linear mixed-effects models and\nthe analysis of nonindependent data: A unified framework to analyze\ncategorical and continuous independent variables that vary\nwithin-subjects and/or within-items. Psychological Methods,\n23(3), 389–411. https://doi.org/10.1037/met0000159\n\n\nBraun, N. (2012). Bayesian football 101. https://web.archive.org/web/20161028142225/http://www.bayesff.com/bayesian101/\n\n\nBrown, M., Grasley, N., & Guido, M. (2025). Do sports bettors\nneed consumer protection? Evidence from a field experiment. https://mattbrownecon.github.io/assets/papers/jmp/sportsbetting.pdf\n\n\nBryan, J., Hester, J., Robinson, D., Wickham, H., Dervieux, C., &\nPosit. (2025). Reprex do’s and don’ts. https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\n\n\nBulus, M. (2023). pwrss: Statistical\npower and sample size calculation tools. https://doi.org/10.32614/CRAN.package.pwrss\n\n\nBulus, M., Dong, N., Kelcey, B., & Spybrook, J. (2021).\nPowerUpR: Power analysis tools for multilevel\nrandomized experiments. https://doi.org/10.32614/CRAN.package.PowerUpR\n\n\nBürkner, P.-C. (2017). brms: An\nR package for Bayesian multilevel models using\nStan. Journal of Statistical Software,\n80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel\nmodeling with the R package brms. The R Journal, 10(1),\n395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2024). brms:\nBayesian regression models using Stan. https://doi.org/10.32614/CRAN.package.brms\n\n\nCapitaine, L. (2020). LongituRF: Random forests for\nlongitudinal data. https://doi.org/10.32614/CRAN.package.LongituRF\n\n\nCarl, S. (2024). nflplotR:\nNFL logo plots in ggplot2 and\ngt. https://doi.org/10.32614/CRAN.package.nflplotR\n\n\nCarl, S., & Baldwin, B. (2024). nflfastR: Functions to efficiently access\nNFL play by play data. https://doi.org/10.32614/CRAN.package.nflfastR\n\n\nCarl, S., & Sharpe, L. (2025). nflseedR: Functions to efficiently simulate and\nevaluate NFL seasons. https://doi.org/10.32614/CRAN.package.nflseedR\n\n\nCarroll, B., Palmer, P., & Thorn, J. (2023). The hidden game of\nfootball: A revolutionary approach to the game and its statistics.\nUniversity of Chicago Press.\n\n\nChakravarthy, P. (2012). Optimizing draft strategies in fantasy\nfootball. https://harvardsportsanalysis.wordpress.com/wp-content/uploads/2012/04/fantasyfootballdraftanalysis1.pdf\n\n\nChalmers, R. P. (2025). SimDesign: Structure for\norganizing Monte Carlo simulation designs. https://doi.org/10.32614/CRAN.package.SimDesign\n\n\nChalmers, R. P., & Adkins, M. C. (2020). Writing effective and\nreliable Monte Carlo simulations with the\nSimDesign package. The Quantitative Methods for\nPsychology, 16(4), 248–280. https://doi.org/10.20982/tqmp.16.4.p248\n\n\nChampely, S. (2020). pwr: Basic\nfunctions for power analysis. https://doi.org/10.32614/CRAN.package.pwr\n\n\nChang, W. (2018). R graphics cookbook: Practical recipes for\nvisualizing data (2nd ed.). O’Reilly Media. https://r-graphics.org\n\n\nChang, W., Cheng, J., Allaire, J., Sievert, C., Schloerke, B., Xie, Y.,\nAllen, J., McPherson, J., Dipert, A., & Borges, B. (2024). shiny: Web application framework for\nR. https://doi.org/10.32614/CRAN.package.shiny\n\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of\nthe American Statistical Association, 116(536), 2009–2022.\nhttps://doi.org/10.1080/01621459.2020.1758115\n\n\nChekroud, A. (2017). nomogrammer: Fagan’s nomograms with ggplot2. https://github.com/achekroud/nomogrammer\n\n\nChristensen, R. H. B. (2024). ordinal:\nRegression models for ordinal data. https://doi.org/10.32614/CRAN.package.ordinal\n\n\nClark, K. (2018). The NFL’s analytics\nrevolution has arrived. https://www.theringer.com/2018/12/19/nfl/nfl-analytics-revolution\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral\nsciences (2nd ed.). Lawrence Erlbaum Associates, Publishers. https://doi.org/10.4324/9780203771587\n\n\nCongelio, B. J. (2023). Introduction to NFL analytics\nwith R. CRC Press. https://bradcongelio.com/nfl-analytics-with-r-book\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for\nWindows. Wiley-Blackwell.\n\n\nCritcher, C. R., & Rosenzweig, E. L. (2014). The performance\nheuristic: A misguided reliance on past success when predicting\nprospects for improvement. Journal of Experimental Psychology:\nGeneral, 143(2), 480–485. https://doi.org/10.1037/a0034129\n\n\nCsárdi, G., Hester, J., Wickham, H., Chang, W., Morgan, M., &\nTenenbaum, D. (2024). remotes:\nR package installation from remote repositories, including\nGitHub. https://doi.org/10.32614/CRAN.package.remotes\n\n\nD’Onofrio, B. M., Sjölander, A., Lahey, B. B., Lichtenstein, P., &\nÖberg, A. S. (2020). Accounting for confounding in observational\nstudies. Annual Review of Clinical Psychology, 16(1),\n25–48. https://doi.org/10.1146/annurev-clinpsy-032816-045030\n\n\nDana, J., & Thomas, R. (2006). In defense of clinical judgment … and\nmechanical prediction. Journal of Behavioral Decision Making,\n19(5), 413–428. https://doi.org/10.1002/bdm.537\n\n\nDawes, R. M. (1979). The robust beauty of improper linear models in\ndecision making. American Psychologist, 34(7),\n571–582. https://doi.org/10.1037/0003-066X.34.7.571\n\n\nDawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus\nactuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573\n\n\nDelignette-Muller, M. L., & Dutang, C. (2015). fitdistrplus: An R package for\nfitting distributions. Journal of Statistical Software,\n64(4), 1–34. https://doi.org/10.18637/jss.v064.i04\n\n\nDelignette-Muller, M.-L., Dutang, C., & Siberchicot, A. (2025).\nfitdistrplus: Help to fit of a\nparametric distribution to non-censored or censored data. https://doi.org/10.32614/CRAN.package.fitdistrplus\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., &\nMeijer, R. R. (2018). Selection procedures in sports:\nImproving predictions of athletes’ future performance.\nEuropean Journal of Sport Science, 18(9), 1191–1198.\nhttps://doi.org/10.1080/17461391.2018.1480662\n\n\nDigitale, J. C., Martin, J. N., & Glymour, M. M. (2022). Tutorial on\ndirected acyclic graphs. Journal of Clinical Epidemiology,\n142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001\n\n\nDinno, A. (2014). Gently clarifying the application of\nHorn’s parallel analysis to principal component analysis\nversus factor analysis. http://archives.pdx.edu/ds/psu/10527\n\n\nEastwell, P. (2014). Understanding hypotheses, predictions, laws, and\ntheories. Science Education Review, 13(1), 16–21. https://eric.ed.gov/?id=EJ1057150\n\n\nEddy, D. M. (1982). Probabilistic reasoning in clinical medicine:\nProblems and opportunities. In D. Kahneman, P. Slovic, & A. Tversky\n(Eds.), Judgment under uncertainty: Heuristics and biases (pp.\n249–267). Cambridge University Press. https://doi.org/10.1017/CBO9780511809477.019\n\n\nEnke, B. (2020). What you see is all there is. The Quarterly Journal\nof Economics, 135(3), 1363–1398. https://doi.org/10.1093/qje/qjaa012\n\n\nFabri, A. (2022). powerjoin: Extensions\nof dplyr and fuzzyjoin join functions. https://doi.org/10.32614/CRAN.package.powerjoin\n\n\nFantasy Football Analytics. (2025). FFAnalytics web app\naccuracy tab. https://fantasyfootballanalytics.net/ffanalytics-web-app-accuracy-tab\n\n\nFantasy Sports & Gaming Association. (2023). Industry\ndemographics. https://thefsga.org/industry-demographics/\n\n\nFarrington, D. P., & Loeber, R. (1989). Relative improvement over\nchance (RIOC) and phi as measures of predictive efficiency\nand strength of association in 2×2 tables. Journal of Quantitative\nCriminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737\n\n\nFowler, J. (2015). Why the Steelers hired a\nCarnegie Mellon professor for advanced analytics. https://www.espn.com/blog/pittsburgh-steelers/post/_/id/14521/why-the-steelers-hired-a-carnegie-mellon-professor-for-advanced-analytics\n\n\nFox, C. R., & Tversky, A. (1998). A belief-based account of decision\nunder uncertainty. Management Science, 44(7), 879–895.\nhttps://doi.org/10.1287/mnsc.44.7.879\n\n\nFox, J., & Weisberg, S. (2019). An R companion to\napplied regression (Third). Sage. https://www.john-fox.ca/Companion\n\n\nFox, J., Weisberg, S., & Price, B. (2024). car: Companion to applied regression. https://doi.org/10.32614/CRAN.package.car\n\n\nFox, L. (2021). How the NFL uses analytics, according\nto the lead analyst of a Super Bowl champion. https://www.forbes.com/sites/liamfox/2021/08/12/how-the-nfl-uses-analytics-according-to-the-lead-analyst-of-a-super-bowl-champion\n\n\nFraley, C., Raftery, A. E., & Scrucca, L. (2024). mclust: Gaussian mixture modelling for model-based\nclustering, classification, and density estimation. https://doi.org/10.32614/CRAN.package.mclust\n\n\nFree, H., Groenewold, M. R., & Luckhaupt, S. E. (2020). Lifetime\nprevalence of self-reported work-related health problems among\nUS workers—United States, 2018. MMWR.\nMorbidity and Mortality Weekly Report, 69(13), 361–365. https://doi.org/10.15585/mmwr.mm6913a1\n\n\nFreeman, M. K. (2017). An introduction to hierarchical\nmodeling. http://mfviz.com/hierarchical-models/\n\n\nFrick, H., Chow, F., Kuhn, M., Mahoney, M., Silge, J., & Wickham, H.\n(2025). rsample: General resampling\ninfrastructure. https://doi.org/10.32614/CRAN.package.rsample\n\n\nFrick, H., Vaughan, D., & Kuhn, M. (2025). hardhat: Construct modeling packages. https://doi.org/10.32614/CRAN.package.hardhat\n\n\nFriedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization\npaths for generalized linear models via coordinate descent. Journal\nof Statistical Software, 33(1), 1–22. https://doi.org/10.18637/jss.v033.i01\n\n\nFriedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K.,\nSimon, N., & Yang, J. (2025). glmnet: Lasso and elastic-net regularized\ngeneralized linear models. https://doi.org/10.32614/CRAN.package.glmnet\n\n\nGabry, J., Češnovar, R., & Johnson, A. (2024). cmdstanr: R interface to\nCmdStan. https://mc-stan.org/cmdstanr\n\n\nGandrud, C. (2020). Reproducible research with R and\nR studio (3rd ed.). CRC Press. https://www.routledge.com/Reproducible-Research-with-R-and-RStudio/Gandrud/p/book/9780367143985\n\n\nGarb, H. N., & Wood, J. M. (2019). Methodological advances in\nstatistical prediction. Psychological Assessment,\n31(12), 1456–1466. https://doi.org/10.1037/pas0000673\n\n\nGarnier, S. (2024). viridis:\nColorblind-friendly color maps for R. https://doi.org/10.32614/CRAN.package.viridis\n\n\nGarnier, S., Ross, N., Rudis, B., Sciaini, M., Camargo, A. P., &\nScherer, C. (2024). viridis(Lite) -\ncolorblind-friendly color maps for R. https://doi.org/10.5281/zenodo.4679423\n\n\nGet Up ESPN. (2021). @nfldraftscout on\nthe Cowboys’ interest in drafting Kyle\nPitts. https://x.com/GetUpESPN/status/1380165126108672001\n\n\nGetty, D., Li, H., Yano, M., Gao, C., & Hosoi, A. E. (2018). Luck\nand the law: Quantifying chance in fantasy sports and other contests.\nSIAM Review, 60(4), 869–887. https://doi.org/10.1137/16m1102094\n\n\nGilli, M., Maringer, D., & Schumann, E. (2019). Numerical\nmethods and optimization in finance (Second). Elsevier/Academic\nPress. https://www.enricoschumann.net/NMOF\n\n\nGilovich, T., Vallone, R., & Tversky, A. (1985). The hot hand in\nbasketball: On the misperception of random sequences. Cognitive\nPsychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6\n\n\nGoldschmied, N., Ratkovich, T., & Raphaeli, M. (2025). Brief report:\nExploring the icing the kicker strategy in the NFL. Journal of\nApplied Sport Psychology, 37(3), 389–396. https://doi.org/10.1080/10413200.2024.2437166\n\n\nGoldstein, J. (2013). Cat beats investors in stock market\nchallenge. https://www.npr.org/sections/money/2013/01/14/169326326/housecat-beats-investors-in-stock-market-challenge\n\n\nGonzalez Sanchez, A., Martinez, S., Yurko, R., Elmore, R., &\nMacdonald, B. (2024). Beyond the box score: Does icing the field goal\nkicker work in the NFL? CHANCE, 37(3), 41–48. https://doi.org/10.1080/09332480.2024.2415841\n\n\nGoodman, S. (2008). A dirty dozen: Twelve p-value\nmisconceptions. Seminars in Hematology, 45(3),\n135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nGoodman, Z. T., Casline, E., Jensen-Doss, A., Ehrenreich-May, J., &\nBainter, S. A. (2022). shinyDLRs: A\ndashboard to facilitate derivation of diagnostic likelihood ratios.\nPsychological Assessment, 34(6), 558–569. https://doi.org/10.1037/pas0001114\n\n\nGoodwin, L. D., & Leech, N. L. (2006). Understanding correlation:\nFactors that affect the size of r. The Journal of\nExperimental Education, 74(3), 249–266. https://doi.org/10.3200/JEXE.74.3.249-266\n\n\nGray, J. C., & MacKillop, J. (2015). Impulsive delayed reward\ndiscounting as a genetically-influenced target for drug abuse\nprevention: A critical evaluation. Frontiers in Psychology,\n6. https://doi.org/10.3389/fpsyg.2015.01104\n\n\nGrove, W. M., & Meehl, P. E. (1996). Comparative efficiency of\ninformal (subjective, impressionistic) and formal (mechanical,\nalgorithmic) prediction procedures: The clinical–statistical\ncontroversy. Psychology, Public Policy, and Law, 2(2),\n293–323. https://doi.org/10.1037/1076-8971.2.2.293\n\n\nGrove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C.\n(2000). Clinical versus mechanical prediction: A meta-analysis.\nPsychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19\n\n\nGuo, J., Gabry, J., Goodrich, B., Johnson, A., Weber, S., & Badr, H.\nS. (2025). rstan: R\ninterface to Stan. https://doi.org/10.32614/CRAN.package.rstan\n\n\nHarrell, Jr., F. E. (2025). rms:\nRegression modeling strategies. https://doi.org/10.32614/CRAN.package.rms\n\n\nHarris, C. (2012). How to make VBD work for you.\nhttps://www.espn.com/fantasy/football/ffl/story?page=nfldk2k12_vbdwork\n\n\nHarstad, A. (2023). The best draft strategy for 2023 (and every\nother year). https://www.footballguys.com/article/2023-draft-adp-fallers\n\n\nHitchings, J. (2012). Moneyball: Using modern portfolio theory to\nwin your fantasy sports league. https://eng.wealthfront.com/2012/01/17/moneyball-using-modern-portfolio-theory-to-win-your-fantasy-sports-league\n\n\nHo, T., & Carl, S. (2024). nflreadr:\nDownload nflverse data. https://doi.org/10.32614/CRAN.package.nflreadr\n\n\nHo, T., & Carl, S. (2025a). Articles. https://nflreadr.nflverse.com/articles/index.html\n\n\nHo, T., & Carl, S. (2025b). Data dictionary - combine. https://nflreadr.nflverse.com/articles/dictionary_combine.html\n\n\nHo, T., & Carl, S. (2025c). Data dictionary - contracts. https://nflreadr.nflverse.com/articles/dictionary_contracts.html\n\n\nHo, T., & Carl, S. (2025d). Data dictionary - depth charts.\nhttps://nflreadr.nflverse.com/articles/dictionary_depth_charts.html\n\n\nHo, T., & Carl, S. (2025e). Data dictionary - draft picks.\nhttps://nflreadr.nflverse.com/articles/dictionary_draft_picks.html\n\n\nHo, T., & Carl, S. (2025f). Data dictionary - ESPN\nQBR. https://nflreadr.nflverse.com/articles/dictionary_espn_qbr.html\n\n\nHo, T., & Carl, S. (2025g). Data dictionary - FF\nopportunity. https://nflreadr.nflverse.com/articles/dictionary_ff_opportunity.html\n\n\nHo, T., & Carl, S. (2025h). Data dictionary - FF\nplayer IDs. https://nflreadr.nflverse.com/articles/dictionary_ff_playerids.html\n\n\nHo, T., & Carl, S. (2025i). Data dictionary - FF\nrankings. https://nflreadr.nflverse.com/articles/dictionary_ff_rankings.html\n\n\nHo, T., & Carl, S. (2025j). Data dictionary - FTN\ncharting. https://nflreadr.nflverse.com/articles/dictionary_ftn_charting.html\n\n\nHo, T., & Carl, S. (2025k). Data dictionary - injuries. https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n\n\nHo, T., & Carl, S. (2025l). Data dictionary - next gen\nstats. https://nflreadr.nflverse.com/articles/dictionary_nextgen_stats.html\n\n\nHo, T., & Carl, S. (2025m). Data dictionary -\nparticipation. https://nflreadr.nflverse.com/articles/dictionary_participation.html\n\n\nHo, T., & Carl, S. (2025n). Data dictionary -\nPBP. https://nflreadr.nflverse.com/articles/dictionary_pbp.html\n\n\nHo, T., & Carl, S. (2025o). Data dictionary - PFR passing.\nhttps://nflreadr.nflverse.com/articles/dictionary_pfr_passing.html\n\n\nHo, T., & Carl, S. (2025p). Data dictionary - player stats.\nhttps://nflreadr.nflverse.com/articles/dictionary_player_stats.html\n\n\nHo, T., & Carl, S. (2025q). Data dictionary - player stats\ndefense. https://nflreadr.nflverse.com/articles/dictionary_player_stats_def.html\n\n\nHo, T., & Carl, S. (2025r). Data dictionary - rosters. https://nflreadr.nflverse.com/articles/dictionary_rosters.html\n\n\nHo, T., & Carl, S. (2025s). Data dictionary - schedules. https://nflreadr.nflverse.com/articles/dictionary_schedules.html\n\n\nHo, T., & Carl, S. (2025t). Data dictionary - snap counts.\nhttps://nflreadr.nflverse.com/articles/dictionary_snap_counts.html\n\n\nHoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting\npersonal events. Journal of Experimental Psychology: Learning,\nMemory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719\n\n\nHollenbeck, J. R., & Wright, P. M. (2017). Harking, sharking, and\ntharking: Making the case for post hoc analysis of scientific data.\nJournal of Management, 43(1), 5–18. https://doi.org/10.1177/0149206316679487\n\n\nHolmes, S., & Chatterjee, S. (2023). XICOR:\nAssociation measurement through cross rank increments. https://doi.org/10.32614/CRAN.package.XICOR\n\n\nHopper, T. (2014). Can we do better than r-squared? https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared\n\n\nHough, S. E. (2016). Predicting the unpredictable: The tumultuous\nscience of earthquake prediction. Princeton University Press.\n\n\nHu, J., & Szymczak, S. (2023). A review on longitudinal data\nanalysis with random forest. Briefings in Bioinformatics,\n24(2). https://doi.org/10.1093/bib/bbad002\n\n\nHuff, D. (2023). How to lie with statistics. Penguin UK.\n\n\nHyndman, R. J. (2014). Alternative to MAPE when the\ndata is not a time series. https://stats.stackexchange.com/a/108963/20338\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting:\nPrinciples and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nHyndman, R. J., Athanasopoulos, G., Bergmeir, C., Caceres, G., Chhay,\nL., Kuroptev, K., O’Hara-Wild, M., Petropoulos, F., Razbash, S., Wang,\nE., & Yasmeen, F. (2024). forecast:\nForecasting functions for time series and linear models. https://doi.org/10.32614/CRAN.package.forecast\n\n\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series\nforecasting: The forecast package for\nR. Journal of Statistical Software,\n27(3), 1–22. https://doi.org/10.18637/jss.v027.i03\n\n\nIacobucci, D., Schneider, M. J., Popovich, D. L., & Bakamitsos, G.\nA. (2016). Mean centering helps alleviate “micro” but not\n“macro” multicollinearity. Behavior Research\nMethods, 48(4), 1308–1317. https://doi.org/10.3758/s13428-015-0624-x\n\n\nInternational Society of Genetic Genealogy. (2022). Autosomal DNA\nstatistics. https://isogg.org/wiki/Autosomal_DNA_statistics\n\n\nJacinto, S. B., Lewis, C. C., Braga, J. N., & Scott, K. (2018). A\nconceptual model for generating and validating in-session clinical\njudgments. Psychotherapy Research, 28(1), 91–105. https://doi.org/10.1080/10503307.2016.1169329\n\n\nJackson-Wood, M. (2017). statistical test\nflowchart. https://www.statsflowchart.co.uk\n\n\nJak, S., Jorgensen, T. D., Verdam, M. G. E., Oort, F. J., & Elffers,\nL. (2020). Analytical power calculations for structural equation\nmodeling: A tutorial and shiny app. Behavior Research Methods.\nhttps://doi.org/10.3758/s13428-020-01479-0\n\n\nJohnson, J. E. V., & Bruce, A. C. (2001). Calibration of subjective\nprobability judgments in a naturalistic setting. Organizational\nBehavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949\n\n\nJones, J. M. (2024). Football retains dominant position as favorite\nU.S. sport. https://news.gallup.com/poll/610046/football-retains-dominant-position-favorite-sport.aspx\n\n\nKahneman, D. (2011). Thinking, fast and slow. Farrar,\nStraus, and Giroux.\n\n\nKarch, J. D. (2025a). lavaangui: A web-based\ngraphical interface for specifying lavaan\nmodels by drawing path diagrams. Structural Equation Modeling: A\nMultidisciplinary Journal, 1–12. https://doi.org/10.1080/10705511.2024.2420678\n\n\nKarch, J. D. (2025b). lavaangui:\nGraphical user interface with integrated diagrammer for lavaan. https://doi.org/10.32614/CRAN.package.lavaangui\n\n\nKartes, J. (2024). Which fantasy football projections are most\naccurate? https://fantasyfootballanalytics.net/2024/12/which-fantasy-football-projections-are-most-accurate.html\n\n\nKartes, J. (2025). Fantasy football projections: Exploring\npositional bias in projections. https://fantasyfootballanalytics.net/2025/07/fantasy-football-projections-exploring-positional-bias-in-projections.html\n\n\nKassambara, A. (2017). Practical guide to cluster analysis in\nR: Unsupervised machine learning (Vol.\n1). Sthda.\n\n\nKay, M. (2024). tidybayes: Tidy data and\ngeoms for Bayesian models. https://doi.org/10.32614/CRAN.package.tidybayes\n\n\nKeren, G. (1987). Facing uncertainty in the game of bridge: A\ncalibration study. Organizational Behavior and Human Decision\nProcesses, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1\n\n\nKessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., &\nZubizarreta, J. R. (2020). Suicide prediction models: A critical review\nof recent research with recommendations for the way forward.\nMolecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0\n\n\nKievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013).\nSimpson’s paradox in psychological science: A practical guide.\nFrontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513\n\n\nKilin, I. (2022). The best charts for color blind viewers. https://www.datylon.com/blog/data-visualization-for-colorblind-readers\n\n\nKline, R. B. (2023). Principles and practice of structural equation\nmodeling (5th ed.). Guilford Publications.\n\n\nKline, R. B. (2024). How to evaluate local fit (residuals) in large\nstructural equation models. International Journal of\nPsychology, 59(6), 1293–1306. https://doi.org/10.1002/ijop.13252\n\n\nKoehler, D. J., Brenner, L., & Griffin, D. (2002). The calibration\nof expert judgment: Heuristics and biases beyond the laboratory. In T.\nGilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics and\nbiases: The psychology of intuitive judgment. Cambridge University\nPress. https://doi.org/10.1017/CBO9780511808098.041\n\n\nKoriat, A., Lichtenstein, S., & Fischhoff, B. (1980). Reasons for\nconfidence. Journal of Experimental Psychology: Human Learning and\nMemory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107\n\n\nKotrba, V. (2020). Heuristics in fantasy sports: Is it profitable to\nstrategize based on favourite of the match? Mind & Society,\n19(1), 195–206. https://doi.org/10.1007/s11299-020-00231-7\n\n\nKuhn, M. (2025). tune: Tidy tuning\ntools. https://doi.org/10.32614/CRAN.package.tune\n\n\nKuhn, M., & Frick, H. (2025). dials:\nTools for creating tuning parameter values. https://doi.org/10.32614/CRAN.package.dials\n\n\nKuhn, M., & Vaughan, D. (2025). parsnip: A common API to modeling and\nanalysis functions. https://doi.org/10.32614/CRAN.package.parsnip\n\n\nKuhn, M., Vaughan, D., & Hvitfeldt, E. (2025). yardstick: Tidy characterizations of model\nperformance. https://doi.org/10.32614/CRAN.package.yardstick\n\n\nKuhn, M., & Wickham, H. (2020). Tidymodels: A\ncollection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org\n\n\nKuhn, M., & Wickham, H. (2025). tidymodels: Easily install and load the tidymodels\npackages. https://doi.org/10.32614/CRAN.package.tidymodels\n\n\nKuhn, M., Wickham, H., & Hvitfeldt, E. (2025). recipes: Preprocessing and feature engineering\nsteps for modeling. https://doi.org/10.32614/CRAN.package.recipes\n\n\nKuznetsova, A., Bruun Brockhoff, P., & Haubo Bojesen Christensen, R.\n(2020). lmerTest: Tests in linear mixed\neffects models. https://doi.org/10.32614/CRAN.package.lmerTest\n\n\nLarrick, R. P., Mannes, A. E., & Soll, J. B. (2024). The social\npsychology of the wisdom of crowds (with a new section on recent\nadvances). In F. M. Federspiel, G. Montibeller, & M. Seifert (Eds.),\nBehavioral decision analysis (pp. 121–143). Springer. https://doi.org/10.1007/978-3-031-44424-1_7\n\n\nLederer, D. J., Bell, S. C., Branson, R. D., Chalmers, J. D., Marshall,\nR., Maslove, D. M., Ost, D. E., Punjabi, N. M., Schatz, M., Smyth, A.\nR., Stewart, P. W., Suissa, S., Adjei, A. A., Akdis, C. A., Azoulay, É.,\nBakker, J., Ballas, Z. K., Bardin, P. G., Barreiro, E., … Vincent, J.-L.\n(2019). Control of confounding and reporting of results in causal\ninference studies. Guidance for authors from editors of respiratory,\nsleep, and critical care journals. Annals of the American Thoracic\nSociety, 16(1), 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS\n\n\nLee, M. D., & Liu, S. (2022). Drafting strategies in fantasy\nfootball: A study of competitive sequential human decision making.\nJudgment and Decision Making, 17(4), 691–719. https://doi.org/10.1017/S1930297500008901\n\n\nLenth, R. V. (2025). emmeans: Estimated\nmarginal means, aka least-squares\nmeans. https://doi.org/10.32614/CRAN.package.emmeans\n\n\nLewis, M. (2009). The no-stats all-star. https://www.nytimes.com/2009/02/15/magazine/15Battier-t.html\n\n\nLilienfeld, S. O. (2007). Psychological treatments that cause harm.\nPerspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x\n\n\nLilienfeld, S. O., Lynn, S. J., & Lohr, J. M. (2015). Science\nand pseudoscience in clinical psychology: Initial thoughts, reflections,\nand considerations (S. O. Lilienfeld, S. J. Lynn, & J. M. Lohr,\nEds.; 2nd ed., pp. 1–16). Guilford Publications.\n\n\nLindhiem, O., Petersen, I. T., Mentch, L. K., & Youngstrom, E. A.\n(2020). The importance of calibration in clinical psychology.\nAssessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055\n\n\nLishinski, A. (2024). lavaanPlot: Path\ndiagrams for lavaan models via\nDiagrammeR. https://doi.org/10.32614/CRAN.package.lavaanPlot\n\n\nLochbaum, M., Stoner, E., Hefner, T., Cooper, S., Lane, A. M., &\nTerry, P. C. (2022). Sport psychology and performance meta-analyses: A\nsystematic review of the literature. PLOS ONE, 17(2),\ne0263408. https://doi.org/10.1371/journal.pone.0263408\n\n\nLong, J. A. (2024). interactions:\nComprehensive, user-friendly toolkit for probing interactions. https://doi.org/10.32614/CRAN.package.interactions\n\n\nLong, J. D., & Teetor, P. (2019). R cookbook: Proven recipes for\ndata analysis, statistics, and graphics (2nd ed.). O’Reilly Media.\nhttps://rc2e.com\n\n\nLuchman, J. (2024). domir: Tools to\nsupport relative importance analysis. https://doi.org/10.32614/CRAN.package.domir\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., & Makowski, D. (2020).\nExtracting, computing and exploring the parameters of statistical models\nusing R. Journal of Open Source Software,\n5(53), 2445. https://doi.org/10.21105/joss.02445\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., &\nMakowski, D. (2021). performance: An\nR package for assessment, comparison and testing of\nstatistical models. Journal of Open Source Software,\n6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nLüdecke, D., Makowski, D., Ben-Shachar, M. S., Patil, I., Højsgaard, S.,\n& Wiernik, B. M. (2025). parameters:\nProcessing of model parameters. https://doi.org/10.32614/CRAN.package.parameters\n\n\nLüdecke, D., Makowski, D., Ben-Shachar, M. S., Patil, I., Waggoner, P.,\nWiernik, B. M., & Thériault, R. (2025). performance: Assessment of regression models\nperformance. https://doi.org/10.32614/CRAN.package.performance\n\n\nLy, N. (2015). The rules of American football -\nEXPLAINED! (NFL). https://www.youtube.com/watch?v=Ddwp1HyEFRE\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J.\n(2011). On the predictive efficiency of past performance and physical\nability: The case of the National Football League.\nHuman Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218\n\n\nMaechler, M., Todorov, V., Ruckstuhl, A., Salibian-Barrera, M., Koller,\nM., & Conceicao, E. L. T. (2024). robustbase: Basic robust statistics. https://doi.org/10.32614/CRAN.package.robustbase\n\n\nMagnusson, K. (2013). Creating a typical textbook illustration of\nstatistical power using either ggplot or\nbase graphics. https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics\n\n\nMagnusson, K. (2014). Understanding statistical power and\nsignificance testing. https://rpsychologist.com/d3/nhst/\n\n\nMagnusson, K. (2015). Distribution of p-values\nwhen comparing two groups. https://rpsychologist.com/d3/pdist\n\n\nMagnusson, K. (2021). Understanding p-values\nthrough simulations. https://rpsychologist.com/pvalue\n\n\nMagnusson, K. (2023). Interpreting correlations: An interactive\nvisualization. https://rpsychologist.com/correlation\n\n\nMakowski, D., Ben-Shachar, M. S., Patil, I., & Lüdecke, D. (2020).\nMethods and algorithms for correlation analysis in R.\nJournal of Open Source Software, 5(51), 2306. https://doi.org/10.21105/joss.02306\n\n\nMakowski, D., Wiernik, B. M., Patil, I., Lüdecke, D., Ben-Shachar, M.\nS., & Thériault, R. (2025). correlation: Methods for correlation\nanalysis. https://doi.org/10.32614/CRAN.package.correlation\n\n\nMakridakis, S., Hogarth, R. M., & Gaba, A. (2009). Forecasting and\nuncertainty in the economic and business world. International\nJournal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012\n\n\nMannes, A. E., Soll, J. B., & Larrick, R. P. (2014). The wisdom of\nselect crowds. Journal of Personality and Social Psychology,\n107(2), 276–299. https://doi.org/10.1037/a0036677\n\n\nMassey, C., & Thaler, R. H. (2013). The loser’s curse: Decision\nmaking and market efficiency in the National Football\nLeague draft. Management Science, 59(7),\n1479–1495. https://doi.org/10.1287/mnsc.1120.1657\n\n\nMathieu, J. E., Aguinis, H., Culpepper, S. A., & Chen, G. (2012).\nUnderstanding and estimating the power to detect cross-level interaction\neffects in multilevel modeling. Journal of Applied Psychology,\n97(5), 951–966. https://doi.org/10.1037/a0028380\n\n\nMayer, M. (2024). missRanger: Fast\nimputation of missing values. https://doi.org/10.32614/CRAN.package.missRanger\n\n\nMazerolle, M. J. (2025). AICcmodavg: Model selection\nand multimodel inference based on (Q)AIC(c). https://doi.org/10.32614/CRAN.package.AICcmodavg\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree:\nThe case of r and d. Psychological Methods,\n11(4), 386–401. https://doi.org/10.1037/1082-989X.11.4.386\n\n\nMcNeish, D., & Wolf, M. G. (2023). Dynamic fit index cutoffs for\nconfirmatory factor analysis models. Psychological Methods,\n28(1), 61–88. https://doi.org/10.1037/met0000425\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula?\nJournal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks:\nSir Karl, Sir\nRonald, and the slow progress of soft psychology.\nJournal of Consulting and Clinical Psychology, 46(4),\n806–834. https://doi.org/10.1037/0022-006x.46.4.806\n\n\nMeehl, P. E. (1986). Causes and effects of my disturbing little book.\nJournal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6\n\n\nMeehl, P. E., & Rosen, A. (1955). Antecedent probability and the\nefficiency of psychometric signs, patterns, or cutting scores.\nPsychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070\n\n\nMiller, J. B., & Sanjurjo, A. (2014). A cold shower for the hot hand\nfallacy. Innocenzo Gasparini Institute for Economic Research.\nhttps://repec.unibocconi.it/igier/igi/wp/2014/518.pdf\n\n\nMiller, J. B., & Sanjurjo, A. (2024). A cold shower for the hot hand\nfallacy: Robust evidence from controlled settings. The Review of\nEconomics and Statistics, 106(6), 1607–1619. https://doi.org/10.1162/rest_a_01280\n\n\nMiller, R. M. (2013). Cognitive bias in fantasy sports: Is your\nbrain sabotaging your team? Xlibris Press.\n\n\nMlodinow, L. (2008). The drunkard’s walk: How randomness rules our\nlives. Pantheon Books.\n\n\nMoore, D. A., & Healy, P. J. (2008). The trouble with\noverconfidence. Psychological Review, 115(2), 502–517.\nhttps://doi.org/10.1037/0033-295X.115.2.502\n\n\nMorley, S. K., Brito, T. V., & Welling, D. T. (2018). Measures of\nmodel performance based on the log accuracy ratio. Space\nWeather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669\n\n\nMoshagen, M., & Bader, M. (2024). semPower: General power analysis\nfor structural equation models. Behavior Research Methods,\n56(4), 2901–2922. https://doi.org/10.3758/s13428-023-02254-7\n\n\nMoskowitz, T. J., & Wertheim, L. J. (2011). Scorecasting: The\nhidden influences behind how sports are played and games are won.\nThree Rivers Press.\n\n\nMotz, B. (2013). Fantasy football: A touchdown for undergraduate\nstatistics education. Proceedings of the Games, Learning, and\nSociety Conference, 9.0, 222–228. https://doi.org/10.1184/R1/6686804.v1\n\n\nMotz, B. (2018). GitHub repository for COLL\nC-105: Prediction, Probability, and\nPigskin. https://github.com/bmotz/prediction-probability-and-pigskin\n\n\nMurayama, K., Usami, S., & Sakaki, M. (2022).\nSummary-statistics-based power analysis: A new and practical method to\ndetermine sample size for mixed-effects modeling. Psychological\nMethods, 27(6), 1014–1038. https://doi.org/10.1037/met0000330\n\n\nMurphy, A. H., & Winkler, R. L. (1984). Probability forecasting in\nmeterology. Journal of the American Statistical Association,\n79(387), 489–500. https://doi.org/10.2307/2288395\n\n\nNakamura, J., & Csikszentmihalyi, M. (2021). The experience of flow\ntheory and research. In C. R. Snyder, S. J. Lopez, L. M. Edwards, &\nS. C. Marques (Eds.), The Oxford handbook of positive\npsychology (pp. 279–296). Oxford University Press.\n\n\nNFL Fantasy Football. (2020). How to play fantasy football for\nBEGINNERS. https://www.youtube.com/watch?v=XhrBapdhLEc\n\n\nNFL Films Presents. (2014). Playing fantasy football for college\ncredit?? Welcome to C105 - Prediction, Probability,\n& Pigskin. https://www.facebook.com/watch/?v=10155572257183615\n\n\nNiles, B. (2022). Do players perform better in fantasy football in a\ncontract year? https://www.4for4.com/2022/preseason/do-players-perform-better-fantasy-football-contract-year\n\n\nNivison, A. (2021). Florida TE Kyle Pitts draws\ncomparison to Lebron James. https://247sports.com/article/kyle-pitts-lebron-james-2021-nfl-draft-florida-gators-football-163882176\n\n\nNuñez, J. R., Anderton, C. R., & Renslow, R. S. (2018). Optimizing\ncolormaps with consideration for color vision deficiency to enable\naccurate interpretation of scientific data. PLOS ONE,\n13(7), e0199239. https://doi.org/10.1371/journal.pone.0199239\n\n\nNYT 4th Down Bot. (2014). 4th down: When to go for it and why.\nhttps://www.nytimes.com/2014/09/05/upshot/4th-down-when-to-go-for-it-and-why.html\n\n\nO’Connell, R. (2022). Without the Wonderlic, the\nN.F.L. finds other ways to test\nfootball I.Q. https://www.nytimes.com/2022/03/02/sports/football/nfl-wonderlic-test.html\n\n\nOkabe, M., & Ito, K. (2008). Color universal design\n(CUD)- how to make figures and presentations that are\nfriendly to colorblind people. https://jfly.uni-koeln.de/color/\n\n\nOskamp, S. (1965). Overconfidence in case-study judgments. Journal\nof Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125\n\n\nPartnow, S. (2021). The midrange theory: Basketball’s evolution in\nthe age of analytics. Triumph Books.\n\n\nPedersen, T. L. (2024). patchwork: The\ncomposer of plots. https://doi.org/10.32614/CRAN.package.patchwork\n\n\nPelechrinis, K., & Winston, W. (2022). The hot hand in the wild.\nPLOS ONE, 17(1), e0261890. https://doi.org/10.1371/journal.pone.0261890\n\n\nPetersen, I. T. (2014). Identify sleepers in fantasy football using\nstatistics and wisdom of the crowd. https://fantasyfootballanalytics.net/2014/06/identify-sleepers-using-wisdom-crowd.html\n\n\nPetersen, I. T. (2016). Which are more accurate: Fantasy football\nrankings or projections? https://fantasyfootballanalytics.net/2016/04/accuracy-of-rankings-vs-projections.html\n\n\nPetersen, I. T. (2017). Who has the best fantasy football\nprojections? 2017 update. https://fantasyfootballanalytics.net/2017/03/best-fantasy-football-projections-2017.html\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With\napplied examples in R. Chapman and\nHall/CRC. https://doi.org/10.1201/9781003357421\n\n\nPetersen, I. T. (2025a). petersenlab: A\ncollection of R functions by the Petersen\nLab. https://doi.org/10.32614/CRAN.package.petersenlab\n\n\nPetersen, I. T. (2025b). Principles of psychological assessment:\nWith applied examples in R. University of Iowa\nLibraries. https://doi.org/10.25820/work.007199\n\n\nPeterson, R. A. (2021). Finding optimal normalizing transformations via\nbestNormalize. The R\nJournal, 13(1), 310–329. https://doi.org/10.32614/RJ-2021-041\n\n\nPeterson, R. A. (2023). bestNormalize:\nNormalizing transformation functions. https://doi.org/10.32614/CRAN.package.bestNormalize\n\n\nPeterson, R. A., & Cavanaugh, J. E. (2020). Ordered quantile\nnormalization: A semiparametric transformation built for the\ncross-validation era. Journal of Applied Statistics,\n47(13-15), 2312–2327. https://doi.org/10.1080/02664763.2019.1630372\n\n\nPosner, K., Brown, G. K., Stanley, B., Brent, D. A., Yershova, K. V.,\nOquendo, M. A., Currier, G. W., Melvin, G. A., Greenhill, L., Shen, S.,\n& Mann, J. J. (2011). The Columbia–Suicide Severity Rating\nScale: Initial validity and internal consistency findings from\nthree multisite studies with adolescents and adults. American\nJournal of Psychiatry, 168(12), 1266–1277. https://doi.org/10.1176/appi.ajp.2011.10111704\n\n\nPro Football Reference. (2024). 2024 NFL advanced stats. https://www.pro-football-reference.com/years/2024/advanced.htm\n\n\nPro Football Reference. (2025). About our advanced stats. https://www.pro-football-reference.com/about/advanced_stats.htm\n\n\nR Core Team. (2025). R: A language and environment for\nstatistical computing. R Foundation for Statistical\nComputing. https://www.R-project.org\n\n\nRader, C. A., Larrick, R. P., & Soll, J. B. (2017). Advice as a form\nof social influence: Informational motives and the consequences for\naccuracy. Social and Personality Psychology Compass,\n11(8), e12329. https://doi.org/10.1111/spc3.12329\n\n\nRaiche, G., & Magis, D. (2025). nFactors: Parallel analysis and other non\ngraphical solutions to the Cattell scree test. https://doi.org/10.32614/CRAN.package.nFactors\n\n\nRedelmeier, D. A., & Tibshirani, R. J. (1997). Association between\ncellular-telephone calls and motor vehicle collisions. New England\nJournal of Medicine, 336(7), 453–458. https://doi.org/10.1056/NEJM199702133360701\n\n\nReed, T. (2016). In an NFL divided over analytics,\nCleveland Browns look to make numbers add up in their\nfavor. https://www.cleveland.com/browns/2016/01/in_an_nfl_divided_over_analyti.html\n\n\nRevelle, W. (2025). psych: Procedures\nfor psychological, psychometric, and personality research. https://doi.org/10.32614/CRAN.package.psych\n\n\nRevelle, W., & Rocklin, T. (1979). Very simple structure: An\nalternative procedure for estimating the optimal number of interpretable\nfactors. Multivariate Behavioral Research, 14(4),\n403–414. https://doi.org/10.1207/s15327906mbr1404_2\n\n\nRice, M. E., Harris, G. T., & Lang, C. (2013). Validation of and\nrevision to the VRAG and SORAG: The\nViolence Risk Appraisal Guide—Revised\n(VRAG-R). Psychological Assessment,\n25(3), 951–965. https://doi.org/10.1037/a0032878\n\n\nRipley, B., & Venables, B. (2025). MASS: Support\nfunctions and datasets for Venables and Ripley’s MASS. https://doi.org/10.32614/CRAN.package.MASS\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez,\nJ.-C., & Müller, M. (2011). pROC: An\nopen-source package for R and S+ to analyze\nand compare ROC curves. BMC Bioinformatics,\n12, 77. https://doi.org/10.1186/1471-2105-12-77\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez,\nJ.-C., & Müller, M. (2023). pROC:\nDisplay and analyze ROC curves. https://doi.org/10.32614/CRAN.package.pROC\n\n\nRobinson, D., Hayes, A., & Couch, S. (2025). broom: Convert statistical objects into tidy\ntibbles. https://doi.org/10.32614/CRAN.package.broom\n\n\nRobitzsch, A., Grund, S., & Henke, T. (2024). miceadds: Some additional multiple imputation\nfunctions, especially for mice. https://doi.org/10.32614/CRAN.package.miceadds\n\n\nRohrer, J. M. (2018). Thinking clearly about correlations and causation:\nGraphical causal models for observational data.\nAdvances in Methods and Practices in Psychological Science,\n1(1), 27–42. https://doi.org/10.1177/2515245917745629\n\n\nRomer, D. (2006). Do firms maximize? Evidence from professional\nfootball. Journal of Political Economy, 114(2),\n340–365. https://doi.org/10.1086/501171\n\n\nRosalsky, G. (2023). Should we invest more in weather forecasting?\nIt may save your life. https://www.npr.org/sections/money/2023/07/11/1186458991/should-we-invest-more-in-weather-forecasting-it-may-save-your-life\n\n\nRosenthal, G. (2018). Super Bowl LII: How the 2017\nPhiladelphia Eagles were built. https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753\n\n\nRosseel, Y. (2012). lavaan: An\nR package for structural equation modeling. Journal of\nStatistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02\n\n\nRosseel, Y., Jorgensen, T. D., & De Wilde, L. (2024). lavaan: Latent variable analysis. https://doi.org/10.32614/CRAN.package.lavaan\n\n\nRuscio, J., & Roche, B. (2012). Determining the number of factors to\nretain in an exploratory factor analysis using comparison data of known\nfactorial structure. Psychological Assessment, 24(2),\n282–292. https://doi.org/10.1037/a0025697\n\n\nRusso, J. E., & Schoemaker, P. J. (1992). Managing overconfidence.\nSloan Management Review, 33(2), 7.\n\n\nRyan, J. (2013). Beating the NBA draft: Does any team\noutperform expectations? https://harvardsportsanalysis.org/2013/11/beating-the-nba-draft-does-any-team-outperform-expectations\n\n\nRyan, J. A., & Ulrich, J. M. (2024). quantmod: Quantitative financial modelling\nframework. https://doi.org/10.32614/CRAN.package.quantmod\n\n\nSalmon, M. (2018). Where to get help with your R\nquestion? https://masalmon.eu/2018/07/22/wheretogethelp/\n\n\nSchalter, T. (2022). The NFL preseason is not\npredictive — but it can often seem that way. https://fivethirtyeight.com/features/the-nfl-preseason-is-not-predictive-but-it-can-often-seem-that-way\n\n\nScherer, C. (2021). Beyond bar and box plots. https://z3tt.github.io/beyond-bar-and-box-plots\n\n\nSchloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M.,\nThoen, E., Elberg, A., & Crowley, J. (2025).\nGGally: Extension to ggplot2. https://doi.org/10.32614/CRAN.package.GGally\n\n\nSchmidt, F. L., & Hunter, J. E. (1996). Measurement error in\npsychological research: Lessons from 26 research scenarios.\nPsychological Methods, 1(2), 199–223. https://doi.org/10.1037/1082-989X.1.2.199\n\n\nSchoemann, A. M., Boulton, A. J., & Short, S. D. (2017). Determining\npower and sample size for simple and complex mediation models.\nSocial Psychological and Personality Science, 8(4),\n379–386. https://doi.org/10.1177/1948550617715068\n\n\nSchumann, E. (2011--2024). Numerical methods and optimization in\nfinance (NMOF) manual. Package version 2.10-1). https://enricoschumann.net/NMOF\n\n\nSchumann, E. (2024). NMOF: Numerical methods and\noptimization in finance. https://doi.org/10.32614/CRAN.package.NMOF\n\n\nSchwabish, J. (2021). Better data visualizations: A guide for\nscholars, researchers, and wonks. Columbia University Press. https://doi.org/10.7312/schw19310\n\n\nSchwartz, A. (2006). Diagnostic test calculator. http://araw.mede.uic.edu/cgi-bin/testcalc.pl\n\n\nScrucca, L. (2011). Model-based SIR for dimension\nreduction. Computational Statistics & Data Analysis,\n5(11), 3010–3026. https://doi.org/10.1016/j.csda.2011.05.006\n\n\nScrucca, L. (2020). msir: Model-based\nsliced inverse regression. https://doi.org/10.32614/CRAN.package.msir\n\n\nScrucca, L., Fraley, C., Murphy, T. B., & Raftery, A. E. (2023).\nModel-based clustering, classification, and density estimation using\nmclust in R. Chapman;\nHall/CRC. https://doi.org/10.1201/9781003277965\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002).\nExperimental and quasi-experimental designs for generalized causal\ninference. Houghton Mifflin.\n\n\nSharpe, L. (2020a). NFL data sets. https://github.com/nflverse/nfldata/blob/master/DATASETS.md\n\n\nSharpe, L. (2020b). NFL data sets - draft values.\nhttps://github.com/nflverse/nfldata/blob/master/DATASETS.md#draft_values\n\n\nSharpe, L. (2020c). NFL data sets - rosters. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#rosters\n\n\nSharpe, L. (2020d). NFL data sets - standings. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#standings\n\n\nSharpe, L. (2020e). NFL data sets - trades. https://github.com/nflverse/nfldata/blob/master/DATASETS.md#trades\n\n\nSherman, A., & Goldner, K. (2021). Sharpstack: Cholesky\ncorrelations for building better lineups. https://assets-global.website-files.com/5f1af76ed86d6771ad48324b/607a4434a565aa7763bd1312_AndyAsh-Sharpstack-RPpaper.pdf\n\n\nSievert, C. (2020). Interactive web-based data visualization with\nR, plotly, and shiny. Chapman; Hall/CRC. https://plotly-r.com\n\n\nSievert, C., Parmer, C., Hocking, T., Chamberlain, S., Ram, K.,\nCorvellec, M., & Despouy, P. (2024). plotly: Create interactive web graphics via plotly.js. https://doi.org/10.32614/CRAN.package.plotly\n\n\nSigal, M. J., & Chalmers, R. P. (2016). Play it again: Teaching\nstatistics with Monte Carlo simulation. Journal of\nStatistics Education, 24(3), 136–156. https://doi.org/10.1080/10691898.2016.1246953\n\n\nSignorell, A. (2025). DescTools: Tools for descriptive\nstatistics. https://doi.org/10.32614/CRAN.package.DescTools\n\n\nSigrist, F., Gyger, T., & Kuendig, P. (2025). gpboost: Combining tree-boosting with gaussian\nprocess and mixed effects models. https://doi.org/10.32614/CRAN.package.gpboost\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions\nfail–but some don’t. Penguin.\n\n\nSimoiu, C., Sumanth, C., Mysore, A., & Goel, S. (2019). Studying the\n\"wisdom of crowds\" at scale. Proceedings of the AAAI Conference on\nHuman Computation and Crowdsourcing, 7(1), 171–179. https://doi.org/10.1609/hcomp.v7i1.5271\n\n\nSkala, D. (2008). Overconfidence in psychology and finance–an\ninterdisciplinary literature review. Bank i Kredyt, 4,\n33–50.\n\n\nSmith, B., Sharma, P., & Hooper, P. (2006). Decision making in\nonline fantasy sports communities. Interactive Technology and Smart\nEducation, 3(4), 347–360. https://doi.org/10.1108/17415650680000072\n\n\nSmith, G. (2016). The Sports Illustrated cover\njinx. https://www.psychologytoday.com/us/blog/what-the-luck/201610/the-sports-illustrated-cover-jinx\n\n\nSpector, P. E., & Brannick, M. T. (2010). Methodological urban\nlegends: The misuse of statistical control variables.\nOrganizational Research Methods, 14(2), 287–305. https://doi.org/10.1177/1094428110369842\n\n\nSpielman, R. M., Jenkins, W. J., & Lovett, M. D. (2020).\nPsychology (2nd ed.). OpenStax. https://openstax.org/details/books/psychology-2e\n\n\nSpinu, V., Grolemund, G., & Wickham, H. (2024). lubridate: Make dealing with dates a little\neasier. https://doi.org/10.32614/CRAN.package.lubridate\n\n\nStack Overflow. (2018). How to make a great R\nreproducible example. https://stackoverflow.com/a/5963610\n\n\nStack Overflow. (2025). How to create a minimal, reproducible\nexample. https://stackoverflow.com/help/minimal-reproducible-example\n\n\nstatistica. (2023a). Fantasy sports in the U.S.-\nstatistics & facts. https://www.statista.com/topics/10895/fantasy-sports-in-the-us/\n\n\nstatistica. (2023b). Most watched sports leagues in the United\nStates in 2023, by minutes watched. https://www.statista.com/statistics/1430289/most-watched-sports-leagues-usa/\n\n\nStevens, R. J., & Poppe, K. K. (2020). Validation of clinical\nprediction models: What does the “calibration slope” really\nmeasure? Journal of Clinical Epidemiology, 118, 93–99.\nhttps://doi.org/10.1016/j.jclinepi.2019.09.016\n\n\nSteyerberg, E. W., & Vergouwe, Y. (2014). Towards better clinical\nprediction models: Seven steps for development and an ABCD for\nvalidation. European Heart Journal, 35(29), 1925–1931.\nhttps://doi.org/10.1093/eurheartj/ehu207\n\n\nStrauss, M. E., & Smith, G. T. (2009). Construct validity: Advances\nin theory and methodology. Annual Review of Clinical\nPsychology, 5(1), 1–25. https://doi.org/10.1146/annurev.clinpsy.032408.153639\n\n\nStrayer, D. L., & Drew, F. A. (2004). Profiles in driver\ndistraction: Effects of cell phone conversations on younger and older\ndrivers. Human Factors, 46(4), 640–649. https://doi.org/10.1518/hfes.46.4.640.56806\n\n\nStrayer, D. L., & Johnston, W. A. (2001). Driven to distraction:\nDual-task studies of simulated driving and conversing on a cellular\ntelephone. Psychological Science, 12(6), 462–466. https://doi.org/10.1111/1467-9280.00386\n\n\nSurowiecki, J. (2005). The wisdom of crowds. Anchor Books.\n\n\nTanney, M. (2021). R in sports analytics. https://www.youtube.com/watch?v=1zCDWtNEucI\n\n\nTay, J. K., Narasimhan, B., & Hastie, T. (2023). Elastic net\nregularization paths for all generalized linear models. Journal of\nStatistical Software, 106(1), 1–31. https://doi.org/10.18637/jss.v106.i01\n\n\nTetlock, P. E. (2017). Expert political judgment: How good is it?\nHow can we know? - New edition. Princeton University\nPress.\n\n\nTextor, J., van der Zander, B., & Ankan, A. (2023). dagitty: Graphical analysis of structural causal\nmodels. https://doi.org/10.32614/CRAN.package.dagitty\n\n\nTextor, J., van der Zander, B., Gilthorpe, M. S., Liśkiewicz, M., &\nEllison, G. T. (2016). Robust causal inference using directed acyclic\ngraphs: The R package ’dagitty’. International Journal of\nEpidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nThe Associated Press. (2008). Janikowski gives Raiders\nwin over Jets in overtime. https://www.nfl.com/news/janikowski-gives-raiders-win-over-jets-in-overtime-09000d5d80bc3910\n\n\nTheussl, S., & Hornik, K. (2024). Rglpk:\nR/GNU linear programming kit interface. https://doi.org/10.32614/CRAN.package.Rglpk\n\n\nTodorov, V., & Filzmoser, P. (2009). An object-oriented framework\nfor robust multivariate analysis. Journal of Statistical\nSoftware, 32(3), 1–47. https://doi.org/10.18637/jss.v032.i03\n\n\nTofallis, C. (2015). A better measure of relative prediction accuracy\nfor model selection and model estimation. Journal of the Operational\nResearch Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103\n\n\nTotalProSports.com. (2017). 10 most ridiculous things ever said by\nStephen A. Smith or Skip Bayless. https://www.youtube.com/watch?v=lTjBuEPcLlc\n\n\nTrafimow, D., Hyman, M. R., & Kostyk, A. (in press). Enhancing\npredictive power by unamalgamating multi-item scales. Psychological\nMethods. https://doi.org/10.1037/met0000599\n\n\nTreat, T. A., & Viken, R. J. (2023). Measuring test performance with\nsignal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M.\nMcMullen, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA\nhandbook of research methods in psychology: Foundations, planning,\nmeasures, and psychometrics (2nd ed., Vol. 1, pp. 837–858).\nAmerican Psychological Association. https://doi.org/10.1037/0000318-038\n\n\nTufte, E. R. (2001). The visual display of quantitative\ninformation. Graphics Press.\n\n\nTungate, A., Andersen, D., & Petersen, I. T. (2025). ffanalytics: Scrape data for fantasy\nfootball. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nTüzen, M. F. (2025). Explained vs. Predictive power:\nR², adjusted R², and beyond. https://mfatihtuzen.netlify.app/posts/2025-04-30_rsquared/\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty:\nHeuristics and biases. Science, 185(4157), 1124–1131.\nhttps://doi.org/10.1126/science.185.4157.1124\n\n\nUlrich, J. (2023). TTR: Technical trading rules.\nhttps://doi.org/10.32614/CRAN.package.TTR\n\n\nUnderwood, A. (2019). 15 ways analytics has changed sports. https://stacker.com/stories/sports/15-ways-analytics-has-changed-sports\n\n\nUrsenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D.,\nKosteniuk, J., & Spiteri, R. J. (2019). Scoring algorithms for a\ncomputer-based cognitive screening tool: An illustrative example of\noverfitting machine learning approaches and the impact on estimates of\nclassification accuracy. Psychological Assessment,\n31(11), 1377–1382. https://doi.org/10.1037/pas0000764\n\n\nUsami, S., & Murayama, K. (2018). Time-specific errors in growth\ncurve modeling: Type-1 error inflation and a possible solution with\nmixed-effects models. Multivariate Behavioral Research,\n53(6), 876–897. https://doi.org/10.1080/00273171.2018.1504273\n\n\nvan Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputation by chained equations\nin R. Journal of Statistical Software,\n45(3), 1–67. https://doi.org/10.18637/jss.v045.i03\n\n\nvan Buuren, S., & Groothuis-Oudshoorn, K. (2024). mice: Multivariate imputation by chained\nequations. https://doi.org/10.32614/CRAN.package.mice\n\n\nVaughan, D., & Couch, S. (2025). workflows: Modeling workflows. https://doi.org/10.32614/CRAN.package.workflows\n\n\nVigen, T. (2024). spurious correlations:\ncorrelation is not causation. https://www.tylervigen.com/spurious-correlations\n\n\nWagner, C., & Vinaimont, T. (2010). Evaluating the wisdom of crowds.\nIssues in Information Systems, 11(1), 724–732. http://iacis.org/iis/2010/724-732_LV2010_1546.pdf\n\n\nWalder, S. (2020). 2020 NFL analytics survey: Which\nteams are most, least analytically inclined? https://www.espn.com/nfl/story/_/id/29939438/2020-nfl-analytics-survey-which-teams-most-least-analytically-inclined\n\n\nWang, Y. A., & Rhemtulla, M. (2021). Power analysis for parameter\nestimation in structural equation modeling: A discussion and tutorial.\nAdvances in Methods and Practices in Psychological Science,\n4(1), 1–17. https://doi.org/10.1177/2515245920918253\n\n\nWeinsztok, S., Brassard, S., Balodis, I., Martin, L. E., & Amlung,\nM. (2021). Delay discounting in established and proposed behavioral\naddictions: A systematic review and meta-analysis. Frontiers in\nBehavioral Neuroscience, 15. https://doi.org/10.3389/fnbeh.2021.786358\n\n\nWetzels, R., Tutschkow, D., Dolan, C., Sluis, S. van der, Dutilh, G.,\n& Wagenmakers, E.-J. (2016). A Bayesian test for the\nhot hand phenomenon. Journal of Mathematical Psychology,\n72, 200–209. https://doi.org/10.1016/j.jmp.2015.12.003\n\n\nWhite, M. H., & Sheldon, K. M. (2014). The contract year syndrome in\nthe NBA and MLB: A classic\nundermining pattern. Motivation and Emotion, 38(2),\n196–205. https://doi.org/10.1007/s11031-013-9389-7\n\n\nWickham, H. (2023). tidyverse: Easily\ninstall and load the Tidyverse. https://doi.org/10.32614/CRAN.package.tidyverse\n\n\nWickham, H. (2024). ggplot2: Elegant graphics for data analysis\n(3rd ed.). Springer. https://ggplot2-book.org\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D.,\nFrançois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M.,\nPedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J.,\nRobinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to\nthe tidyverse. Journal of Open Source\nSoftware, 4(43), 1686. https://doi.org/10.21105/joss.01686\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K.,\nWilke, C., Woo, K., Yutani, H., Dunnington, D., & van den Brand, T.\n(2024). ggplot2: Create elegant data\nvisualisations using the grammar of graphics. https://doi.org/10.32614/CRAN.package.ggplot2\n\n\nWilke, C. O. (2024). ggridges: Ridgeline\nplots in ggplot2. https://doi.org/10.32614/CRAN.package.ggridges\n\n\nWilke, C. O., & Wiernik, B. M. (2022). ggtext: Improved text rendering support for ggplot2. https://doi.org/10.32614/CRAN.package.ggtext\n\n\nWilliams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., &\nSakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific\nreview of evidential value. Clinical Psychology: Science and\nPractice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331\n\n\nWolf, M. G., & McNeish, D. (2022). dynamic: DFI cutoffs for latent\nvariable models. https://doi.org/10.32614/CRAN.package.dynamic\n\n\nWood, S. N. (2017). Generalized additive models: An introduction\nwith R (2nd ed.). CRC press. https://doi.org/10.1201/9781315370279\n\n\nWood, S. N. (2025). mgcv: Mixed\nGAM computation vehicle with automatic smoothness\nestimation. https://doi.org/10.32614/CRAN.package.mgcv\n\n\nWoodland, L. M., & Woodland, B. M. (2015). The National\nFootball League season wins total betting market: The impact of\nheuristics on behavior. Southern Economic Journal,\n82(1), 38–54. https://doi.org/10.4284/0038-4038-2013.145\n\n\nWright, M. N. (2024). ranger: A fast\nimplementation of random forests. https://doi.org/10.32614/CRAN.package.ranger\n\n\nWright, M. N., & Ziegler, A. (2017). ranger: A fast implementation of random forests\nfor high dimensional data in C++ and R.\nJournal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01\n\n\nWuertz, D., Setz, T., Chalabi, Y., & Theussl, S. (2023). fPortfolio: Rmetrics - portfolio\nselection and optimization. https://doi.org/10.32614/CRAN.package.fPortfolio\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical\ncontrol requires causal justification. Advances in Methods and\nPractices in Psychological Science, 5(2),\n25152459221095823. https://doi.org/10.1177/25152459221095823\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R\nMarkdown cookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2024). R markdown\ncookbook. CRC Press. https://bookdown.org/yihui/rmarkdown-cookbook\n\n\nYahoo! Sports. (2024). How cognitive bias affects your fantasy draft\nstrategy with neuroscience professor Dr. Renee Miller.\nhttps://www.youtube.com/watch?v=gmpLFWs5ae0\n\n\nYoungstrom, E. A., Halverson, T. F., Youngstrom, J. K., Lindhiem, O.,\n& Findling, R. L. (2018). Evidence-based assessment from simple\nclinical judgments to statistical learning: Evaluating a range of\noptions using pediatric bipolar disorder as a diagnostic challenge.\nClinical Psychological Science, 6(2), 243–265. https://doi.org/10.1177/2167702617741845\n\n\nYutani, H. (2023). gghighlight:\nHighlight lines and points in ggplot2.\nhttps://doi.org/10.32614/CRAN.package.gghighlight\n\n\nZhang, Z., & Yuan, K.-H. (2018). Practical statistical power\nanalysis using WebPower and R. ISDSA\nPress. https://doi.org/10.35566/power",
    "crumbs": [
      "References"
    ]
  }
]