[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "",
    "text": "Preface\nThis is a book in progress—it is incomplete. I will continue to add to and update it as I am able.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-contribute",
    "href": "index.html#sec-contribute",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "How to Contribute",
    "text": "How to Contribute\nThis is an open-access textbook. My goal is to share data analysis strategies for free! Anyone is welcome to contribute to the project. If you would like to contribute, please consider one of the following:\n\n\nopen an issue or create a pull request on the book’s GitHub repository.\n\nbuy me a coffee—Support me in developing this (free!) resource for fantasy football analytics… Even a cup of coffee helps me stay awake!\n\n(or use PayPal)\n\nThe GitHub repository for the book is located here: https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook. If you have data or analysis examples that are you willing to share and include in the book, feel free to contact me.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-openAccess",
    "href": "index.html#sec-openAccess",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Open Access",
    "text": "Open Access\nThis is an open-access book. This means that it is freely available for anyone to access.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-license",
    "href": "index.html#sec-license",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "License",
    "text": "License\n\n\nCreative Commons License\n\nThe online version of this book is licensed under the Creative Commons Attribution License. In short, you can use my work as long as you cite it.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-citation",
    "href": "index.html#sec-citation",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Citation",
    "text": "Citation\nThe APA-style citation for the book is:\nPetersen, I. T. (2024). Fantasy football analytics: Statistics, prediction, and empiricism Using R. Version 0.0.1. University of Iowa Libraries. https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook. [INSERT DOI LINK]\n\nThe BibTeX citation for the book is:\n@book{petersenFantasyFootballAnalytics,\n  title = {Fantasy football analytics: Statistics, prediction, and empiricism Using R},\n  author = {Petersen, Isaac T.},\n  year = {2024},\n  publisher = {{University of Iowa Libraries}},\n  note = {Version 0.0.1},\n  doi = {INSERT},\n  isbn = {INSERT},\n  url = {https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-author",
    "href": "index.html#sec-author",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "About the Author",
    "text": "About the Author\nI am an Associate Professor in the Department of Psychological and Brain Sciences at the University of Iowa. I am a licensed psychologist with expertise in child clinical psychology. Why am I writing about fantasy football and data analysis? Because fantasy football involves the intersection of two things I love: sports and statistics.\nThrough my training, I have learned the value of statistics for answering important questions that I find interesting. In graduate training, I came to the realization that statistics are relevant not only for psychology and science, but also for domains that I enjoy as hobbies, including sports and fantasy sports. I have played in a longstanding fantasy football league for over 20 years (since my junior year of high school) with old friends from high school. I wanted to apply what I was learning about statistics to help others improve their performance in fantasy football and to help people—including those who might not otherwise be interested—to learn statistics. So I began blogging online about the value of applying statistics to improve decision making in fantasy football. Apparently, many people were interested in learning statistics when they could apply them to a domain that they find interesting like fantasy football. My blog eventually became FantasyFootballAnalytics.net, a website that uses advanced statistics to help people win their fantasy football leagues.\nIn terms of my R and statistics background, I have published many peer-reviewed publications that employ advanced statistical methods, have published a book on psychological assessment (Petersen, 2024b, 2024c) that includes applied examples in R, and have published the petersenlab R package (Petersen, 2024a) on the Comprehensive R Archive Network (CRAN). Several sections in this book come from Petersen (2024c). I am also a co-author of the ffanalytics R package (Andersen et al., 2024) that provides free utilities for downloading fantasy football projections and additional fantasy-relevant data, and for calculating projected points given your league settings.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-accessibility",
    "href": "index.html#sec-accessibility",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Accessibility",
    "text": "Accessibility\nI strive to follow principles of accessibility (archived at https://perma.cc/8XJ9-Q6QJ) to make the book content accessible to people with visual impairments and physical disabilities. If there are additional ways I can make the content more accessible, please let me know.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-acknowledgments",
    "href": "index.html#sec-acknowledgments",
    "title": "Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI thank Dr. Benjamin Motz, who provided consultation and many helpful resources based on his fantasy football statistics class. I also thank key members of FantasyFootballAnalytics.net, including Val Pinskiy, Andrew Tungate, Dennis Andersen, and Adam Peterson, who helped develop and provide fantasy football-related resources and who helped sharpen my thinking about the topic. I also thank Professor Patrick Carroll, who taught me the value of statistics for answering important questions.\n\n\n\n\nAndersen, D., Petersen, I. T., & Tungate, A. (2024). ffanalytics: Scrape data for fantasy football. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nPetersen, I. T. (2024a). petersenlab: A collection of R functions by the Petersen Lab. https://github.com/DevPsyLab/petersenlab\n\n\nPetersen, I. T. (2024c). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 About this Book\nHow can we use information to make predictions about uncertain events? This book is about empiricism (basing theories on observed data) and judgment, prediction, and decision making in the context of uncertainty. The book provides an introduction to modern analytical techniques used to make informed predictions, test theories, and draw conclusions from a given dataset.\nThis book was originally written for a undergraduate-level course entitled, “Fantasy Football: Predictive Analytics and Empiricism”. The chapters provide an overview of topics that each could have its own class and textbook, such as causal inference, factor analysis, cluster analysis, principal component analysis, machine learning, cognitive biases, modern portfolio theory, data visualization, simulation, etc. The book gives readers an overview of the breadth of the approaches to prediction and empiricism. As a consequence, the book does not cover any one technique or approach in great depth.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whatIsFantasyFootball",
    "href": "intro.html#sec-whatIsFantasyFootball",
    "title": "1  Introduction",
    "section": "1.2 What is Fantasy Football?",
    "text": "1.2 What is Fantasy Football?\nFantasy football is an online game where participants assemble (i.e., “draft”) imaginary teams composed of real-life National Football League (NFL) players. In this game, participants compete against their opponents (e.g., friends/coworkers/classmates), accumulating points based on players’ actual statistical performances in games. The goal is to outscore one’s opponent each week to win matches and ultimately claim victory in the league.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-whyFantasyFootball",
    "href": "intro.html#sec-whyFantasyFootball",
    "title": "1  Introduction",
    "section": "1.3 Why Focus on Fantasy Football?",
    "text": "1.3 Why Focus on Fantasy Football?\nI was fortunate to have an excellent instructor who taught me the value of learning statistics to answer interesting and important questions. That is, I do not find statistics intrinsically interesting; rather, I find them interesting because of what they allow me to do. Many students find statistics intimidating in part because of how it is typically taught—with examples like dice rolls and coin flips that are (seemingly irrelevant and) boring to students. My contention is that applied examples are a more effective lens to teach many concepts in psychology and data analysis. It can be more engaging and relatable to learn statistics in the applied context of sports, a domain that is more intuitive to many. Many people play fantasy sports. This book involves applying statistics to a particular domain (football). People actually want to learn statistical principles and methods when they can apply them to interesting questions (e.g., sports). In my opinion [and supported by evidence; Motz (2013)], this is a much more effective way of engaging people and teaching statistics than in the context of abstract coin flips and dice rolls. Fantasy football relies heavily on prediction—trying to predict which players will perform best and selecting them accordingly. In this way, fantasy football provides a plethora of decision making opportunities in the face of uncertainty, and a wealth of data for analyzing these decisions. However, unlike many other applied domains in psychology, fantasy football (1) allows a person to see the accuracy of their predictions on a timely basis and (2) provides a safe environment for friendly competition. Thus, it provides a unique domain to evaluate—and improve—the accuracy of various prediction models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-educationalValue",
    "href": "intro.html#sec-educationalValue",
    "title": "1  Introduction",
    "section": "1.4 Educational Value",
    "text": "1.4 Educational Value\nSkills in statistics, statistical programming, and data analysis are highly valuable. This book includes practical and conceptual tools that build a foundation for critical thinking. The book aims to help readers evaluate theory in the light of evidence (and vice versa) and to refine decision making in the context of uncertainty. Readers will learn about the ways that psychological science (and related disciplines) poses questions, formulates hypotheses, designs studies to test those questions, and interprets the findings, collectively with the aim to answer questions, improve decision making, and solve problems.\nOf course, this is not a traditional psychology textbook. However, the book incorporates important psychological concepts, such as cognitive biases in judgment and prediction, etc. In the modern world of big data, research and society need people who know how to make sense of the information around us. Psychology is in a prime position to teach applied statistics to a wide variety of students, most of whom will not have careers as psychologists. Psychology can teach the importance of statistics given humans’ cognitive biases. It can also teach about how these biases can influence how people interpret statistics. This book will teach readers the applications of statistics (prediction) and research methods (empiricism) to answer questions they find interesting, while applying scientific and psychological rigor.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-objectives",
    "href": "intro.html#sec-objectives",
    "title": "1  Introduction",
    "section": "1.5 Learning Objectives",
    "text": "1.5 Learning Objectives\nThis book aims to help readers accomplish the following learning objectives:\n\nApply empirical inference and appreciate the value it provides over speculative supposition.\nAsk educated questions when confronted with decisions in the face of uncertainty.\nUnderstand human decision making, including common heuristics and cognitive biases and how to mitigate them analytically.\nEngage in critical thinking about causality, including devising plausible alternative explanations for observed effects.\nUnderstand causal inference including confounding, causal pathways, and counterfactuals.\nThink empirically about human behavior and performance.\nDescribe the strengths and weaknesses of humans versus computers in prediction scenarios.\nApply basic skills in statistical programming using R to manipulate and summarize datasets and to conduct data analysis.\nCritically evaluate the strengths and limitations of different statistical models and methodologies used in predicting uncertain events, enhancing their understanding of statistical inference and model selection.\nUse various analytical techniques for predicting the outcome of uncertain events, and for uncovering latent causes of patterns in observed data.\nInterpret findings from various statistical approaches and evaluate the accuracy of predictions.\nEngage in iterative problem-solving processes, refining analytical approaches based on feedback and outcomes, and adapting strategies accordingly.\nCommunicate statistical findings and analyses in both written and oral formats, demonstrating proficiency in presenting complex information to diverse audiences.\nMake sense of big data.\nUse practical analytical skills that can be applied in future research and job settings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-disclosures",
    "href": "intro.html#sec-disclosures",
    "title": "1  Introduction",
    "section": "1.6 Disclosures",
    "text": "1.6 Disclosures\nI am the Owner of Fantasy Football Analytics, LLC, which operates https://fantasyfootballanalytics.net.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-disclaimer",
    "href": "intro.html#sec-disclaimer",
    "title": "1  Introduction",
    "section": "1.7 Disclaimer",
    "text": "1.7 Disclaimer\n“This material probably won’t win you fantasy football championships. You could take what we learn and apply it to fantasy football and you might become 5 percent more likely to win. Or… Consider the broader relevance of this. You could learn data analysis and figure out ways to apply it to other systems. And you could be making a six-figure salary within the next five years.” – Benjamin Motz, Ph.D.\nHere is a video of a Professor Benjamin Motz that describes the value of teaching statistics through the lens of fantasy football:\n\n\nVideo\nVideo of Prof. Benjamin Motz Discussing Statistics and His Fantasy Football Course.\n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nMotz, B. (2013). Fantasy football: A touchdown for undergraduate statistics education. Proceedings of the Games, Learning, and Society Conference, 9.0, 222–228. https://doi.org/10.1184/R1/6686804.v1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html",
    "href": "fantasy-football.html",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "2.1 Football\nFootball is the most widely watched sport in the United States.1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#sec-football",
    "href": "fantasy-football.html#sec-football",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "2.1.1 The Objective\nThe goal in football is for a team to score more points than their opponent. A game lasts 60 minutes, and it is separated into four 15-minute quarters. The team with the most points when the time runs out wins.\nHere is a video that provides a brief introduction to American football:\n\n\nVideo\nVideo that Provides An Introduction to American Football.\n\n\n\n\n2.1.2 The Roster\n\n2.1.2.1 Overview\nEach team has 11 players on the field at a time. The particular players who are on the field will depend on the situation, but usually includes one of the three subsets of players:\n\nOffense\nDefense\nSpecial Teams\n\nAn example formation is depicted in Figure 2.1.\n\n\n\n\n\n\nFigure 2.1: An Example Football Formation for the Offense and Defense. The solid line indicates the line of scrimmage. The arrow indicates the direction the offense tries to advance the ball.\n\n\n\n\n\n2.1.2.2 Offense\nThe offense is on the field when the team has the ball.\nPlayers on offense include:\n\nQuarterback (QB)\nRunning Back (RB)\n\nHalfback (HB) or Tailback (TB)\nFullback (FB)\n\nWide Receiver (WR)\nTight End (TE)\nOffensive Linemen (OL), part of the “Offensive Line”\n\nCenter (C)\nOffensive Guard (OG)\nOffensive Tackle (OT)\n\n\nThe quarterback is the most important player on the offense. They help lead the team down the field. The quarterback receives the ball from the Center at the beginning of the play, and they can either hand the ball off (typically to a Running Back or Fullback), pass the ball (typically to a Wide Receiver or Tight End), or run the ball. Quarterbacks tend to have a strong arm for throwing the ball far and accurately. Some quarterbacks are fast and are considered “dual threats” to pass or run.\nRunning Backs take a hand-off from the Quarterback to execute a running play (i.e., a rush). They may also catch short passes from the Quarterback or help protect (i.e., block for) the Quarterback from the defensive players who are trying to tackle the Quarterback. Halfbacks and Tailbacks tend to be quick and agile. Fullbacks tend to be strong and powerful.\nWide Receivers catch passes from the Quarterback to execute a passing play. On running plays, they provide protection for the player running the ball (e.g., the Running Back) so the ball carrier can get as far as possible without being tackled. Wide receivers tend to be tall, fast, have good hands (can catch the ball well), and can jump high.\nTight Ends block for running and passing plays, and they catch passes from the Quarterback. Tight ends tend to be strong and have good hands.\nOffensive Linemen block for running and passing plays. On passing plays, they provide protection for the Quarterback so the Quarterback has time to pass the ball without being tackled. On running plays, they provide protection for the player running the ball (e.g., the Running Back) so the ball carrier can get as far as possible without being tackled. Offensive Linemen tend to be large so they can provide adequate protection for the Quarterback and Running Back.\n\n\n2.1.2.3 Defense\nThe defense is on the field when the team does not have the ball (i.e., when the opposing team has the ball).\nPlayers on defense include:\n\nDefensive Linemen (DL), part of the “Defensive Line”\n\nDefensive End (DE)\nDefensive Tackle (DT)\n\nLinebacker (LB)\n\nMiddle (or Inside) Linebacker (MLB)\nOutside Linebacker (OLB)\n\nDefensive Back (DB), part of the “Secondary”\n\nCornerback (CB)\nSafety (S)\n\nFree Safety (FS)\nStrong Safety (SS)\n\n\n\nThe players on the defense attempt to tackle the offensive players for as short of gains as possible and attempt to prevent completed passes.\nOn passing plays, Defensive Linemen try to apply pressure to the Quarterback and try to tackle the Quarterback behind the line of scrimmage before the Quarterback can throw the ball (i.e., a sack). On rushing plays, Defensive Linemen try to tackle the ball carrier to prevent the ball carrier from advancing the ball (i.e., gaining yards). Defensive Linemen tend to be large yet quick so they can apply pressure to the Quarterback.\nLinebackers are versatile in that, on a given play, they may attempt to a) “blitz” to sack the Quarterback, b) stop the Running Back, or c) prevent a completed pass. Linebackers tend to be strong yet agile.\nDefensive Backs are specialist pass defenders. The main role of Cornerbacks is to cover the Wide Receivers. Safeties serve as the last line of defense for longer passes. Defensive Backs tend to be quick and agile.\n\n\n2.1.2.4 Special Teams\nThe special teams involves specialist players who are on the field during all kicking plays including kickoffs, field goals, and punts.\nPlayers on special teams include:\n\nKicker (K)\nPunter (P)\nHolder\nLong Snapper\nPunt Returner\nKick Returner\nand other players intended to block for or to tackle the ball carrier\n\nOn a field goal attempt, the Long Snapper snaps the ball to the Holder, who holds the ball for the Kicker. The Kicker attempts field goals and, during kickoffs, kicks the ball to the opposing team. During kickoffs, the Kick Returner catches the kicked ball and returns it for as many yards as possible. During a punt play, the Long Snapper snaps the ball to the Punter who kicks (i.e., punts) the ball to the opposing team. The Punt Returner catches the punted ball and returns it for as many yards as possible.\n\n\n\n2.1.3 The Field\nThe football field is rectangular and is 120 yards long and 53 1/3 yards wide (109.73 m x 48.77 m).2 At each end of the 120-yard field is a team’s end zone. Each end zone is 10 yards long (9.14 m). Thus, the distance from one end zone to the other end zone is 100 yards (91.44 m). Behind each end zone is a field goal post. A diagram of a football field is depicted in Figure 2.2.\n\n\n\n\n\n\nFigure 2.2: A Diagram of a Football Field. The yard markers depict the distance from the nearest end zone. The orange shaded area is called the “red zone”, where chances of scoring points are highest. The original figure was modified to depict field goal posts. (Figure retrieved from https://commons.wikimedia.org/wiki/File:American_football_field.svg)\n\n\n\n\n\n2.1.4 The Gameplay\nAt the beginning of the game, there is a coin flip to determine which teams receives the ball first and which team takes which side of the field. During the kickoff, the kicking team kicks the ball to the receiving team, who has the option to return the kick. The offense starts their possession at the 25 yard line—if there is no return (i.e., a touchback)—or wherever the kick returner is tackled or goes out of bounds.\nThe team with the ball (i.e., the offense) has four opportunities (“downs”) to advance the ball (i.e., gain) 10 yards. A team can advance the ball either by running it or by throwing (i.e., passing) and catching it. At the end of a rushing play, the ball advances to wherever the ball carrier is tackled or goes out of bounds (i.e., wherever the player is “down”). At the end of a passing play, if the thrown ball is caught (i.e., a completed pass), the ball advances to wherever the ball carrier is tackled or goes out of bounds. If the thrown ball is not caught in bounds before the ball hits the ground (i.e., an incomplete pass), the ball does not advance. Wherever the ball is advanced to dictates where the next play begins. The yard position on the field where the next play takes place from is known as the “line of scrimmage”. Neither team can cross the line the line of scrimmage until the next play begins. To begin the play, the ball is placed on the line of scrimmage and the Center gives (or “snaps”) the ball to the Quarterback.\nIf the team advances the ball 10 or more yards within four downs, the team receives a “first down” and is awarded a new set of downs—four more downs to advance the ball 10 more yards. If the team advances the ball all the way to the other team’s end zone, they score a touchdown. If the team fails to advance the ball 10 or more yards within four downs, the team loses the ball, and the other team takes possession at that spot on the field. There are risks of giving the other team the ball with a short distance to score. Thus, on fourth down, instead of trying to advance the ball for a first down, a team may choose to kick a field goal—to get points—or to punt.\nA field goal involves a kicker kicking the ball with an intent to kick the ball through the field goal posts (“uprights”). To score points by making a field goal, the kicked ball must go between the uprights (extended vertically) and over the cross bar.\nPunting involves a punter kicking the ball to the other team with an intent to give their opponent worse field position, thus making it harder for the other team to score. The punting team tries to pin the opponent as close as possible to the opponent’s end zone (i.e., as far as possible from the own team’s end zone), so they have a longer distance to go to score a touchdown.\nThere are multiple ways that ball possession can switch from the offense to the other team. After scoring a touchdown, field goal, or safety, there is a kickoff, in which the scoring team kicks the ball to the opponent. Another way that the ball switches possession to the other team is if the team commits a turnover. The defense can force a turnover by an interception, fumble recovery, or turnover on downs. A turnover due to an interception occurs when a defensive player catches the quarterback’s pass. A turnover due to a fumble recovery occurs when an offensive player, who had possession of the ball, loses the ball before being down or scoring a touchdown and the ball is recovered by the opponent. A turnover on downs occurs when the team attempts on fourth down to achieve the remainder of the needed 10 yards to go but fails.\nOther football-related situations include tackles for loss and sacks. A tackle for loss occurs when a ball carrier is tackled behind the line of scrimmage. A sack occurs when a Quarterback is tackled with the ball behind the line of scrimmage. A pass defended occurs when a defensive player knocks down the ball in the air so that the indended receiver cannot catch the ball.\n\n\n2.1.5 The Scoring\nThe goal of the team with the ball (i.e., the offense) is to score points. It can do this by either advancing the ball into the other team’s end zone (6 points) or by kicking a field goal (3 points). Advancing the ball in the other team’s end zone is called a touchdown. After a touchdown, the offense chooses to attempt either a point-after-touchdown (PAT) or a two-point conversion. A PAT is a short kick attempt from the 15-yard line (i.e., 15 yards away from the end zone) that, if it goes through the goal posts (“uprights”) and over the cross bar, is worth 1 point. A two-point conversion is a single-scoring opportunity from the 3-yard line (i.e., 3 yards away from the end zone). If the offense scores (i.e., advances the ball into the end zone) from the 3-yard line, the team is awarded 2 points.\nA team can kick a field goal from any distance as long as the kick goes through the goal posts. The current record for the longest field goal is 66 yards (by Justin Tucker in 2021).\nA safety occurs when the offense is tackled with the ball in their own end zone. When a safety occurs, the opposing team (i.e., defense) is awarded two points and the ball.\n\n\n2.1.6 Glossary of Terms\n\nrunning play (“run”) or rushing play (or “rush”)—the attempt by an offensive player, typically the Running Back or Quarterback, to advance the ball “on the ground” by running it—not by passing it forward\npassing play (or “pass”)—the attempt by an offensive player, typically the Quarterback, to advance the ball by throwing it forward to an offensive player\npassing attempt—the attempt to advance the ball by passing it (i.e., a thrown pass)\nrushing attempt—the attempt to advance the ball by running it\npassing completion—a thrown pass that is succesfully caught by an offensive player\npassing incompletion—a thrown pass that is not caught by an offensive player\npassing yards—the distance (in yards) the player advanced the ball by throwing it\nrushing yards—the distance (in yards) the player advanced the ball by running it\nreceving yards—the distance (in yards) the player advanced the ball by catching thrown passes and then running with it further upfield\nkick/punt return yards—the distance (in yards) the player advanced the ball by returning kicks or punts\nturnover return yards—the distance (in yards) the player advanced the ball by returning turnovers\nreception—a pass that is caught by the offensive player\ntouchdown—advancing the ball into the opponent’s end zone either by a) throwing a completed pass that ends up in the end zone, b) running it into the end zone, c) catching it in the end zone, or d) catching it and then running it into the end zone\npassing touchdown—advancing the ball into the opponent’s end zone either by throwing a completed pass that ends up in the end zone\nrushing touchdown—advancing the ball into the opponent’s end zone either by running it into the end zone\nreceiving touchdown—advancing the ball into the opponent’s end zone either by catching it in the end zone or by catching it and then running it into the end zone\nkick/punt return touchdown—advancing the ball into the opponent’s end zone when returning a kick or punt\nturnover return touchdown—advancing the ball into the opponent’s end zone when returning a turnover (i.e., interception or fumble)\ntwo-point conversion—a single-scoring opportunity from the 3-yard line (i.e., 3 yards away from the end zone) that is an option given to a team that scores a touchdown; if the offense scores (i.e., advances the ball into the end zone) from the 3-yard line, the team is awarded 2 points\nblock—when the defense/special teams blocks a kick or field goal by hitting the ball just after it is kicked to prevent the ball from going far\nkickoff—the kicking team kicks the ball to the receiving team, who has the option to return the kick\nfield goal—a kicker kicks the ball with an intent to kick the ball through the field goal posts (“uprights”). To score points by making a field goal, the kicked ball must go between the uprights (extended vertically) and over the cross bar. If the field goal attempt is successful, the team gains 3 points.\npoint after touchdown (PAT)—a short kick attempt from the 15-yard line (i.e., 15 yards away from the end zone) that, if it goes through the goal posts (“uprights”) and over the cross bar, is worth 1 point\nextra point returned—if the defense/special teams returns the ball into the opponent’s end zone during a point after touchdown (PAT) attempt, it is worth 2 points\npunt—a punter kicks the ball to the other team with an intent to give their opponent worse field position, thus making it harder for the other team to score\nfumble lost—when an offensive player, who had possession of the ball, loses the ball before being down or scoring a touchdown and the ball is recovered by the opponent\nfumble forced—when a defensive player knocks the ball out of the hands of an offensive player, who had possession of the ball\nfumble recovery—when a defensive player recovers a fumble by the opponent\ninterception—when a defensive player catches a pass from an offensive player\ntackle—when a player brings down the ball carrier\ntackle solo—when a player is the main tackler (i.e., the primary player to bring down the ball carrier)\ntackle assist—when a player is one of two or more players who, together, bring down the ball carrier\ntackle for loss—when an offensive player is tackled with the ball behind the line of scrimmage\nsack—when a Quarterback is tackled with the ball behind the line of scrimmage\npass defended—when a defensive player knocks down the ball in the air so that the indended receiver cannot catch the ball\nsafety—when the offense is tackled with the ball in their own end zone",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#sec-fantasyFootball",
    "href": "fantasy-football.html#sec-fantasyFootball",
    "title": "2  Intro to Football and Fantasy",
    "section": "2.2 Fantasy Football",
    "text": "2.2 Fantasy Football\n\n2.2.1 Overview of Fantasy Football\nFantasy football is one of the most widely played games in the history of games. It is estimated that around 62 million people play fantasy sports3, of whom around 29 million play fantasy football.4 As noted in the Introduction, fantasy football is an online game where participants assemble (i.e., “draft”) imaginary teams composed of real-life National Football League (NFL) players.5 The participants are in charge of managing and making strategic decisions for their imaginary team to have the best possible team that will score the most points. Thus, the participants are called “managers”. Managers make decisions such as selecting which players to draft, selecting which players to play (i.e., “start”) on a weekly basis, identifying players to pick up from the remaining pool of available players (i.e., waiver wire), and making trades with other teams. Fantasy football relies heavily on prediction—trying to predict which players will perform best and selecting them accordingly.\nHere is a video that provides a brief introduction to fantasy football:\n\n\nVideo\nVideo that Provides an Introduction to Fantasy Football.\n\n\n\n\n2.2.2 The Fantasy League\nA fantasy football “league” is composed of various imaginary (i.e., “fantasy”) teams—and their associated manager. In the fantasy league, the managers’ fantasy teams play against each other. A fantasy league is commonly composed of 8, 10, or 12 fantasy teams, but leagues can have more or fewer teams.\n\n\n2.2.3 The Roster of a Fantasy Team\nOn a given roster, a manager has a “starting lineup” and a “bench”. Each week, the manager decides which players on their roster to put in the starting lineup, and which to keep on the bench. In many leagues, a starting lineup is composed of offensive players, a kicker, and defense/special teams:\nOffensive players:\n\n\n\nTable 2.1: Offensive Players in the Starting Lineup\n\n\n\n\n\n\n\n\n\nPosition\nTypical Number of Players in Starting Lineup\n\n\n\n\nQuarterback (QB)\n1\n\n\nRunning Back (RB)\n2\n\n\nWide Receiver (WR)\n2\n\n\nTight End (TE)\n1\n\n\nFlex Position\n1\n\n\n\n\n\n\nA “flex position” is a flexible position that can involve a player from various positions: e.g., a Running Back, Wide Receiver, or Tight End.\nKickers:\n\none Kicker (K)\n\nDefense/Special Teams:\n\none Team Defense (DST/D/DEF) or multiple Individual Defensive Players (IDP)\n\n\n\n2.2.4 Scoring\n\n2.2.4.1 Scoring Overview\nIn the game of fantasy football, managers accumulate points on a weekly basis based on players’ actual statistical performances in NFL games. Managers receive points for only those players who are on their starting lineup (not players on their bench). A manager’s goal is to outscore their opponent each week to win matches and ultimately claim victory in the league. Scoring settings can differ from league to league.\nBelow are common scoring settings for fantasy leagues.\n\n\n2.2.4.2 Offensive Players\n\n\n\nTable 2.2: Common Scoring Settings for Offensive Players\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nRushing or receiving TD\n6\n\n\nReturning a kick or punt for a TD\n6\n\n\nReturning or recovering a fumble for a TD\n6\n\n\nPassing TD\n4\n\n\nPassing INT\n−2\n\n\nFumble lost\n−2\n\n\nRushing, passing, or receiving 2-point conversion\n2\n\n\nRushing or receiving yards\n1 point per 10 yards\n\n\nPassing yards\n1 point per 25 yards\n\n\n\n\n\n\nNote: “TD” = touchdown; “INT” = interception\nOther common (but not necessarily standard) statistical categories include:\n\nreceptions (called “point per reception” [PPR] leagues)\nreturn yards\npassing attempts\nrushing attempts\n\n\n\n2.2.4.3 Kickers\n\n\n\nTable 2.3: Common Scoring Settings for Kickers\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nFG made: 50+ yards\n5\n\n\nFG made: 40–49 yards\n4\n\n\nFG made: 39 yards or less\n3\n\n\nRushing, passing, or receiving 2-point conversion\n2\n\n\nPoint after touchdown attempt made\n1\n\n\nPoint after touchdown attempt missed\n−1\n\n\nMissed FG: 0–39 yards\n−2\n\n\nMissed FG: 40–49 yards\n−1\n\n\n\n\n\n\nNote: “FG” = field goal\n\n\n2.2.4.4 Team Defense/Special Teams\n\n\n\nTable 2.4: Common Scoring Settings for Team Defense/Special Teams\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nDefensive or special teams TD\n3\n\n\nInterception\n2\n\n\nFumble recovery\n2\n\n\nBlocked punt, PAT, or FG\n2\n\n\nSafety\n2\n\n\nSack\n1\n\n\n\n\n\n\nNote: “TD” = touchdown; “PAT” = point after touchdown; “FG” = field goal\n\n\n2.2.4.5 Individual Defensive Players\n\n\n\nTable 2.5: Common Scoring Settings for Individual Defensive Players\n\n\n\n\n\n\n\n\n\nStatistical category\nPoints\n\n\n\n\nTackle solo\n1\n\n\nTackle assist\n0.5\n\n\nTackle for loss\n1\n\n\nSack\n2\n\n\nInterception\n4\n\n\nFumble forced\n2\n\n\nFumble recovery\n2\n\n\nTD\n6\n\n\nSafety\n2\n\n\nPass defended\n1\n\n\nBlocked kick\n2\n\n\nExtra point returned\n2\n\n\n\n\n\n\nNote: “TD” = touchdown\nOther common (but not necessarily standard) statistical categories include:\n\nturnover return yards\n\n\n\n2.2.4.6 Common Scoring Abbreviations\n\n“TD” = touchdown\n“INT” = interception\n“yds” = yards\n“ATT” = attempts\n“2-pt conversion” = two-point conversion\n“FG” = field goal\n“PAT” = point after touchdown (i.e., extra point/point after attempt)\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "fantasy-football.html#footnotes",
    "href": "fantasy-football.html#footnotes",
    "title": "2  Intro to Football and Fantasy",
    "section": "",
    "text": "https://news.gallup.com/poll/610046/football-retains-dominant-position-favorite-sport.aspx (archived at https://perma.cc/X2UG-RAAK); https://www.statista.com/statistics/1430289/most-watched-sports-leagues-usa/ (archived at https://perma.cc/JNU6-S96A)↩︎\nOne yard is equal to three feet. A yard is just smaller than a meter (0.9144 meters).↩︎\nhttps://thefsga.org/industry-demographics/ (archived at https://perma.cc/9PB8-ZDJJ)↩︎\nhttps://www.statista.com/topics/10895/fantasy-sports-in-the-us/ (archived at https://perma.cc/8YSN-UUNT)↩︎\nFantasy leagues are also available for baseball, basketball, and many other sports.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro to Football and Fantasy</span>"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "",
    "text": "3.1 Initial Setup\nTo get started, follow the following steps:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-initialSetup",
    "href": "getting-started.html#sec-initialSetup",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "",
    "text": "Install R: https://cran.r-project.org\nInstall RStudio Desktop: https://posit.co/download/rstudio-desktop\n\nAfter installing RStudio, open RStudio and run the following code in the console to install several key R packages:\n\nCodeinstall.packages(\n  c(\"petersenlab\",\"remotes\",\"nflreadr\",\"nflfastR\",\"nfl4th\",\"nflplotR\",\n  \"gsisdecoder\",\"progressr\",\"lubridate\",\"tidyverse\",\"psych\"))\n\n\n\n\nSome necessary packages, including the ffanalytics package, are hosted in GitHub and need to be installed using the following code (after installing the remotes package above):\n\nCoderemotes::install_github(\"FantasyFootballAnalytics/ffanalytics\")\n\n\n\n\n\n\n\n\n\n\nNote 3.1: If you are in Dr. Petersen’s class\n\n\n\nIf you are in Dr. Petersen’s class, also perform the following steps:\n\nSet up a free account on GitHub.com.\nDownload GitHub Desktop: https://desktop.github.com\n\nMake sure you are logged into your GitHub account on GitHub.com.\nGo to the following GitHub repository: https://github.com/isaactpetersen/QuartoBlogFantasyFootball and complete the following steps:\nClick “Use this Template” (in the top right of the screen) &gt; “Create a new repository”\nMake sure the checkbox is selected for the following option: “Include all branches”\nMake sure your Owner account is selected\nSpecify the repository name to whatever you want, such as FantasyFootballBlog\n\nType a brief description, such as Files for my fantasy football blog\n\nKeep the repository public (this is necessary for generating your blog)\nSelect “Create repository”\nAfter creating the new repository, make sure you are on the page of of your new repository and complete the following steps:\nClick “Settings” (in the topof the screen)\nClick “Actions” (in the left sidebar) &gt; “General”\nMake sure the following are selected: - “Read and write permissions” (under “Workflow permissions”) - “Allow GitHub Actions to create and approve pull requests” - then click “Save”\nClick “Pages” (in the left sidebar)\nMake sure the following are selected: - “Deploy from a branch” (under “Source”) - “gh-pages/(root)” (under “Branch”) - then click “Save”\nClone the repository to your local computer by clicking “Code” &gt; “Open with GitHub Desktop”, select the folder where you want the repository to be saved on your local computer, and click “Clone”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-installingPackages",
    "href": "getting-started.html#sec-installingPackages",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.2 Installing Packages",
    "text": "3.2 Installing Packages\nYou can install R packages using the following syntax:\n\nCodeinstall.packages(\"INSERT_PACKAGE_NAME_HERE\")\n\n\nFor instance, you can use the following code to install the nflreadr package:\n\nCodeinstall.packages(\"nflreadr\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-loadPackages",
    "href": "getting-started.html#sec-loadPackages",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.3 Load Packages",
    "text": "3.3 Load Packages\n\nCodelibrary(\"ffanalytics\")\nlibrary(\"nflreadr\")\nlibrary(\"nflfastR\")\nlibrary(\"nfl4th\")\nlibrary(\"nflplotR\")\nlibrary(\"progressr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-functionsArguments",
    "href": "getting-started.html#sec-functionsArguments",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.4 Using Functions and Arguments",
    "text": "3.4 Using Functions and Arguments\nYou can learn about a particular function and its arguments by entering a question mark before the name of the function:\n\nCode?NAME_OF_FUNCTION()\n\n\nBelow, we provide examples for how to learn about and use functions and arguments, by using the seq() function as an example. The seq() function creates a sequence of numbers. To learn about the seq() function, which creates a sequence of numbers, you can execute the following command:\n\nCode?seq()\n\n\nThis is what the documentation shows for the seq() function in the Usage section:\n\nCodeseq(\n  from = 1,\n  to = 1,\n  by = ((to - from)/(length.out - 1)),\n  length.out = NULL,\n  along.with = NULL,\n  ...)\n\n\nBased on this information, we know that the seq() function takes the following arguments:\n\nfrom\nto\nby\nlength.out\nalong.with\n...\n\nThe arguments have default values that are used if the user does not specify values for the arguments. The default values are provided in the Usage section and are in Table 3.1:\n\n\nTable 3.1: Arguments and defaults for the seq() function. Arguments with a default of NULL are not used unless a value is provided by the user.\n\n\n\nArgument\nDefault Value for Argument\n\n\n\nfrom\n1\n\n\nto\n1\n\n\nby\n((to - from)/(length.out - 1))\n\n\nlength.out\nNULL\n\n\nalong.with\nNULL\n\n\n\n\n\n\nWhat each argument represents (i.e., the meaning of from, to, by, etc.) is provided in the Arguments section of the documentation. You can specify a function and its arguments either by providing values for each argument in the order indicated by the function, or by naming its arguments.\nHere is an example of providing values to the arguments in the order indicated by the function, to create a sequence of numbers from 1 to 9:\n\nCodeseq(1, 9)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nHere is an example of providing values to the arguments by naming its arguments:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 1)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nIf you provide values to arguments by naming the arguments, you can reorder the arguments and get the same answer:\n\nCodeseq(\n  by = 1,\n  to = 9,\n  from = 1)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\nThere are various combinations of arguments that one could use to obtain the same result. For instance, here is code to generate a sequence from 1 to 9 by 2:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 2)\n\n[1] 1 3 5 7 9\n\n\nOr, alternatively, you could specify the length of the desired sequence (5 values):\n\nCodeseq(\n  from = 1,\n  to = 9,\n  length.out = 5)\n\n[1] 1 3 5 7 9\n\n\nIf you want to generate a series with decimal values, you could specify a long desired sequence of 81 values:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  length.out = 81)\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n[20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n[39] 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6\n[58] 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 8.5\n[77] 8.6 8.7 8.8 8.9 9.0\n\n\nThis is equivalent to specifying a sequence from 1 to 9 by 0.1:\n\nCodeseq(\n  from = 1,\n  to = 9,\n  by = 0.1)\n\n [1] 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8\n[20] 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7\n[39] 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6\n[58] 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 8.4 8.5\n[77] 8.6 8.7 8.8 8.9 9.0\n\n\nHopefully, that provides an example for how to learn about a particular function, its arguments, and how to use them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-downloadFootballData",
    "href": "getting-started.html#sec-downloadFootballData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.5 Download Football Data",
    "text": "3.5 Download Football Data\nBelow, we provide examples for how to download various types of National Football League (NFL) data. For additional resources, Congelio (2023) provides a helpful introductory text for working with NFL data in R. We save each data file after downloading it, so we can use the data in subsequent chapters.\n\n3.5.1 Players\n\nCodenfl_players &lt;- progressr::with_progress(\n  nflreadr::load_players())\n\n\n\nCodesave(\n  nfl_players,\n  file = \"./data/nfl_players.RData\"\n)\n\n\n\n3.5.2 Teams\n\nCodenfl_teams &lt;- progressr::with_progress(\n  nflreadr::load_teams(current = TRUE))\n\n\n\nCodesave(\n  nfl_teams,\n  file = \"./data/nfl_teams.RData\"\n)\n\n\n\n3.5.3 Player Info\n\n3.5.4 Rosters\nA Data Dictionary for rosters is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_rosters.html\n\nCodenfl_rosters &lt;- progressr::with_progress(\n  nflreadr::load_rosters(seasons = TRUE))\n\nnfl_rosters_weekly &lt;- progressr::with_progress(\n  nflreadr::load_rosters_weekly(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_rosters,\n  file = \"./data/nfl_rosters.RData\"\n)\n\nsave(\n  nfl_rosters_weekly,\n  file = \"./data/nfl_rosters_weekly.RData\"\n)\n\n\n\n3.5.5 Game Schedules\nA Data Dictionary for game schedules data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_schedules.html\n\nCodenfl_schedules &lt;- progressr::with_progress(\n  nflreadr::load_schedules(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_schedules,\n  file = \"./data/nfl_schedules.RData\"\n)\n\n\n\n3.5.6 The Combine\nA Data Dictionary for data from the combine is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_combine.html\n\nCodenfl_combine &lt;- progressr::with_progress(\n  nflreadr::load_combine(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_combine,\n  file = \"./data/nfl_combine.RData\"\n)\n\n\n\n3.5.7 Draft Picks\nA Data Dictionary for draft picks data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_draft_picks.html\n\nCodenfl_draftPicks &lt;- progressr::with_progress(\n  nflreadr::load_draft_picks(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_draftPicks,\n  file = \"./data/nfl_draftPicks.RData\"\n)\n\n\n\n3.5.8 Depth Charts\nA Data Dictionary for data from weekly depth charts is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_depth_charts.html\n\nCodenfl_depthCharts &lt;- progressr::with_progress(\n  nflreadr::load_depth_charts(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_depthCharts,\n  file = \"./data/nfl_depthCharts.RData\"\n)\n\n\n\n3.5.9 Play-By-Play Data\nTo download play-by-play data from prior weeks and seasons, we can use the load_pbp() function of the nflreadr package. We add a progress bar using the with_progress() function from the progressr package because it takes a while to run. A Data Dictionary for the play-by-play data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_pbp.html\n\n\n\n\n\n\nNote 3.2: Downloading play-by-play data\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_pbp &lt;- progressr::with_progress(\n  nflreadr::load_pbp(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_pbp,\n  file = \"./data/nfl_pbp.RData\"\n)\n\n\n\n3.5.10 4th Down Data\n\n\n\n\n\n\nNote 3.3: Downloading 4th down data\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_4thdown &lt;- nfl4th::load_4th_pbp(seasons = 2014:2023)\n\n\n\nCodesave(\n  nfl_4thdown,\n  file = \"./data/nfl_4thdown.RData\"\n)\n\n\n\n3.5.11 Participation\nA Data Dictionary for the participation data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_participation.html\n\nCodenfl_participation &lt;- progressr::with_progress(\n  nflreadr::load_participation(\n    seasons = TRUE,\n    include_pbp = TRUE))\n\n\n\nCodesave(\n  nfl_participation,\n  file = \"./data/nfl_participation.RData\"\n)\n\n\n\n3.5.12 Historical Weekly Actual Player Statistics\nWe can download historical week-by-week actual player statistics using the load_player_stats() function from the nflreadr package. A Data Dictionary for statistics for offensive players is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_player_stats.html. A Data Dictionary for statistics for defensive players is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_player_stats_def.html.\n\nCodenfl_actualStats_offense_weekly &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"offense\"))\n\nnfl_actualStats_defense_weekly &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"defense\"))\n\nnfl_actualStats_kicking_weekly &lt;- progressr::with_progress(\n  nflreadr::load_player_stats(\n    seasons = TRUE,\n    stat_type = \"kicking\"))\n\n\n\nCodesave(\n  nfl_actualStats_offense_weekly, nfl_actualStats_defense_weekly, nfl_actualStats_kicking_weekly,\n  file = \"./data/nfl_actualStats_weekly.RData\"\n)\n\n\n\n3.5.13 Injuries\nA Data Dictionary for injury data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_injuries.html\n\nCodenfl_injuries &lt;- progressr::with_progress(\n  nflreadr::load_injuries(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_injuries,\n  file = \"./data/nfl_injuries.RData\"\n)\n\n\n\n3.5.14 Snap Counts\nA Data Dictionary for snap counts data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_snap_counts.html\n\nCodenfl_snapCounts &lt;- progressr::with_progress(\n  nflreadr::load_snap_counts(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_snapCounts,\n  file = \"./data/nfl_snapCounts.RData\"\n)\n\n\n\n3.5.15 ESPN QBR\nA Data Dictionary for ESPN QBR data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_espn_qbr.html\n\nCodenfl_espnQBR_seasonal &lt;- progressr::with_progress(\n  nflreadr::load_espn_qbr(\n    seasons = TRUE,\n    summary_type = c(\"season\")))\n\nnfl_espnQBR_weekly &lt;- progressr::with_progress(\n  nflreadr::load_espn_qbr(\n    seasons = TRUE,\n    summary_type = c(\"weekly\")))\n\nnfl_espnQBR_weekly$game_week &lt;- as.character(nfl_espnQBR_weekly$game_week)\n\nnfl_espnQBR &lt;- bind_rows(\n  nfl_espnQBR_seasonal,\n  nfl_espnQBR_weekly\n)\n\n\n\nCodesave(\n  nfl_espnQBR,\n  file = \"./data/nfl_espnQBR.RData\"\n)\n\n\n\n3.5.16 NFL Next Gen Stats\nA Data Dictionary for NFL Next Gen Stats data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_nextgen_stats.html\n\nCodenfl_nextGenStats_pass_weekly &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"passing\")))\n\nnfl_nextGenStats_rush_weekly &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"rushing\")))\n\nnfl_nextGenStats_rec_weekly &lt;- progressr::with_progress(\n  nflreadr::load_nextgen_stats(\n    seasons = TRUE,\n    stat_type = c(\"receiving\")))\n\nnfl_nextGenStats_weekly &lt;- bind_rows(\n  nfl_nextGenStats_pass_weekly,\n  nfl_nextGenStats_rush_weekly,\n  nfl_nextGenStats_rec_weekly\n)\n\n\n\nCodesave(\n  nfl_nextGenStats_weekly,\n  file = \"./data/nfl_nextGenStats_weekly.RData\"\n)\n\n\n\n3.5.17 Advanced Stats from PFR\nA Data Dictionary for PFR passing data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_pfr_passing.html\n\nCodenfl_advancedStatsPFR_pass_seasonal &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"pass\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_pass_weekly &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"pass\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_rush_seasonal &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rush\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_rush_weekly &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rush\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_rec_seasonal &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rec\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_rec_weekly &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"rec\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR_def_seasonal &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"def\"),\n    summary_level = c(\"season\")))\n\nnfl_advancedStatsPFR_def_weekly &lt;- progressr::with_progress(\n  nflreadr::load_pfr_advstats(\n    seasons = TRUE,\n    stat_type = c(\"def\"),\n    summary_level = c(\"week\")))\n\nnfl_advancedStatsPFR &lt;- bind_rows(\n  nfl_advancedStatsPFR_pass_seasonal,\n  nfl_advancedStatsPFR_pass_weekly,\n  nfl_advancedStatsPFR_rush_seasonal,\n  nfl_advancedStatsPFR_rush_weekly,\n  nfl_advancedStatsPFR_rec_seasonal,\n  nfl_advancedStatsPFR_rec_weekly,\n  nfl_advancedStatsPFR_def_seasonal,\n  nfl_advancedStatsPFR_def_weekly,\n)\n\n\n\nCodesave(\n  nfl_advancedStatsPFR,\n  file = \"./data/nfl_advancedStatsPFR.RData\"\n)\n\n\n\n3.5.18 Player Contracts\nA Data Dictionary for player contracts data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_contracts.html\n\nCodenfl_playerContracts &lt;- progressr::with_progress(\n  nflreadr::load_contracts())\n\n\n\nCodesave(\n  nfl_playerContracts,\n  file = \"./data/nfl_playerContracts.RData\"\n)\n\n\n\n3.5.19 FTN Charting Data\nA Data Dictionary for FTN Charting data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_ftn_charting.html\n\nCodenfl_ftnCharting &lt;- progressr::with_progress(\n  nflreadr::load_ftn_charting(seasons = TRUE))\n\n\n\nCodesave(\n  nfl_ftnCharting,\n  file = \"./data/nfl_ftnCharting.RData\"\n)\n\n\n\n3.5.20 Fantasy Player IDs\nA Data Dictionary for fantasy player ID data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_playerids.html\n\nCodenfl_playerIDs &lt;- progressr::with_progress(\n  nflreadr::load_ff_playerids())\n\n\n\nCodesave(\n  nfl_playerIDs,\n  file = \"./data/nfl_playerIDs.RData\"\n)\n\n\n\n3.5.21 FantasyPros Rankings\nA Data Dictionary for FantasyPros ranking data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_rankings.html\n\nCode#nfl_rankings &lt;- progressr::with_progress( # currently throws error\n#  nflreadr::load_ff_rankings(type = \"all\"))\n\nnfl_rankings_draft &lt;- progressr::with_progress(\n  nflreadr::load_ff_rankings(type = \"draft\"))\n\nnfl_rankings_weekly &lt;- progressr::with_progress(\n  nflreadr::load_ff_rankings(type = \"week\"))\n\nnfl_rankings &lt;- bind_rows(\n  nfl_rankings_draft,\n  nfl_rankings_weekly\n)\n\n\n\nCodesave(\n  nfl_rankings,\n  file = \"./data/nfl_rankings.RData\"\n)\n\n\n\n3.5.22 Expected Fantasy Points\nA Data Dictionary for expected fantasy points data is located at the following link: https://nflreadr.nflverse.com/articles/dictionary_ff_opportunity.html\n\nCodenfl_expectedFantasyPoints_weekly &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"weekly\",\n    model_version = \"latest\"\n  ))\n\nnfl_expectedFantasyPoints_pass &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"pbp_pass\",\n    model_version = \"latest\"\n  ))\n\nnfl_expectedFantasyPoints_rush &lt;- progressr::with_progress(\n  nflreadr::load_ff_opportunity(\n    seasons = TRUE,\n    stat_type = \"pbp_rush\",\n    model_version = \"latest\"\n  ))\n\nnfl_expectedFantasyPoints_weekly$season &lt;- as.integer(nfl_expectedFantasyPoints_weekly$season)\n\nnfl_expectedFantasyPoints_offense &lt;- bind_rows(\n  nfl_expectedFantasyPoints_pass,\n  nfl_expectedFantasyPoints_rush\n)\n\n\n\nCodesave(\n  nfl_expectedFantasyPoints_weekly,\n  file = \"./data/nfl_expectedFantasyPoints_weekly.RData\"\n)\n\nsave(\n  nfl_expectedFantasyPoints_offense,\n  file = \"./data/nfl_expectedFantasyPoints_offense.RData\"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-dataDictionary",
    "href": "getting-started.html#sec-dataDictionary",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.6 Data Dictionary",
    "text": "3.6 Data Dictionary\nData Dictionaries are metadata that describe the meaning of the variables in a datset. You can find Data Dictionaries for the various NFL datasets at the following link: https://nflreadr.nflverse.com/articles/index.html.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createDF",
    "href": "getting-started.html#sec-createDF",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.7 Create a Data Frame",
    "text": "3.7 Create a Data Frame\nHere is an example of creating a data frame:\n\nCodeplayers &lt;- data.frame(\n  ID = 1:12,\n  name = c(\n    \"Ken Cussion\",\n    \"Ben Sacked\",\n    \"Chuck Downfield\",\n    \"Ron Ingback\",\n    \"Rhonda Ball\",\n    \"Hugo Long\",\n    \"Lionel Scrimmage\",\n    \"Drew Blood\",\n    \"Chase Emdown\",\n    \"Justin Time\",\n    \"Spike D'Ball\",\n    \"Isac Ulooz\"),\n  position = c(\"QB\",\"QB\",\"QB\",\"RB\",\"RB\",\"WR\",\"WR\",\"WR\",\"WR\",\"TE\",\"TE\",\"LB\"),\n  age = c(40, 30, 24, 20, 18, 23, 27, 32, 26, 23, NA, 37)\n  )\n\nfantasyPoints &lt;- data.frame(\n  ID = c(2, 7, 13, 14),\n  fantasyPoints = c(250, 170, 65, 15)\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-variableNames",
    "href": "getting-started.html#sec-variableNames",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.8 Variable Names",
    "text": "3.8 Variable Names\nTo see the names of variables in a data frame, use the following syntax:\n\nCodenames(nfl_players)\n\n [1] \"status\"                   \"display_name\"            \n [3] \"first_name\"               \"last_name\"               \n [5] \"esb_id\"                   \"gsis_id\"                 \n [7] \"suffix\"                   \"birth_date\"              \n [9] \"college_name\"             \"position_group\"          \n[11] \"position\"                 \"jersey_number\"           \n[13] \"height\"                   \"weight\"                  \n[15] \"years_of_experience\"      \"team_abbr\"               \n[17] \"team_seq\"                 \"current_team_id\"         \n[19] \"football_name\"            \"entry_year\"              \n[21] \"rookie_year\"              \"draft_club\"              \n[23] \"college_conference\"       \"status_description_abbr\" \n[25] \"status_short_description\" \"gsis_it_id\"              \n[27] \"short_name\"               \"smart_id\"                \n[29] \"headshot\"                 \"draft_number\"            \n[31] \"uniform_number\"           \"draft_round\"             \n[33] \"season\"                  \n\nCodenames(players)\n\n[1] \"ID\"       \"name\"     \"position\" \"age\"     \n\nCodenames(fantasyPoints)\n\n[1] \"ID\"            \"fantasyPoints\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-logicalOperators",
    "href": "getting-started.html#sec-logicalOperators",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.9 Logical Operators",
    "text": "3.9 Logical Operators\n\n3.9.1 Is Equal To: ==\n\n\nCodeplayers$position == \"RB\"\n\n [1] FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n3.9.2 Is Not Equal To: !=\n\n\nCodeplayers$position != \"RB\"\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\n\n3.9.3 Is Greater Than: &gt;\n\n\nCodeplayers$age &gt; 30\n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE    NA  TRUE\n\n\n\n3.9.4 Is Less Than: &lt;\n\n\nCodeplayers$age &lt; 30\n\n [1] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA FALSE\n\n\n\n3.9.5 Is Greater Than or Equal To: &gt;=\n\n\nCodeplayers$age &gt;= 30\n\n [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE    NA  TRUE\n\n\n\n3.9.6 Is Less Than or Equal To: &lt;=\n\n\nCodeplayers$age &lt;= 30\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE    NA FALSE\n\n\n\n3.9.7 Is In a Value of Another Vector: %in%\n\n\nCodeplayers$position %in% c(\"RB\",\"WR\")\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\n\n3.9.8 Is Not In a Value of Another Vector: !(%in%)\n\n\nCode!(players$position %in% c(\"RB\",\"WR\"))\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\n\n3.9.9 Is Missing: is.na()\n\n\nCodeis.na(players$age)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n3.9.10 Is Not Missing: !is.na()\n\n\nCode!is.na(players$age)\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n\n\n\n3.9.11 And: &\n\n\nCodeplayers$position == \"WR\" & players$age &gt; 26\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\n\n3.9.12 Or: |\n\n\nCodeplayers$position == \"WR\" | players$age &gt; 23\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE    NA  TRUE",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-subset",
    "href": "getting-started.html#sec-subset",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.10 Subset",
    "text": "3.10 Subset\nTo subset a data frame, use brackets to specify the subset of rows and columns to keep, where the value/vector before the comma specifies the rows to keep, and the value/vector after the comma specifies the columns to keep:\n\nCodedataframe[rowsToKeep, columnsToKeep]\n\n\nYou can subset by using any of the following:\n\nnumeric indices of the rows/columns to keep (or drop)\nnames of the rows/columns to keep (or drop)\nvalues of TRUE and FALSE corresponding to which rows/columns to keep\n\n\n3.10.1 One Variable\nTo subset one variable, use the following syntax:\n\nCodeplayers$name\n\n [1] \"Ken Cussion\"      \"Ben Sacked\"       \"Chuck Downfield\"  \"Ron Ingback\"     \n [5] \"Rhonda Ball\"      \"Hugo Long\"        \"Lionel Scrimmage\" \"Drew Blood\"      \n [9] \"Chase Emdown\"     \"Justin Time\"      \"Spike D'Ball\"     \"Isac Ulooz\"      \n\n\nor:\n\nCodeplayers[,\"name\"]\n\n [1] \"Ken Cussion\"      \"Ben Sacked\"       \"Chuck Downfield\"  \"Ron Ingback\"     \n [5] \"Rhonda Ball\"      \"Hugo Long\"        \"Lionel Scrimmage\" \"Drew Blood\"      \n [9] \"Chase Emdown\"     \"Justin Time\"      \"Spike D'Ball\"     \"Isac Ulooz\"      \n\n\n\n3.10.2 Particular Rows of One Variable\nTo subset one variable, use the following syntax:\n\nCodeplayers$name[which(players$position == \"RB\")]\n\n[1] \"Ron Ingback\" \"Rhonda Ball\"\n\n\nor:\n\nCodeplayers[which(players$position == \"RB\"), \"name\"]\n\n[1] \"Ron Ingback\" \"Rhonda Ball\"\n\n\n\n3.10.3 Particular Columns (Variables)\nTo subset particular columns/variables, use the following syntax:\n\n3.10.3.1 Base R\n\n\nCodesubsetVars &lt;- c(\"name\",\"age\")\n\nplayers[,c(2,4)]\n\n\n  \n\n\nCodeplayers[,c(\"name\",\"age\")]\n\n\n  \n\n\nCodeplayers[,subsetVars]\n\n\n  \n\n\n\nOr, to drop columns:\n\nCodedropVars &lt;- c(\"name\",\"age\")\n\nplayers[,-c(2,4)]\n\n\n  \n\n\nCodeplayers[,!(names(players) %in% c(\"name\",\"age\"))]\n\n\n  \n\n\nCodeplayers[,!(names(players) %in% dropVars)]\n\n\n  \n\n\n\n\n3.10.3.2 Tidyverse\n\nCodeplayers %&gt;%\n  select(name, age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(name:age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(all_of(subsetVars))\n\n\n  \n\n\n\nOr, to drop columns:\n\nCodeplayers %&gt;%\n  select(-name, -age)\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(-c(name:age))\n\n\n  \n\n\nCodeplayers %&gt;%\n  select(-all_of(dropVars))\n\n\n  \n\n\n\n\n3.10.4 Particular Rows\nTo subset particular rows, use the following syntax:\n\n3.10.4.1 Base R\n\n\nCodesubsetRows &lt;- c(4,5)\n\nplayers[c(4,5),]\n\n\n  \n\n\nCodeplayers[subsetRows,]\n\n\n  \n\n\nCodeplayers[which(players$position == \"RB\"),]\n\n\n  \n\n\n\n\n3.10.4.2 Tidyverse\n\nCodeplayers %&gt;%\n  filter(position == \"WR\")\n\n\n  \n\n\nCodeplayers %&gt;%\n  filter(position == \"WR\", age &lt;= 26)\n\n\n  \n\n\nCodeplayers %&gt;%\n  filter(position == \"WR\" | age &gt;= 26)\n\n\n  \n\n\n\n\n3.10.5 Particular Rows and Columns\nTo subset particular rows and columns, use the following syntax:\n\n3.10.5.1 Base R\n\n\nCodeplayers[c(4,5), c(2,4)]\n\n\n  \n\n\nCodeplayers[subsetRows, subsetVars]\n\n\n  \n\n\nCodeplayers[which(players$position == \"RB\"), subsetVars]\n\n\n  \n\n\n\n\n3.10.5.2 Tidyverse\n\nCodeplayers %&gt;%\n  filter(position == \"RB\") %&gt;%\n  select(all_of(subsetVars))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-viewData",
    "href": "getting-started.html#sec-viewData",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.11 View Data",
    "text": "3.11 View Data\n\n3.11.1 All Data\nTo view data, use the following syntax:\n\nCodeView(players)\n\n\n\n3.11.2 First 6 Rows/Elements\nTo view only the first six rows (if a data frame) or elements (if a vector), use the following syntax:\n\nCodehead(nfl_players)\n\n\n  \n\n\nCodehead(nfl_players$display_name)\n\n[1] \"'Omar Ellison\"    \"A'Shawn Robinson\" \"A.J. Arcuri\"      \"A.J. Bouye\"      \n[5] \"A.J. Brown\"       \"A.J. Cann\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-dataCharacteristics",
    "href": "getting-started.html#sec-dataCharacteristics",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.12 Data Characteristics",
    "text": "3.12 Data Characteristics\n\n3.12.1 Data Structure\n\nCodestr(nfl_players)\n\nnflvrs_d [20,039 × 33] (S3: nflverse_data/tbl_df/tbl/data.table/data.frame)\n $ status                  : chr [1:20039] \"RET\" \"ACT\" \"ACT\" \"RES\" ...\n $ display_name            : chr [1:20039] \"'Omar Ellison\" \"A'Shawn Robinson\" \"A.J. Arcuri\" \"A.J. Bouye\" ...\n $ first_name              : chr [1:20039] \"'Omar\" \"A'Shawn\" \"A.J.\" \"Arlandus\" ...\n $ last_name               : chr [1:20039] \"Ellison\" \"Robinson\" \"Arcuri\" \"Bouye\" ...\n $ esb_id                  : chr [1:20039] \"ELL711319\" \"ROB367960\" \"ARC716900\" \"BOU651714\" ...\n $ gsis_id                 : chr [1:20039] \"00-0004866\" \"00-0032889\" \"00-0037845\" \"00-0030228\" ...\n $ suffix                  : chr [1:20039] NA NA NA NA ...\n $ birth_date              : chr [1:20039] NA \"1995-03-21\" NA \"1991-08-16\" ...\n $ college_name            : chr [1:20039] NA \"Alabama\" \"Michigan State\" \"Central Florida\" ...\n $ position_group          : chr [1:20039] \"WR\" \"DL\" \"OL\" \"DB\" ...\n $ position                : chr [1:20039] \"WR\" \"DT\" \"T\" \"CB\" ...\n $ jersey_number           : int [1:20039] 84 91 61 24 11 60 6 81 63 20 ...\n $ height                  : num [1:20039] 73 76 79 72 72 75 76 69 76 72 ...\n $ weight                  : int [1:20039] 200 330 320 191 226 325 220 190 280 183 ...\n $ years_of_experience     : chr [1:20039] \"2\" \"8\" \"2\" \"8\" ...\n $ team_abbr               : chr [1:20039] \"LAC\" \"NYG\" \"LA\" \"CAR\" ...\n $ team_seq                : int [1:20039] NA 1 NA 1 1 1 1 NA NA NA ...\n $ current_team_id         : chr [1:20039] \"4400\" \"3410\" \"2510\" \"0750\" ...\n $ football_name           : chr [1:20039] NA \"A'Shawn\" \"A.J.\" \"A.J.\" ...\n $ entry_year              : int [1:20039] NA 2016 2022 2013 2019 2015 2019 NA NA NA ...\n $ rookie_year             : int [1:20039] NA 2016 2022 2013 2019 2015 2019 NA NA NA ...\n $ draft_club              : chr [1:20039] NA \"DET\" \"LA\" NA ...\n $ college_conference      : chr [1:20039] NA \"Southeastern Conference\" \"Big Ten Conference\" \"American Athletic Conference\" ...\n $ status_description_abbr : chr [1:20039] NA \"A01\" \"A01\" \"R01\" ...\n $ status_short_description: chr [1:20039] NA \"Active\" \"Active\" \"R/Injured\" ...\n $ gsis_it_id              : int [1:20039] NA 43335 54726 40688 47834 42410 48335 NA NA NA ...\n $ short_name              : chr [1:20039] NA \"A.Robinson\" \"A.Arcuri\" \"A.Bouye\" ...\n $ smart_id                : chr [1:20039] \"3200454c-4c71-1319-728e-d49d3d236f8f\" \"3200524f-4236-7960-bf20-bc060ac0f49c\" \"32004152-4371-6900-5185-8cdd66b2ad11\" \"3200424f-5565-1714-cb38-07c822111a12\" ...\n $ headshot                : chr [1:20039] NA \"https://static.www.nfl.com/image/private/f_auto,q_auto/league/qgiwxchd1lmgszfunys8\" NA \"https://static.www.nfl.com/image/private/f_auto,q_auto/league/cpgi2hbhnmvs1oczkzas\" ...\n $ draft_number            : int [1:20039] NA 46 261 NA 51 67 NA NA NA NA ...\n $ uniform_number          : chr [1:20039] NA \"91\" \"61\" \"24\" ...\n $ draft_round             : chr [1:20039] NA NA NA NA ...\n $ season                  : int [1:20039] NA NA NA NA NA NA NA NA NA NA ...\n - attr(*, \"nflverse_type\")= chr \"players\"\n - attr(*, \"nflverse_timestamp\")= POSIXct[1:1], format: \"2024-03-01 01:18:40\"\n\n\n\n3.12.2 Data Dimensions\nNumber of rows and columns:\n\nCodedim(nfl_players)\n\n[1] 20039    33\n\n\nNumber of rows:\n\nCodenrow(nfl_players)\n\n[1] 20039\n\n\nNumber of columns:\n\nCodencol(nfl_players)\n\n[1] 33\n\n\n\n3.12.3 Number of Elements\n\nCodelength(nfl_players$display_name)\n\n[1] 20039\n\n\n\n3.12.4 Number of Missing Elements\n\nCodelength(nfl_players$college_name[which(is.na(nfl_players$college_name))])\n\n[1] 12127\n\n\n\n3.12.5 Number of Non-Missing Elements\n\nCodelength(nfl_players$college_name[which(!is.na(nfl_players$college_name))])\n\n[1] 7912\n\nCodelength(na.omit(nfl_players$college_name))\n\n[1] 7912",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-createNewVars",
    "href": "getting-started.html#sec-createNewVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.13 Create New Variables",
    "text": "3.13 Create New Variables\nTo create a new variable, use the following syntax:\n\nCodeplayers$newVar &lt;- NA\n\n\nHere is an example of creating a new variable:\n\nCodeplayers$newVar &lt;- 1:nrow(players)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-recodeVars",
    "href": "getting-started.html#sec-recodeVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.14 Recode Variables",
    "text": "3.14 Recode Variables\nHere is an example of recoding a variable:\n\nCodeplayers$oldVar1 &lt;- NA\nplayers$oldVar1[which(players$position == \"QB\")] &lt;- \"quarterback\"\nplayers$oldVar1[which(players$position == \"RB\")] &lt;- \"running back\"\nplayers$oldVar1[which(players$position == \"WR\")] &lt;- \"wide receiver\"\nplayers$oldVar1[which(players$position == \"TE\")] &lt;- \"tight end\"\n\nplayers$oldVar2 &lt;- NA\nplayers$oldVar2[which(players$age &lt; 30)] &lt;- \"young\"\nplayers$oldVar2[which(players$age &gt;= 30)] &lt;- \"old\"\n\n\nRecode multiple variables:\n\nCodeplayers %&gt;%\n  mutate(across(c(\n    oldVar1:oldVar2),\n    ~ case_match(\n      .,\n      c(\"quarterback\",\"old\",\"running back\") ~ 0,\n      c(\"wide receiver\",\"tight end\",\"young\") ~ 1)))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#renameVars",
    "href": "getting-started.html#renameVars",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.15 Rename Variables",
    "text": "3.15 Rename Variables\n\nCodeplayers &lt;- players %&gt;% \n  rename(\n    newVar1 = oldVar1,\n    newVar2 = oldVar2)\n\n\nUsing a vector of variable names:\n\nCodevarNamesFrom &lt;- c(\"oldVar1\",\"oldVar2\")\nvarNamesTo &lt;- c(\"newVar1\",\"newVar2\")\n\nplayers &lt;- players %&gt;% \n  rename_with(~ varNamesTo, all_of(varNamesFrom))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#convertVarTypes",
    "href": "getting-started.html#convertVarTypes",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.16 Convert the Types of Variables",
    "text": "3.16 Convert the Types of Variables\nOne variable:\n\nCodeplayers$factorVar &lt;- factor(players$ID)\nplayers$numericVar &lt;- as.numeric(players$age)\nplayers$integerVar &lt;- as.integer(players$newVar1)\nplayers$characterVar &lt;- as.character(players$newVar2)\n\n\nMultiple variables:\n\nCodeplayers %&gt;%\n  mutate(across(c(\n    ID,\n    age),\n    as.numeric))\n\n\n  \n\n\nCodeplayers %&gt;%\n  mutate(across(\n    age:newVar1,\n    as.character))\n\n\n  \n\n\nCodeplayers %&gt;%\n  mutate(across(where(is.factor), as.character))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-merging",
    "href": "getting-started.html#sec-merging",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.17 Merging/Joins",
    "text": "3.17 Merging/Joins\n\n3.17.1 Overview\nMerging (also called joining) merges two data objects using a shared set of variables called “keys.” The keys are the variable(s) that uniquely identify each row (i.e., they account for the levels of nesting). In some data objects, the key might be the player’s identification number (e.g., player_id). However, some data objects have multiple keys. For instance, in long form data objects, each participant may have multiple rows corresponding to multiple seasons. In this case, the keys may be player_id and season. If a participant has multiple rows corresponding to seasons and games/weeks, the keys are player_id, season, and week. In general, each row should have a value on each of the keys; there should be no missingness in the keys.\nTo merge two objects, the key(s) that will be used to match the records must be present in both objects. The keys are used to merge the variables in object 1 (x) with the variables in object 2 (y). Different merge types select different rows to merge.\nNote: if the two objects include variables with the same name (apart from the keys), R will not know how you want each to appear in the merged object. So, it will add a suffix (e.g., .x, .y) to each common variable to indicate which object (i.e., object x or object y) the variable came from, where object x is the first object—i.e., the object to which object y (the second object) is merged. In general, apart from the keys, you should not include variables with the same name in two objects to be merged. To prevent this, either remove or rename the shared variable in one of the objects, or include the shared variable as a key. However, as described above, you should include it as a key only if it uniquely identifies each row in terms of levels of nesting.\n\n3.17.2 Data Before Merging\nHere are the data in the players object:\n\nCodeplayers\n\n\n  \n\n\nCodedim(players)\n\n[1] 12 10\n\n\nThe data are structured in ID form. That is, every row in the dataset is uniquely identified by the variable, ID.\nHere are the data in the fantasyPoints object:\n\nCodefantasyPoints\n\n\n  \n\n\nCodedim(fantasyPoints)\n\n[1] 4 2\n\n\n\n3.17.3 Types of Joins\n\n3.17.3.1 Visual Overview of Join Types\nBelow is a visual that depicts various types of merges/joins. Object x is the circle labeled as x. Object y is the circle labeled as y. The area of overlap in the Venn diagram indicates the rows on the keys that are shared between the two objects (e.g., the same player_id, season, and week). The non-overlapping area indicates the rows on the keys that are unique to each object. The shaded blue area indicates which rows (on the keys) are kept in the merged object from each of the two objects, when using each of the merge types. For instance, a left outer join keeps the shared rows and the rows that are unique to object x, but it drops the rows that are unique to object y.\n\n\nTypes of merges/joins\n\n\n3.17.3.2 Full Outer Join\nA full outer join includes all rows in x or y. It returns columns from x and y. Here is how to merge two data frames using a full outer join (i.e., “full join”):\n\nCodefullJoinData &lt;- full_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nfullJoinData\n\n\n  \n\n\nCodedim(fullJoinData)\n\n[1] 14 11\n\n\n\n3.17.3.3 Left Outer Join\nA left outer join includes all rows in x. It returns columns from x and y. Here is how to merge two data frames using a left outer join (“left join”):\n\nCodeleftJoinData &lt;- left_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nleftJoinData\n\n\n  \n\n\nCodedim(leftJoinData)\n\n[1] 12 11\n\n\n\n3.17.3.4 Right Outer Join\nA right outer join includes all rows in y. It returns columns from x and y. Here is how to merge two data frames using a right outer join (“right join”):\n\nCoderightJoinData &lt;- right_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nrightJoinData\n\n\n  \n\n\nCodedim(rightJoinData)\n\n[1]  4 11\n\n\n\n3.17.3.5 Inner Join\nAn inner join includes all rows that are in both x and y. An inner join will return one row of x for each matching row of y, and can duplicate values of records on either side (left or right) if x and y have more than one matching record. It returns columns from x and y. Here is how to merge two data frames using an inner join:\n\nCodeinnerJoinData &lt;- inner_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\ninnerJoinData\n\n\n  \n\n\nCodedim(innerJoinData)\n\n[1]  2 11\n\n\n\n3.17.3.6 Semi Join\nA semi join is a filter. A left semi join returns all rows from x with a match in y. That is, it filters out records from x that are not in y. Unlike an inner join, a left semi join will never duplicate rows of x, and it includes columns from only x (not from y). Here is how to merge two data frames using a left semi join:\n\nCodesemiJoinData &lt;- semi_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nsemiJoinData\n\n\n  \n\n\nCodedim(semiJoinData)\n\n[1]  2 10\n\n\n\n3.17.3.7 Anti Join\nAn anti join is a filter. A left anti join returns all rows from x without a match in y. That is, it filters out records from x that are in y. It returns columns from only x (not from y). Here is how to merge two data frames using a left anti join:\n\nCodeantiJoinData &lt;- anti_join(\n  players,\n  fantasyPoints,\n  by = \"ID\")\n\nantiJoinData\n\n\n  \n\n\nCodedim(antiJoinData)\n\n[1] 10 10\n\n\n\n3.17.3.8 Cross Join\nA cross join combines each row in x with each row in y.\n\nCodecrossJoinData &lt;- cross_join(\n  players,\n  fantasyPoints)\n\ncrossJoinData\n\n\n  \n\n\nCodedim(crossJoinData)\n\n[1] 48 12",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-longToWide",
    "href": "getting-started.html#sec-longToWide",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.18 Transform Data from Long to Wide",
    "text": "3.18 Transform Data from Long to Wide\nDepending on the analysis, it may be important to restructure the data to be in long or wide form. When the data are in wide form, each player has only one row. When the data are in long form, each player has multiple rows—e.g., a row for each game. The data structure is called wide or long form because a dataset in wide form has more columns and fewer rows (i.e., it appears wider and shorter), whereas a dataset in long form has more rows and fewer columns (i.e., it appears narrower and taller).\nHere are the data in the nfl_actualStats_offense_weekly object. The data are structured in “player-season-week form”. That is, every row in the dataset is uniquely identified by the variables, player_id, season, and week. This is an example of long form, because each player has multiple rows.\nOriginal data:\n\nCodedataLong &lt;- nfl_actualStats_offense_weekly %&gt;% \n  select(player_id, player_display_name, season, week, fantasy_points)\n\ndim(dataLong)\n\n[1] 129739      5\n\nCodenames(dataLong)\n\n[1] \"player_id\"           \"player_display_name\" \"season\"             \n[4] \"week\"                \"fantasy_points\"     \n\n\nBelow, we widen the data by two variables (season and week), using tidyverse, so that the data are now in “player form” (where each row is uniquely identified by the player_id variable):\n\nCodedataWide &lt;- dataLong %&gt;% \n  pivot_wider(\n    names_from = c(season, week),\n    names_glue = \"{.value}_{season}_week{week}\",\n    values_from = fantasy_points)\n\ndim(dataWide)\n\n[1] 4021  530\n\nCodenames(dataWide)\n\n  [1] \"player_id\"                  \"player_display_name\"       \n  [3] \"fantasy_points_1999_week1\"  \"fantasy_points_1999_week2\" \n  [5] \"fantasy_points_1999_week4\"  \"fantasy_points_1999_week7\" \n  [7] \"fantasy_points_1999_week8\"  \"fantasy_points_1999_week9\" \n  [9] \"fantasy_points_1999_week10\" \"fantasy_points_1999_week11\"\n [11] \"fantasy_points_1999_week12\" \"fantasy_points_1999_week13\"\n [13] \"fantasy_points_1999_week14\" \"fantasy_points_1999_week15\"\n [15] \"fantasy_points_1999_week16\" \"fantasy_points_1999_week5\" \n [17] \"fantasy_points_1999_week6\"  \"fantasy_points_1999_week17\"\n [19] \"fantasy_points_1999_week18\" \"fantasy_points_1999_week3\" \n [21] \"fantasy_points_1999_week19\" \"fantasy_points_1999_week20\"\n [23] \"fantasy_points_1999_week21\" \"fantasy_points_2000_week1\" \n [25] \"fantasy_points_2000_week12\" \"fantasy_points_2000_week14\"\n [27] \"fantasy_points_2000_week15\" \"fantasy_points_2000_week6\" \n [29] \"fantasy_points_2000_week10\" \"fantasy_points_2000_week4\" \n [31] \"fantasy_points_2000_week5\"  \"fantasy_points_2000_week7\" \n [33] \"fantasy_points_2000_week8\"  \"fantasy_points_2000_week9\" \n [35] \"fantasy_points_2000_week11\" \"fantasy_points_2000_week13\"\n [37] \"fantasy_points_2000_week2\"  \"fantasy_points_2000_week16\"\n [39] \"fantasy_points_2000_week17\" \"fantasy_points_2000_week3\" \n [41] \"fantasy_points_2000_week18\" \"fantasy_points_2000_week19\"\n [43] \"fantasy_points_2000_week21\" \"fantasy_points_2000_week20\"\n [45] \"fantasy_points_2001_week15\" \"fantasy_points_2001_week17\"\n [47] \"fantasy_points_2001_week1\"  \"fantasy_points_2001_week3\" \n [49] \"fantasy_points_2001_week4\"  \"fantasy_points_2001_week5\" \n [51] \"fantasy_points_2001_week6\"  \"fantasy_points_2001_week9\" \n [53] \"fantasy_points_2001_week11\" \"fantasy_points_2001_week12\"\n [55] \"fantasy_points_2001_week13\" \"fantasy_points_2001_week14\"\n [57] \"fantasy_points_2001_week16\" \"fantasy_points_2001_week2\" \n [59] \"fantasy_points_2001_week7\"  \"fantasy_points_2001_week8\" \n [61] \"fantasy_points_2001_week10\" \"fantasy_points_2001_week19\"\n [63] \"fantasy_points_2001_week18\" \"fantasy_points_2001_week20\"\n [65] \"fantasy_points_2001_week21\" \"fantasy_points_2002_week3\" \n [67] \"fantasy_points_2002_week1\"  \"fantasy_points_2002_week2\" \n [69] \"fantasy_points_2002_week4\"  \"fantasy_points_2002_week6\" \n [71] \"fantasy_points_2002_week7\"  \"fantasy_points_2002_week8\" \n [73] \"fantasy_points_2002_week9\"  \"fantasy_points_2002_week5\" \n [75] \"fantasy_points_2002_week10\" \"fantasy_points_2002_week11\"\n [77] \"fantasy_points_2002_week12\" \"fantasy_points_2002_week13\"\n [79] \"fantasy_points_2002_week14\" \"fantasy_points_2002_week15\"\n [81] \"fantasy_points_2002_week16\" \"fantasy_points_2002_week17\"\n [83] \"fantasy_points_2002_week19\" \"fantasy_points_2002_week20\"\n [85] \"fantasy_points_2002_week21\" \"fantasy_points_2002_week18\"\n [87] \"fantasy_points_2003_week2\"  \"fantasy_points_2003_week4\" \n [89] \"fantasy_points_2003_week5\"  \"fantasy_points_2003_week7\" \n [91] \"fantasy_points_2003_week8\"  \"fantasy_points_2003_week9\" \n [93] \"fantasy_points_2003_week10\" \"fantasy_points_2003_week11\"\n [95] \"fantasy_points_2003_week13\" \"fantasy_points_2003_week14\"\n [97] \"fantasy_points_2003_week15\" \"fantasy_points_2003_week17\"\n [99] \"fantasy_points_2003_week1\"  \"fantasy_points_2003_week3\" \n[101] \"fantasy_points_2003_week6\"  \"fantasy_points_2003_week12\"\n[103] \"fantasy_points_2003_week16\" \"fantasy_points_2003_week18\"\n[105] \"fantasy_points_2003_week19\" \"fantasy_points_2003_week20\"\n[107] \"fantasy_points_2003_week21\" \"fantasy_points_2004_week2\" \n[109] \"fantasy_points_2004_week5\"  \"fantasy_points_2004_week6\" \n[111] \"fantasy_points_2004_week10\" \"fantasy_points_2004_week16\"\n[113] \"fantasy_points_2004_week17\" \"fantasy_points_2004_week1\" \n[115] \"fantasy_points_2004_week3\"  \"fantasy_points_2004_week7\" \n[117] \"fantasy_points_2004_week8\"  \"fantasy_points_2004_week9\" \n[119] \"fantasy_points_2004_week11\" \"fantasy_points_2004_week12\"\n[121] \"fantasy_points_2004_week13\" \"fantasy_points_2004_week14\"\n[123] \"fantasy_points_2004_week15\" \"fantasy_points_2004_week4\" \n[125] \"fantasy_points_2004_week19\" \"fantasy_points_2004_week20\"\n[127] \"fantasy_points_2004_week21\" \"fantasy_points_2004_week18\"\n[129] \"fantasy_points_2005_week1\"  \"fantasy_points_2005_week2\" \n[131] \"fantasy_points_2005_week3\"  \"fantasy_points_2005_week4\" \n[133] \"fantasy_points_2005_week6\"  \"fantasy_points_2005_week7\" \n[135] \"fantasy_points_2005_week8\"  \"fantasy_points_2005_week10\"\n[137] \"fantasy_points_2005_week11\" \"fantasy_points_2005_week13\"\n[139] \"fantasy_points_2005_week14\" \"fantasy_points_2005_week15\"\n[141] \"fantasy_points_2005_week16\" \"fantasy_points_2005_week17\"\n[143] \"fantasy_points_2005_week5\"  \"fantasy_points_2005_week9\" \n[145] \"fantasy_points_2005_week12\" \"fantasy_points_2005_week18\"\n[147] \"fantasy_points_2005_week19\" \"fantasy_points_2005_week20\"\n[149] \"fantasy_points_2005_week21\" \"fantasy_points_2006_week4\" \n[151] \"fantasy_points_2006_week1\"  \"fantasy_points_2006_week2\" \n[153] \"fantasy_points_2006_week3\"  \"fantasy_points_2006_week10\"\n[155] \"fantasy_points_2006_week11\" \"fantasy_points_2006_week12\"\n[157] \"fantasy_points_2006_week13\" \"fantasy_points_2006_week14\"\n[159] \"fantasy_points_2006_week5\"  \"fantasy_points_2006_week6\" \n[161] \"fantasy_points_2006_week7\"  \"fantasy_points_2006_week15\"\n[163] \"fantasy_points_2006_week16\" \"fantasy_points_2006_week17\"\n[165] \"fantasy_points_2006_week8\"  \"fantasy_points_2006_week9\" \n[167] \"fantasy_points_2006_week18\" \"fantasy_points_2006_week19\"\n[169] \"fantasy_points_2006_week20\" \"fantasy_points_2006_week21\"\n[171] \"fantasy_points_2007_week1\"  \"fantasy_points_2007_week2\" \n[173] \"fantasy_points_2007_week3\"  \"fantasy_points_2007_week5\" \n[175] \"fantasy_points_2007_week9\"  \"fantasy_points_2007_week16\"\n[177] \"fantasy_points_2007_week17\" \"fantasy_points_2007_week14\"\n[179] \"fantasy_points_2007_week4\"  \"fantasy_points_2007_week6\" \n[181] \"fantasy_points_2007_week7\"  \"fantasy_points_2007_week8\" \n[183] \"fantasy_points_2007_week10\" \"fantasy_points_2007_week11\"\n[185] \"fantasy_points_2007_week12\" \"fantasy_points_2007_week13\"\n[187] \"fantasy_points_2007_week15\" \"fantasy_points_2007_week19\"\n[189] \"fantasy_points_2007_week21\" \"fantasy_points_2007_week18\"\n[191] \"fantasy_points_2007_week20\" \"fantasy_points_2008_week8\" \n[193] \"fantasy_points_2008_week1\"  \"fantasy_points_2008_week2\" \n[195] \"fantasy_points_2008_week3\"  \"fantasy_points_2008_week4\" \n[197] \"fantasy_points_2008_week5\"  \"fantasy_points_2008_week6\" \n[199] \"fantasy_points_2008_week7\"  \"fantasy_points_2008_week14\"\n[201] \"fantasy_points_2008_week10\" \"fantasy_points_2008_week11\"\n[203] \"fantasy_points_2008_week12\" \"fantasy_points_2008_week13\"\n[205] \"fantasy_points_2008_week15\" \"fantasy_points_2008_week16\"\n[207] \"fantasy_points_2008_week17\" \"fantasy_points_2008_week9\" \n[209] \"fantasy_points_2008_week19\" \"fantasy_points_2008_week18\"\n[211] \"fantasy_points_2008_week20\" \"fantasy_points_2008_week21\"\n[213] \"fantasy_points_2009_week9\"  \"fantasy_points_2009_week11\"\n[215] \"fantasy_points_2009_week2\"  \"fantasy_points_2009_week3\" \n[217] \"fantasy_points_2009_week5\"  \"fantasy_points_2009_week7\" \n[219] \"fantasy_points_2009_week12\" \"fantasy_points_2009_week13\"\n[221] \"fantasy_points_2009_week14\" \"fantasy_points_2009_week15\"\n[223] \"fantasy_points_2009_week16\" \"fantasy_points_2009_week17\"\n[225] \"fantasy_points_2009_week1\"  \"fantasy_points_2009_week4\" \n[227] \"fantasy_points_2009_week8\"  \"fantasy_points_2009_week6\" \n[229] \"fantasy_points_2009_week10\" \"fantasy_points_2009_week18\"\n[231] \"fantasy_points_2009_week19\" \"fantasy_points_2009_week20\"\n[233] \"fantasy_points_2009_week21\" \"fantasy_points_2010_week2\" \n[235] \"fantasy_points_2010_week3\"  \"fantasy_points_2010_week4\" \n[237] \"fantasy_points_2010_week1\"  \"fantasy_points_2010_week7\" \n[239] \"fantasy_points_2010_week17\" \"fantasy_points_2010_week6\" \n[241] \"fantasy_points_2010_week8\"  \"fantasy_points_2010_week10\"\n[243] \"fantasy_points_2010_week13\" \"fantasy_points_2010_week14\"\n[245] \"fantasy_points_2010_week15\" \"fantasy_points_2010_week16\"\n[247] \"fantasy_points_2010_week5\"  \"fantasy_points_2010_week20\"\n[249] \"fantasy_points_2010_week12\" \"fantasy_points_2010_week11\"\n[251] \"fantasy_points_2010_week18\" \"fantasy_points_2010_week19\"\n[253] \"fantasy_points_2010_week21\" \"fantasy_points_2010_week9\" \n[255] \"fantasy_points_2011_week17\" \"fantasy_points_2011_week13\"\n[257] \"fantasy_points_2011_week14\" \"fantasy_points_2011_week16\"\n[259] \"fantasy_points_2011_week15\" \"fantasy_points_2011_week1\" \n[261] \"fantasy_points_2011_week2\"  \"fantasy_points_2011_week3\" \n[263] \"fantasy_points_2011_week4\"  \"fantasy_points_2011_week5\" \n[265] \"fantasy_points_2011_week6\"  \"fantasy_points_2011_week7\" \n[267] \"fantasy_points_2011_week9\"  \"fantasy_points_2011_week10\"\n[269] \"fantasy_points_2011_week11\" \"fantasy_points_2011_week12\"\n[271] \"fantasy_points_2011_week19\" \"fantasy_points_2011_week8\" \n[273] \"fantasy_points_2011_week18\" \"fantasy_points_2011_week20\"\n[275] \"fantasy_points_2011_week21\" \"fantasy_points_2012_week12\"\n[277] \"fantasy_points_2012_week13\" \"fantasy_points_2012_week2\" \n[279] \"fantasy_points_2012_week3\"  \"fantasy_points_2012_week4\" \n[281] \"fantasy_points_2012_week5\"  \"fantasy_points_2012_week7\" \n[283] \"fantasy_points_2012_week8\"  \"fantasy_points_2012_week9\" \n[285] \"fantasy_points_2012_week11\" \"fantasy_points_2012_week16\"\n[287] \"fantasy_points_2012_week1\"  \"fantasy_points_2012_week6\" \n[289] \"fantasy_points_2012_week10\" \"fantasy_points_2012_week14\"\n[291] \"fantasy_points_2012_week15\" \"fantasy_points_2012_week17\"\n[293] \"fantasy_points_2012_week19\" \"fantasy_points_2012_week20\"\n[295] \"fantasy_points_2012_week21\" \"fantasy_points_2012_week18\"\n[297] \"fantasy_points_2013_week1\"  \"fantasy_points_2013_week2\" \n[299] \"fantasy_points_2013_week3\"  \"fantasy_points_2013_week4\" \n[301] \"fantasy_points_2013_week5\"  \"fantasy_points_2013_week7\" \n[303] \"fantasy_points_2013_week8\"  \"fantasy_points_2013_week9\" \n[305] \"fantasy_points_2013_week10\" \"fantasy_points_2013_week11\"\n[307] \"fantasy_points_2013_week12\" \"fantasy_points_2013_week13\"\n[309] \"fantasy_points_2013_week14\" \"fantasy_points_2013_week15\"\n[311] \"fantasy_points_2013_week16\" \"fantasy_points_2013_week17\"\n[313] \"fantasy_points_2013_week6\"  \"fantasy_points_2013_week19\"\n[315] \"fantasy_points_2013_week20\" \"fantasy_points_2013_week21\"\n[317] \"fantasy_points_2013_week18\" \"fantasy_points_2014_week3\" \n[319] \"fantasy_points_2014_week4\"  \"fantasy_points_2014_week16\"\n[321] \"fantasy_points_2014_week17\" \"fantasy_points_2014_week1\" \n[323] \"fantasy_points_2014_week2\"  \"fantasy_points_2014_week5\" \n[325] \"fantasy_points_2014_week6\"  \"fantasy_points_2014_week7\" \n[327] \"fantasy_points_2014_week8\"  \"fantasy_points_2014_week9\" \n[329] \"fantasy_points_2014_week10\" \"fantasy_points_2014_week11\"\n[331] \"fantasy_points_2014_week12\" \"fantasy_points_2014_week13\"\n[333] \"fantasy_points_2014_week14\" \"fantasy_points_2014_week15\"\n[335] \"fantasy_points_2014_week19\" \"fantasy_points_2014_week20\"\n[337] \"fantasy_points_2014_week21\" \"fantasy_points_2014_week18\"\n[339] \"fantasy_points_2015_week4\"  \"fantasy_points_2015_week5\" \n[341] \"fantasy_points_2015_week11\" \"fantasy_points_2015_week12\"\n[343] \"fantasy_points_2015_week13\" \"fantasy_points_2015_week14\"\n[345] \"fantasy_points_2015_week15\" \"fantasy_points_2015_week16\"\n[347] \"fantasy_points_2015_week1\"  \"fantasy_points_2015_week2\" \n[349] \"fantasy_points_2015_week3\"  \"fantasy_points_2015_week6\" \n[351] \"fantasy_points_2015_week8\"  \"fantasy_points_2015_week9\" \n[353] \"fantasy_points_2015_week10\" \"fantasy_points_2015_week17\"\n[355] \"fantasy_points_2015_week19\" \"fantasy_points_2015_week20\"\n[357] \"fantasy_points_2015_week21\" \"fantasy_points_2015_week7\" \n[359] \"fantasy_points_2015_week18\" \"fantasy_points_2016_week5\" \n[361] \"fantasy_points_2016_week6\"  \"fantasy_points_2016_week7\" \n[363] \"fantasy_points_2016_week8\"  \"fantasy_points_2016_week10\"\n[365] \"fantasy_points_2016_week11\" \"fantasy_points_2016_week12\"\n[367] \"fantasy_points_2016_week13\" \"fantasy_points_2016_week14\"\n[369] \"fantasy_points_2016_week15\" \"fantasy_points_2016_week16\"\n[371] \"fantasy_points_2016_week17\" \"fantasy_points_2016_week19\"\n[373] \"fantasy_points_2016_week20\" \"fantasy_points_2016_week21\"\n[375] \"fantasy_points_2016_week1\"  \"fantasy_points_2016_week2\" \n[377] \"fantasy_points_2016_week3\"  \"fantasy_points_2016_week4\" \n[379] \"fantasy_points_2016_week9\"  \"fantasy_points_2016_week18\"\n[381] \"fantasy_points_2017_week1\"  \"fantasy_points_2017_week2\" \n[383] \"fantasy_points_2017_week3\"  \"fantasy_points_2017_week4\" \n[385] \"fantasy_points_2017_week5\"  \"fantasy_points_2017_week6\" \n[387] \"fantasy_points_2017_week7\"  \"fantasy_points_2017_week8\" \n[389] \"fantasy_points_2017_week10\" \"fantasy_points_2017_week11\"\n[391] \"fantasy_points_2017_week12\" \"fantasy_points_2017_week13\"\n[393] \"fantasy_points_2017_week14\" \"fantasy_points_2017_week15\"\n[395] \"fantasy_points_2017_week16\" \"fantasy_points_2017_week17\"\n[397] \"fantasy_points_2017_week19\" \"fantasy_points_2017_week20\"\n[399] \"fantasy_points_2017_week21\" \"fantasy_points_2017_week9\" \n[401] \"fantasy_points_2017_week18\" \"fantasy_points_2018_week1\" \n[403] \"fantasy_points_2018_week2\"  \"fantasy_points_2018_week3\" \n[405] \"fantasy_points_2018_week4\"  \"fantasy_points_2018_week5\" \n[407] \"fantasy_points_2018_week6\"  \"fantasy_points_2018_week7\" \n[409] \"fantasy_points_2018_week8\"  \"fantasy_points_2018_week9\" \n[411] \"fantasy_points_2018_week10\" \"fantasy_points_2018_week12\"\n[413] \"fantasy_points_2018_week13\" \"fantasy_points_2018_week14\"\n[415] \"fantasy_points_2018_week15\" \"fantasy_points_2018_week16\"\n[417] \"fantasy_points_2018_week17\" \"fantasy_points_2018_week19\"\n[419] \"fantasy_points_2018_week20\" \"fantasy_points_2018_week21\"\n[421] \"fantasy_points_2018_week11\" \"fantasy_points_2018_week18\"\n[423] \"fantasy_points_2019_week1\"  \"fantasy_points_2019_week2\" \n[425] \"fantasy_points_2019_week3\"  \"fantasy_points_2019_week4\" \n[427] \"fantasy_points_2019_week5\"  \"fantasy_points_2019_week6\" \n[429] \"fantasy_points_2019_week7\"  \"fantasy_points_2019_week8\" \n[431] \"fantasy_points_2019_week9\"  \"fantasy_points_2019_week11\"\n[433] \"fantasy_points_2019_week12\" \"fantasy_points_2019_week13\"\n[435] \"fantasy_points_2019_week14\" \"fantasy_points_2019_week15\"\n[437] \"fantasy_points_2019_week16\" \"fantasy_points_2019_week17\"\n[439] \"fantasy_points_2019_week18\" \"fantasy_points_2019_week10\"\n[441] \"fantasy_points_2019_week19\" \"fantasy_points_2019_week20\"\n[443] \"fantasy_points_2019_week21\" \"fantasy_points_2020_week1\" \n[445] \"fantasy_points_2020_week2\"  \"fantasy_points_2020_week3\" \n[447] \"fantasy_points_2020_week4\"  \"fantasy_points_2020_week5\" \n[449] \"fantasy_points_2020_week6\"  \"fantasy_points_2020_week7\" \n[451] \"fantasy_points_2020_week8\"  \"fantasy_points_2020_week9\" \n[453] \"fantasy_points_2020_week10\" \"fantasy_points_2020_week11\"\n[455] \"fantasy_points_2020_week12\" \"fantasy_points_2020_week14\"\n[457] \"fantasy_points_2020_week15\" \"fantasy_points_2020_week16\"\n[459] \"fantasy_points_2020_week17\" \"fantasy_points_2020_week18\"\n[461] \"fantasy_points_2020_week19\" \"fantasy_points_2020_week20\"\n[463] \"fantasy_points_2020_week21\" \"fantasy_points_2020_week13\"\n[465] \"fantasy_points_2021_week1\"  \"fantasy_points_2021_week2\" \n[467] \"fantasy_points_2021_week3\"  \"fantasy_points_2021_week4\" \n[469] \"fantasy_points_2021_week5\"  \"fantasy_points_2021_week6\" \n[471] \"fantasy_points_2021_week7\"  \"fantasy_points_2021_week8\" \n[473] \"fantasy_points_2021_week10\" \"fantasy_points_2021_week11\"\n[475] \"fantasy_points_2021_week12\" \"fantasy_points_2021_week13\"\n[477] \"fantasy_points_2021_week14\" \"fantasy_points_2021_week15\"\n[479] \"fantasy_points_2021_week16\" \"fantasy_points_2021_week17\"\n[481] \"fantasy_points_2021_week18\" \"fantasy_points_2021_week19\"\n[483] \"fantasy_points_2021_week20\" \"fantasy_points_2021_week9\" \n[485] \"fantasy_points_2021_week21\" \"fantasy_points_2021_week22\"\n[487] \"fantasy_points_2022_week1\"  \"fantasy_points_2022_week2\" \n[489] \"fantasy_points_2022_week3\"  \"fantasy_points_2022_week4\" \n[491] \"fantasy_points_2022_week5\"  \"fantasy_points_2022_week6\" \n[493] \"fantasy_points_2022_week7\"  \"fantasy_points_2022_week8\" \n[495] \"fantasy_points_2022_week9\"  \"fantasy_points_2022_week10\"\n[497] \"fantasy_points_2022_week12\" \"fantasy_points_2022_week13\"\n[499] \"fantasy_points_2022_week14\" \"fantasy_points_2022_week15\"\n[501] \"fantasy_points_2022_week16\" \"fantasy_points_2022_week17\"\n[503] \"fantasy_points_2022_week18\" \"fantasy_points_2022_week19\"\n[505] \"fantasy_points_2022_week11\" \"fantasy_points_2022_week20\"\n[507] \"fantasy_points_2022_week21\" \"fantasy_points_2022_week22\"\n[509] \"fantasy_points_2023_week1\"  \"fantasy_points_2023_week4\" \n[511] \"fantasy_points_2023_week7\"  \"fantasy_points_2023_week11\"\n[513] \"fantasy_points_2023_week14\" \"fantasy_points_2023_week16\"\n[515] \"fantasy_points_2023_week13\" \"fantasy_points_2023_week15\"\n[517] \"fantasy_points_2023_week17\" \"fantasy_points_2023_week19\"\n[519] \"fantasy_points_2023_week2\"  \"fantasy_points_2023_week3\" \n[521] \"fantasy_points_2023_week5\"  \"fantasy_points_2023_week6\" \n[523] \"fantasy_points_2023_week8\"  \"fantasy_points_2023_week12\"\n[525] \"fantasy_points_2023_week18\" \"fantasy_points_2023_week10\"\n[527] \"fantasy_points_2023_week21\" \"fantasy_points_2023_week22\"\n[529] \"fantasy_points_2023_week9\"  \"fantasy_points_2023_week20\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-wideToLong",
    "href": "getting-started.html#sec-wideToLong",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.19 Transform Data from Wide to Long",
    "text": "3.19 Transform Data from Wide to Long\nConversely, we can also restructure data from wide to long.\nOriginal data:\n\nCodedataWide &lt;- nfl_actualStats_offense_weekly %&gt;% \n  select(player_id, player_display_name, season, week, recent_team, opponent_team)\n\ndim(dataWide)\n\n[1] 129739      6\n\nCodenames(dataWide)\n\n[1] \"player_id\"           \"player_display_name\" \"season\"             \n[4] \"week\"                \"recent_team\"         \"opponent_team\"      \n\n\nData in long form, transformed from wide form using tidyverse:\n\nCodedataLong &lt;- dataWide %&gt;% \n  pivot_longer(\n    cols = c(recent_team, opponent_team),\n    names_to = \"role\",\n    values_to = \"team\")\n\ndim(dataLong)\n\n[1] 259478      6\n\nCodenames(dataLong)\n\n[1] \"player_id\"           \"player_display_name\" \"season\"             \n[4] \"week\"                \"role\"                \"team\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-loops",
    "href": "getting-started.html#sec-loops",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.20 Loops",
    "text": "3.20 Loops\nIf you want to perform the same computation multiple times, it can be faster to do it in a loop compared to writing out the same computation many times. For instance, here is a loop that runs from 1 to 12 (the number of players in the players object), incrementing by 1 after each iteration. The loop prints each element of a vector (i.e., the player’s name) and the loop index (i) that indicates where the loop is in terms of its iterations:\n\nCodefor(i in 1:length(players$ID)){\n  print(paste(\"The loop is at index:\", i, sep = \" \"))\n  print(paste(\"My favorite player is:\", players$name[i], sep = \" \"))\n}\n\n[1] \"The loop is at index: 1\"\n[1] \"My favorite player is: Ken Cussion\"\n[1] \"The loop is at index: 2\"\n[1] \"My favorite player is: Ben Sacked\"\n[1] \"The loop is at index: 3\"\n[1] \"My favorite player is: Chuck Downfield\"\n[1] \"The loop is at index: 4\"\n[1] \"My favorite player is: Ron Ingback\"\n[1] \"The loop is at index: 5\"\n[1] \"My favorite player is: Rhonda Ball\"\n[1] \"The loop is at index: 6\"\n[1] \"My favorite player is: Hugo Long\"\n[1] \"The loop is at index: 7\"\n[1] \"My favorite player is: Lionel Scrimmage\"\n[1] \"The loop is at index: 8\"\n[1] \"My favorite player is: Drew Blood\"\n[1] \"The loop is at index: 9\"\n[1] \"My favorite player is: Chase Emdown\"\n[1] \"The loop is at index: 10\"\n[1] \"My favorite player is: Justin Time\"\n[1] \"The loop is at index: 11\"\n[1] \"My favorite player is: Spike D'Ball\"\n[1] \"The loop is at index: 12\"\n[1] \"My favorite player is: Isac Ulooz\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-calculations",
    "href": "getting-started.html#sec-calculations",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.21 Calculations",
    "text": "3.21 Calculations\n\n3.21.1 Historical Actual Player Statistics\nIn addition to week-by-week actual player statistics, we can also compute historical actual player statistics as a function of different timeframes, including season-by-season and career statistics.\n\n3.21.1.1 Career Statistics\nFirst, we can compute the players’ career statistics using the calculate_player_stats(), calculate_player_stats_def(), and calculate_player_stats_kicking() functions from the nflfastR package for offensive players, defensive players, and kickers, respectively.\n\n\n\n\n\n\nNote 3.4: Calculating players’ career statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodenfl_actualStats_offense_career &lt;- nflfastR::calculate_player_stats(\n  nfl_pbp,\n  weekly = FALSE)\n\nnfl_actualStats_defense_career &lt;- nflfastR::calculate_player_stats_def(\n  nfl_pbp,\n  weekly = FALSE)\n\nnfl_actualStats_kicking_career &lt;- nflfastR::calculate_player_stats_kicking(\n  nfl_pbp,\n  weekly = FALSE)\n\n\n\n3.21.1.2 Season-by-Season Statistics\nSecond, we can compute the players’ season-by-season statistics.\n\nCodeseasons &lt;- unique(nfl_pbp$season)\n\nnfl_pbp_seasonalList &lt;- list()\nnfl_actualStats_offense_seasonalList &lt;- list()\nnfl_actualStats_defense_seasonalList &lt;- list()\nnfl_actualStats_kicking_seasonalList &lt;- list()\n\n\n\n\n\n\n\n\nNote 3.5: Calculating players’ season-by-season statistics\n\n\n\nNote: the following code takes a while to run.\n\n\n\nCodepb &lt;- txtProgressBar(\n  min = 0,\n  max = length(seasons),\n  style = 3)\n\nfor(i in 1:length(seasons)){\n  # Subset play-by-play data by season\n  nfl_pbp_seasonalList[[i]] &lt;- nfl_pbp %&gt;% \n    dplyr::filter(season == seasons[i])\n  \n  # Compute actual statistics by season\n  nfl_actualStats_offense_seasonalList[[i]] &lt;- \n    nflfastR::calculate_player_stats(\n      nfl_pbp_seasonalList[[i]],\n      weekly = FALSE)\n\n  nfl_actualStats_defense_seasonalList[[i]] &lt;- \n    nflfastR::calculate_player_stats_def(\n      nfl_pbp_seasonalList[[i]],\n      weekly = FALSE)\n\n  nfl_actualStats_kicking_seasonalList[[i]] &lt;- \n    nflfastR::calculate_player_stats_kicking(\n      nfl_pbp_seasonalList[[i]],\n      weekly = FALSE)\n\n  nfl_actualStats_offense_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualStats_defense_seasonalList[[i]]$season &lt;- seasons[i]\n  nfl_actualStats_kicking_seasonalList[[i]]$season &lt;- seasons[i]\n  \n  print(\n    paste(\"Completed computing projections for season: \", seasons[i], sep = \"\"))\n\n  # Update the progress bar\n  setTxtProgressBar(pb, i)\n}\n\n# Close the progress bar\nclose(pb)\n\nnfl_actualStats_offense_seasonal &lt;- nfl_actualStats_offense_seasonalList %&gt;% \n  dplyr::bind_rows()\nnfl_actualStats_defense_seasonal &lt;- nfl_actualStats_defense_seasonalList %&gt;% \n  dplyr::bind_rows()\nnfl_actualStats_kicking_seasonal &lt;- nfl_actualStats_kicking_seasonalList %&gt;% \n  dplyr::bind_rows()\n\n\n\n3.21.1.3 Week-by-Week Statistics\nWe already load players’ week-by-week statistics above. Nevertheless, we could compute players’ weekly statistics from the play-by-play data using the following syntax:\n\nCodenfl_actualStats_offense_weekly &lt;- nflfastR::calculate_player_stats(\n  nfl_pbp,\n  weekly = TRUE)\n\nnfl_actualStats_defense_weekly &lt;- nflfastR::calculate_player_stats_def(\n  nfl_pbp,\n  weekly = TRUE)\n\nnfl_actualStats_kicking_weekly &lt;- nflfastR::calculate_player_stats_kicking(\n  nfl_pbp,\n  weekly = TRUE)\n\n\n\n3.21.2 Historical Actual Fantasy Points\nSpecify scoring settings:\n\n3.21.2.1 Weekly\n\n3.21.2.2 Seasonal\n\n3.21.2.3 Career\n\n3.21.3 Player Age and Experience\n\n3.21.3.1 Weekly\n\nCode# Reshape from wide to long format\nnfl_actualStats_offense_weekly_long &lt;- nfl_actualStats_offense_weekly %&gt;% \n  tidyr::pivot_longer(\n    cols = c(recent_team, opponent_team),\n    names_to = \"role\",\n    values_to = \"team\")\n\n# Perform separate inner join operations for the home_team and away_team\nnfl_actualStats_offense_weekly_home &lt;- dplyr::inner_join(\n  nfl_actualStats_offense_weekly_long,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"home_team\")) %&gt;% \n  mutate(home_away = \"home_team\")\n\nnfl_actualStats_offense_weekly_away &lt;- dplyr::inner_join(\n  nfl_actualStats_offense_weekly_long,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"away_team\")) %&gt;% \n  mutate(home_away = \"away_team\")\n\nnfl_actualStats_defense_weekly_home &lt;- dplyr::inner_join(\n  nfl_actualStats_defense_weekly,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"home_team\")) %&gt;% \n  mutate(home_away = \"home_team\")\n\nnfl_actualStats_defense_weekly_away &lt;- dplyr::inner_join(\n  nfl_actualStats_defense_weekly,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"away_team\")) %&gt;% \n  mutate(home_away = \"away_team\")\n\nnfl_actualStats_kicking_weekly_home &lt;- dplyr::inner_join(\n  nfl_actualStats_kicking_weekly,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"home_team\")) %&gt;% \n  mutate(home_away = \"home_team\")\n\nnfl_actualStats_kicking_weekly_away &lt;- dplyr::inner_join(\n  nfl_actualStats_kicking_weekly,\n  nfl_schedules,\n  by = c(\"season\",\"week\",\"team\" = \"away_team\")) %&gt;% \n  mutate(home_away = \"away_team\")\n\n# Combine the results of the join operations\nnfl_actualStats_offense_weekly_schedules_long &lt;- dplyr::bind_rows(\n  nfl_actualStats_offense_weekly_home,\n  nfl_actualStats_offense_weekly_away)\n\nnfl_actualStats_defense_weekly_schedules_long &lt;- dplyr::bind_rows(\n  nfl_actualStats_defense_weekly_home,\n  nfl_actualStats_defense_weekly_away)\n\nnfl_actualStats_kicking_weekly_schedules_long &lt;- dplyr::bind_rows(\n  nfl_actualStats_kicking_weekly_home,\n  nfl_actualStats_kicking_weekly_away)\n\n# Reshape from long to wide\nplayer_game_gameday_offense &lt;- nfl_actualStats_offense_weekly_schedules_long %&gt;%\n  dplyr::distinct(player_id, season, week, game_id, home_away, team, gameday) %&gt;% #, .keep_all = TRUE\n  tidyr::pivot_wider(\n    names_from = home_away,\n    values_from = team)\n\nplayer_game_gameday_defense &lt;- nfl_actualStats_defense_weekly_schedules_long %&gt;%\n  dplyr::distinct(player_id, season, week, game_id, home_away, team, gameday) %&gt;% #, .keep_all = TRUE\n  tidyr::pivot_wider(\n    names_from = home_away,\n    values_from = team)\n\nplayer_game_gameday_kicking &lt;- nfl_actualStats_kicking_weekly_schedules_long %&gt;%\n  dplyr::distinct(player_id, season, week, game_id, home_away, team, gameday) %&gt;% #, .keep_all = TRUE\n  tidyr::pivot_wider(\n    names_from = home_away,\n    values_from = team)\n\n# Merge player birthdate and the game date\nplayer_game_birthdate_gameday_offense &lt;- dplyr::left_join(\n  player_game_gameday_offense,\n  unique(nfl_players[,c(\"gsis_id\",\"birth_date\")]),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\nplayer_game_birthdate_gameday_defense &lt;- dplyr::left_join(\n  player_game_gameday_defense,\n  unique(nfl_players[,c(\"gsis_id\",\"birth_date\")]),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\nplayer_game_birthdate_gameday_kicking &lt;- dplyr::left_join(\n  player_game_gameday_kicking,\n  unique(nfl_players[,c(\"gsis_id\",\"birth_date\")]),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\nplayer_game_birthdate_gameday_offense$birth_date &lt;- lubridate::ymd(player_game_birthdate_gameday_offense$birth_date)\nplayer_game_birthdate_gameday_offense$gameday &lt;- lubridate::ymd(player_game_birthdate_gameday_offense$gameday)\n\nplayer_game_birthdate_gameday_defense$birth_date &lt;- lubridate::ymd(player_game_birthdate_gameday_defense$birth_date)\nplayer_game_birthdate_gameday_defense$gameday &lt;- lubridate::ymd(player_game_birthdate_gameday_defense$gameday)\n\nplayer_game_birthdate_gameday_kicking$birth_date &lt;- lubridate::ymd(player_game_birthdate_gameday_kicking$birth_date)\nplayer_game_birthdate_gameday_kicking$gameday &lt;- lubridate::ymd(player_game_birthdate_gameday_kicking$gameday)\n\n# Calculate player's age for a given week as the difference between their birthdate and the game date\nplayer_game_birthdate_gameday_offense$age &lt;- lubridate::interval(\n  start = player_game_birthdate_gameday_offense$birth_date,\n  end = player_game_birthdate_gameday_offense$gameday\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\nplayer_game_birthdate_gameday_defense$age &lt;- lubridate::interval(\n  start = player_game_birthdate_gameday_defense$birth_date,\n  end = player_game_birthdate_gameday_defense$gameday\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\nplayer_game_birthdate_gameday_kicking$age &lt;- lubridate::interval(\n  start = player_game_birthdate_gameday_kicking$birth_date,\n  end = player_game_birthdate_gameday_kicking$gameday\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\n# Calculate ageCentered and ageCenteredQuadratic\nplayer_game_birthdate_gameday_offense$ageCentered20 &lt;- player_game_birthdate_gameday_offense$age - 20\nplayer_game_birthdate_gameday_offense$ageCentered20Quadratic &lt;- player_game_birthdate_gameday_offense$ageCentered20 ^ 2\n\nplayer_game_birthdate_gameday_defense$ageCentered20 &lt;- player_game_birthdate_gameday_defense$age - 20\nplayer_game_birthdate_gameday_defense$ageCentered20Quadratic &lt;- player_game_birthdate_gameday_defense$ageCentered20 ^ 2\n\nplayer_game_birthdate_gameday_kicking$ageCentered20 &lt;- player_game_birthdate_gameday_kicking$age - 20\nplayer_game_birthdate_gameday_kicking$ageCentered20Quadratic &lt;- player_game_birthdate_gameday_kicking$ageCentered20 ^ 2\n\n# Merge with player info\nplayer_age_offense &lt;- dplyr::left_join(\n  player_game_birthdate_gameday_offense,\n  nfl_players %&gt;% select(-birth_date, -season),\n  by = c(\"player_id\" = \"gsis_id\"))\n\nplayer_age_defense &lt;- dplyr::left_join(\n  player_game_birthdate_gameday_defense,\n  nfl_players %&gt;% select(-birth_date, -season),\n  by = c(\"player_id\" = \"gsis_id\"))\n\nplayer_age_kicking &lt;- dplyr::left_join(\n  player_game_birthdate_gameday_kicking,\n  nfl_players %&gt;% select(-birth_date, -season),\n  by = c(\"player_id\" = \"gsis_id\"))\n\n# Add game_id to weekly stats to facilitate merging\nnfl_actualStats_game_offense_weekly &lt;- nfl_actualStats_offense_weekly %&gt;% \n  dplyr::left_join(\n    player_age_offense[,c(\"season\",\"week\",\"player_id\",\"game_id\")],\n    by = c(\"season\",\"week\",\"player_id\"))\n\nnfl_actualStats_game_defense_weekly &lt;- nfl_actualStats_defense_weekly %&gt;% \n  dplyr::left_join(\n    player_age_offense[,c(\"season\",\"week\",\"player_id\",\"game_id\")],\n    by = c(\"season\",\"week\",\"player_id\"))\n\nnfl_actualStats_game_kicking_weekly &lt;- nfl_actualStats_kicking_weekly %&gt;% \n  dplyr::left_join(\n    player_age_offense[,c(\"season\",\"week\",\"player_id\",\"game_id\")],\n    by = c(\"season\",\"week\",\"player_id\"))\n\n# Merge with player weekly stats\nplayer_stats_weekly_offense &lt;- dplyr::full_join(\n  player_age_offense %&gt;% select(-position, -position_group),\n  nfl_actualStats_game_offense_weekly,\n  by = c(\"season\",\"week\",\"player_id\",\"game_id\"))\n\nplayer_stats_weekly_defense &lt;- dplyr::full_join(\n  player_age_defense %&gt;% select(-position, -position_group),\n  nfl_actualStats_game_defense_weekly,\n  by = c(\"season\",\"week\",\"player_id\",\"game_id\"))\n\nplayer_stats_weekly_kicking &lt;- dplyr::full_join(\n  player_age_kicking %&gt;% select(-position, -position_group),\n  nfl_actualStats_game_kicking_weekly,\n  by = c(\"season\",\"week\",\"player_id\",\"game_id\"))\n\nplayer_stats_weekly_offense$total_years_of_experience &lt;- as.integer(player_stats_weekly_offense$years_of_experience)\nplayer_stats_weekly_defense$total_years_of_experience &lt;- as.integer(player_stats_weekly_defense$years_of_experience)\nplayer_stats_weekly_kicking$total_years_of_experience &lt;- as.integer(player_stats_weekly_kicking$years_of_experience)\n\nplayer_stats_weekly_offense$years_of_experience &lt;- NULL\nplayer_stats_weekly_defense$years_of_experience &lt;- NULL\nplayer_stats_weekly_kicking$years_of_experience &lt;- NULL\n\ndistinct_seasons_offense &lt;- player_stats_weekly_offense %&gt;%\n  dplyr::select(player_id, season) %&gt;%\n  dplyr::distinct() %&gt;% \n  dplyr::left_join(\n    nfl_players[,c(\"gsis_id\",\"years_of_experience\")],\n    by = c(\"player_id\" = \"gsis_id\")\n  ) %&gt;% \n  dplyr::mutate(total_years_of_experience = as.integer(years_of_experience)) %&gt;% \n  dplyr::select(-years_of_experience)\n\ndistinct_seasons_defense &lt;- player_stats_weekly_defense %&gt;%\n  dplyr::select(player_id, season) %&gt;%\n  dplyr::distinct() %&gt;% \n  dplyr::left_join(\n    nfl_players[,c(\"gsis_id\",\"years_of_experience\")],\n    by = c(\"player_id\" = \"gsis_id\")\n  ) %&gt;% \n  dplyr::mutate(total_years_of_experience = as.integer(years_of_experience)) %&gt;% \n  dplyr::select(-years_of_experience)\n\ndistinct_seasons_kicking &lt;- player_stats_weekly_kicking %&gt;%\n  dplyr::select(player_id, season) %&gt;%\n  dplyr::distinct() %&gt;% \n  dplyr::left_join(\n    nfl_players[,c(\"gsis_id\",\"years_of_experience\")],\n    by = c(\"player_id\" = \"gsis_id\")\n  ) %&gt;% \n  dplyr::mutate(total_years_of_experience = as.integer(years_of_experience)) %&gt;% \n  dplyr::select(-years_of_experience)\n\nyears_of_experience_offense &lt;- distinct_seasons_offense %&gt;% \n  dplyr::arrange(player_id, -season) %&gt;% \n  dplyr::group_by(player_id) %&gt;%\n  dplyr::mutate(years_of_experience = first(total_years_of_experience) - (row_number() - 1)) %&gt;%\n  dplyr::ungroup()\n\nyears_of_experience_defense &lt;- distinct_seasons_defense %&gt;% \n  dplyr::arrange(player_id, -season) %&gt;% \n  dplyr::group_by(player_id) %&gt;%\n  dplyr::mutate(years_of_experience = first(total_years_of_experience) - (row_number() - 1)) %&gt;%\n  dplyr::ungroup()\n\nyears_of_experience_kicking &lt;- distinct_seasons_kicking %&gt;% \n  dplyr::arrange(player_id, -season) %&gt;% \n  dplyr::group_by(player_id) %&gt;%\n  dplyr::mutate(years_of_experience = first(total_years_of_experience) - (row_number() - 1)) %&gt;%\n  dplyr::ungroup()\n\nyears_of_experience_offense$years_of_experience[which(years_of_experience_offense$years_of_experience &lt; 0)] &lt;- 0\nyears_of_experience_defense$years_of_experience[which(years_of_experience_defense$years_of_experience &lt; 0)] &lt;- 0\nyears_of_experience_kicking$years_of_experience[which(years_of_experience_kicking$years_of_experience &lt; 0)] &lt;- 0\n\nplayer_stats_weekly_offense &lt;- player_stats_weekly_offense %&gt;% \n  dplyr::left_join(\n    years_of_experience_offense[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\nplayer_stats_weekly_defense &lt;- player_stats_weekly_defense %&gt;% \n  dplyr::left_join(\n    years_of_experience_offense[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\nplayer_stats_weekly_kicking &lt;- player_stats_weekly_kicking %&gt;% \n  dplyr::left_join(\n    years_of_experience_offense[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\n\n\nCode# Save data\nsave(\n  player_stats_weekly_offense, player_stats_weekly_defense, player_stats_weekly_kicking,\n  file = \"./data/player_stats_weekly.RData\"\n)\n\n\n\n3.21.3.2 Seasonal\n\nCode# Merge player info with seasonal stats\nplayer_stats_seasonal_offense &lt;- dplyr::full_join(\n  nfl_actualStats_offense_seasonal,\n  nfl_players %&gt;% select(-position, -position_group, -season),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\nplayer_stats_seasonal_defense &lt;- dplyr::full_join(\n  nfl_actualStats_defense_seasonal,\n  nfl_players %&gt;% select(-position, -position_group, -season),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\nplayer_stats_seasonal_kicking &lt;- dplyr::full_join(\n  nfl_actualStats_kicking_seasonal,\n  nfl_players %&gt;% select(-position, -position_group, -season),\n  by = c(\"player_id\" = \"gsis_id\")\n)\n\n# Calculate age\nseason_startdate &lt;- nfl_schedules %&gt;% \n  dplyr::group_by(season) %&gt;% \n  dplyr::summarise(startdate = min(gameday, na.rm = TRUE))\n\nplayer_stats_seasonal_offense &lt;- player_stats_seasonal_offense %&gt;% \n  dplyr::left_join(\n    season_startdate,\n    by = \"season\"\n  )\n\nplayer_stats_seasonal_defense &lt;- player_stats_seasonal_defense %&gt;% \n  dplyr::left_join(\n    season_startdate,\n    by = \"season\"\n  )\n\nplayer_stats_seasonal_kicking &lt;- player_stats_seasonal_kicking %&gt;% \n  dplyr::left_join(\n    season_startdate,\n    by = \"season\"\n  )\n\nplayer_stats_seasonal_offense$age &lt;- lubridate::interval(\n  start = player_stats_seasonal_offense$birth_date,\n  end = player_stats_seasonal_offense$startdate\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\nplayer_stats_seasonal_defense$age &lt;- lubridate::interval(\n  start = player_stats_seasonal_defense$birth_date,\n  end = player_stats_seasonal_defense$startdate\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\nplayer_stats_seasonal_kicking$age &lt;- lubridate::interval(\n  start = player_stats_seasonal_kicking$birth_date,\n  end = player_stats_seasonal_kicking$startdate\n) %&gt;% \n  lubridate::time_length(unit = \"years\")\n\n# Calculate ageCentered and ageCenteredQuadratic\nplayer_stats_seasonal_offense$ageCentered20 &lt;- player_stats_seasonal_offense$age - 20\nplayer_stats_seasonal_offense$ageCentered20Quadratic &lt;- player_stats_seasonal_offense$ageCentered20 ^ 2\n\nplayer_stats_seasonal_defense$ageCentered20 &lt;- player_stats_seasonal_defense$age - 20\nplayer_stats_seasonal_defense$ageCentered20Quadratic &lt;- player_stats_seasonal_defense$ageCentered20 ^ 2\n\nplayer_stats_seasonal_kicking$ageCentered20 &lt;- player_stats_seasonal_kicking$age - 20\nplayer_stats_seasonal_kicking$ageCentered20Quadratic &lt;- player_stats_seasonal_kicking$ageCentered20 ^ 2\n\n# Years of experience\nplayer_stats_seasonal_offense$years_of_experience &lt;- NULL\nplayer_stats_seasonal_defense$years_of_experience &lt;- NULL\nplayer_stats_seasonal_kicking$years_of_experience &lt;- NULL\n\nplayer_stats_seasonal_offense &lt;- player_stats_seasonal_offense %&gt;% \n  dplyr::left_join(\n    years_of_experience_offense[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\nplayer_stats_seasonal_defense &lt;- player_stats_seasonal_defense %&gt;% \n  dplyr::left_join(\n    years_of_experience_offense[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\nplayer_stats_seasonal_kicking &lt;- player_stats_seasonal_kicking %&gt;% \n  dplyr::left_join(\n    years_of_experience_offense[,c(\"player_id\",\"season\",\"years_of_experience\")],\n    by = c(\"player_id\",\"season\")\n  )\n\n\n\nCode# Save data\nsave(\n  player_stats_seasonal_offense, player_stats_seasonal_defense, player_stats_seasonal_kicking,\n  file = \"./data/player_stats_seasonal.RData\"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "getting-started.html#sec-gettingStartedSessionInfo",
    "href": "getting-started.html#sec-gettingStartedSessionInfo",
    "title": "3  Getting Started with R for Data Analysis",
    "section": "\n3.22 Session Info",
    "text": "3.22 Session Info\nAt the end of each chapter in which R code is used, I provide the session information, which describes the system and operating system the code was run on and the versions of each package. That way, if you get different results from me, you can see which differ, to help with reproducibility. If you run the (all of) the exact same code as is provided in the text, in the exact same order, with the exact same setup (platform, operating system, package versions, etc.), you should get the exact same answer as is in the text. That is the idea of reproducibility—getting the exact same result with the exact same inputs. Reproducibility is crucial for studies to achieve greater confidence in their findings and to ensure better replicability of findings across studies.\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] forcats_1.0.0          stringr_1.5.1          dplyr_1.1.4           \n [4] purrr_1.0.2            readr_2.1.5            tidyr_1.3.1           \n [7] tibble_3.2.1           ggplot2_3.5.1          tidyverse_2.0.0       \n[10] lubridate_1.9.3        progressr_0.14.0       nflplotR_1.3.1        \n[13] nfl4th_1.0.4           nflfastR_4.6.1         nflreadr_1.4.0        \n[16] ffanalytics_3.1.2.0001\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      xfun_0.46         httr2_1.0.2       htmlwidgets_1.6.4\n [5] websocket_1.4.1   processx_3.8.4    lattice_0.22-6    tzdb_0.4.0       \n [9] vctrs_0.6.5       tools_4.4.1       ps_1.7.7          generics_0.1.3   \n[13] curl_5.2.1        parallel_4.4.1    fansi_1.0.6       pkgconfig_2.0.3  \n[17] Matrix_1.7-0      data.table_1.15.4 gt_0.11.0         readxl_1.4.3     \n[21] lifecycle_1.0.4   compiler_4.4.1    munsell_0.5.1     chromote_0.2.0   \n[25] janitor_2.2.0     codetools_0.2-20  snakecase_0.11.1  rrapply_1.2.7    \n[29] htmltools_0.5.8.1 yaml_2.3.9        later_1.3.2       pillar_1.9.0     \n[33] furrr_0.3.1       cachem_1.1.0      nlme_3.1-164      parallelly_1.37.1\n[37] tidyselect_1.2.1  rvest_1.0.4       digest_0.6.36     stringi_1.8.4    \n[41] future_1.33.2     listenv_0.9.1     splines_4.4.1     fastmap_1.2.0    \n[45] grid_4.4.1        colorspace_2.1-0  cli_3.6.3         magrittr_2.0.3   \n[49] utf8_1.2.4        withr_3.0.0       scales_1.3.0      promises_1.3.0   \n[53] backports_1.5.0   rappdirs_0.3.3    xgboost_1.7.7.1   timechange_0.3.0 \n[57] rmarkdown_2.27    httr_1.4.7        globals_0.16.3    cellranger_1.1.0 \n[61] hms_1.1.3         memoise_2.0.1     evaluate_0.24.0   knitr_1.48       \n[65] mgcv_1.9-1        rlang_1.1.4       Rcpp_1.0.13       glue_1.7.0       \n[69] xml2_1.3.6        jsonlite_1.8.8    R6_2.5.1          fastrmodels_1.0.2\n\n\n\n\n\n\nCongelio, B. J. (2023). Introduction to NFL analytics with R. CRC Press. https://bradcongelio.com/nfl-analytics-with-r-book\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting Started with `R` for Data Analysis</span>"
    ]
  },
  {
    "objectID": "data-visualization.html",
    "href": "data-visualization.html",
    "title": "4  Data Visualization",
    "section": "",
    "text": "4.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationGettingStarted",
    "href": "data-visualization.html#sec-dataVisualizationGettingStarted",
    "title": "4  Data Visualization",
    "section": "",
    "text": "4.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")\nlibrary(\"nflplotR\")\nlibrary(\"plotly\")\n\n\n\n4.1.2 Load Data\n\nCodeload(file = \"./data/nfl_pbp.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 3.21.3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-scatterplot",
    "href": "data-visualization.html#sec-scatterplot",
    "title": "4  Data Visualization",
    "section": "\n4.3 Examples",
    "text": "4.3 Examples\n\n4.3.1 Players\n\n4.3.1.1 Running Back Performance By Player Age\n\nCode# Prepare Data\nrushing_attempts &lt;- nfl_pbp %&gt;% \n  dplyr::filter(season_type == \"REG\") %&gt;% \n  dplyr::filter(\n    rush == 1,\n    rush_attempt == 1,\n    qb_scramble == 0,\n    qb_dropback == 0,\n    !is.na(rushing_yards))\n\nrb_yardsPerCarry &lt;- rushing_attempts %&gt;% \n  dplyr::group_by(rusher_id, season) %&gt;% \n  dplyr::summarise(\n    ypc = mean(rushing_yards, na.rm = TRUE),\n    rush_attempts = n(),\n    .groups = \"drop\") %&gt;% \n  dplyr::ungroup() %&gt;% \n  dplyr::left_join(\n    player_stats_seasonal_offense,\n    by = c(\"rusher_id\" = \"player_id\", \"season\")\n  ) %&gt;% \n  dplyr::filter(\n    position_group == \"RB\",\n    rush_attempts &gt;= 50)\n\n\n\n4.3.1.1.1 Rushing Yards Per Carry\nRushing yards per carry over the course of the season is depicted as a function of the Running Back’s age in Figure 4.12.\n\nCodeplot_ypcByPlayerAge2 &lt;- ggplot2::ggplot(\n  data = rb_yardsPerCarry,\n  aes(\n    x = age,\n    y = ypc)) +\n  geom_point(\n    aes(\n      text = player_display_name,\n      label = season)) +\n  geom_smooth() +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing Yards Per Carry (Season)\",\n    title = \"NFL Rushing Yards Per Carry (Season) by Player Age\",\n    subtitle = \"(minimum 50 rushing attempts)\"\n  ) +\n  theme_classic()\n\nggplotly(plot_ypcByPlayerAge2)\n\n\n\n\n\n\nFigure 4.12: Rushing Yards Per Carry (Season) by Player Age.\n\n\n\n\n4.3.1.1.2 Rushing EPA Per Season\nRushing expected points added (EPA) over the course of the season is depicted as a function of the Running Back’s age in Figure 4.13.\n\nCodeplot_rushEPAbyPlayerAge &lt;- ggplot2::ggplot(\n  data = rb_seasonal,\n  aes(\n    x = age,\n    y = rushing_epa)) +\n  geom_point(\n    aes(\n      text = player_display_name,\n      label = season)) +\n  geom_smooth() +\n  labs(\n    x = \"Running Back's Age (years)\",\n    y = \"Rushing EPA (Season)\",\n    title = \"NFL Rushing Expected Points Added (Season) by Player Age\"\n  ) +\n  theme_classic()\n\nggplotly(plot_rushEPAbyPlayerAge)\n\n\n\n\n\n\nFigure 4.13: Rushing Expected Points Added (Season) by Player Age.\n\n\n\n\n4.3.2 Teams\n\n4.3.2.1 Defensive and Offensive EPA per Play\nExpected points added (EPA) per play by the team with possession.\n\nCodepbp_regularSeason &lt;- nfl_pbp %&gt;% \n  dplyr::filter(\n    season == 2023,\n    season_type == \"REG\") %&gt;%\n  dplyr::filter(!is.na(posteam) & (rush == 1 | pass == 1))\n\nepa_offense &lt;- pbp_regularSeason %&gt;%\n  dplyr::group_by(team = posteam) %&gt;%\n  dplyr::summarise(off_epa = mean(epa, na.rm = TRUE))\n\nepa_defense &lt;- pbp_regularSeason %&gt;%\n  dplyr::group_by(team = defteam) %&gt;%\n  dplyr::summarise(def_epa = mean(epa, na.rm = TRUE))\n\nepa_combined &lt;- epa_offense %&gt;%\n  dplyr::inner_join(\n    epa_defense,\n    by = \"team\")\n\n\nDefensive EPA per play during the 2023 NFL season is depicted as a function of offensive EPA per play in Figure 4.14.\n\nCodeggplot2::ggplot(\n  data = epa_combined,\n  aes(\n    x = off_epa,\n    y = def_epa)) +\n  nflplotR::geom_mean_lines(\n    aes(\n      x0 = off_epa ,\n      y0 = def_epa)) +\n  nflplotR::geom_nfl_logos(\n    aes(\n      team_abbr = team),\n      width = 0.065,\n      alpha = 0.7) +\n  labs(\n    x = \"Offense EPA/play\",\n    y = \"Defense EPA/play\",\n    title = \"2023 NFL Offensive and Defensive EPA per Play\"\n  ) +\n  theme_classic() +\n  scale_y_reverse()\n\n\n\n\n\n\nFigure 4.14: 2023 NFL Offensive and Defensive EPA Per Play.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationConclusion",
    "href": "data-visualization.html#sec-dataVisualizationConclusion",
    "title": "4  Data Visualization",
    "section": "\n4.4 Conclusion",
    "text": "4.4 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-visualization.html#sec-dataVisualizationSessionInfo",
    "href": "data-visualization.html#sec-dataVisualizationSessionInfo",
    "title": "4  Data Visualization",
    "section": "\n4.5 Session Info",
    "text": "4.5 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] plotly_4.10.4   nflplotR_1.3.1  lubridate_1.9.3 forcats_1.0.0  \n [5] stringr_1.5.1   dplyr_1.1.4     purrr_1.0.2     readr_2.1.5    \n [9] tidyr_1.3.1     tibble_3.2.1    ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gt_0.11.0         utf8_1.2.4        generics_0.1.3    xml2_1.3.6       \n [5] lattice_0.22-6    stringi_1.8.4     hms_1.1.3         digest_0.6.36    \n [9] magrittr_2.0.3    evaluate_0.24.0   grid_4.4.1        timechange_0.3.0 \n[13] fastmap_1.2.0     Matrix_1.7-0      jsonlite_1.8.8    mgcv_1.9-1       \n[17] httr_1.4.7        fansi_1.0.6       crosstalk_1.2.1   viridisLite_0.4.2\n[21] scales_1.3.0      lazyeval_0.2.2    cli_3.6.3         rlang_1.1.4      \n[25] nflreadr_1.4.0    splines_4.4.1     munsell_0.5.1     withr_3.0.0      \n[29] cachem_1.1.0      yaml_2.3.9        tools_4.4.1       tzdb_0.4.0       \n[33] memoise_2.0.1     colorspace_2.1-0  vctrs_0.6.5       R6_2.5.1         \n[37] magick_2.8.4      lifecycle_1.0.4   htmlwidgets_1.6.4 ggpath_1.0.1     \n[41] pkgconfig_2.0.3   pillar_1.9.0      gtable_0.3.5      Rcpp_1.0.13      \n[45] glue_1.7.0        data.table_1.15.4 xfun_0.46         tidyselect_1.2.1 \n[49] knitr_1.48        farver_2.1.2      nlme_3.1-164      htmltools_0.5.8.1\n[53] labeling_0.4.3    rmarkdown_2.27    compiler_4.4.1   \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html",
    "href": "player-evaluation.html",
    "title": "5  Player Evaluation",
    "section": "",
    "text": "5.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-playerEvaluationGettingStarted",
    "href": "player-evaluation.html#sec-playerEvaluationGettingStarted",
    "title": "5  Player Evaluation",
    "section": "",
    "text": "5.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#overview",
    "href": "player-evaluation.html#overview",
    "title": "5  Player Evaluation",
    "section": "\n5.2 Overview",
    "text": "5.2 Overview\nEvaluating players for fantasy football could be thought of as similar to the process of evaluating companies when picking stocks to buy. You want to evaluate and compare various assets so that you get the assets with the best value.\nThere are various domains of criteria we can consider when evaluating a football player’s fantasy prospects. Potential domains to consider include:\n\nathletic profile\nhistorical performance\nhealth\nage and career stage\nsituational factors\nmatchups\ncognitive and motivational factors\nfantasy value\n\nThe discussion that follows is based on my and others’ impressions of some of the characteristics that may be valuable to consider when evaluating players. However, the extent to which any factor is actually relevant for predicting future performance is an empirical question and should be evaluated empirically.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAthleticProfile",
    "href": "player-evaluation.html#sec-evalAthleticProfile",
    "title": "5  Player Evaluation",
    "section": "\n5.3 Athletic Profile",
    "text": "5.3 Athletic Profile\nFactors related to a player’s athletic profile include factors such as:\n\nbody shape\n\nheight\nweight\nhand size\nwing span (arm length)\n\n\nbody function\n\nagility\nstrength\nspeed\nacceleration/explosiveness\njumping ability\n\n\n\nIn terms of body shape, we might consider a player’s height, weight, hand size, and wing span (arm length). Height allows players to see over opponents and to reach balls higher in the air. Thus, greater height is particularly valuable for Quarterbacks and Wide Receivers. Heavier players are tougher to budge and to tackle. Greater weight is particularly valuable for Linemen, Fullbacks, and Tight Ends, but it can also be valuable—to a deree—for Quarterbacks, Running Backs, and Wide Receivers. Hand size and wing span is particularly valuable for people catching the ball; thus, a larger hand size and longer wing span are particularly valuable for Wide Receivers and Tight Ends.\nIn terms of body function, we can consider a player’s agility, strength, speed, acceleration/explosiveness, and jumping ability. For Wide Receivers, speed, explosiveness, and jumping ability are particularly valuable. For Running Backs, agility, strength, speed, and explosiveness are particularly valuable.\nMany aspects of a player’s athletic profile, including tests of speed (40-yard dash), strength (bench press), agility (2-yard shuttle run; three cone drill), and jumping ability (vertical jump; broad jump) are available from the National Football League (NFL) Combine, which is especially relevant for evaluating rookies. We demonstrate how to import data from the NFL Combine in Section 3.5.6. There are also calculators that integrate information about body shape and information from the NFL Combine to determine a player’s relative athletic score (RAS) for their position: https://ras.football/ras-calculator/",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-skill",
    "href": "player-evaluation.html#sec-skill",
    "title": "5  Player Evaluation",
    "section": "\n5.4 Skill",
    "text": "5.4 Skill\nWhen scouting players, scouts consider not only the player’s athletic profile, but also their position-relevant skill. For instance, how good are they are reading the defense, passing the ball, running routes, catching balls, making defenders miss tackles, taking care of the ball, consistency, etc. Scouting and evaluating skill is a complicated endeavor, and even the professional scouts frequently make mistakes in their evaluations and predictions. You can certainly read skill evaluations about various players; however, unlike metrics of athletic profile, we do not have direct access to the player’s underlying skill. Some may say, “You know it when you see it.” But, this is not particularly useful when trying to identify players who are undervalued or overvalued—because the skill evaluations are likely already “baked into” a player’s projections. Because we do not have direct access to a player’s skill, we tend to rely on indirect metrics of their ability, such as historical performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalHistoricalPerformance",
    "href": "player-evaluation.html#sec-evalHistoricalPerformance",
    "title": "5  Player Evaluation",
    "section": "\n5.5 Historical Performance",
    "text": "5.5 Historical Performance\n\n5.5.1 Overview\n\n“The best predictor of future behavior is past behavior.” – Unknown\n\n\n“Past performance does not guarantee future results.” – A common disclaimer about investments.\n\nFactors relating to historical performance to consider could include:\n\nperformance in college\n\ndraft position\n\n\nperformance in the NFL\nefficiency\nconsistency\n\nCompared to tests of speed, power, and agility at the NFL Combine, collegiate performance is a stronger predictor of performance in the NFL (Lyons et al., 2011). That is, previous sports performance is the best predictor of future performance (for a review, see Den Hartigh et al., 2018). Thus, it is important to consider a player’s past performance. However, the extent to which historical performance may predict future performance may depend on many factors such as (a) the similarity of the prior situation to the current situation, (b) how long ago the prior situation was, and (c) the extent to which the player (or situation) has changed in the interim. For rookies, the player does not have prior seasons of performance in the NFL to draw upon. Thus, when evaluating rookies, it can be helpful to consider their performance in college or in their prior leagues. However, there are large differences between the situation in college and the situation in the NFL, so prior success in college may not portend future success in the NFL. An indicator that intends to be prognostic of future performance, and that accounts for past performance, is a player’s draft position—that is, how early (or late) was a player selected in the NFL Draft. The earlier a player was selected in the NFL Draft, the greater likelihood that the player will perform well; however, this is somewhat countered by the fact that the teams with the highest draft picks tend to be the worst based on the prior season’s record.\nFor players who have played in the NFL, past performance becomes more relevant because, presumably, the prior situation is more similar (than was their situation in college) to their current situation. Nevertheless, lots of things change from game to game and season to season: injuries, coaches, coaching strategies, teammates, etc. So just because a player performed well or poorly in a given game or season does not necessarily mean that they will perform similarly in subsequent games/seasons. Nevertheless, historical performance is one of the best indicators we have.\nWe demonstrate how to import historical player statistics in Section 3.5.12. We demonstrate how to calculate historical player statistics in Section 3.21.1. We demonstrate how to calculate historical fantasy points in Section 3.21.2.\n\n5.5.2 Efficiency\nIn addition to how many fantasy points a player scores in terms of historical performance, we also care about efficiency and consistency. How efficient were they given the number of opportunities they had? If they were relatively more efficient, they will likely score more points than many of their peers when given more opportunities. If they were relativelly inefficient, their capacity to score fantasy points may be more dependent on touches/opportunities. Efficiency might be operationalized by indicators such as yards per passing attempt, yards per rushing attempt, yards per target, yards per reception, etc.\n\n5.5.3 Consistency\nIn terms of consistency, how consistent was the player they from game to game and from season to season? For instance, we could examine the standard deviations of players’ fantasy points across games in a given season. However, the standard deviation tends to be upwardly biased as the mean increases. So, we can account for the player’s mean fantasy points per game by dividing their game-to-game standard deviation of fantasy points (\\(\\sigma\\)) by their mean fantasy points across games (\\(\\mu\\)). This is known as the coefficient of variation (CV), which is provided in Equation 5.1.\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\tag{5.1}\\]\nPlayers with a lower standard deviation and a lower coefficient of variation (of fantasy points across games) are more consistent. In the example below, Player 2 might be preferable to Player 1 because Player 2 is more consistent; Player 1 is more “boom-or-bust.” Despite showing a similar mean of fantasy points across weeks, Player 2 shows a smaller week-to-week standard deviation and coefficient of variation.\n\nCodeset.seed(1)\n\nplayerScoresByWeek &lt;- data.frame(\n  player1_scores = rnorm(17, mean = 20, sd = 7),\n  player2_scores = rnorm(17, mean = 20, sd = 4),\n  player3_scores = rnorm(17, mean = 10, sd = 4),\n  player4_scores = rnorm(17, mean = 10, sd = 1)\n)\n\nconsistencyData &lt;- data.frame(t(playerScoresByWeek))\n\nweekNames &lt;- paste(\"week\", 1:17, sep = \"\")\n\nnames(consistencyData) &lt;- weekNames\nrow.names(consistencyData) &lt;- NULL\n\nconsistencyData$mean &lt;- rowMeans(consistencyData[,weekNames])\nconsistencyData$sd &lt;- apply(consistencyData, 1, sd)\nconsistencyData$cv &lt;- consistencyData$sd / consistencyData$mean\n\nconsistencyData$player &lt;- c(1, 2, 3, 4)\n\nconsistencyData &lt;- consistencyData %&gt;% \n  select(player, mean, sd, cv, week1:week17)\n\nround(consistencyData, 2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalHealth",
    "href": "player-evaluation.html#sec-evalHealth",
    "title": "5  Player Evaluation",
    "section": "\n5.6 Health",
    "text": "5.6 Health\nHealth-related factors to consider include:\n\ncurrent injury status\ninjury history\n\nIt is also important to consider a player’s past and current health status. In terms of a player’s current health status, it is important to consider whether they are injured or are playing at less than 100% of their typical health. In terms of a player’s prior health status, one can consider their injury history, including the frequency and severity of injuries and their prognosis.\nWe demonstrate how to import injury reports in Section 3.5.13.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalAgeCareerStage",
    "href": "player-evaluation.html#sec-evalAgeCareerStage",
    "title": "5  Player Evaluation",
    "section": "\n5.7 Age and Career Stage",
    "text": "5.7 Age and Career Stage\nAge and career stage-related factors include:\n\nage\nexperience\ntouches\n\nA player’s age is relevant because of important age-related changes in a player’s speed, ability to recover from injury, etc. A player’s experience is relevant because players develop knowledge and skills with greater experience. A player’s prior touches/usage is also relevant, because it speaks to how many hits a player may have taken. For players who take more hits, it may be more likely that their bodies “break down” sooner.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalSituation",
    "href": "player-evaluation.html#sec-evalSituation",
    "title": "5  Player Evaluation",
    "section": "\n5.8 Situational Factors",
    "text": "5.8 Situational Factors\nSituational factors one could consider include:\n\nteam quality\nrole on team\nteammates\nopportunity and usage\n\nsnap count\ntouches/targets\nred zone usage\n\n\n\nFootball is a team sport. A player is embedded within a broader team context; it is important to consider the strength of their team context insofar as it may support— or detract from—a player’s performance. For instance, for a Quarterback, it is important to consider how strong the pass blocking is from the Offensive Line. Will they have enough time to throw the ball, or will they be constantly under pressure to be sacked? It is also important to consider the strength of the pass catchers—the Wide Receivers and Tight Ends. For a Running Back, it is important to consider how strong the run blocking is from the Offensive Line. For a Wide Receiver, it is important to consider how strong the pass blocking is, and how strong the Quarterback is.\nIt is also important to consider a player’s role on the team. Is the player a starter or a backup? Related to this, it is important to consider the strength of one’s teammates. For a given Running Back, if a teammate is better at running the ball, this may take away from how much the player sees the field. For a given Wide Receiver, if a teammate is better at catching the ball, this may take some targets away from the player. However, the team’s top defensive back is often matched up against the team’s top Wide Receiver. So, if the team’s top Wide Receiver is matched up against a particularly strong Defensive Back, the second- and third-best Wide Receivers may more targets than usual.\nIt is also important to consider a player’s opportunity and usage, which are influenced by many factors, including the skill of the player, the skill of their teammates, the role of the player on the team, the coaching style, the strategy of the opposing team, game scripts, etc. In terms of the player’s opportunity and usage, how many snaps do they get? How many touches and/or targets do they receive? Being on the field for more snaps and receiving more touches and/or targets means that the player has more opportunities to score fantasy points. Are they targeted in the red zone? Red zone targets are more likely to lead to touchdown scoring opportunities, which are particularly valuable in fantasy football.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalMatchups",
    "href": "player-evaluation.html#sec-evalMatchups",
    "title": "5  Player Evaluation",
    "section": "\n5.9 Matchups",
    "text": "5.9 Matchups\nMatchup-related factors to consider include:\n\nstrength of schedule\nweekly matchup\n\nAnother aspect to consider is how challenging their matchup(s) and strength of schedule is. For a Quarterback, it is valuable to consider how strong the oppenent’s passing defense is. For a Running Back, how strong is the running defense? For a Wide Receiver, how strong is the passing defense and the Defensive Back that is likely to be assigned to guard them?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalCogMotivational",
    "href": "player-evaluation.html#sec-evalCogMotivational",
    "title": "5  Player Evaluation",
    "section": "\n5.10 Cognitive and Motivational Factors",
    "text": "5.10 Cognitive and Motivational Factors\nOther factors to consider include cognitive and motivational factors. Some coaches refer to these as the “X Factor” or “the intangibles.” However, just as any other construct in psychology, we can devise ways to operationalize them. Insofar as they are observable, they are measurable.\nCognitive and motivational factors one could consider include:\n\nreaction time\nknowledge and intelligence\nwork ethic and mental toughness\nincentives\n\ncontract performance incentives\nwhether they are in a contract year\n\n\n\nA player’s knowledge, intelligence, and reaction time can help them gain an upper-hand even when they may not be the fastest or strongest. A player’s work ethic and mental toughness may help them be resilient and persevere in the face of challenges. Contact-related incentives may lead a player to put forth greater effort. For instance, a contract may have a performance incentive that provides a player greater compensation if they achieve a particular performance milestone (e.g., receiving yards). Another potential incentive is if a player is in what is called their “contract year” (i.e., the last year of their current contract). If a player is in the last year of their current contract, they have an incentive to perform well so they can get re-signed to a new contract.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-fantasyValue",
    "href": "player-evaluation.html#sec-fantasyValue",
    "title": "5  Player Evaluation",
    "section": "\n5.11 Fantasy Value",
    "text": "5.11 Fantasy Value\n\n5.11.1 Sources From Which to Evaluate Fantasy Value\nThere are several sources that one can draw upon to evaluate a player’s fantasy value:\n\nexpert or aggregated rankings\nlayperson rankings\n\nplayers’ Average Draft Position (ADP) in other league snake drafts\n\nplayers’ Average Auction Value (AAV) in other league auction drafts\n\n\n\nexpert or aggregated projections\n\n\n5.11.1.1 Expert Fantasy Rankings\nFantasy rankings (by so-called “experts”) are provided by many sources. To reduce some of the bias due to a given source, some services aggregate projections across sources, consistent with a “wisdom of the crowd” approach. FantasyPros aggregates fantasy rankings across sources. Fantasy Football Analytics creates fantasy rankings from projections that are aggregated across sources (see the webapp here: https://apps.fantasyfootballanalytics.net).\n\n5.11.1.2 Layperson Fantasy Rankings: ADP and AAV\nAverage Draft Position (ADP) and Average Auction Value (AAV), are based on league drafts, mostly composed of everyday people. ADP is based on snake drafts, whereas AAV is based on auction drafts. Thus, ADP and AAV are consistent with a “wisdom of the crowd” approach, and I refer to them as forms of rankings by laypeople. ADP data are provided by FantasyPros. AAV data are also provided by FantasyPros.\n\n5.11.1.3 Projections\nProjections are provided by various sources. Projections (and rankings, for that matter) are a bit of a black box. It is often unclear how they were derived by a particular source. That is, it is unclear how much of the projection was based on statistical analysis versus conjecture.\nTo reduce some of the bias due to a given source, some services aggregate projections across sources, consistent with a “wisdom of the crowd” approach. Projections that are aggregated across sources are provided by Fantasy Football Analytics (see the webapp here: https://apps.fantasyfootballanalytics.net) and by FantasyPros.\n\n5.11.1.4 Benefits of Using Projections Rather than Rankings\nIt is important to keep in mind that rankings, ADP, and AAV are specific to roster and scoring settings of a particular league. For instance, in point-per-reception (PPR) leagues, players who catch lots of passes (Wide Receivers, Tight Ends, and some Running Backs) are valued more highly. As another example, Quarterbacks are valued more highly in 2-Quarterback leagues. Thus, if using rankings, ADP, or AAV, it is important to find ones from leagues that mirror—as closely as possible—your league settings.\nProjected statistics (e.g., projected passing touchdowns) are agnostic to league settings and can thus be used to generate league-specific fantasy projections and rankings. Thus, projected statisitics may be more useful than rankings because they can be used to generate rankings for your particular league settings. For instance, if you know how many touchdowns, yards, and interceptions a Quarterback is a projected to throw (in addition to any other relevant categories for the player, e.g., rushing yards and touchdowns), you can calculate how many fantasy points the Quarterback is expected to gain in your league (or in any league). Thus, you can calculate ranking from projections, but you cannot reverse engineer projections from rankings.\n\n5.11.2 Indices to Evaluate Fantasy Value\nBased on the sources above (rankings, ADP, AAV, and projections), we can derive multiple indices to evaluate fantasy value. There are many potential indices that can be worthwhile to consider, including a player’s:\n\ndropoff\nvalue over replacement player (VORP)\nuncertainty\n\n\n5.11.2.1 Dropoff\nA player’s dropoff is the difference between (a) the player’s projected points and (b) the projected points of the next-best player at that position.\n\n5.11.2.2 Value Over Replacement Player\nBecause players from some positions (e.g., Quarterbacks) tend to score more points than players from other positions (e.g., Wide Receivers), it would be inadvisable to compare players across different positions based on projected points. In order to more fairly compare players across positions, we can consider a player’s value over a typical replacement player at that position (shortened to “value over replacement player”). A player’s value over a replacement player (VORP) is the difference between (a) a player’s projected fantasy points and (b) the fantasy points that you would be expected to get from a typical bench player at that position. Thus, VORP provides an index of how much added value a player provides.\n\n5.11.2.3 Uncertainty\nA player’s uncertainty is how much variability there is in projections or rankings for a given player across sources. For instance, consider a scenario where three experts provide ratings about two players, Player A and Player B. Player A is projected to score 300, 310, and 290 points by experts 1, 2, and 3, respectively. Player B is projected to score 400, 300, and 200 points by experts 1, 2, and 3, respectively. In this case, both players are (on average) projected to score the same number of points (300).\n\nCodeexampleData &lt;- data.frame(\n  player = c(rep(\"A\", 3), rep(\"B\", 3)),\n  expert = c(1:3, 1:3),\n  projectedPoints = c(300, 310, 290, 400, 300, 200)\n)\n\nplayerA_mean &lt;- mean(exampleData$projectedPoints[which(exampleData$player == \"A\")])\nplayerB_mean &lt;- mean(exampleData$projectedPoints[which(exampleData$player == \"B\")])\n\nplayerA_sd &lt;- sd(exampleData$projectedPoints[which(exampleData$player == \"A\")])\nplayerB_sd &lt;- sd(exampleData$projectedPoints[which(exampleData$player == \"B\")])\n\nplayerA_cv &lt;- playerA_mean / playerA_sd\nplayerB_cv &lt;- playerB_mean / playerB_sd\n\n\n\nCodeplayerA_mean\n\n[1] 300\n\nCodeplayerB_mean\n\n[1] 300\n\n\nHowever, the players differ considerably in their uncertainty (i.e., the source-to-source variability in their projections), as operationalized with the standard deviation and coefficient variation of projected points across sources for a given player.\n\nCodeplayerA_sd\n\n[1] 10\n\nCodeplayerB_sd\n\n[1] 100\n\nCodeplayerA_cv\n\n[1] 30\n\nCodeplayerB_cv\n\n[1] 3\n\n\nHere is a depiction of a density plot of projected points for a player with a low, medium, and high uncertainty:\n\nCodeplayerA &lt;- rnorm(1000000, mean = 150, sd = 5)\nplayerB &lt;- rnorm(1000000, mean = 150, sd = 15)\nplayerC &lt;- rnorm(1000000, mean = 150, sd = 30)\n\nmydata &lt;- data.frame(playerA, playerB, playerC)\n\nmydata_long &lt;- mydata %&gt;% \n  pivot_longer(\n    cols = everything(),\n    names_to = \"player\",\n    values_to = \"points\"\n  ) %&gt;% \n  mutate(\n    name = case_match(\n      player,\n      \"playerA\" ~ \"Player A\",\n      \"playerB\" ~ \"Player B\",\n      \"playerC\" ~ \"Player C\",\n    )\n  )\n\nggplot2::ggplot(\n  data = mydata_long,\n  ggplot2::aes(\n    x = points,\n    fill = name\n  )\n) +\n  ggplot2::geom_density(alpha = .3) + \n  ggplot2::labs(\n    x = \"Players' Projected Points\",\n    title = \"Density Plot of Projected Points for Three Players\"\n  ) +\n  ggplot2::theme_classic() +\n  ggplot2::theme(legend.title = element_blank())\n\n\n\n\n\n\nFigure 5.1: Density Plot of Projected Points for Three Players\n\n\n\n\nUncertainty is not necessarily a bad characteristic of a player’s projected points. It just means we have less confidence about how the player may be expected to perform. Thus, players with greater uncertainty are risky and tend to have a higher upside (or ceiling) and a lower downside (or floor).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-evalIntegration",
    "href": "player-evaluation.html#sec-evalIntegration",
    "title": "5  Player Evaluation",
    "section": "\n5.12 Putting it Altogether",
    "text": "5.12 Putting it Altogether\nAfter performing an evaluation of the relevant domain(s) for a given player, then one must integrate the evaluation information across domains to make a judgment about a player’s overall value. When thinking about a player’s value, it can be worth thinking of a player’s upside and a player’s downside. Player that are more consistent may show higher downside but a lower upside. Younger, less experienced players may show a higher upside but a lower downside.\nThe extent to which you prioritize a higher upside versus a higher downside may depend on many factors. For instance, when drafting players, you may prioritize drafting players with the highest downside (i.e., the safest players), whereas you may draft sleepers (i.e., players with higher upside) for your bench. When choosing which players to start in a given week, if you are predicted to beat a team handily, it may make sense to start the players with the highest downside. By contrast, if you are predicted to lose to a team by a good margin, it may make sense to start the players with the highest upside.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "player-evaluation.html#sec-playerEvaluationSessionInfo",
    "href": "player-evaluation.html#sec-playerEvaluationSessionInfo",
    "title": "5  Player Evaluation",
    "section": "\n5.13 Session Info",
    "text": "5.13 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      jsonlite_1.8.8    compiler_4.4.1    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.9        fastmap_1.2.0     R6_2.5.1         \n [9] labeling_0.4.3    generics_0.1.3    knitr_1.48        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.9.0      tzdb_0.4.0        rlang_1.1.4      \n[17] utf8_1.2.4        stringi_1.8.4     xfun_0.46         timechange_0.3.0 \n[21] cli_3.6.3         withr_3.0.0       magrittr_2.0.3    digest_0.6.36    \n[25] grid_4.4.1        hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[29] evaluate_0.24.0   glue_1.7.0        farver_2.1.2      fansi_1.0.6      \n[33] colorspace_2.1-0  rmarkdown_2.27    tools_4.4.1       pkgconfig_2.0.3  \n[37] htmltools_0.5.8.1\n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J. (2011). On the predictive efficiency of past performance and physical ability: The case of the national football league. Human Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Player Evaluation</span>"
    ]
  },
  {
    "objectID": "draft.html",
    "href": "draft.html",
    "title": "6  The Fantasy Draft",
    "section": "",
    "text": "6.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftGettingStarted",
    "href": "draft.html#sec-draftGettingStarted",
    "title": "6  The Fantasy Draft",
    "section": "",
    "text": "6.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftTypes",
    "href": "draft.html#sec-draftTypes",
    "title": "6  The Fantasy Draft",
    "section": "\n6.2 Types of Fantasy Drafts",
    "text": "6.2 Types of Fantasy Drafts\nThere are several types of drafts in fantasy football. The most common types of drafts are snake drafts and auction drafts.\n\n6.2.1 Snake Draft\nIn a snake draft, the participants (i.e., managers) are assigned a draft order. In the first round, the managers draft in that order. In the second round, the managers draft in reverse order. It continues to “snake” in this way, round after round, so that the person who has the first pick in a given round has the last pick in the next round, and whoever has the last pick in a given round has the first pick in the next round.\n\n6.2.2 Auction Draft\nIn an auction draft, the managers are assigned a nomination order and there is a salary cap (e.g., $200). The first manager chooses which player to nominate. Then, the managers bid on that player like in an auction. In order to bid, the manager must raise the price by at least $1. If two managers want to obtain the same player, they may continue to raise the amount until one manager backs out and is no longer to bid by raising the price. The highest bidder wins (i.e., drafts) that player. Then, the second manager nominates a player, and the managers bid on that player. This process repeats until all teams have drafted their allotment of players.\n\n6.2.3 Comparison\nSnake drafts are more common than auction drafts. Snake drafts tend to be quicker than auction drafts. However, auction drafts are more fair than snake drafts. In an auction draft, unlike a snake draft, all players are available to all teams. For instance, in a snake draft, the first 9 players drafted are unavailable to the 10th pick of the first round. So, if you have the 10th pick and want the top-ranked player, this player would not be available to you in the snake draft. However, in the auction draft, every player is available to every manager, so long as the manager is able and willing to bid enough.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftStrategy",
    "href": "draft.html#sec-draftStrategy",
    "title": "6  The Fantasy Draft",
    "section": "\n6.3 Draft Strategy",
    "text": "6.3 Draft Strategy\n\n6.3.1 Overview\nThere is no one “right” draft strategy. As noted by Lee & Liu (2022) in their analysis of fantasy drafts, the effectiveness of any draft strategy depends on the strategies of the other managers in the league. Sometimes it works best to “zig” when everyone else is “zagging”. For instance, if you notice that everyone else is drafting Wide Receivers, this may mean that other managers are over-valuing Wide Receivers, and this could be a nice opportunity to draft a Running Back for good value.\nIn general, you will first want to generate the rankings you will use to select which players to prioritize. You may generate your rankings based one or more of the following:\n\nyour evaluation of players\n\nexpert or aggregated rankings\n\nlayperson rankings\n\nplayers’ Average Draft Position (ADP) in other league drafts (for snake drafts)\nplayers’ Average Auction Value (AAV) in other league drafts (for auction drafts)\n\n\nexpert or aggregated projections\nindices derived from rankings and projections\n\nSection 5.11.1 describes where to obtain aggregated rankings, aggregated projections, ADP, and AAV data.\nAn important concept in the draft is “dropoff”, which is described in Section 5.11.2.1. Dropoff at a given position, is the difference—in terms of projected fantasy points—between (a) the best available player remaining at that position and (b) the second-best available player remaining at that position. If there is a bigger dropoff at a given position, there may be greater value in drafting the top player from that position. For instance, consider the following scenario: “Quarterback A” is projected to score 325 points, and “Quarterback B” is projected to score 320 points. “Tight End A” is projected to score 230 points, and “Tight End B” is projected to score 150 points. In this example, there is a much greater dropoff for Tight Ends than there is for Quarterbacks. Thus, even though “Quarterback A” is projected to score more points than “Tight End A”, “Tight End A” may be more valuable because there is still a good Quarterback available if someone else drafts “Quarterback A”.\nAnother important concept is a player’s value over a typical replacement player at that position (shortened to “value over replacement player”; VORP), which is described in Section 5.11.2.2.\nAnother important concept is a player’s uncertainty, which is described in Section 5.11.2.3.\nIn both snake and auction draft formats, your goal is to draft the team whose weekly starting lineup scores the most points and thus the collection of players with the greatest VORP. For your starting lineup, it may make sense—especially with your earliest selections—when comparing two players with equivalent VORP, to prioritize players with higher consistency and lower uncertainty, because they may be considered “safer” with a higher floor. However, when drafting players for your bench, it make make more sense to prioritize high-risk, high reward players with greater uncertainty, because they may have a higher ceiling. Players with a higher ceiling have a potential to be “sleepers”—players who are valued low (i.e., with a high ADP or low AAV) and who outperform their valuation. Note that, although players with greater uncertainty are high-risk, high-reward players, selecting this kind of a player for your bench (i.e., in a late round or for a small cost) is a lower risk selection, because you have less to lose with later/lower-cost picks. That is, even though the player is higher risk, selecting a higher risk player for your bench is a lower risk decision.\nThe Spurs in the National Basketball Association (NBA) were well-reputed for excelling in this draft strategy (archived at https://perma.cc/X7NW-WZC6). They frequently used their second-round picks to draft high-risk, high-reward players. Sometimes, the secound round pick was a bust, but they have little to lose with a failed second round pick. Other times, their second round picks—including Willie Anderson, DeJuan Blair, Goran Dragic, Luis Scola, and Manu Ginóbili—greatly outperformed expectations. Thanks, in part, to this draft strategy, the team showed strong extended success for nearly three decades from 1989 through the late-2010s.\nHowever, the draft strategies to achieve the “optimal lineup” differ between snake versus auction drafts.\n\n6.3.2 Snake Draft\nIn general, your goal is to draft the team whose weekly starting lineup has the greatest VORP. Consequently, you are often looking to pick the player with the highest VORP at a given selection, while keeping in mind (a) the dropoff of players at other positions and (b) which players may be available at subsequent picks so that you do not sacrifice too much later value with a given selection. For instance, if a particular Quarterback has a slightly higher VORP than a particular Running Back, but the Quarterback is likely to be available at the manager’s next pick but the Running Back is likely to be unavailable at their next pick, it might make more sense to draft the Running Back.\n\n6.3.3 Auction Draft\nAccording to an analysis by the Harvard Sports Analysis Collective (archived at https://perma.cc/P7RX-92UU), the majority of the manager’s salary cap should be spent on the starting lineup, and you should spend less on bench players. This is known as the “stars and scrubs” draft strategy. Based on the analysis, the author recommended applying a 10% premium to the top players and a 10% discount to the lower-tiered players. The idea behind the approach is that a player on your bench does not contribute to the team’s points and, thus, most players drafted to your bench do not contribute much to the team’s points throughout the season. That said, bench players can be important in the case of a starter’s injury or under-performance. So, it is recommended to draft starters with lower uncertainty who are safer. In contrast to your starting lineup, you may look to draft players on your bench who have greater uncertainty for their high reward potential in a low-risk selection given the lower price.\nAn alternative to the “stars and scrubs” approach is to wait to draft more “high-value” players after other managers have over-paid for players.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "draft.html#sec-draftSessionInfo",
    "href": "draft.html#sec-draftSessionInfo",
    "title": "6  The Fantasy Draft",
    "section": "\n6.4 Session Info",
    "text": "6.4 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.1    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.1       htmltools_0.5.8.1 rmarkdown_2.27    knitr_1.48       \n [9] jsonlite_1.8.8    xfun_0.46         digest_0.6.36     rlang_1.1.4      \n[13] evaluate_0.24.0  \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nLee, M. D., & Liu, S. (2022). Drafting strategies in fantasy football: A study of competitive sequential human decision making. Judgment and Decision Making, 17(4), 691–719. https://doi.org/10.1017/S1930297500008901",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Fantasy Draft</span>"
    ]
  },
  {
    "objectID": "research-methods.html",
    "href": "research-methods.html",
    "title": "7  Research Methods",
    "section": "",
    "text": "7.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsGettingStarted",
    "href": "research-methods.html#sec-researchMethodsGettingStarted",
    "title": "7  Research Methods",
    "section": "",
    "text": "7.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-sampleVsPopulation",
    "href": "research-methods.html#sec-sampleVsPopulation",
    "title": "7  Research Methods",
    "section": "\n7.2 Sample vs Population",
    "text": "7.2 Sample vs Population\nIn research, it is important to distinguish between the sample and the target population. The target population is who you want your study’s findings to generalize to. For instance, if we want our findings to lead to inferences we can draw regarding all current NFL players, then NFL players are our target population. However, despite our best efforts to recruit all NFL players into our study, we may not succeed in doing that. The participants (i.e., people or players) who we successfully recruit to be in our study represent our sample. The number of participants in the study is our sample size.\nIt is rare for the sample to include all people who are in the target population. It can be costly to recruit large samples, and many potential participants may decline to participate for a variety of reasons (insufficient time, lack of interest in the study, distrust of scientists, etc.). Thus, our goals are (a) to recruit as many people from the population as possible and (b) for the sample to be as representative of the population as possible.\nFor increasing the representativeness of the sample (with respect to the population), we might conduct a random sample, in which each person in the population (i.e., each NFL player) has equal likelihood of being selected. For instance, we might randomly select 250 players to recruit to the study. True random samples, though strong in aspiration, are difficult and costly to achieve. In reality, many researchers conduct convenience sampling. A convenience sample is recruited because it is convenient (i.e., less costly and time-consuming).\nFor instance, many studies examine college students—in part, because they are easy to recruit. If our target population is NFL players but we are unable to recruit NFL players into our study, we could easily recruit a large sample of college students. Although the convenience sample may afford a very large sample, the college student sample may not be representative of the target population (NFL players). Thus, the findings in our study may not generalize to NFL players—that is, what we learn in college students may not apply in the same way among NFL players. For instance, if we learn that consumption of sports drinks (compared to drinking only water) improves running speed among college students, that may not be the case among NFL players.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchDesigns",
    "href": "research-methods.html#sec-researchDesigns",
    "title": "7  Research Methods",
    "section": "\n7.3 Research Designs",
    "text": "7.3 Research Designs\nThere are three broad types of research designs:\n\nexperiment\ncorrelational/observational study\ncase study\n\n\n7.3.1 Experiment\nIn an experiment, there are one or more things (i.e., variables) that we manipulate to see how the manipulation influences the process of interest. The variable that we manipulate is the independent variable. By contast, the dependent variable is the variable that we evaluate to determine whether it was influenced by the manipulation (i.e., by the independent variable).Besides the independent and dependent variables, the researcher attempts to hold everything else constant through processes including standardization and random assignment. Standardization involves using the same procedures to assess each participant, so that scores can be fairly compared across participants (and groups). Random assignment involves randomly assigning participants to conditions of the independent variable, so the people in each condition are comparable and do not differ systematically.\nFor instance, we may be interested to evaluate whether players perform better (e.g., run faster) if they drink a sports drink compared when they drink only water. Our hypothesis might be that players will be expected to perform better when they drink a sports drink (compared to when they drink only water). To this this research question and hypothesis, we might conduct an experiment by randomly assigning some players during practice to receive a sports drink and some players to receive only water. In this case, our independent variable is whether the player receives a sports drink. Our dependent variable might be their 40-yard dash time during practice.\n\n7.3.2 Correlational/Observational Study\nIn a correlational (aka observational) study, we do not manipulate a variable to see how the manipulation influences another variable. Instead, we examine how two variables, a predictor and an outcome variable, are associated. The hypothesized cause is called the predictor variable. The hypothesized effect is called the outcome variable. In this way, the predictor variable is similar to the independent variable, and the outcome variable is similar to the dependent variable. However, unlike the independent and dependent variables in an experiment, the predictor and outcome variables in a correlational study are not manipulated.\nFor instance, to use a correlational study to test the possibility that players who drink sports drinks perform better than players who drink only water, we could examine whether the players who drink sports drinks during a game score more fantasy points than players who drink only water during the game. In this case, our predictor variable is whether the players drinks sports drinks during a game. Our outcome variable is the number of fantasy points the player scored.\n\n7.3.2.1 Correlation Does Not Imply Causation\nAs the maxim goes, “correlation does not imply causation”—just because two variables are associated does not necessarily mean that they are causally related.\nJust because X is associated with Y does not mean that X causes Y. Consider that you find an association between variables X and Y.\nThere are several reasons why you might observe an association between X and Y:\n\n\nX causes Y\n\n\nY causes X\n\n\nX and Y are bidirectional: X causes Y and Y causes X\n\na third variable (i.e., confound), Z, influences both X and Y\n\nthe association between X and Y is spurious\n\nFor instance, one possibility is that the association we observed reflects our hypothesis that X causes Y, as depicted in Figure 7.1. That is, consumption of more sports drink may improve players’ performance.\n\n\n\n\n\nFigure 7.1: Hypothesized Causal Effect Based on an Observed Association Between X and Y, Such That X Causes Y.\n\n\nHowever, a second possibility is that the association reflects the opposite direction of effect, where Y actually causes X, as depicted in Figure 7.2. For instance, greater performance may lead players to drink more sports drink (rather than the reverse).\n\n\n\n\n\nFigure 7.2: Reverse (Opposite) Direction of Effect From the Hypothesized Effect, Where Y Causes X.\n\n\nA third possibility is that the association reflects a bidirectional effect, where X causes Y and Y causes X, as depicted in Figure 7.3. For instance, consumption of more sports drink may improve players’ performance, and greater performance in turn may lead players to drink more sports drink.\n\n\n\n\n\nFigure 7.3: Bidirectional Effect Between X and Y, such that X causes Y and Y causes X.\n\n\nA fourth possibility is that the association could reflect the influence of a third variable. If a third variable is a common cause of each and accounts for their association, it is a confound. An observed association between X and Y could reflect a confound—i.e., a cause (Z) that influences both X and Y, which explains why X and Y are correlated even though they are not causally related. A third variable confound that is a common cause of both X and Y is depicted in Figure 7.4. For instance, it may not be that sport drink consumption per se influences player performance; rather, it may be that players who are more intelligent or have more financial resources tend to drink more sports drinks and also tend to perform better. In this case, intelligence or financial resources may be a confound that influences both sports drink consumption and player performance, but sports drink consumptions—though correlated with player performance—does not influence player performance.\nFor another example, consider that ice cream sales are associated with shark attacks. It is unlikely that more people eating ice creams leads to shark attacks. There is a likely a third variable—heat waves—that is a confound because it influences both ice cream sales and shark attacks and explains their association.\n\n\n\n\n\nFigure 7.4: Confounded Association Between X and Y due to a Common Cause, Z.\n\n\nLastly, the association might be spurious. It might just reflect random variation (i.e., chance), and that when tested on an independent sample, what appeared as an association in the original dataset may not hold when testing the association in a new dataset.\n\n7.3.3 Case Study\nIn a case study, we assess a small sample of individuals (commonly only one person or a few people), often with rich qualitative information. Themes may be coded from the qualitative information, which may help inform inferences about whether some process may have played a role in influencing the outcome of interest. The inferences are then drawn in a subjective, qualitative way. Testimonials and anecdotes are examples are case studies.\nFor instance, to use a case study to evalute the possibilty that players who drink sports drinks perform better than players who drink only water, we could conduct an in-depth interview with a player. In the interview, we might ask the player how they performed in games with versus without a sports drink and have them discuss whether they believe the sports drink improved their performance (and if so, how). Then, based on the player’s responses, we might code the responses to extract themes and to make a qualitative judgement of whether or not the player likely performed better during games in which they had a sports drink.\n\n7.3.4 Other Features of the Research Design\n\n7.3.4.1 Number of Timepoints\nIn addition to whether the research design is an experiment, correlational/observational study, or a case study, a research design can also have one or multiple timepoints. The differing number of timepoints allow studies to be characterized as one of the following:\n\ncross-sectional\nlongitudinal\n\n\n7.3.4.1.1 Cross-Sectional\nA cross-sectional study is a study with one timepoint.\nFor instance, in a cross-sectional study evaluating whether having a sports drink improves player performance, we might assess players’ drinking behavior and performance during only game 1.\nCross-sectional studies are more common than longitudinal studies because cross-sectional studies are less costly and time-consuming. They can provide a helpful starting point to test findings more rigorously in subsequent longitudinal studies.\n\n7.3.4.1.2 Longitudinal Design\nA longitudinal study is a study with more than one timepoint. When the same measures are assessed at each of multiple timepoints, we refer to this as a “repeated measures” design.\nIn a longitudinal study evaluating whether having a sports drink improves player performance, we might assess players’ drinking behavior and performance during each game of the season, and possibly across multiple seasons.\nLongitudinal studies are less common than cross-sectional studies because longitudinal studies are more costly and time-consuming. Nevertheless, longitudinal studies can allow us test our hypotheses more rigorously, because they can allow us to test whether changes in the predictor/indepdnent variable leads to changes in the outcome/dependent variable. Thus, compared to cross-sectional studies, longitudinal studies can provide greater confidence in causal inferences.\n\n7.3.4.2 Within- or Between-Subject\nA research design can also be within-subject, between-subject, or both. A study can involve both within-subject and between-subject comparisons if one predictor/independent variable is within-subject and another predictor/independent variable is between-subject.\n\n7.3.4.2.1 Within-Subject Design\nA within-subject design is one in which each participant (i.e., person or player) receives multiple levels of the independent variable (or predictor).\nFor instance, in an experiment evaluating whether having a sports drink improves player performance, we might assign players to drink the sports drink in the first half of the game and to drink only water in the second half of the game. Or we could assign some of the players to drink sports drink in the first half and water in the second half, and assign the other players to drink water in the first half and sports drink in the second half.\nIn a correlational study evaluating whether having a sports drink improves player performance, we might evaluate how within-person changes in sports drink consumption are associated with within-person changes in performance. That is, we could evaluate, when a given player has a sports drink (or more sports drinks), do they perform better than when the same individual has only water (or fewer sports drinks)?\nWithin-subject designs tend to have greater statistical power than between-subject designs. However, within-subject designs often have carryover effects. For instance, consider the study in which we assign players to drink only water in the first and third quarters and to drink sports drink in the second and fourth quarters (an A-B-A-B design). Drinking sports drink in the second quarter could increase how much hydration a player has throughout the rest of the game, which could lead to altered performance in the third and fourth quarters that is not due to what they drink in third and fourth quarters.\n\n7.3.4.2.2 Between-Subject Design\nA between-subject design is one in which each participant (i.e., person or player) receives only one level of the independent variable.\nFor instance, in an experiment evaluating whether having a sports drink improves player performance, we might assign some players to drink the sports drink but the other players to drink only water.\nIn a correlational study evaluating whether having a sports drink improves player performance, we might evaluate whether people who drink sports drinks tend to perform better than players who drink only water. Or, we could evaluate whether players who drink more sports drinks perform better than players who drink fewer sports drinks (i.e., whether the number of sports drinks consumed during a game is correlated with player performance).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchDesignValidity",
    "href": "research-methods.html#sec-researchDesignValidity",
    "title": "7  Research Methods",
    "section": "\n7.4 Research Design Validity",
    "text": "7.4 Research Design Validity\nResearch design validity involves the accuracy of inferences from a study. There are three types of research design validity:\n\ninternal validity\nexternal validity\nconclusion validity\n\n\n7.4.1 Internal Validity\nInternal validity is the extent to which we can be confident that the associations identified in the study are causal.\n\n7.4.2 External Validity\nExternal validity is the extent to which we can be confident that findings from the study play out similarly in the real world—that is, the findings generalize to the target population.\n\n7.4.3 Tradeoffs Between Internal and External Validity\nThere is a tradeoff between internal and external validity—a single research design cannot have both high internal and high external validity. Each study and design has weaknesses. Some research designs are better suited for making causal inferences, whereas other designs tend to be better suited for making inferences that generalize to the real world. The research design that is best suited to making causal inferences is an experiment because it is the design in which the researcher has the greatest control over the variables. Thus, experiments tend to have higher internal validity than other research designs. However, by manipulating one variable and holding everything else constant, the research takes place in a very standardized fashion that can become like studying a process in a vacuum. So, even if a process is theoretically causal in a vacuum, it may act differently in the real world when it interacts with other processes.\nCorrelational designs have greater capacity for external validity than experimental designs because the participants can be observed in their natural environments to evaluate how variables are related in the real world. However, the greater external validity comes at a cost of lower internal validity. Correlational designs are not well-positioned to make causal inferences. Correlational studies can account for potential confounds using covariates or for the reverse direction of effect using longitudinal designs, but the researcher has less control over the variables than in an experiment.\nAs the internal validity of a study’s design increases, its external validity tends to decrease. The greater control we have over variables (and, therefore, have greater confidence about causal inferences), the lower the likelihood that the findings reflect what happens in the real world because it is studying things in a metaphorical vacuum. Because no single research design can have both high internal and external validity, scientific inquiry needs a combination of many different research designs so we can be more confident in our inferences—experimental designs for making causal inferences and correlational designs for making inferences that are more likely to reflect the real world.\nCase studies, because they have smaller sample sizes and inferences drawn in a subjective, qualitative way, tend to have lower external validity than both experimental and correlational studies. Case studies also tend to have lower internal validity because they have less control over variables, and thus fail to remove the possibility of illusory correlations, potential confounds, or the reverse direction of effect. Thus, case studies are among the weakest forms of evidence. Nevertheless, case studies can still be useful for generating hypotheses that can then be tested empirically with a larger sample in experimental or correlational studies.\n\n7.4.4 Conclusion Validity\nConclusion validity is the extent to which a study’s conclusions are reasonable about the association among variables based on the data. That is, were the correct statistical analyses performed, and are the interpretations of the findings from those analyses correct?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-mediationModeration",
    "href": "research-methods.html#sec-mediationModeration",
    "title": "7  Research Methods",
    "section": "\n7.5 Mediation vs Moderation",
    "text": "7.5 Mediation vs Moderation\nBoth types of effects involve (at least) three variables:\n\nAn independent/predictor variable, which will be labeled as X.\nA dependent/outcome variable, which will be labeled as Y.\nThe mediator or moderator variable, which will be labeled as M.\n\nA mnemonic to help remember the difference between mediation and moderation is in Figure 7.5.\n\n\n\n\n\nFigure 7.5: Mediation Versus Moderation Mnemonic.\n\n\n\n7.5.1 Mediation\n\n7.5.1.1 Overview\nMediation is a causal chain of events, where one variable (a mediator variable) at least partially explains (or accounts for) the association between two other variables (the predictor variable and the outcome variable). In mediation, a predictor (X) leads to a mediator (M), which leads to an outcome (Y). Mediation answers the question of, “Why (or how) does X influence Y? A mediator (M) is a variable that helps explain the assocation between two other variables, and it answers the question of why/how X influences Y. That is, the mediator is the variable that helps explain how/why X is related to Y. In other words, you can think of the mediator as the mechanism that helps explain why X has an impact on Y. The association between X and Y gets smaller when accounting for M. Visually this can be written as in Figure 7.6:\n\n\n\n\n\nFigure 7.6: Mediation.\n\n\nwhere X is causing M, which in turn is causing Y. In other words, X leads to M, and M leads to Y.\nFor instance, if we determine that consuming sports drinks improves player performance, we may want to know how/why. That is, what is the mechanism that leads consumption of sports drinks to improve player performance? We might hypothesize that consumption of sports drink helps increase a player’s hydration, which in turn will improve the player’s performance. In this case, increased hydration mediates (i.e., helps explain or account for) the effect of the sports drink consumption on improved player performance.\nQuestion: Why/how does sports drink consumpion lead players to perform better?\nAnswer: increased hydration\nAs a picture, we can draw this assocation as in Figure 7.7:\n\n\n\n\n\nFigure 7.7: Mediation Example.\n\n\n\n7.5.1.2 Types of Mediation\n\n7.5.1.2.1 Full Mediation\nWhen one mechanism fully accounts for the effect of the predictor variable on the outcome variable, this is known as full mediation, as depicted in Figure 11.15:\n\n\n\n\n\nFigure 7.8: Full Mediation.\n\n\n\n7.5.1.2.2 Partial Mediation\nWhen a single process partially—but does not fully—accounts for the effect of the predictor variable on the outcome variable; this is known as partial mediation and is depicted in Figure 11.16:\n\n\n\n\n\nFigure 7.9: Partial Mediation.\n\n\n\n7.5.1.2.3 Multiple Mediators\nIn addition, there can be multiple mediators/mechanisms that account for the effect of a predictor variable on an outcome variable, as depicted in Figure 7.10:\n\n\n\n\n\nFigure 7.10: Multiple Mediators.\n\n\n\n7.5.2 Moderation (i.e., Interaction)\n\n7.5.2.1 Overview\nModeration (sometimes called an “interaction”), on the other hand, occurs when there is a variable or condition (M; called a “moderator”) that changes the assocation between X and Y. That is, the effect of the predictor variable on the outcome variable differs at different levels of the moderator variable. In these cases, X and M work together to have an effect on Y; here X does not have a direct effect on M. Moderation answers the question of, “For whom does X influence Y?” If X influences Y more strongly for some people or in some circumstances, we would say that there is an interaction such that the effect of X on Y depends on M, as depicted in Figure 7.11:\n\n\n\n\n\nFigure 7.11: Moderation.\n\n\nFor example, if the effect of consuming sports drinks on player performance differs for Quarterbacks and Wide Receivers, the interaction could be depicted in Figures 7.12 and 7.13:\n\n\n\n\n\nFigure 7.12: Moderation Example: Path Diagram.\n\n\n\n\n\n\n\nFigure 7.13: Moderation Example: Interaction Graph.\n\n\nAn interaction can be identified visually by non-parallel lines at different levels of the moderator. In this example, the player’s position moderates the effect consuming sports drinks on player performance. In particular, there is a strong positive association between consuming sports drinks and player performance for Wide Receivers (as evidenced by the upward slope of the best-fit regression line), whereas there is no association between consuming sports drinks and player performance for Quarterbacks (as evidenced by the flat line).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-levelsOfMeasurement",
    "href": "research-methods.html#sec-levelsOfMeasurement",
    "title": "7  Research Methods",
    "section": "\n7.6 Levels of Measurement",
    "text": "7.6 Levels of Measurement\nIt is important to know the levels of measurement of your data, because the level(s) of measurement of your data constrain the types of comparisons and analyses that you can meaningfully perform. There are four levels of measurement that any variable can have:\n\nnominal\nordinal\ninterval\nratio\n\nEach is described below:\n\n7.6.1 Nominal\nA variable is considered nominal if it is composed of qualitative classifications. You cannot meaningfully evaluate whether one number in the variable is larger than another number in the variable because higher numbers do not reflect higher levels of the concept. Examples of nominal variables include:\n\nsex (e.g., 1 = male; 2 = female)\nrace (e.g., 1 = American Indian; 2 = Asian; 3 = Black; 4 = Pacific Islander; 5 = White)\nethnicity (e.g., 0 = Non-Hispanic/Latino; 1 = Hispanic/Latino)\nzip code\njersey number\n\nA football player’s jersey number is an example of a nominal variable. A jersey number of 7 is not higher on whatever concept of interest compared to a jersey number of 6.\nTo examine the central tendency of a nominal variable, you can determine the mode, but you cannot calculate a mean or median.\n\n7.6.2 Ordinal\nA variable is considered ordinal if the classifications are ordered. However, ordinal variables do not have equally spaced intervals. Examples of ordinal intervals include:\n\nlikert response scales (e.g., 1 = strongly disagree; 2 = disagree; 3 = neutral; 4 = agree; 5 = strongly agree)\neducational attainment (e.g., 1 = no formal education; 2 = elementary school; 3 = middle school; 4 = high school; 5 = college; 6 = graduate degree)\nacademic grades on A–F scale (e.g., 1 = A; 2 = B; 3 = C; 4 = D; 5 = F)\nplayer rank (1 = 1st; 2 = 2nd; 3 = 3rd, etc.)\n\nA football player’s fantasy rank is an example of an ordinal variable. A player with a fantasy rank of 1 has a higher rank than a player with a rank of 2, but it is not known how far apart each player is—i.e., the intervals do not all reflect the same distance. For instance, the distance between the top-ranked player and the 2nd-best player might be 30 points, whereas the distance between the 2nd-best player and the 3rd-best player might be 2 points.\nTo examine the central tendency of ordinal data, the median and mode are most appropriate; however, the mean may be used (unlike for nominal data).\n\n7.6.3 Interval\nA variable is considered interval if the classifications are ordered (similar to ordinal data) and have equally spaced intervals (unlike ordinal data). However, interval variables do not have a meaningful zero that reflects absence. Examples of interval data include:\n\ntemperature on the Fahrenheit or Celsius scale\ntime of day\n\nFor instance, the temperature difference between 80 and 90 degrees Fahrenheit is the same as the temperature difference between 90 and 100 degrees Fahrenheit. However, 0 degrees Fahrenheit does not reflect absence of temperature/heat.\nInterval data can be meaningfully added or subtracted. For instance, if a game starts at 4 pm and ends at 7 pm, you know the game lasted 3 hours (\\(7 - 4 = 3\\)). However, interval data cannot be meaningfully multiplied or divided. For instance, 100 degrees Fahrenheit is not twice as hot as 50 degrees Fahrenheit.\nTo examine the central tendency of interval data, you can compute the mean, median, or mode.\n\n7.6.4 Ratio\nA variable is considered ratio if the classifications are ordered (similar to ordinal data), have equally spaced intervals (like interval data), and have an absolute zero point that reflects absence of the concept. Examples of ratio data include:\n\ntemperature on the Kelvin scale\nheight\nweight\nage\ndistance\nspeed\nvolume\ntime elapsed\nincome\nstock price\nyears of formal education\npoints in football\n\nFor instance, points in football has order, equally spaced intervals, and an absolute zero—a team cannot score less than zero points, and zero points reflects absence of points (though it could be argued to be interval data because zero points does not reflect absence of skill.)\nRatio data can be meaningfully added, subtracted, multiplied, or divided. A player who weighs 350 pounds weighs twice as much as someone who weighs 175 pounds.\nTo examine the central tendency of ratio data, you can compute the mean, median, or mode.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-psychometrics",
    "href": "research-methods.html#sec-psychometrics",
    "title": "7  Research Methods",
    "section": "\n7.7 Psychometrics",
    "text": "7.7 Psychometrics\nBelow, I provide brief discussions of various aspects of measurement reliability and validity. For more information on these and other aspects of psychometrics, see Petersen (2024a) and Petersen (2024b).\n\n7.7.1 Measurement Reliability\nThe reliability of a measure’s scores deals with the consistency of measurement. This book focuses on the following types of reliability:\n\ntest–retest reliability\ninter-rater reliability\nintra-rater reliability\ninternal consistency\nparallel-forms reliability\n\nFor more information on these and other aspects of reliability, see https://isaactpetersen.github.io/Principles-Psychological-Assessment/reliability.html (Petersen, 2024a, 2024b).\n\n7.7.1.1 Test–Retest Reliability\nTest–retest reliability evaluates the consistency of scores across time. For a construct that is expected to be stable across time (e.g., hand size in adults), we would expect our measurements to be consistent across time. The consistency of scores across time can be examined in terms of relative or absolute test–retest reliability. Relative test–retest reliability—i.e., the consistency of individual differences across time—is commonly evaluated using the coefficient of stability (i.e., the Pearson correlation coefficent). Absolute test–retest reliability—i.e., the absolute consistency of people’s scores across time—is commonly evaluated using the coefficient of repeatability.\n\n7.7.1.2 Inter-Rater Reliability\nInter-rater reliability evaluates the consistency of scores across raters. For instance, if we have a strong measure for assessing college players’ aptitude to succeed in the NFL, the measure should yield a similar score for a given player regardless of which (trained) rater (e.g., coach or talent scout) uses it to rate the player. The consistency of scores across raters is commonly evaluated using the intraclass correlation coefficient (for continuous variables) and Cohen’s kappa (\\(\\kappa\\); for categorical variables).\n\n7.7.1.3 Intra-Rater Reliability\nIntra-rater reliability evaluates the consistency of scores within a given rater. If we have a strong measure for assessing college players’ aptitude to succeed in the NFL, the measure should yield a similar score for a given player from the same (trained) rater (e.g., coach or talent scout) each time they rate the same player (assuming the player’s aptitude has not changed). The consistency of scores within raters can be evaluated using similar approaches as those evaluating inter-rater reliability.\n\n7.7.1.4 Internal Consistency\nInternal consistency evaluates the consistency of scores across items within a measure. If we develop a strong questionnaire measure to assess a college players’ aptitude to succeed in the NFL, the scores should be relatively consistent across items. The consistency of scores across items within a measure is commonly evaluated using Cronbach’s alpha (\\(\\alpha\\)) or McDonald’s omega (\\(\\omega\\)).\n\n7.7.1.5 Parallel-Forms Reliability\nParallel-forms reliability evaluates the consistency of scores across different but equivalent forms of a measure. If we develop two equivalent versions of the Wonderlic Contemporary Cognitive Ability Test (Form A and Form B) so that players sitting next to each other do not receive the same items, we would expect a player’s score on Form A would be similar to their score on Form B. Parallel-forms reliability is is commonly evaluated using the coefficient of equivalence (i.e., the Pearson correlation coefficent).\n\n7.7.2 Measurement Validity\nThe validity of a measure’s scores deals with the accuracy of measurement. This book focuses on the following types of validity:\n\nface validity\ncontent validity\n\ncriterion-related validity\n\nconcurrent (criterion-related) validity\npredictive (criterion-related) validity\n\n\nconstruct validity\nconvergent validity\ndiscriminant validity\nincremental validity\necological validity\n\nFor more information on these and other aspects of validity, see https://isaactpetersen.github.io/Principles-Psychological-Assessment/validity.html (Petersen, 2024a, 2024b).\n\n7.7.2.1 Face Validity\nFace validity evaluates the extent to which a measure “looks like” (on its face) it assesses the construct of interest. For instance, if a measure is developed to assess aptitude of Wide Receivers for the position, it would be considered to have face validity if everyday (lay) people believe that it assesses aptitude for being a successful Wide Receiver.\n\n7.7.2.2 Content Validity\nContent validity evaluates the extent to which the measure assesses the full breadth of the content, as determined by context experts. For the measure to have content validity, it should not have gaps (missing content facets) or intrusions (facets of other constructs). For instance, a strong measure for assessing a player’s aptitude to succeed in the NFL might need to include a player’s speed, strength, size, lateral quickness, etc. If the measure is missing their speed, this would be a content gap. If the measure assesses a construct-irrelevant facet (e.g., their attractiveness), this would be a content intrusion.\n\n7.7.2.3 Criterion-Related Validity\nCriterion-related validity evaluates the extent to which the measure’s scores are related to meaningful variables of interest. Criterion-related validity is commonly evaluated using a Pearson correlation or some form of regression.\nThere are two types of criterion-related validity:\n\nconcurrent (criterion-related) validity\npredictive (criterion-related) validity\n\n\n7.7.2.3.1 Concurrent (Criterion-Related) Validity\nConcurrent criterion-related validity (aka concurrent validity) evaluates the extent to which the measure’s scores are related to meaningful variables of interest assessed at the same point in time. That is, concurrent validity could evaluate whether current player statistics (e.g., passing yards) are associated with their fantasy points.\n\n7.7.2.3.2 Predictive (Criterion-Related) Validity\nPredictive criterion-related validity (aks predictive validity) evaluates the extent to which the measure’s scores are related to meaningful variables of interest that are assessed at a later point in time. For example, predictive validity could evaluate whether scores on the measure we developed to assess a player’s aptitude to succeed in the NFL predicts later performance in the NFL.\n\n7.7.2.4 Construct Validity\nConstruct validity evaluates the extent to which the measure’s scores accurately assess the construct of interest. If we develop a measure with intent to assess aptitude for being a successful Running Back, and it appears to more accurately assess aptitude for being a successful Wide Receiver, then our measure has poor construct validity for assessing aptitude for being a successful Running Back. Construct validity subsumes convergent and discriminant validity, in addition to all of the other forms of measurement validity.\n\n7.7.2.5 Convergent Validity\nConvergent validity evaluates the extent to which the measure’s scores are related to other measures of the same construct. For instance, if we develop a new measure to assess intelligence, its scores should be related to scores from other measures designed to assess intelligence (e.g., Wonderlic Contemporary Cognitive Ability Test).\n\n7.7.2.6 Discriminant Validity\nDiscriminant validity evaluates the extent to which the measure’s scores are unrelated to measures of the different constructs. For instance, if we develop a new measure to assess intelligence, its scores should be less strongly associated with measures of other constructs (e.g., measures of happiness).\n\n7.7.2.7 Incremental Validity\nIncremental validity evalutes the extent to which the measure’s scores provide an increase in predictive accuracy compared to other information that is easily and cheaply available. That is, in order to be useful, a strong measure should tell us something that we did not already know. For instance, if we develop a strong measure of intelligence, it should result in increased predictive accuracy (for success in the NFL) compared to when just relying on the Wonderlic Contemporary Cognitive Ability Test.\n\n7.7.2.8 Ecological Validity\nEcological validity evaluates the extent to which the measures’ scores are indicative of the behavior of a person in the natural environment. For instance, measures of a players’ speed during a game has higher ecological validity (and is more predictive of their performance) than their speed during the NFL Combine (Lyons et al., 2011). For instance, compared to tests of speed, power, and agility at the NFL Combine, collegiate performance is a stronger predictor of performance in the NFL (Lyons et al., 2011). That is, previous sports performance is the best predictor of future performance (for a review, see Den Hartigh et al., 2018).\n\n7.7.3 Reliability vs Validity\nReliability and validity are different but related. Reliability refers to the consistency of scores, whereas accuracy refers to the accuracy of scores. Validity depends on reliability. Reliability is necessary—but insufficient for—validity. That is, consistency is necessary—but insufficient for—accuracy. As depicted in Figure 7.14, a measure can be no more valid than it is reliable. A measure can be consistent but inaccurate; however, a measure cannot be accurate but inconsistent.\n\n\n\n\n\nFigure 7.14: Reliability Versus Validity.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsConclusion",
    "href": "research-methods.html#sec-researchMethodsConclusion",
    "title": "7  Research Methods",
    "section": "\n7.8 Conclusion",
    "text": "7.8 Conclusion\nThere are various types of research designs. Each type of research design differs in the extent to which it supports the ability to draw causal inferences (internal validity) versus the extent to which it supports the ability to identify processes that generalize to the real-world (external validity). In addition, it is important to understand the distinction between sample and population, and the distinction between mediation and moderation. It is also important to consider the levels of measurement used because they constrain the types of analyses that may be performed. In addition, it is important to consider the psychometrics of measurements, including multiple aspects of reliability (consistency) and validity (accuracy).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "research-methods.html#sec-researchMethodsSessionInfo",
    "href": "research-methods.html#sec-researchMethodsSessionInfo",
    "title": "7  Research Methods",
    "section": "\n7.9 Session Info",
    "text": "7.9 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.1    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.1       htmltools_0.5.8.1 rmarkdown_2.27    knitr_1.48       \n [9] jsonlite_1.8.8    xfun_0.46         digest_0.6.36     rlang_1.1.4      \n[13] evaluate_0.24.0  \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J. (2011). On the predictive efficiency of past performance and physical ability: The case of the national football league. Human Performance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nPetersen, I. T. (2024a). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Research Methods</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html",
    "href": "basic-statistics.html",
    "title": "8  Basic Statistics",
    "section": "",
    "text": "8.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsGettingStarted",
    "href": "basic-statistics.html#sec-basicStatsGettingStarted",
    "title": "8  Basic Statistics",
    "section": "",
    "text": "8.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"DescTools\")\nlibrary(\"pwr\")\nlibrary(\"pwrss\")\nlibrary(\"WebPower\")\nlibrary(\"grid\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-descriptiveStatistics",
    "href": "basic-statistics.html#sec-descriptiveStatistics",
    "title": "8  Basic Statistics",
    "section": "\n8.2 Descriptive Statistics",
    "text": "8.2 Descriptive Statistics\nDescriptive statistics are used to describe data. For instance, they may be used to describe the center, spread, or shape of the data. There are various indices of each.\n\n8.2.1 Center\nIndices to describe the center (central tendency) of a variable’s data include:\n\nmean\nmedian\nHodges-Lehmann statistic (aka pseudomedian)\nmode\nweighted mean\nweighted median\n\nThe mean of \\(X\\) (written as: \\(\\bar{X}\\)) is calculated as in Equation 8.5:\n\\[\n\\bar{X} = \\frac{\\sum X_i}{n} = \\frac{X_1 + X_2 + ... + X_n}{n}\n\\tag{8.1}\\]\n\nCodeexampleValues &lt;- c(0, 0, 10, 15, 20, 30, 1000)\nexampleValues_mean &lt;- apa(mean(exampleValues), 2)\n\n\nThat is, to compute the mean, sum all of the values and divide by the number of values (\\(n\\)). One issue with the mean is that it is sensitive to extreme (outlying) values. For instance, the mean of the values of 0, 0, 10, 15, 20, 30, and 1000 is 153.57.\n\nCodeexampleValues_median &lt;- median(exampleValues)\n\n\nThe median is determined as the value at the 50th percentile (i.e., the value that is higher than 50% of the values and is lower than the other 50% of values). Compared to the mean, the median is less influenced by outliers. The median of the values of 0, 0, 10, 15, 20, 30, and 1000 is 15.\n\nCodeexampleValues_pseudomedian &lt;- DescTools::HodgesLehmann(exampleValues)\n\n\nThe Hodges-Lehmann statistic (aka pseudomedian) is computed as the median of all pairwise means, and it is also robust to outliers. The pseudomedian of the values of 0, 0, 10, 15, 20, 30, and 1000 is 15.\n\nCodeexampleValues_mode &lt;- petersenlab::Mode(exampleValues)\n\n\nThe mode is the most common/frequent value. The mode of the values of 0, 0, 10, 15, 20, 30, and 1000 is 0. The petersenlab package (Petersen, 2024a) contains the Mode() function for computing the mode of a set of data.\nIf you want to give some values more weight to others, you can calculate a weighted mean and a weighted median (or other quantile), while assigning a weight to each value.\nBelow is R code to estimate each:\n\nCode#mean(data, na.rm = TRUE)\n#median(data, na.rm = TRUE)\n#DescTools::HodgesLehmann(exampleValues, na.rm = TRUE)\n#petersenlab::Mode(exampleValues)\n#weighted.mean(data, weights, na.rm = TRUE)\n#DescTools::Quantile(data, weights, na.rm = TRUE)\n\n\n\n8.2.2 Spread\nIndices to describe the spread (variability) of a variable’s data include:\n\nstandard deviation\nvariance\nrange\nminimum and maximum\ninterquartile range (IQR)\nmedian absolute deviation\n\nThe (sample) variance of \\(X\\) (written as: \\(s^2\\)) is calculated as in Equation 8.2:\n\\[\ns^2 = \\frac{\\sum (X_i - \\bar{X})^2}{n-1}\n\\tag{8.2}\\]\nwhere \\(X_i\\) is each data point, \\(\\bar{X}\\) is the mean of \\(X\\), and \\(n\\) is the number of data points.\nThe (sample) standard deviation of \\(X\\) (written as: \\(s\\)) is calculated as in Equation 8.3:\n\\[\ns = \\sqrt{\\frac{\\sum (X_i - \\bar{X})^2}{n-1}}\n\\tag{8.3}\\]\nThe range is calculated of \\(X\\) is calculated as in Equation 8.4:\n\\[\n\\text{range} = \\text{maximum} - \\text{minimum}\n\\tag{8.4}\\]\nThe interquartile range (IQR) is calculated as in Equation 8.5:\n\\[\n\\text{IQR} = Q_3 - Q_1\n\\tag{8.5}\\]\nwhere \\(Q_3\\) is the score at the third quartile (i.e., 75th percentile), and \\(Q_1\\) is the score at the first quartile (i.e., 25th percentile).\nThe median absolute deviation (MAD) is the median of all deviations from the median, and is calculated as in Equation 8.6:\n\\[\n\\text{MAD} = \\text{median}(|X_i - \\tilde{X}|)\n\\tag{8.6}\\]\nwhere \\(\\tilde{X}\\) is the median of X. Compared to the standard deviation, the median absolute deviation is more robust to outliers.\nBelow is R code to estimate each:\n\n8.2.3 Shape\nIndices to describe the shape of a variable’s data include:\n\nskewness\nkurtosis\n\nBelow is R code to estimate each:\n\n8.2.4 Combination\nTo estimate multiple indices of center, spread, and shape of the data, you can use the following code:\n\nCode#psych::describe(mydata)\n\n#mydata %&gt;% \n#  summarise(across(\n#      everything(),\n#      .fns = list(\n#        n = ~ length(na.omit(.)),\n#        missingness = ~ mean(is.na(.)) * 100,\n#        M = ~ mean(., na.rm = TRUE),\n#        SD = ~ sd(., na.rm = TRUE),\n#        min = ~ min(., na.rm = TRUE),\n#        max = ~ max(., na.rm = TRUE),\n#        range = ~ max(., na.rm = TRUE) - min(., na.rm = TRUE),\n#        IQR = ~ IQR(., na.rm = TRUE),\n#        MAD = ~ mad(., na.rm = TRUE),\n#        median = ~ median(., na.rm = TRUE),\n#        pseudomedian = ~ DescTools::HodgesLehmann(., na.rm = TRUE),\n#        mode = ~ petersenlab::Mode(., multipleModes = \"mean\"),\n#        skewness = ~ psych::skew(., na.rm = TRUE),\n#        kurtosis = ~ psych::kurtosi(., na.rm = TRUE)),\n#      .names = \"{.col}.{.fn}\")) %&gt;%\n#    pivot_longer(\n#      cols = everything(),\n#      names_to = c(\"variable\",\"index\"),\n#      names_sep = \"\\\\.\") %&gt;% \n#    pivot_wider(\n#      names_from = index,\n#      values_from = value)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-scoresAndScales",
    "href": "basic-statistics.html#sec-scoresAndScales",
    "title": "8  Basic Statistics",
    "section": "\n8.3 Scores and Scales",
    "text": "8.3 Scores and Scales\nThere are many different types of scores and scales. This book focuses on raw scores and z-scores. For information on other scores and scales, including percentile ranks, T-scores, standard scores, scaled scores, and stanine scores, see here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/scoresScales.html#scoreTransformation (Petersen, 2024b).\n\n8.3.1 Raw Scores\nRaw scores are the original data on the original metric. Thus, raw scores are considered unstandardized. For example, raw scores that represent the players’ age may range from 20 to 40. Raw scores depend on the construct and unit; thus raw scores may not be comparable across variables.\n\n8.3.2 z Scores\nz scores have a mean of zero and a standard deviation of one. z scores are frequently used to render scores across variables more comparable. Thus, z scores are considered a form of a standardized score.\nz scores are calculated using Equation 8.7:\n\\[\nz = \\frac{X - \\bar{X}}{\\sigma}\n\\tag{8.7}\\]\nwhere \\(X\\) is the observed score, \\(\\bar{X}\\) is the mean observed score, and \\(\\sigma\\) is the standard deviation of the observed scores.\nYou can easily convert a variable to a z score using the scale() function:\n\nCodescale(variable)\n\n\nWith a standard normal curve, 68% of scores fall within one standard deviation of the mean. 95% of scores fall within two standard deviations of the mean. 99.7% of scores fall within three standard deviations of the mean.\nThe area under a normal curve within one standard deviation of the mean is calculated below using the pnorm() function, which calculates the cumulative density function for a normal curve.\n\nCodestdDeviations &lt;- 1\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.6826895\n\n\nThe area under a normal curve within one standard deviation of the mean is depicted in Figure 8.1.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 8.1: Density of Standard Normal Distribution. The blue region represents the area within one standard deviation of the mean.\n\n\n\n\nThe area under a normal curve within two standard deviations of the mean is calculated below:\n\nCodestdDeviations &lt;- 2\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.9544997\n\n\nThe area under a normal curve within two standard deviations of the mean is depicted in Figure 8.2.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 8.2: Density of Standard Normal Distribution. The blue region represents the area within two standard deviations of the mean.\n\n\n\n\nThe area under a normal curve within three standard deviations of the mean is calculated below:\n\nCodestdDeviations &lt;- 3\n\npnorm(stdDeviations) - pnorm(stdDeviations * -1)\n\n[1] 0.9973002\n\n\nThe area under a normal curve within three standard deviations of the mean is depicted in Figure 8.3.\n\nCodex &lt;- seq(-4, 4, length = 200)\ny &lt;- dnorm(x, mean = 0, sd = 1)\nplot(x, y, type = \"l\",\n     xlab = \"z Score\",\n     ylab = \"Normal Density\")\n\nx &lt;- seq(stdDeviations * -1, stdDeviations, length = 100)\ny &lt;- dnorm(x, mean = 0, sd = 1)\npolygon(c(stdDeviations * -1, x, stdDeviations),\n        c(0, y, 0),\n        col = \"blue\")\n\n\n\n\n\n\nFigure 8.3: Density of Standard Normal Distribution. The blue region represents the area within three standard deviations of the mean.\n\n\n\n\nIf you want to determine the z score associated with a particular percentile in a normal distribution, you can use the qnorm() function. For instance, the z score associated with the 37th percentile is:\n\nCodeqnorm(.37)\n\n[1] -0.3318533",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-inferentialStatistics",
    "href": "basic-statistics.html#sec-inferentialStatistics",
    "title": "8  Basic Statistics",
    "section": "\n8.4 Inferential Statistics",
    "text": "8.4 Inferential Statistics\nInferential statistics are used to draw inferences regarding whether there is (a) a difference in level on variable across groups or (b) an association between variables. For instance, inferential statistics may be used to evaluate whether Quarterbacks tend to have longer careers compared to Running Backs. Or, they could be used to evaluate whether number of carries is associated with injury likelihood. To apply inferential statistics, we make use of the null hypothesis (\\(H_0\\)) and the alternative hypothesis (\\(H_1\\)).\n\n8.4.1 Null Hypothesis Significance Testing\nTo draw statistical inferences, the frequentist statistics paradigm leverages null hypothesis significance testing. Frequentist statistics is the most widely used statistical paradigm. However, frequentist statistics is not the only statistical paradigm. Other statistical paradigms exist, including Bayesian statistics, which is based on Bayes’ theorem. This chapter focuses on the frequentist approach to hypothesis testing, known as null hypothesis significance testing. We discuss Bayesian statistics in Chapter 14.\n\n8.4.1.1 Null Hypothesis (\\(H_0\\))\nWhen testing whether there are differences in level across groups on a variable of interest, the null hypothesis (\\(H_0\\)) is that there is no difference in level across groups. For instance, when testing whether Quarterbacks tend to have longer careers compared to Running Backs, the null hypothesis (\\(H_0\\)) is that Quarterbacks do not systematically differ from Running Backs in the length of their career.\nWhen testing whether there is an association between variables, the null hypothesis (\\(H_0\\)) is that there is no association between the variables. For instance, when testing whether number of carries is associated with injury likelihood, the null hypothesis (\\(H_0\\)) is that there is no association between number of carries and injury likelihood.\n\n8.4.1.2 Alternative Hypothesis (\\(H_1\\))\nThe alternative hypothesis (\\(H_1\\)) is the researcher’s hypothesis that they want to evaluate. An alternative hypothesis (\\(H_1\\)) might be directional (i.e., one-sided) or non-directional (i.e., two-sided).\nDirectional hypotheses specify a particular direction, such as which group will have larger scores or which direction (positive or negative) two variables will be associated. Examples of directional hypotheses include:\n\nQuarterbacks have longer careers compared to Running Backs\nNumber of carries is positively associated with injury likelihood\n\nNon-directional hypotheses do not specify a particular direction. For instance, non-directional hypotheses may state that two groups differ but do not specify which group will have larger scores. Or, non-directional hypotheses may state that two variables are associated but do not state what the sign is of the association—i.e., positive or negative. Examples of non-directional hypotheses include:\n\nQuarterbacks differ in the length of their careers compared to Running Backs\nNumber of carries is associated with injury likelihood\n\n8.4.1.3 Statistical Significance\nIn science, statistical significance is evaluated with the p-value. The p-value does not represent the probability that you observed the result by chance. The p-value represents a conditional probability—it examines the probability of one event given another event. In particular, the p-value evaluates the likelihood that you would detect a result as at least as extreme as the one observed (in terms of the magnitude of the difference or of the association) given that the null hypothesis (\\(H_0\\)) is true.\nThis can be expressed in conditional probability notation, \\(P(A | B)\\), which is the probability (likelihood) of event A occurring given that event B occurred (or given condition B).\nThe conditional probability notation for a left-tailed directional test (i.e., Quarterbacks have shorter careers than Running Backs; or number of carries is negatively associated with injury likelihood) is in Equation 8.8.\n\\[\np\\text{-value} = P(T \\le t | H_0)\n\\tag{8.8}\\]\nwhere \\(T\\) is the test statistic of interest (e.g., the distribution of \\(t\\)-, \\(r-\\), or \\(F\\) values, depending on the test) and \\(t\\) is the observed test statistic (e.g., \\(t\\)-, \\(r-\\), or \\(F\\)-coefficient, depending on the test).\nThe conditional probability notation for a right-tailed directional test (i.e., Quarterbacks have longer careers than Running Backs; or number of carries is positively associated with injury likelihood) is in Equation 8.9.\n\\[\np\\text{-value} = P(T \\ge t | H_0)\n\\tag{8.9}\\]\nThe conditional probability notation for a two-tailed non-directional test (i.e., Quarterbacks differ in the length of their careers compared to Running Backs; or number of carries is associated with injury likelihood) is in Equation 8.10.\n\\[\np\\text{-value} = 2 \\times \\text{min}(P(T \\le t | H_0), P(T \\ge t | H_0))\n\\tag{8.10}\\]\nwhere min(a, b) is the smaller number of a and b.\nIf the distribution of the test statistic is symmetric around zero, the p-value for the two-tailed non-directional test simplifies to Equation 8.11.\n\\[\np\\text{-value} = 2 \\times P(T \\ge |t| | H_0)\n\\tag{8.11}\\]\nNevertheless, to be conservative (i.e., to avoid false positive/Type I errors), many researchers use two-tailed p-values regardless whether their hypothesis is one- or two-tailed.\nFor a test of group differences, the p-value evaluates the likelihood that you would observe a difference as large or larger than the one you observed between the groups if there were no systematic difference between the groups, as depicted in Figure 8.4. For instance, when evaluating whether Quarterbacks have longer careers than Running Backs, and you observed a mean difference of 0.03 years, the p-value evaluates the likelihood that you would observe a difference as larger or larger than 0.03 years between the groups if Quarterbacks do not differ from Running Backs in terms of the length of their career.\nCodeset.seed(52242)\n\nnObserved &lt;- 1000\nnPopulation &lt;- 1000000\n\nobservedGroups &lt;- data.frame(\n  score = c(rnorm(nObserved, mean = 47, sd = 3), rnorm(nObserved, mean = 52, sd = 3)),\n  group = as.factor(c(rep(\"Group 1\", nObserved), rep(\"Group 2\", nObserved)))\n)\n\npopulationGroups &lt;- data.frame(\n  score = c(rnorm(nPopulation, mean = 50, sd = 3.03), rnorm(nPopulation, mean = 50, sd = 3)),\n  group = as.factor(c(rep(\"Group 1\", nPopulation), rep(\"Group 2\", nPopulation)))\n)\n\nggplot2::ggplot(\n  data = observedGroups,\n  mapping = aes(\n    x = score,\n    fill = group,\n    color = group\n  )\n) +\n  geom_density(alpha = 0.5) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_fill_manual(values = c(\"red\",\"blue\")) +\n  geom_vline(xintercept = mean(observedGroups$score[which(observedGroups$group == \"Group 1\")])) +\n  geom_vline(xintercept = mean(observedGroups$score[which(observedGroups$group == \"Group 2\")])) +\n  ggplot2::labs(\n    x = \"Score\",\n    y = \"Frequency\",\n    title = \"What is the probability my data would look like this...\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    #plot.title.position = \"plot\"\n    legend.position = \"inside\",\n    legend.margin = margin(0, 0, 0, 0),\n    legend.justification.top = \"left\",\n    legend.justification.left = \"top\",\n    legend.justification.bottom = \"right\",\n    legend.justification.inside = c(1, 1),\n    legend.location = \"plot\")\n\nggplot2::ggplot(\n  data = populationGroups,\n  mapping = aes(\n    x = score,\n    fill = group,\n    color = group\n  )\n) +\n  geom_density(alpha = 0.5) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_fill_manual(values = c(\"red\",\"blue\")) +\n  geom_vline(xintercept = mean(populationGroups$score[which(populationGroups$group == \"Group 1\")])) +\n  geom_vline(xintercept = mean(populationGroups$score[which(populationGroups$group == \"Group 2\")])) +\n  ggplot2::labs(\n    x = \"Score\",\n    y = \"Frequency\",\n    title = \"...if in the population, the groups were really this:\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    #plot.title.position = \"plot\",\n    legend.position = \"inside\",\n    legend.margin = margin(0, 0, 0, 0),\n    legend.justification.top = \"left\",\n    legend.justification.left = \"top\",\n    legend.justification.bottom = \"right\",\n    legend.justification.inside = c(1, 1),\n    legend.location = \"plot\")\n\n\n\n\n\n\n\n\n\n(a) What is the probability my data would look like this…\n\n\n\n\n\n\n\n\n\n(b) …if in the population, the groups were really this?\n\n\n\n\n\n\nFigure 8.4: Interpretation of p-Values When Examining The Differences Between Groups. The vertical black lines reflect the group means.\n\n\nFor a test of whether two variables are associated, the p-value evaluates the likelihood that you would observe an association as strong or stronger than the one you observed between the groups if there were no association between the variables, as depicted in Figure 8.5. For instance, when evaluating whether number of carries is positively associated with injury likelihood, and you observed a correlation coefficient of \\(r = .25\\) between number of carries and injury likelihood, the p-value evaluates the likelihood that you would observe a correlation as strong or stronger than \\(r = .25\\) between the variables if number of carries is not associated with injury likelihood.\nCodeset.seed(52242)\n\nobservedCorrelation &lt;- 0.9\n\ncorrelations &lt;- data.frame(criterion = rnorm(2000))\ncorrelations$sample &lt;- NA\ncorrelations$sample[1:100] &lt;- complement(correlations$criterion[1:100], observedCorrelation)\ncorrelations$population &lt;- complement(correlations$criterion, 0)\n\nggplot2::ggplot(\n  data = correlations,\n  mapping = aes(\n    x = sample,\n    y = criterion\n  )\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(\n    limits = c(-3.5,3)\n  ) +\n  annotate(\n    x = 0,\n    y = 4,\n    label = paste(\"italic(r) != \", 0, sep = \"\"),\n    parse = TRUE,\n    geom = \"text\",\n    size = 7) + \n  ggplot2::labs(\n    x = \"Predictor Variable\",\n    y = \"Outcome Variable\",\n    title = \"What is the probability my data would look like this...\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\nggplot2::ggplot(\n  data = correlations,\n  mapping = aes(\n    x = population,\n    y = criterion\n  )\n) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  scale_x_continuous(\n    limits = c(-2.5,2.5)\n  ) +\n  annotate(\n    x = 0,\n    y = 4,\n    label = paste(\"italic(r) == '\", \"0.00\", \"'\", sep = \"\"),\n    parse = TRUE,\n    geom = \"text\",\n    size = 7) + \n  ggplot2::labs(\n    x = \"Predictor Variable\",\n    y = \"Outcome Variable\",\n    title = \"...if in the population, the association was really this:\"\n  ) +\n  ggplot2::theme_classic(\n    base_size = 16) +\n  ggplot2::theme(\n    legend.title = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\n(a) What is the probability my data would look like this…\n\n\n\n\n\n\n\n\n\n(b) …if in the population, the association was really this?\n\n\n\n\n\n\nFigure 8.5: Interpretation of p-Values When Examining The Association Between Variables.\n\n\nUsing what is called null-hypothesis significance testing (NHST), we consider an effect to be statistically significant if the p-value is less than some threshold, called the alpha level. In science, we typically want to be conservative because a false positive (i.e., Type I error) is considered more problematic than a false negative (i.e., Type II error). That is, we would rather say an effect does not exist when it really does than to say an effect does exist when it really does not. Thus, we typically set the alpha level to a low value, commonly .05. Then, we would consider an effect to be statistically significant if the p-value is less than .05. That is, there is a small chance (5%; or 1 in 20 times) that we would observe an effect at least as extreme as the effect observed, if the null hypothesis were true. So, you might expect around 5% of tests where the null hypothesis is true to be statistically significant just by chance. We could lower the rate of Type II (i.e., false negative) errors—i.e., we could detect more effects—if we set the alpha level to a higher value (e.g., .10); however, raising the alpha level would raise the possibility of Type I (false positive) errors.\nIf the p-value is less than .05, we reject the null hypothesis (\\(H_0\\)) that there was no difference or association. Thus, we conclude that there was a statistically significant (non-zero) difference or association. If the p-value is greater than .05, we fail to reject the null hypothesis; the difference/association was not statistically significant. Thus, we do not have confidence that there was a difference or association. However, we do not accept the null hypothesis; it could be there we did not observe an effect because we did not have adequate power to detect the effect—e.g., if the effect size was small, the data were noisy, and the sample size was small and/or unrepresentative.\nThere are four general possibilities of decision making outcomes when performing null-hypothesis significance testing:\n\nWe (correctly) reject the null hypothesis when it is in fact false (\\(1 - \\beta\\)). This is a true positive. For instance, we may correctly determine that Quarterbacks have longer careers than Running Backs.\nWe (correctly) fail to reject the null hypothesis when it is in fact true (\\(1 - \\alpha\\)). This is a true negative. For instance, we may correctly determine that Quarterbacks do not have longer careers than Running Backs.\nWe (incorrectly) reject the null hypothesis when it is in fact true (\\(\\alpha\\)). This is a false positive. When performing null hypothesis testing, a false positive is known as a Type I error. For instance, we may incorrectly determine that Quarterbacks have longer careers than Running Backs when, in fact, Quarterbacks and Running Backs do not differ in their career length.\nWe (incorrectly) fail to reject the null hypothesis when it is in fact false (\\(\\beta\\)). This is a false negative. When performing null hypothesis testing, a false negative is known as a Type II error. For instance, we may incorrectly determine that Quarterbacks and Running Backs do not differ in their career length when, in fact, Quarterbacks have longer careers than Running Backs.\n\nA two-by-two confusion matrix for null-hypothesis significance testing is in Figure 8.6.\n\n\n\n\n\nFigure 8.6: A Two-by-Two Confusion Matrix for Null-Hypothesis Significance Testing.\n\n\nIn statistics, power is the probability of detecting an effect, if, in fact, the effect exists. Otherwise said, power is the probability of rejecting the null hypothesis, if, in fact, the null hypothesis is false. Power is influenced by several variables:\n\nthe sample size (N): the larger the N, the greater the power\n\nfor group comparisons, the power depends on the sample size of each group\n\n\nthe effect size: the larger the effect, the greater the power\n\nfor group comparisons, larger effect sizes reflect:\n\nlarger between-group variance, and\nsmaller within-group variance (i.e., strong measurement precision, i.e., reliability)\n\n\n\n\nthe alpha level: the researcher specifies the alpha level (though it is typically set at .05); the higher the alpha level, the greater the power; however, the higher we set the alpha level, the higher the likelihood of Type I errors (false positives)\none- versus two-tailed tests: one-tailed tests have higher power than two-tailed tests\n\nwithin-subject versus between-subject comparisons: within-subject designs tend to have greater power than between-subject designs\n\n\nA plot of statistical power is in Figure 8.7.\n\nCodem1 &lt;- 0  # mu H0\nsd1 &lt;- 1.5 # sigma H0\nm2 &lt;- 3.5 # mu HA\nsd2 &lt;- 1.5 # sigma HA\n \nz_crit &lt;- qnorm(1-(0.05/2), m1, sd1)\n \n# set length of tails\nmin1 &lt;- m1-sd1*4\nmax1 &lt;- m1+sd1*4\nmin2 &lt;- m2-sd2*4\nmax2 &lt;- m2+sd2*4          \n# create x sequence\nx &lt;- seq(min(min1,min2), max(max1, max2), .01)\n# generate normal dist #1\ny1 &lt;- dnorm(x, m1, sd1)\n# put in data frame\ndf1 &lt;- data.frame(\"x\" = x, \"y\" = y1)\n# generate normal dist #2\ny2 &lt;- dnorm(x, m2, sd2)\n# put in data frame\ndf2 &lt;- data.frame(\"x\" = x, \"y\" = y2)\n \n# Alpha polygon\ny.poly &lt;- pmin(y1,y2)\npoly1 &lt;- data.frame(x=x, y=y.poly)\npoly1 &lt;- poly1[poly1$x &gt;= z_crit, ] \npoly1&lt;-rbind(poly1, c(z_crit, 0))  # add lower-left corner\n \n# Beta polygon\npoly2 &lt;- df2\npoly2 &lt;- poly2[poly2$x &lt;= z_crit,] \npoly2&lt;-rbind(poly2, c(z_crit, 0))  # add lower-left corner\n \n# power polygon; 1-beta\npoly3 &lt;- df2\npoly3 &lt;- poly3[poly3$x &gt;= z_crit,] \npoly3 &lt;-rbind(poly3, c(z_crit, 0))  # add lower-left corner\n \n# combine polygons. \npoly1$id &lt;- 3 # alpha, give it the highest number to make it the top layer\npoly2$id &lt;- 2 # beta\npoly3$id &lt;- 1 # power; 1 - beta\npoly &lt;- rbind(poly1, poly2, poly3)\npoly$id &lt;- factor(poly$id,  labels=c(\"power\",\"beta\",\"alpha\"))\n\n# plot with ggplot2\nggplot(poly, aes(x,y, fill=id, group=id)) +\n  geom_polygon(show.legend=F, alpha=I(8/10)) +\n  # add line for treatment group\n  geom_line(data=df1, aes(x,y, color=\"H0\", group=NULL, fill=NULL), linewidth=1.5, show_guide=F) + \n  # add line for treatment group. These lines could be combined into one dataframe.\n  geom_line(data=df2, aes(color=\"HA\", group=NULL, fill=NULL),linewidth=1.5, show_guide=F) +\n  # add vlines for z_crit\n  geom_vline(xintercept = z_crit, linewidth=1, linetype=\"dashed\") +\n  # change colors \n  scale_color_manual(\"Group\", \n                     values= c(\"HA\" = \"#981e0b\",\"H0\" = \"black\")) +\n  scale_fill_manual(\"test\", values= c(\"alpha\" = \"#0d6374\",\"beta\" = \"#be805e\",\"power\"=\"#7cecee\")) +\n  # beta arrow\n  annotate(\"segment\", x=0.1, y=0.045, xend=1.3, yend=0.01, arrow = arrow(length = unit(0.3, \"cm\")), linewidth=1) +\n  annotate(\"text\", label=\"beta\", x=0, y=0.05, parse=T, size=8) +\n  # alpha arrow\n  annotate(\"segment\", x=4, y=0.043, xend=3.4, yend=0.01, arrow = arrow(length = unit(0.3, \"cm\")), linewidth=1) +\n  annotate(\"text\", label=\"frac(alpha,2)\", x=4.2, y=0.05, parse=T, size=8) +\n  # power arrow\n  annotate(\"segment\", x=6, y=0.2, xend=4.5, yend=0.15, arrow = arrow(length = unit(0.3, \"cm\")), linewidth=1) +\n  annotate(\"text\", label=expression(paste(1-beta, \"  (\\\"power\\\")\")), x=6.1, y=0.21, parse=T, size=8) +\n  # H_0 title\n  annotate(\"text\", label=\"H[0]\", x=m1, y=0.28, parse=T, size=8) +\n  # H_a title\n  annotate(\"text\", label=\"H[1]\", x=m2, y=0.28, parse=T, size=8) +\n  ggtitle(\"Statistical Power\") +\n  # remove some elements\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.background = element_blank(),\n    plot.background = element_rect(fill=\"white\"),\n    panel.border = element_blank(),\n    axis.line = element_blank(),\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    plot.title = element_text(size=22))\n\n\n\n\n\n\nFigure 8.7: Statistical Power (Adapted from Kristoffer Magnusson: https://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics; archived at https://perma.cc/FG3J-85L6). The dashed line represents the critical value or threshold.\n\n\n\n\nInteractive visualizations by Kristoffer Magnusson on p-values and null-hypothesis significance testing are below:\n\n\nhttps://rpsychologist.com/pvalue/ (archived at https://perma.cc/JP9F-9ZVY)\n\nhttps://rpsychologist.com/d3/pdist/ (archived at https://perma.cc/BE96-8LSJ)\n\nhttps://rpsychologist.com/d3/nhst/ (archived at https://perma.cc/ZU9A-37F3)\n\nTwelve misconceptions about p-values (Goodman, 2008) are in Table 8.1.\n\n\nTable 8.1: Twelve Misconceptions About p-Values from Goodman (2008). Goodman also provides a discussion about why each statement is false.\n\n\n\n\n\n\n\nNumber\nMisconception\n\n\n\n1\nIf \\(p = .05\\), the null hypothesis has only a 5% chance of being true.\n\n\n2\nA nonsignificant difference (eg, \\(p &gt; .05\\)) means there is no difference between groups.\n\n\n3\nA statistically significant finding is clinically important.\n\n\n4\nStudies with \\(p\\)-values on opposite sides of .05 are conflicting.\n\n\n5\nStudies with the same \\(p\\)-value provide the same evidence against the null hypothesis.\n\n\n6\n\n\\(p = .05\\) means that we have observed data that would occur only 5% of the time under the null hypothesis.\n\n\n7\n\n\\(p = .05\\) and \\(p &lt; .05\\) mean the same thing.\n\n\n8\n\n\\(p\\)-values are properly written as inequalities (e.g., “\\(p \\le .05\\)” when \\(p = .015\\)).\n\n\n9\n\n\\(p = .05\\) means that if you reject the null hypothesis, the probability of a Type I error is only 5%.\n\n\n10\nWith a \\(p = .05\\) threshold for significance, the chance of a Type I error will be 5%.\n\n\n11\nYou should use a one-sided \\(p\\)-value when you don’t care about a result in one direction, or a difference in that direction is impossible.\n\n\n12\nA scientific conclusion or treatment policy should be based on whether or not the \\(p\\)-value is significant.\n\n\n\n\n\n\nThat is, the p-value is not:\n\nthe probability that the effect was due to chance\nthe probability that the null hypothesis is true\nthe size of the effect\nthe importance of the effect\nwhether the effect is true, real, or causal\n\nStatistical significance involves the consistency of an effect/association/difference; it suggests that the association/difference is reliably non-zero. However, just because something is statistically significant does not mean that it is important. For instance, consider that we discover that players who consume sports drink before a game tend to perform better than players who do not (\\(p &lt; .05\\)). However, what if consumption of sports drinks is associated with an average improvement of 0.002 points per game. A small effect such as this might be detectable with a large sample size. This effect would be considered to be reliable/consistent because it is statistically significant. However, such an effect is so small that it results in differences that are not practically important. Thus, in addition to statistical significance, it is also important to consider practical significance.\n\n8.4.2 Practical Significance\nPractical significance deals with how large or important the effect/association/difference is. It is based on the magnitude of the effect, called the effect size. Effect size can be quantified in various ways including:\n\nCohen’s \\(d\\)\n\nStandardized regression coefficient (beta; \\(\\beta\\))\nCorrelation coefficient (\\(r\\))\nCohen’s \\(\\omega\\) (omega)\nCohen’s \\(f\\)\n\nCohen’s \\(f^2\\)\n\nCoefficient of determination (\\(R^2\\))\nEta squared (\\(\\eta^2\\))\nPartial eta squared (\\(\\eta_p^2\\))\n\n\n8.4.2.1 Cohen’s \\(d\\)\n\nCohen’s \\(d\\) is calculated as in Equation 8.12:\n\\[\n\\begin{aligned}\n  d &= \\frac{\\text{mean difference}}{\\text{pooled standard deviation}} \\\\\n   &= \\frac{\\bar{X_1} - \\bar{X_2}}{s} \\\\\n\\end{aligned}\n\\tag{8.12}\\]\nwhere:\n\\[\ns = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\tag{8.13}\\]\nwhere \\(n_1\\) and \\(n_2\\) is the sample size of group 1 and group 2, respectively, and \\(s_1\\) and \\(s_2\\) is the standard deviation of group 1 and group 2, respectively.\n\n8.4.2.2 Standardized Regression Coefficient (Beta; \\(\\beta\\))\nThe standardized regression coefficient (beta; \\(\\beta\\)) is used in multiple regression, and is calculated as in Equation 8.14:\n\\[\n\\beta_x = B_x \\times \\frac{s_x}{s_y}\n\\tag{8.14}\\]\nwhere \\(B_x\\) is the unstandardized regression coefficient of the predictor variable \\(x\\) in predicting the outcome variable \\(y\\), \\(s_x\\) is the standard deviation of \\(x\\), and \\(s_y\\) is the standard deviation of \\(y\\).\n\n8.4.2.3 Correlation Coefficient (\\(r\\))\nThe formula for the correlation coefficient is in Chapter 9.\n\n8.4.2.4 Cohen’s \\(\\omega\\)\n\nCohen’s \\(\\omega\\) is used in chi-square tests, and is calculated as in Equation 8.15:\n\\[\n\\omega = \\sqrt{\\frac{\\chi^2}{N} - \\frac{df}{N}}\n\\tag{8.15}\\]\nwhere \\(\\chi^2\\) is the chi-square statistic from the test, \\(N\\) is the sample size, and \\(df\\) is the degrees of freedom.\n\n8.4.2.5 Cohen’s \\(f\\)\n\nCohen’s \\(f\\) is commonly used in ANOVA, and is calculated as in Equation 8.16:\n\\[\n\\begin{aligned}\n  f &= \\sqrt{\\frac{R^2}{1 - R^2}} \\\\\n    &= \\sqrt{\\frac{\\eta^2}{1 - \\eta^2}}\n\\end{aligned}\n\\tag{8.16}\\]\n\n8.4.2.6 Cohen’s \\(f^2\\)\n\nCohen’s \\(f^2\\) is commonly used in regression, and is calculated as in Equation 8.17:\n\\[\n\\begin{aligned}\n  f^2 &= \\frac{R^2}{1 - R^2} \\\\\n      &= \\frac{\\eta^2}{1 - \\eta^2}\n\\end{aligned}\n\\tag{8.17}\\]\nTo calculate the effect size of a particular predictor, you can calculate \\(\\Delta f^2\\) as in Equation 8.18:\n\\[\n\\begin{aligned}\n  \\Delta f^2 &= \\frac{R^2_{\\text{model}} - R^2_{\\text{reduced}}}{1 - R^2_{\\text{model}}} \\\\\n             &= \\frac{\\eta^2_{\\text{model}} - \\eta^2_{\\text{reduced}}}{1 - \\eta^2_{\\text{model}}}\n\\end{aligned}\n\\tag{8.18}\\]\nwhere \\(R^2_{\\text{model}}\\) is the \\(R^2\\) of the model with the predictor variable of interest and \\(R^2_{\\text{reduced}}\\) is the \\(R^2\\) of the model without the predictor variable of interest.\n\n8.4.2.7 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome variable that is explained by the predictor variable(s). \\(R^2\\) is commonly used in regression, and is calculated as in Equation 8.19:\n\\[\n\\begin{aligned}\n  R^2 &= 1 - \\frac{\\sum (Y_i - \\hat{Y}_i)^2}{\\sum (Y_i - \\bar{Y})^2} \\\\\n      &= 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{\\text{sum of squared residuals}}{\\text{total sum of squares}} \\\\\n      &= \\frac{f^2}{1 + f^2} \\\\\n      &= \\eta^2 \\\\\n      &= \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\n\\end{aligned}\n\\tag{8.19}\\]\nwhere \\(Y_i\\) is the observed value of the outcome variable for the \\(i\\)th observation, \\(\\hat{Y}_i\\) is the model predicted value for the \\(i\\)th observation, \\(\\bar{Y}\\) is the mean of the observed values of the outcome variable. The total sum of squares is an index of the total variation in the outcome variable.\n\n8.4.2.8 Eta Squared (\\(\\eta^2\\)) and Partial Eta Squared (\\(\\eta_p^2\\))\nLike \\(R^2\\), eta squared (\\(\\eta^2\\)) reflects the proportion of variance in the dependent variable that is explained by the independent variable(s). \\(\\eta^2\\) is commonly used in ANOVA, and is calculated as in Equation 8.20:\n\\[\n\\begin{aligned}\n  \\eta^2 &= \\frac{SS_{\\text{effect}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}} \\\\\n      &= 1 - \\frac{\\text{sum of squared residuals}}{\\text{total sum of squares}} \\\\\n      &= \\frac{f^2}{1 + f^2} \\\\\n      &= R^2\n\\end{aligned}\n\\tag{8.20}\\]\nwhere \\(SS_{\\text{effect}}\\) is the sum of squares for the effect of interest and SS_{} is the total sum of squares.\nPartial eta squared (\\(\\eta_p^2\\)) reflects the proportion of variance in the dependent variable that is explained by the independent variable while controlling for the other independent variables. \\(\\eta_p^2\\) is commonly used in ANOVA, and is calculated as in Equation 8.21:\n\\[\n\\eta_p^2 = \\frac{SS_{\\text{effect}}}{SS_{\\text{effect}} + SS_{\\text{error}}}\n\\tag{8.21}\\]\nwhere \\(SS_{\\text{effect}}\\) is the sum of squares for the effect of interest and SS_{} is the sum of squares for the residual error term.\n\n8.4.2.9 Effect Size Thresholds\nEffect size thresholds (Cohen, 1988; McGrath & Meyer, 2006) for small, medium, and large effect sizes are in Table 8.2.\n\n\nTable 8.2: Effect Size Thresholds for Small, Medium, and Large Effect Sizes.\n\n\n\n\n\n\n\n\n\nEffect Size Index\nSmall\nMedium\nLarge\n\n\n\nCohen’s \\(d\\)\n\n\\(\\ge |.20|\\)\n\\(\\ge |.50|\\)\n\\(\\ge |.80|\\)\n\n\nStandardized regression coefficient (beta; \\(\\beta\\))\n\\(\\ge |.10|\\)\n\\(\\ge |.24|\\)\n\\(\\ge |.37|\\)\n\n\nCorrelation coefficient (\\(r\\))\n\\(\\ge |.10|\\)\n\\(\\ge |.24|\\)\n\\(\\ge |.37|\\)\n\n\nCohen’s \\(\\omega\\)\n\n\\(\\ge .10\\)\n\\(\\ge .30\\)\n\\(\\ge .50\\)\n\n\nCohen’s \\(f\\)\n\n\\(\\ge .10\\)\n\\(\\ge .25\\)\n\\(\\ge .40\\)\n\n\nCohen’s \\(f^2\\)\n\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .16\\)\n\n\nCoefficient of determination (\\(R^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)\n\n\nEta squared (\\(\\eta^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)\n\n\nPartial eta squared (\\(\\eta_p^2\\))\n\\(\\ge .01\\)\n\\(\\ge .06\\)\n\\(\\ge .14\\)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-statisticalDecisionTree",
    "href": "basic-statistics.html#sec-statisticalDecisionTree",
    "title": "8  Basic Statistics",
    "section": "\n8.5 Statistical Decision Tree",
    "text": "8.5 Statistical Decision Tree\nA statistical decision tree is a flowchart or decision tree that depicts which statistical test to use given the purpose of analysis, the type of data, etc. An example statistical decision tree is depicted in Figure 8.8.\n\n\n\n\n\nFigure 8.8: A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R. & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including the addition of several statistical tests. Note: “Interval” as a level of measurement includes data with an “interval” or higher level of measurement; thus, it also includes data with a “ratio” level of measurement.\n\n\nThis statistical decision tree can be generally summarized such that associations are examined with the correlation/regression family, and differences are examined with the t-test/ANOVA family, as depicted in Figure 8.9.\n\n\n\n\n\nFigure 8.9: Summary of A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure.\n\n\nHowever, many statistical tests can be re-formulated in a regression framework, as in Figure 8.10.\n\n\n\n\n\nFigure 8.10: A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure, Re-Formulated in a Regression Framework. Adapted from: https://commons.wikimedia.org/wiki/File:InferentialStatisticalDecisionMakingTrees.pdf. The original source is: Corston, R. & Colman, A. M. (2000). A crash course in SPSS for Windows. Wiley-Blackwell. Changes were made to the original, including re-formulating the tests in a regression framework.\n\n\nBoth associations and differences can be examined with the regression family, which greatly simplifies our summary of the statistical decision tree, as depicted in Figure 8.11.\n\n\n\n\n\nFigure 8.11: Summary of A Statistical Decision Tree For Choosing an Appropriate Statistical Procedure.\n\n\nThus, in most cases, the regression framework can be used to examine most questions regarding associations between variables or differences between groups.\nFor an online, interactive statistical decision tree to help you decide which statistical analysis to use, see here: https://www.statsflowchart.co.uk",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-statisticalTests",
    "href": "basic-statistics.html#sec-statisticalTests",
    "title": "8  Basic Statistics",
    "section": "\n8.6 Statistical Tests",
    "text": "8.6 Statistical Tests\n\n8.6.1 t-Test\nThere are several t-tests:\n\none-sample t-test\ntwo-samples t-test\n\nindependent samples t-test\npaired samples t-test\n\n\n\nA one-sample t-test is used to evaluate whether a sample mean differs systematically from a particular value. The null hypothesis is that the sample mean does not differ systematically from the pre-specified value. The alternative hypothesis is that the sample mean differs systematically from the pre-specified value. For instance, let’s say you want to test out a new draft strategy. You could participate in a mock draft and draft players using the new strategy. Then, you could use a one-sample t-test to evaluate whether your new draft strategy yields players with more projected points than the average of players’ projected points for other teams.\nTwo-samples t-tests are used to test for differences between scores of two groups. If the two groups are independent, the independent samples t-test is used. If the two groups involve paired samples, the paired samples t-test is used. The null hypothesis is that the mean of group 1 does not differ systematically from the mean of group 2. The alternative hypothesis is that the mean of group 1 differs systematically from the mean of group 2. For instance, you could use an independent-samples t-test if you want to examine whether Quarterbacks tend to have have longer careers than Running Backs. By contrast, you could use a paired samples t-test if you want to examine whether Quarterbacks tend to score more points in the second year of their contract compared to their rookie year, because the same subjects were assessed twice (i.e., a within-subject design).\n\n8.6.2 Analysis of Variance\nAnalysis of variance (ANOVA) allows examining whether groups differ systematically as a function of one or more factors. There are multiple variants of ANOVA:\n\none-way ANOVA\nfactorial ANOVA\nrepeated measures ANOVA (RM-ANOVA)\nmultivariate ANOVA (MANOVA)\n\nLike two-samples t-tests, ANOVA allows examining whether groups differ as a function of an independent variable. However, unlike a t-test, ANOVA allows examining multiple multiple independent variables and more than two groups. The null hypothesis is that the the groups’ mean value does not differ systematically. The alternative hypothesis is that the groups’ mean value differs systematically.\nA one-way ANOVA examines whether two or more groups differ as a function of an independent variable. For instance, you could use a one-way ANOVA to evaluate if you want to evaluate whether multiple positions differ in their length of career. Factorial ANOVA examines whether two or more groups differ as a function of multiple independent variables. For instance, you could use factorial ANOVA to evaluate whether one’s length of career depends on one’s position and weight. Repeated measures ANOVA examines whether scores differ across repeated measures (e.g., across time) for the same participants. For instance, you could use repeated-measures ANOVA to evaluate whether rookies score more points as the season progresses. Multivariate ANOVA examines whether multiple dependent variables differ as a function of one or more factor(s). For instance, you could use MANOVA to evaluate whether one’s contract length and pay differ as a function of one’s position.\n\n8.6.3 Correlation\nCorrelation examines the association between a predictor and outcome variable. The null hypothesis is that the the two variables are not associated. The alternative hypothesis is that the two variables are associated.\nThe Pearson correlation coefficient (\\(r\\)) is calculated as in Equation 8.22:\n\\[\nr = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2 \\sum (Y_i - \\bar{Y})^2}}\n\\tag{8.22}\\]\nwhere \\(X\\) is the predictor variable and \\(Y\\) is the outcome variable.\n\n8.6.4 (Multiple) Regression\nRegression, like correlation, examines the association between a predictor and outcome variable. However, unlike correlation, regression allows multiple predictor variables.\nRegression with a single predictor takes the form in Equation 10.1. A regression line is depicted in Figure 10.4. Multiple regression (i.e., regression with multiple predictors) takes the form in Equation 10.2.\nThe null hypothesis is that the the predictor variable(s) are not associated with the outcome variable. The alternative hypothesis is that the predictor variable(s) are associated with the outcome variable.\n\n8.6.5 Chi-Square Test\nThere are two primary types of chi-square tests:\n\nchi-square goodness-of-fit test\nchi-square test for association (aka test of independence)\n\nThe chi-square goodness-of-fit test evaluates whether a set of categorical data came from a specified distribution. The null hypothesis is that the data came from the specified distribution. The alternative hypothesis is that the data did not come from the specified distribution.\nThe chi-square test for association evaluates whether two categorical variables are associated. The null hypothesis is that the two variables are not associated. The alternative hypothesis is that the two variables are associated.\n\n8.6.6 Formulating Statistical Tests in Terms of Partitioned Variance\nMany statistical tests can be formulated in terms of partitioned variance.\nFor instance, the t statistic from the independent-samples t-test and the F statistic from ANOVA can be thought of as the ratio of between-group variance to within-group variance, as in Equation 8.23:\n\\[\nt \\text{ or } F = \\frac{\\text{between-group variance}}{\\text{within-group variance}}\n\\tag{8.23}\\]\nThe correlation coefficient can be thought of as the ratio of shared variance (i.e., covariance) to total variance, as in Equation 8.24:\n\\[\nr = \\frac{\\text{shared variance}}{\\text{total variance}}\n\\tag{8.24}\\]\nThe coefficient of determination (\\(R^2\\)) is the proportion of variance in the outcome variable that is explained by the predictor variables. \\(\\eta^2\\) is the proportion of variance in the dependent variable that is explained by the independent variables. The coefficient of determination and \\(\\eta^2\\) can be expressed as the ratio of variance explained in the outcome or dependent variable to the total variance in the outcome or dependent variable, as in Equation 8.25:\n\\[\nR^2 \\text{ or } \\eta^2 = \\frac{\\text{variance explained in the outcome variable}}{\\text{total variance in the outcome variable}}\n\\tag{8.25}\\]\n\n8.6.7 Critical Value\nThe critical value is the test value for a given test, above which the effect is considered to be statistically significant. The critical value for statistical significance for each test can be determined based on the degrees of freedom and alpha level. The degrees of freedom (df) refer to the number of values in the calculation of a test statistic that are free to vary.\n\nCodealpha &lt;- .05\nN &lt;- 200\nnGroup1 &lt;- 150\nnGroup2 &lt;- 150\nnumGroups &lt;- 4\nnumLevelsFactorA &lt;- 3\nnumLevelsFactorB &lt;- 4\nnumMeasurements &lt;- 4\nnumPredictors &lt;- 5\nnumCategories &lt;- 6\nnumRows &lt;- 5\nnumColumns &lt;- 2\n\n\n\n8.6.7.1 One-Sample t-Test\nFor a one-sample t-test, the degrees of freedom is in Equation 8.26:\n\\[\ndf = N - 1\n\\tag{8.26}\\]\nwhere \\(N\\) is sample size.\n\nCodedf_oneSampleTtest &lt;- N - 1\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_oneSampleTtest)\n\n[1] 1.652547\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_oneSampleTtest)\n\n[1] 1.971957\n\n\n\n8.6.7.2 Independent-Samples t-Test\nFor an independent-samples t-test, the degrees of freedom is in Equation 8.27:\n\\[\ndf = n_1 + n_2 - 2\n\\tag{8.27}\\]\nwhere \\(n_1\\) is the sample size of group 1 and \\(n_2\\) is the sample size of group 2.\n\nCodedf_independentSamplesTtest &lt;- nGroup1 + nGroup2 - 2\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_independentSamplesTtest)\n\n[1] 1.649983\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_independentSamplesTtest)\n\n[1] 1.967957\n\n\n\n8.6.7.3 Paired-Samples t-Test\nFor a paired-samples t-test, the degrees of freedom is in Equation 8.28:\n\\[\ndf = N - 1\n\\tag{8.28}\\]\nwhere \\(N\\) is sample size (i.e., the number of paired observations).\n\nCodedf_pairedSamplesTtest &lt;- N - 1\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_pairedSamplesTtest)\n\n[1] 1.652547\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_pairedSamplesTtest)\n\n[1] 1.971957\n\n\n\n8.6.7.4 One-Way ANOVA\nFor a one-way ANOVA, the degrees of freedom is in Equation 8.29:\n\\[\n\\begin{aligned}\n  df_\\text{between} &= g - 1 \\\\\n  df_\\text{within} &= N - g\n\\end{aligned}\n\\tag{8.29}\\]\nwhere \\(N\\) is sample size and \\(g\\) is the number of groups.\n\nCodedf_betweenOneWayANOVA &lt;- numGroups - 1\ndf_withinOneWayANOVA &lt;- N - numGroups\n\n\nOne-tailed test:\n\nCodeqf(1 - alpha, df_betweenOneWayANOVA, df_withinOneWayANOVA)\n\n[1] 2.650677\n\n\nTwo-tailed test:\n\nCodeqf(1 - alpha/2, df_betweenOneWayANOVA, df_withinOneWayANOVA)\n\n[1] 3.183378\n\n\n\n8.6.7.5 Factorial ANOVA\nFor a factorial two-way ANOVA, the degrees of freedom is in Equation 8.30:\n\\[\n\\begin{aligned}\n  df_\\text{Factor A} &= a - 1 \\\\\n  df_\\text{Factor B} &= b - 1 \\\\\n  df_\\text{Interaction} &= (a - 1)(b - 1) \\\\\n  df_\\text{error} &= ab(N - 1)\n\\end{aligned}\n\\tag{8.30}\\]\nwhere \\(N\\) is sample size, \\(a\\) is the number of levels for factor A, and \\(b\\) is the number of levels for factor B.\n\nCodedf_factorA &lt;- numLevelsFactorA - 1\ndf_factorB &lt;- numLevelsFactorB - 1\ndf_interaction &lt;- df_factorA * df_factorB\ndf_error &lt;- numLevelsFactorA * numLevelsFactorB * (N - 1)\n\n\nFactor A (one-tailed test):\n\nCodeqf(1 - alpha, df_factorA, df_error)\n\n[1] 2.999494\n\n\nFactor B (one-tailed test):\n\nCodeqf(1 - alpha, df_factorB, df_error)\n\n[1] 2.608629\n\n\nInteraction (one-tailed test):\n\nCodeqf(1 - alpha, df_interaction, df_error)\n\n[1] 2.102376\n\n\nFactor A (two-tailed test):\n\nCodeqf(1 - alpha/2, df_factorA, df_error)\n\n[1] 3.694584\n\n\nFactor B (two-tailed test):\n\nCodeqf(1 - alpha/2, df_factorB, df_error)\n\n[1] 3.121587\n\n\nInteraction (two-tailed test):\n\nCodeqf(1 - alpha/2, df_interaction, df_error)\n\n[1] 2.413504\n\n\n\n8.6.7.6 Repeated Measures ANOVA\nFor a repeated measures ANOVA, the degrees of freedom is in Equation 8.31:\n\\[\n\\begin{aligned}\n  df_1 &= T - 1 \\\\\n  df_2 &= (T - 1)(N - 1)\n\\end{aligned}\n\\tag{8.31}\\]\nwhere \\(N\\) is sample size and \\(T\\) is the number of measurements (i.e., the number of levels of the within-person factor: e.g., timepoints or conditions).\n\nCodedf1_RMANOVA &lt;- numMeasurements - 1\ndf2_RMANOVA &lt;- (numMeasurements - 1) * (N - 1)\n\n\nOne-tailed test:\n\nCodeqf(1 - alpha, df1_RMANOVA, df2_RMANOVA)\n\n[1] 2.619828\n\n\nTwo-tailed test:\n\nCodeqf(1 - alpha/2, df1_RMANOVA, df2_RMANOVA)\n\n[1] 3.138017\n\n\n\n8.6.7.7 Correlation\nFor a correlation, the degrees of freedom is in Equation 8.32:\n\\[\ndf = N - 2\n\\tag{8.32}\\]\nwhere \\(N\\) is sample size.\n\nCodedf_correlation &lt;- N - 2\n\n\nOne-tailed test:\n\nCodeqt(1 - alpha, df_correlation)\n\n[1] 1.652586\n\n\nTwo-tailed test:\n\nCodeqt(1 - alpha/2, df_correlation)\n\n[1] 1.972017\n\n\n\n8.6.7.8 Multiple Regression\nFor multiple regression, the degrees of freedom is in Equation 8.33:\n\\[\n\\begin{aligned}\n  df_1 &= p \\\\\n  df_2 &= N - p - 1\n\\end{aligned}\n\\tag{8.33}\\]\nwhere \\(N\\) is sample size and \\(p\\) is the number of predictors.\n\nCodedf1_regression &lt;- numPredictors\ndf2_regression &lt;- N - numPredictors - 1\n\n\nOne-tailed test:\n\nCodeqf(1 - alpha, df1_regression, df2_regression)\n\n[1] 2.260647\n\n\nTwo-tailed test:\n\nCodeqf(1 - alpha/2, df1_regression, df2_regression)\n\n[1] 2.63243\n\n\n\n8.6.7.9 Chi-Square Goodness-of-Fit Test\nFor the chi-square goodness-of-fit test, the degrees of freedom is in Equation 8.34:\n\\[\ndf = c - 1\n\\tag{8.34}\\]\nwhere \\(c\\) is the number of categories.\n\nCodedf_chisquareGOF &lt;- numCategories - 1\n\n\nOne-tailed test:\n\nCodeqchisq(1 - alpha, df_chisquareGOF)\n\n[1] 11.0705\n\n\nTwo-tailed test:\n\nCodeqchisq(1 - alpha/2, df_chisquareGOF)\n\n[1] 12.8325\n\n\n\n8.6.7.10 Chi-Square Test for Association\nFor the chi-square test for association, the degrees of freedom is in Equation 8.35:\n\\[\ndf = (r - 1) \\times (c - 1)\n\\tag{8.35}\\]\nwhere \\(r\\) is the number of rows in the contingency table and \\(c\\) is the number of columns in the contingency table.\n\nCodedf_chisquareAssociation &lt;- (numRows - 1) * (numColumns - 1)\n\n\nOne-tailed test:\n\nCodeqchisq(1 - alpha, df_chisquareAssociation)\n\n[1] 9.487729\n\n\nTwo-tailed test:\n\nCodeqchisq(1 - alpha/2, df_chisquareAssociation)\n\n[1] 11.14329\n\n\n\n8.6.8 Statistical Power\nAs described above, statistical power is the probability of detecting an effect, if, in fact, the effect exists. Statistical power for a given test can be calculated based on three factors:\n\neffect size\nsample size\nalpha level\n\nKnowing any three of the following, you can calculate the fourth: statistical power, effect size, sample size, and alpha level. Below is R code for calculating power for each of various statistical tests (i.e., a power analysis). For free point-and-click software for calculating statistical power, see G*Power: https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower.html\n\nCodepower &lt;- .8\neffectSize_d &lt;- .5\neffectSize_r &lt;- .24\neffectSize_beta &lt;- .24\neffectSize_f &lt;- .25\neffectSize_fSquared &lt;- .06\neffectSize_omega &lt;- .3\n\n\nWhen designing a study, it is important to consider power and the sample size needed to detect the hypothesized effect size. If your sample size is too small and you do not detect an effect (i.e., \\(p &gt; .05\\)), you do not know whether your failure to detect the effect was because a) the effect does not exist, or b) the effect exists but you did not have enough power to detect it.\n\n8.6.8.1 One-Sample t-Test\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9999998\n    alternative = two.sided\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"one.sample\",\n  alternative = \"two.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 200\n              d = 0.1990655\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n8.6.8.2 Independent-Samples t-Test\n\n8.6.8.2.1 Balanced Group Sizes\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9987689\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"two.sample\",\n  alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 200\n              d = 0.2808267\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n8.6.8.2.2 Unbalanced Group Sizes\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  n1 = nGroup1,\n  n2 = nGroup2,\n  d = effectSize_d,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 150\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9907677\n    alternative = two.sided\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  power = power,\n  n1 = nGroup1,\n  d = effectSize_d,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 40.22483\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t2n.test(\n  power = power,\n  n1 = nGroup1,\n  n2 = nGroup2,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 150\n             n2 = 150\n              d = 0.3245459\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n8.6.8.3 Paired-Samples t-Test\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.t.test(\n  n = N,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 200\n              d = 0.5\n      sig.level = 0.05\n          power = 0.9999998\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  d = effectSize_d,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 33.36713\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.t.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  type = \"paired\",\n  alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 200\n              d = 0.1990655\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n8.6.8.4 One-Way ANOVA\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999962\n\nNOTE: n is number in each group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numGroups)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.117038\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nThe power analysis code above assumes the groups are of equal size (i.e., a balanced design). If the design is unbalanced (i.e., there are different numbers of participants in each group), it may be necessary to conduct a power analysis via a simulation. Below is an example of evaluating the statistical power for detecting an effect unbalanced designs via simulation:\n\nCodenSim &lt;- 1000 # number of simulations\n\n# Function to generate data and perform ANOVA\nsimulate_anova &lt;- function(nGroup1, nGroup2, f, alpha) {\n  # Means for each group\n  mean1 &lt;- 0\n  mean2 &lt;- f * sqrt((nGroup1 + nGroup2) / 2)\n  \n  # Generate data\n  group1 &lt;- rnorm(nGroup1, mean = mean1, sd = 1)\n  group2 &lt;- rnorm(nGroup2, mean = mean2, sd = 1)\n  \n  # Combine data\n  data &lt;- data.frame(\n    value = c(group1, group2),\n    group = factor(rep(c(\"Group1\", \"Group2\"), c(nGroup1, nGroup2)))\n  )\n  \n  # Perform ANOVA\n  aov_result &lt;- aov(value ~ group, data = data)\n  p_value &lt;- summary(aov_result)[[1]][[\"Pr(&gt;F)\"]][1]\n  \n  # Check if p-value is less than alpha\n  return(p_value &lt; alpha)\n}\n\n# Run simulations\nset.seed(52242) # for reproducibility\npowerSimulationOneWayAnova &lt;- replicate(\n  nSim,\n  simulate_anova(\n    nGroup1 = 10,\n    nGroup2 = 25,\n    f = effectSize_f,\n    alpha = alpha))\n\n# Estimate power\nmean(powerSimulationOneWayAnova)\n\n[1] 0.774\n\n\n\n8.6.8.5 Factorial ANOVA\nThe power analysis code below assumes the groups are of equal size (i.e., a balanced design). If the design is unbalanced (i.e., there are different numbers of participants in each group), it may be necessary to conduct a power analysis via a simulation. See Section 8.6.8.4 for an example power analysis simulation for one-way ANOVA.\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999238\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 0.9999962\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  n = N,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 200\n              f = 0.25\n      sig.level = 0.05\n          power = 1\n\nNOTE: n is number in each group\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  f = effectSize_f,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 32.05196\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorA)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 200\n              f = 0.1270373\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 200\n              f = 0.117038\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\nCodepwr::pwr.anova.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  k = numLevelsFactorA + numLevelsFactorB)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 7\n              n = 200\n              f = 0.09889082\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\n\n8.6.8.6 Repeated Measures ANOVA\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.8484718\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.8536292\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n      n    f ng nm nscor alpha     power\n    200 0.25  4  4     1  0.05 0.6756298\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    178.3971 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    175.7692 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  ng = numGroups,\n  nm = numMeasurements,\n  f = effectSize_f,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n           n    f ng nm nscor alpha power\n    253.2369 0.25  4  4     1  0.05   0.8\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 0)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2358259  4  4     1  0.05   0.8\n\nNOTE: Power analysis for between-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 1)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2342726  4  4     1  0.05   0.8\n\nNOTE: Power analysis for within-effect test\nURL: http://psychstat.org/rmanova\n\nCodeWebPower::wp.rmanova(\n  power = power,\n  n = N,\n  ng = numGroups,\n  nm = numMeasurements,\n  alpha = alpha,\n  type = 2)\n\nRepeated-measures ANOVA analysis\n\n      n         f ng nm nscor alpha power\n    200 0.2817486  4  4     1  0.05   0.8\n\nNOTE: Power analysis for interaction-effect test\nURL: http://psychstat.org/rmanova\n\n\n\n8.6.8.7 Correlation\nSolving for statistical power achieved (given effect size, sample size per group, and alpha level):\n\nCodepwr::pwr.r.test(\n  n = N,\n  r = effectSize_r,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 200\n              r = 0.24\n      sig.level = 0.05\n          power = 0.9310138\n    alternative = two.sided\n\n\nSolving for sample size per group needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.r.test(\n  power = power,\n  r = effectSize_r,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 133.1299\n              r = 0.24\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSolving for the minimum detectable effect size (given sample size per group, power, and alpha level):\n\nCodepwr::pwr.r.test(\n  power = power,\n  n = N,\n  sig.level = alpha,\n  alternative = \"two.sided\")\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 200\n              r = 0.1965767\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n8.6.8.8 Multiple Regression\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.f2.test(\n  f2 = effectSize_fSquared,\n  sig.level = alpha,\n  u = numPredictors,\n  v = N - numPredictors - 1)\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 194\n             f2 = 0.06\n      sig.level = 0.05\n          power = 0.7548031\n\nCodepwrss::pwrss.t.reg(\n  n = N,\n  beta1 = effectSize_beta,\n  k = numPredictors,\n  alpha = alpha,\n  alternative = \"not equal\")\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.936 \n  n = 200 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 194 \n Non-centrality parameter = 3.496 \n Type I error rate = 0.05 \n Type II error rate = 0.064 \n\n\nSolving for sample size needed (given effect size, power, and alpha level)—\\(v = N - \\text{numberOfPredictors} - 1\\); thus, \\(N = v + \\text{numberOfPredictors} + 1\\):\n\nCodemultipleRegressionSampleSizeModel &lt;- pwr::pwr.f2.test(\n  power = power,\n  f2 = effectSize_fSquared,\n  sig.level = alpha,\n  u = numPredictors)\n\nmultipleRegressionSampleSizeModel\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 213.3947\n             f2 = 0.06\n      sig.level = 0.05\n          power = 0.8\n\nCodevNeeded &lt;- multipleRegressionSampleSizeModel$v\nsampleSizeNeeded &lt;- vNeeded + numPredictors + 1\nsampleSizeNeeded\n\n[1] 219.3947\n\nCodepwrss::pwrss.t.reg(\n  power = power,\n  beta1 = effectSize_beta,\n  k = numPredictors,\n  alpha = alpha,\n  alternative = \"not equal\")\n\n Linear Regression Coefficient (t Test) \n H0: beta1 = beta0 \n HA: beta1 != beta0 \n ------------------------------ \n  Statistical power = 0.8 \n  n = 131 \n ------------------------------ \n Alternative = \"not equal\" \n Degrees of freedom = 124.427 \n Non-centrality parameter = 2.823 \n Type I error rate = 0.05 \n Type II error rate = 0.2 \n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.f2.test(\n  power = power,\n  sig.level = alpha,\n  u = numPredictors,\n  v = N - numPredictors - 1)\n\n\n     Multiple regression power calculation \n\n              u = 5\n              v = 194\n             f2 = 0.06597765\n      sig.level = 0.05\n          power = 0.8\n\n\n\n8.6.8.9 Chi-Square Goodness-of-Fit Test\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  N = N,\n  w = effectSize_omega,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 200\n             df = 5\n      sig.level = 0.05\n          power = 0.9269225\n\nNOTE: N is the number of observations\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  w = effectSize_omega,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 142.529\n             df = 5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  N = N,\n  df = numCategories - 1,\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.2532543\n              N = 200\n             df = 5\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\n\n8.6.8.10 Chi-Square Test for Association\nSolving for statistical power achieved (given effect size, sample size, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  N = N,\n  w = effectSize_omega,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 200\n             df = 4\n      sig.level = 0.05\n          power = 0.9431195\n\nNOTE: N is the number of observations\n\n\nSolving for sample size needed (given effect size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  w = effectSize_omega,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.3\n              N = 132.6143\n             df = 4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\nSolving for the minimum detectable effect size (given sample size, power, and alpha level):\n\nCodepwr::pwr.chisq.test(\n  power = power,\n  N = N,\n  df = (numRows - 1)*(numColumns - 1),\n  sig.level = alpha)\n\n\n     Chi squared power calculation \n\n              w = 0.2442875\n              N = 200\n             df = 4\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations\n\n\n\n8.6.8.11 Multilevel Modeling\nPower analysis for multilevel modeling approaches is more complicated than it is for other statistical analyses, such as correlation, multiple regression, t-tests, ANOVA, etc.\nThere are free web applications for calculating power in multilevel modeling:\n\nhttps://aguinis.shinyapps.io/ml_power/\nhttps://koumurayama.shinyapps.io/tmethod_mlm/\nhttps://webpower.psychstat.org/wiki/models/index\n\n8.6.8.12 Path Analysis, Factor Analysis, and Structural Equation Modeling\nPower analysis for latent variable modeling approaches like structural equation modeling (SEM) is more complicated than it is for other statistical analyses, such as correlation, multiple regression, t-tests, ANOVA, etc.\nI provide an example of power analysis in SEM using Monte Carlo simulation in R here: https://isaactpetersen.github.io/Principles-Psychological-Assessment/sem.html#monteCarloPowerAnalysis (Petersen, 2024b).\nThere are also free web applications for calculating power in SEM:\n\nhttps://sjak.shinyapps.io/power4SEM/\nhttps://sempower.shinyapps.io/sempower/\nhttps://yilinandrewang.shinyapps.io/pwrSEM/\nhttps://webpower.psychstat.org/wiki/models/index\n\n8.6.8.13 Mediation and Moderation\nThere are free tools for calculating power for tests of mediation and moderation:\n\nhttps://schoemanna.shinyapps.io/mc_power_med/\n\nhttps://www.causalevaluation.org/power-analysis.html (web application: https://powerupr.shinyapps.io/index/)\nhttps://webpower.psychstat.org/wiki/models/index",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsConclusion",
    "href": "basic-statistics.html#sec-basicStatsConclusion",
    "title": "8  Basic Statistics",
    "section": "\n8.7 Conclusion",
    "text": "8.7 Conclusion\nDescriptive statistics are used to describe the data, including the center, spread, or shape of data. Inferential statistics are used to draw inferences regarding differences between groups or associations between variables. Null hypothesis signficance testing is a framework for inferential statistics, in which there is a null hypothesis and alternative hypothesis. The null hypothesis is that there is no difference between groups or that there is no association between variables. Statistical significance is evaluated with a \\(p\\)-value, which represents the probability of obtaining a result at least as extreme as the result observed if the null hypothesis is true. Effects with p-values less than .05 are considered statistically significant. However, it is also important to consider practical significance and effect sizes. When designing a study, it is important to consider statistical power and the sample size needed to detect the hypothesized effect size. You can determine a study’s power based on the effect size, sample size, and alpha level.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "basic-statistics.html#sec-basicStatsSessionInfo",
    "href": "basic-statistics.html#sec-basicStatsSessionInfo",
    "title": "8  Basic Statistics",
    "section": "\n8.8 Session Info",
    "text": "8.8 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] grid      parallel  stats     graphics  grDevices utils     datasets \n[8] methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   WebPower_0.9.4    PearsonDS_1.3.1  \n[13] lavaan_0.6-18     lme4_1.1-35.5     Matrix_1.7-0      MASS_7.3-60.2    \n[17] pwrss_0.3.1       pwr_1.3-0         DescTools_0.99.54 petersenlab_1.0.3\n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.3          mnormt_2.1.1       gridExtra_2.3      gld_2.6.6         \n [5] readxl_1.4.3       rlang_1.1.4        magrittr_2.0.3     e1071_1.7-14      \n [9] compiler_4.4.1     mgcv_1.9-1         vctrs_0.6.5        reshape2_1.4.4    \n[13] quadprog_1.5-8     pkgconfig_2.0.3    fastmap_1.2.0      backports_1.5.0   \n[17] labeling_0.4.3     pbivnorm_0.6.0     utf8_1.2.4         rmarkdown_2.27    \n[21] tzdb_0.4.0         nloptr_2.1.1       xfun_0.46          jsonlite_1.8.8    \n[25] psych_2.4.6.26     cluster_2.1.6      R6_2.5.1           stringi_1.8.4     \n[29] RColorBrewer_1.1-3 boot_1.3-30        rpart_4.1.23       cellranger_1.1.0  \n[33] Rcpp_1.0.13        knitr_1.48         base64enc_0.1-3    splines_4.4.1     \n[37] nnet_7.3-19        timechange_0.3.0   tidyselect_1.2.1   rstudioapi_0.16.0 \n[41] yaml_2.3.9         lattice_0.22-6     plyr_1.8.9         withr_3.0.0       \n[45] evaluate_0.24.0    foreign_0.8-86     proxy_0.4-27       pillar_1.9.0      \n[49] checkmate_2.3.1    stats4_4.4.1       generics_0.1.3     mix_1.0-12        \n[53] hms_1.1.3          munsell_0.5.1      scales_1.3.0       rootSolve_1.8.2.4 \n[57] minqa_1.2.7        xtable_1.8-4       class_7.3-22       glue_1.7.0        \n[61] Hmisc_5.1-3        lmom_3.0           tools_4.4.1        data.table_1.15.4 \n[65] Exact_3.3          mvtnorm_1.2-5      mitools_2.4        colorspace_2.1-0  \n[69] nlme_3.1-164       htmlTable_2.4.3    Formula_1.2-5      cli_3.6.3         \n[73] fansi_1.0.6        expm_0.999-9       viridisLite_0.4.2  gtable_0.3.5      \n[77] digest_0.6.36      htmlwidgets_1.6.4  farver_2.1.2       htmltools_0.5.8.1 \n[81] lifecycle_1.0.4    httr_1.4.7        \n\n\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates, Publishers. https://doi.org/10.4324/9780203771587\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nGoodman, S. (2008). A dirty dozen: Twelve p-value misconceptions. Seminars in Hematology, 45(3), 135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree: The case of r and d. Psychological Methods, 11(4), 386–401. https://doi.org/10.1037/1082-989X.11.4.386\n\n\nPetersen, I. T. (2024a). petersenlab: A collection of R functions by the Petersen Lab. https://github.com/DevPsyLab/petersenlab\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Statistics</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "9  Correlation Analysis",
    "section": "",
    "text": "9.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationGettingStarted",
    "href": "correlation.html#sec-correlationGettingStarted",
    "title": "9  Correlation Analysis",
    "section": "",
    "text": "9.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"XICOR\")\nlibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationOverview",
    "href": "correlation.html#sec-correlationOverview",
    "title": "9  Correlation Analysis",
    "section": "\n9.2 Overview of Correlation",
    "text": "9.2 Overview of Correlation\nCorrelation is an index of the association between variables. Covariance is the association between variables and in an unstandardized metric that differs for variables with different scales. By contrast, correlation is in a standarized metric that does not differ for variables with different scales. When examining the association between variables that are interval or ratio levels of measurement, Pearson correlation is used. When examining the association between variables that are ordinal in level of measurement, Spearman correlation is used. Pearson correlation is an index of the linear association between variables. If a nonlinear association is present, other indices like xi [\\(\\xi\\); Chatterjee (2021)] and distance correlation coefficients are better suited to detect the association.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#the-correlation-coefficient-r",
    "href": "correlation.html#the-correlation-coefficient-r",
    "title": "9  Correlation Analysis",
    "section": "\n9.3 The Correlation Coefficient (\\(r\\))",
    "text": "9.3 The Correlation Coefficient (\\(r\\))\nThe formula for the correlation coefficient is in Equation 8.22.\nThe correlation coefficient ranges from −1.0 to +1.0. The correlation coefficient (\\(r\\)) tells you two things: (1) the direction (sign) of the association (positive or negative) and (2) the magnitude of the association. If the correlation coefficient is positive, the association is positive. If the correlation coefficient is negative, the association is negative. If the association is positive, as X increases, Y increases (or conversely, as X decreases, Y decreases). If the association is negative, as X increases, Y decreases (or conversely, as X decreases, Y increases). The smaller the absolute value of the correlation coefficient (i.e., the closer the \\(r\\) value is to zero), the weaker the association and the flatter the slope of the best-fit line in a scatterplot. The larger the absolute value of the correlation coefficient (i.e., the closer the absolute value of the \\(r\\) value is to one), the stronger the association and the steeper the slope of the best-fit line in a scatterplot. See Figure 9.1 for a range of different correlation coefficients and what some example data may look like for each direction and strength of association.\n\nCodeset.seed(52242)\ncorrelations &lt;- data.frame(criterion = rnorm(1000))\n\ncorrelations$v1 &lt;- complement(correlations$criterion, -1)\ncorrelations$v2 &lt;- complement(correlations$criterion, -.9)\ncorrelations$v3 &lt;- complement(correlations$criterion, -.8)\ncorrelations$v4 &lt;- complement(correlations$criterion, -.7)\ncorrelations$v5 &lt;- complement(correlations$criterion, -.6)\ncorrelations$v6 &lt;- complement(correlations$criterion, -.5)\ncorrelations$v7 &lt;- complement(correlations$criterion, -.4)\ncorrelations$v8 &lt;- complement(correlations$criterion, -.3)\ncorrelations$v9 &lt;- complement(correlations$criterion, -.2)\ncorrelations$v10 &lt;-complement(correlations$criterion, -.1)\ncorrelations$v11 &lt;-complement(correlations$criterion, 0)\ncorrelations$v12 &lt;-complement(correlations$criterion, .1)\ncorrelations$v13 &lt;-complement(correlations$criterion, .2)\ncorrelations$v14 &lt;-complement(correlations$criterion, .3)\ncorrelations$v15 &lt;-complement(correlations$criterion, .4)\ncorrelations$v16 &lt;-complement(correlations$criterion, .5)\ncorrelations$v17 &lt;-complement(correlations$criterion, .6)\ncorrelations$v18 &lt;-complement(correlations$criterion, .7)\ncorrelations$v19 &lt;-complement(correlations$criterion, .8)\ncorrelations$v20 &lt;-complement(correlations$criterion, .9)\ncorrelations$v21 &lt;-complement(correlations$criterion, 1)\n\npar(mfrow = c(7,3), mar = c(1, 0, 1, 0))\n\n# -1.0\nplot(correlations$criterion, correlations$v1, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v1)$estimate, 2))))\nabline(lm(v1 ~ criterion, data = correlations), col = \"black\")\n\n# -.9\nplot(correlations$criterion, correlations$v2, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v2)$estimate, 2))))\nabline(lm(v2 ~ criterion, data = correlations), col = \"black\")\n\n# -.8\nplot(correlations$criterion, correlations$v3, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v3)$estimate, 2))))\nabline(lm(v3 ~ criterion, data = correlations), col = \"black\")\n\n# -.7\nplot(correlations$criterion, correlations$v4, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v4)$estimate, 2))))\nabline(lm(v4 ~ criterion, data = correlations), col = \"black\")\n\n# -.6\nplot(correlations$criterion, correlations$v5, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v5)$estimate, 2))))\nabline(lm(v5 ~ criterion, data = correlations), col = \"black\")\n\n# -.5\nplot(correlations$criterion, correlations$v6, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v6)$estimate, 2))))\nabline(lm(v6 ~ criterion, data = correlations), col = \"black\")\n\n# -.4\nplot(correlations$criterion, correlations$v7, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v7)$estimate, 2))))\nabline(lm(v7 ~ criterion, data = correlations), col = \"black\")\n\n# -.3\nplot(correlations$criterion, correlations$v8, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v8)$estimate, 2))))\nabline(lm(v8 ~ criterion, data = correlations), col = \"black\")\n\n# -.2\nplot(correlations$criterion, correlations$v9, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v9)$estimate, 2))))\nabline(lm(v9 ~ criterion, data = correlations), col = \"black\")\n\n# -.1\nplot(correlations$criterion, correlations$v10, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v10)$estimate, 2))))\nabline(lm(v10 ~ criterion, data = correlations), col = \"black\")\n\n# 0.0\nplot(correlations$criterion, correlations$v11, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v11)$estimate, 2))))\nabline(lm(v11 ~ criterion, data = correlations), col = \"black\")\n\n# 0.1\nplot(correlations$criterion, correlations$v12, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v12)$estimate, 2))))\nabline(lm(v12 ~ criterion, data = correlations), col = \"black\")\n\n# 0.2\nplot(correlations$criterion, correlations$v13, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v13)$estimate, 2))))\nabline(lm(v13 ~ criterion, data = correlations), col = \"black\")\n\n# 0.3\nplot(correlations$criterion, correlations$v14, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v14)$estimate, 2))))\nabline(lm(v14 ~ criterion, data = correlations), col = \"black\")\n\n# 0.4\nplot(correlations$criterion, correlations$v15, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v15)$estimate, 2))))\nabline(lm(v15 ~ criterion, data = correlations), col = \"black\")\n\n# 0.5\nplot(correlations$criterion, correlations$v16, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v16)$estimate, 2))))\nabline(lm(v16 ~ criterion, data = correlations), col = \"black\")\n\n# 0.6\nplot(correlations$criterion, correlations$v17, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v17)$estimate, 2))))\nabline(lm(v17 ~ criterion, data = correlations), col = \"black\")\n\n# 0.7\nplot(correlations$criterion, correlations$v18, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v18)$estimate, 2))))\nabline(lm(v18 ~ criterion, data = correlations), col = \"black\")\n\n# 0.8\nplot(correlations$criterion, correlations$v19, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v19)$estimate, 2))))\nabline(lm(v19 ~ criterion, data = correlations), col = \"black\")\n\n# 0.9\nplot(correlations$criterion, correlations$v20, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v20)$estimate, 2))))\nabline(lm(v20 ~ criterion, data = correlations), col = \"black\")\n\n# 1.0\nplot(correlations$criterion, correlations$v21, xaxt = \"n\", yaxt = \"n\", xlab = \"\" , ylab = \"\",\n     main = substitute(paste(italic(r), \" = \", x, sep = \"\"), list(x = round(cor.test(x = correlations$criterion, y = correlations$v21)$estimate, 2))))\nabline(lm(v21 ~ criterion, data = correlations), col = \"black\")\n\ninvisible(dev.off()) #par(mfrow = c(1,1))\n\n\n\n\n\n\nFigure 9.1: Correlation Coefficients.\n\n\n\n\nSee Figure 9.2 for the interpretation of the magnitude and direction (sign) of various correlation coefficients.\n\nCodelibrary(\"patchwork\")\n\nset.seed(52242)\ncorrelations2 &lt;- data.frame(criterion = rnorm(15))\n\ncorrelations2$v1 &lt;- complement(correlations2$criterion, -1)\ncorrelations2$v2 &lt;- complement(correlations2$criterion, -.9)\ncorrelations2$v3 &lt;- complement(correlations2$criterion, -.8)\ncorrelations2$v4 &lt;- complement(correlations2$criterion, -.7)\ncorrelations2$v5 &lt;- complement(correlations2$criterion, -.6)\ncorrelations2$v6 &lt;- complement(correlations2$criterion, -.5)\ncorrelations2$v7 &lt;- complement(correlations2$criterion, -.4)\ncorrelations2$v8 &lt;- complement(correlations2$criterion, -.3)\ncorrelations2$v9 &lt;- complement(correlations2$criterion, -.2)\ncorrelations2$v10 &lt;-complement(correlations2$criterion, -.1)\ncorrelations2$v11 &lt;-complement(correlations2$criterion, 0)\ncorrelations2$v12 &lt;-complement(correlations2$criterion, .1)\ncorrelations2$v13 &lt;-complement(correlations2$criterion, .2)\ncorrelations2$v14 &lt;-complement(correlations2$criterion, .3)\ncorrelations2$v15 &lt;-complement(correlations2$criterion, .4)\ncorrelations2$v16 &lt;-complement(correlations2$criterion, .5)\ncorrelations2$v17 &lt;-complement(correlations2$criterion, .6)\ncorrelations2$v18 &lt;-complement(correlations2$criterion, .7)\ncorrelations2$v19 &lt;-complement(correlations2$criterion, .8)\ncorrelations2$v20 &lt;-complement(correlations2$criterion, .9)\ncorrelations2$v21 &lt;-complement(correlations2$criterion, 1)\n\n# -1.0\np1 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v1\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Perfect Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−1.0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.9\np2 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v2\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Strong Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.9\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.5\np3 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v6\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Moderate Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.5\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# -0.2\np4 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v9\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Weak Negative Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"−.2\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.0\np5 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v11\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"No Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.2\np6 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v13\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Weak Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".2\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.5\np7 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v16\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Moderate Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".5\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 0.9\np8 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v20\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Strong Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \".9\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n# 1.0\np9 &lt;- ggplot(\n  data = correlations2,\n  mapping = aes(\n    x = criterion,\n    y = v21\n  )\n) + \n  geom_point() +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE) +\n  labs(\n    title = \"Perfect Positive Association\",\n    subtitle = expression(paste(italic(\"r\"), \" = \", \"1.0\"))\n  ) +\n  theme_classic(\n    base_size = 12) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\np1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +\n  plot_layout(\n    ncol = 3,\n    heights = 1,\n    widths = 1)\n\n\n\n\n\n\nFigure 9.2: Interpretation of the Magnitude and Direction (Sign) of Correlation Coefficients.\n\n\n\n\nInteractive visualizations by Kristoffer Magnusson on p-values and null-hypothesis significance testing are below:\n\n\nhttps://rpsychologist.com/correlation/ (archived at https://perma.cc/G8YR-VCM4)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationExamples",
    "href": "correlation.html#sec-correlationExamples",
    "title": "9  Correlation Analysis",
    "section": "\n9.4 Examples",
    "text": "9.4 Examples\n\n9.4.1 Covariance\n\n9.4.2 Pearson Correlation\n\n9.4.3 Spearman Correlation\n\n9.4.4 Nonlinear Correlation\n\n9.4.5 Correlation Matrix\n\n9.4.6 Correlogram",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationOutliers",
    "href": "correlation.html#sec-correlationOutliers",
    "title": "9  Correlation Analysis",
    "section": "\n9.5 Impact of Outliers",
    "text": "9.5 Impact of Outliers",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlation-correlationAndCausation",
    "href": "correlation.html#sec-correlation-correlationAndCausation",
    "title": "9  Correlation Analysis",
    "section": "\n9.6 Correlation Does Not Imply Causation",
    "text": "9.6 Correlation Does Not Imply Causation\nAs described in Section 7.3.2.1, correlation does not imply causation. There are several reasons (described in Section 7.3.2.1) that, just because X is correlated with Y does not necessarily mean that X causes Y. However, correlation can still be useful. In order for two processes to be causally related, they must be associated. That is, association is necessary but insufficient for causality.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationConclusion",
    "href": "correlation.html#sec-correlationConclusion",
    "title": "9  Correlation Analysis",
    "section": "\n9.7 Conclusion",
    "text": "9.7 Conclusion\nCorrelation is an index of the association between variables. The correlation coefficient (\\(r\\)) ranges from −1 to +1, and indicates the sign and magnitude of the association. Although correlation does not imply causation, identifying associations between variables can still be useful because association is a necessary (but insufficient) condition for causality.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "correlation.html#sec-correlationSessionInfo",
    "href": "correlation.html#sec-correlationSessionInfo",
    "title": "9  Correlation Analysis",
    "section": "\n9.8 Session Info",
    "text": "9.8 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] patchwork_1.2.0   lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n [5] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n [9] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   XICOR_0.4.1      \n[13] petersenlab_1.0.3\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1   psych_2.4.6.26     viridisLite_0.4.2  farver_2.1.2      \n [5] fastmap_1.2.0      digest_0.6.36      rpart_4.1.23       timechange_0.3.0  \n [9] lifecycle_1.0.4    cluster_2.1.6      magrittr_2.0.3     compiler_4.4.1    \n[13] rlang_1.1.4        Hmisc_5.1-3        tools_4.4.1        utf8_1.2.4        \n[17] yaml_2.3.9         data.table_1.15.4  knitr_1.48         labeling_0.4.3    \n[21] htmlwidgets_1.6.4  mnormt_2.1.1       plyr_1.8.9         RColorBrewer_1.1-3\n[25] foreign_0.8-86     withr_3.0.0        R.oo_1.26.0        nnet_7.3-19       \n[29] grid_4.4.1         stats4_4.4.1       fansi_1.0.6        lavaan_0.6-18     \n[33] xtable_1.8-4       colorspace_2.1-0   scales_1.3.0       cli_3.6.3         \n[37] mvtnorm_1.2-5      rmarkdown_2.27     generics_0.1.3     rstudioapi_0.16.0 \n[41] reshape2_1.4.4     tzdb_0.4.0         DBI_1.2.3          rtf_0.4-14.1      \n[45] splines_4.4.1      parallel_4.4.1     base64enc_0.1-3    mitools_2.4       \n[49] vctrs_0.6.5        Matrix_1.7-0       jsonlite_1.8.8     hms_1.1.3         \n[53] Formula_1.2-5      htmlTable_2.4.3    glue_1.7.0         stringi_1.8.4     \n[57] gtable_0.3.5       quadprog_1.5-8     munsell_0.5.1      pillar_1.9.0      \n[61] psychTools_2.4.3   htmltools_0.5.8.1  R6_2.5.1           mix_1.0-12        \n[65] evaluate_0.24.0    pbivnorm_0.6.0     lattice_0.22-6     R.methodsS3_1.8.2 \n[69] backports_1.5.0    Rcpp_1.0.13        gridExtra_2.3      nlme_3.1-164      \n[73] checkmate_2.3.1    mgcv_1.9-1         xfun_0.46          pkgconfig_2.0.3   \n\n\n\n\n\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of the American Statistical Association, 116(536), 2009–2022. https://doi.org/10.1080/01621459.2020.1758115\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Correlation Analysis</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html",
    "href": "multiple-regression.html",
    "title": "10  Multiple Regression",
    "section": "",
    "text": "10.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionGettingStarted",
    "href": "multiple-regression.html#sec-multipleRegressionGettingStarted",
    "title": "10  Multiple Regression",
    "section": "",
    "text": "10.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"tidyverse\")\nlibrary(\"knitr\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionOverview",
    "href": "multiple-regression.html#sec-multipleRegressionOverview",
    "title": "10  Multiple Regression",
    "section": "\n10.2 Overview of Multiple Regression",
    "text": "10.2 Overview of Multiple Regression\nMultiple regression examines the association between multiple predictor variables and one outcome variable. It allows obtaining a more accurate estimate of the unique contribution of a given predictor variable, by controlling for other variables (covariates).\nRegression with one predictor variable takes the form of Equation 10.1:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\epsilon\n\\tag{10.1}\\]\nwhere \\(y\\) is the outcome variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, \\(x_1\\) is the predictor variable, and \\(\\epsilon\\) is the error term.\nA regression line is depicted in Figure 10.4.\n\n\n\n\n\nFigure 10.1: A Regression Best-Fit Line.\n\n\nRegression with multiple predictors—i.e., multiple regression—takes the form of Equation 10.2:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon\n\\tag{10.2}\\]\nwhere \\(p\\) is the number of predictor variables.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionComponents",
    "href": "multiple-regression.html#sec-multipleRegressionComponents",
    "title": "10  Multiple Regression",
    "section": "\n10.3 Components",
    "text": "10.3 Components\n\n\n\\(B\\) = unstandardized coefficient: direction and magnitude of the estimate (original scale)\n\n\\(\\beta\\) (beta) = standardized coefficient: direction and magnitude of the estimate (standard deviation scale)\n\n\\(SE\\) = standard error: uncertainty of unstandardized estimate\n\nThe unstandardized regression coefficient (\\(B\\)) is interpreted such that, for every unit change in the predictor variable, there is a __ unit change in the outcome variable. For instance, when examining the association between age and fantasy points, if the unstandardized regression coefficient is 2.3, players score on average 2.3 more points for each additional year of age. (In reality, we might expect a nonlinear, inverted-U-shaped association between age and fantasy points such that players tend to reach their peak in the middle of their careers.) Unstandardized regression coefficients are tied to the metric of the raw data. Thus, a large unstandardized regression coefficient for two variables may mean completely different things. Holding the strength of the association constant, you tend to see larger unstandardized regression coefficients for variables with smaller units and smaller unstandardized regression coefficients for variables with larger units.\nStandardized regression coefficients can be obtained by standardizing the variables to z-scores so they all have a mean of zero and standard deviation of one. The standardized regression coefficient (\\(\\beta\\)) is interpreted such that, for every standard deviation change in the predictor variable, there is a __ standard deviation change in the outcome variable. For instance, when examining the association between age and fantasy points, if the standardized regression coefficient is 0.1, players score on average 0.1 standard deviation more points for each additional standard deviation of their year of age. Standardized regression coefficients—though not the case in all instances—tend to fall between [−1, 1]. Thus, standardized regression coefficients tend to be more comparable across variables and models compared to unstandardized regression coefficients. In this way, standardized regression coefficients provide a meaningful index of effect size.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-assumptionsRegression",
    "href": "multiple-regression.html#sec-assumptionsRegression",
    "title": "10  Multiple Regression",
    "section": "\n10.4 Assumptions of Multiple Regression",
    "text": "10.4 Assumptions of Multiple Regression\nLinear regression models make the following assumptions:\n\nthere is a linear association between the predictor variables and the outcome variable\nthere is homoscedasticity of the residuals; the residuals do not differ as a function of the predictor variables or as a function of the outcome variable\nthe residuals are independent; they are uncorrelated with each other\nthe residuals are normally distributed",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionRSquared",
    "href": "multiple-regression.html#sec-multipleRegressionRSquared",
    "title": "10  Multiple Regression",
    "section": "\n10.5 Coefficient of Determination (\\(R^2\\))",
    "text": "10.5 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions: \\(R^2 = \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\\). Various formulas for \\(R^2\\) are in Equation 8.19. Larger \\(R^2\\) values indicate greater accuracy. Multiple regression can be conceptualized with overlapping circles (similar to a venn diagram), where the non-overlapping portions of the circles reflect nonshared variance and the overlapping portions of the circles reflect shared variance, as in Figure 10.4.\n\n\n\n\n\nFigure 10.2: Conceptual Depiction of Proportion of Variance Explained (\\(R^2\\)) in an Outcome Variable (\\(Y\\)) by Multiple Predictors (\\(X1\\) and \\(X2\\)) in Multiple Regression. The size of each circle represents the variable’s variance. The proportion of variance in \\(Y\\) that is explained by the predictors is depicted by the areas in orange. The dark orange space (\\(G\\)) is where multiple predictors explain overlapping variance in the outcome. Overlapping variance that is explained in the outcome (\\(G\\)) will not be recovered in the regression coefficients when both predictors are included in the regression model. From Petersen (2024a) and Petersen (2024b).\n\n\nOne issue with \\(R^2\\) is that it increases as the number of predictors increases, which can lead to overfitting if using \\(R^2\\) as an index to compare models for purposes of selecting the “best-fitting” model. Consider the following example (adapted from Petersen (2024b)) in which you have one predictor variable and one outcome variable, as shown in Table 10.1.\n\n\n\nTable 10.1: Example Data of Predictor (x1) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\n\n\n\n7\n1\n\n\n13\n2\n\n\n29\n7\n\n\n10\n2\n\n\n\n\n\n\n\n\nUsing the data, the best fitting regression model is: \\(y =\\) 3.98 \\(+\\) 3.59 \\(\\cdot x_1\\). In this example, the \\(R^2\\) is 0.98. The equation is not a perfect prediction, but with a single predictor variable, it captures the majority of the variance in the outcome.\nNow consider the following example where you add a second predictor variable to the data above, as shown in Table 10.2.\n\n\n\nTable 10.2: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\nx2\n\n\n\n7\n1\n3\n\n\n13\n2\n5\n\n\n29\n7\n1\n\n\n10\n2\n2\n\n\n\n\n\n\n\n\nWith the second predictor variable, the best fitting regression model is: \\(y =\\) 0.00 + 4.00 \\(\\cdot x_1 +\\) 1.00 \\(\\cdot x_2\\). In this example, the \\(R^2\\) is 1.00. The equation with the second predictor variable provides a perfect prediction of the outcome.\nProviding perfect prediction with the right set of predictor variables is the dream of multiple regression. So, using multiple regression, we often add predictor variables to incrementally improve prediction. Knowing how much variance would be accounted for by random chance follows Equation 10.3:\n\\[\nE(R^2) = \\frac{K}{n-1}\n\\tag{10.3}\\]\nwhere \\(E(R^2)\\) is the expected value of \\(R^2\\) (the proportion of variance explained), \\(K\\) is the number of predictor variables, and \\(n\\) is the sample size. The formula demonstrates that the more predictor variables in the regression model, the more variance will be accounted for by chance. With many predictor variables and a small sample, you can account for a large share of the variance merely by chance.\nAs an example, consider that we have 13 predictor variables to predict fantasy performance for 43 players. Assume that, with 13 predictor variables, we explain 38% of the variance (\\(R^2 = .38; r = .62\\)). We explained a lot of the variance in the outcome, but it is important to consider how much variance could have been explained by random chance: \\(E(R^2) = \\frac{K}{n-1} = \\frac{13}{43 - 1} = .31\\). We expect to explain 31% of the variance, by chance, in the outcome. So, 82% of the variance explained was likely spurious (i.e., \\(\\frac{.31}{.38} = .82\\)). As the sample size increases, the spuriousness decreases.\nTo account for the number of predictor variables in the model, we can use a modified version of \\(R^2\\) called adjusted \\(R^2\\) (\\(R^2_{adj}\\)). Adjusted \\(R^2\\) (\\(R^2_{adj}\\)) accounts for the number of predictor variables in the model, based on how much would be expected to be accounted for by chance to penalize overfitting. Adjusted \\(R^2\\) (\\(R^2_{adj}\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictor variables in the model. The formula for adjusted \\(R^2\\) (\\(R^2_{adj}\\)) is in Equation 10.4:\n\\[\nR^2_{adj} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1}\n\\tag{10.4}\\]\nwhere \\(p\\) is the number of predictor variables in the model, and \\(n\\) is the sample size.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-overfitting",
    "href": "multiple-regression.html#sec-overfitting",
    "title": "10  Multiple Regression",
    "section": "\n10.6 Overfitting",
    "text": "10.6 Overfitting\nStatistical models applied to big data (e.g., data with many predictor variables) can overfit the data, which means that the statistical model accounts for error variance, which will not generalize to future samples. So, even though an overfitting statistical model appears to be accurate because it is accounting for more variance, it is not actually that accurate—it will predict new data less accurately than how accurately it accounts for the data with which the model was built. In the case of fantasy football analytics, this is especially relevant because there are hundreds if not thousands of variables we could consider for inclusion and many, many players when considering historical data.\nConsider an example where you develop an algorithm to predict players’ fantasy performance based on 2023 data using hundreds of predictor variables. To some extent, these predictor variables will likely account for true variance (i.e., signal) and error variance (i.e., noise). If we were to apply the same algorithm based on the 2023 prediction model to 2024 data, the prediction model would likely predict less accurately than with 2023 data. The regression coefficients in the\nIn Figure 10.3, the blue line represents the true distribution of the data, and the red line is an overfitting model:\n\nCodeset.seed(52242)\n\nsampleSize &lt;- 200\nquadraticX &lt;- rnorm(sampleSize)\nquadraticY &lt;- quadraticX ^ 2 + rnorm(sampleSize)\nquadraticData &lt;- cbind(quadraticX, quadraticY) %&gt;%\n  data.frame %&gt;%\n  arrange(quadraticX)\n\nquadraticModel &lt;- lm(\n  quadraticY ~ quadraticX + I(quadraticX ^ 2),\n  data = quadraticData)\n\nquadraticNewData &lt;- data.frame(\n  quadraticX = seq(\n    from = min(quadraticData$quadraticX),\n    to = max(quadraticData$quadraticY),\n    length.out = sampleSize))\n\nquadraticNewData$quadraticY &lt;- predict(\n  quadraticModel,\n  newdata = quadraticNewData)\n\nloessFit &lt;- loess(\n  quadraticY ~ quadraticX,\n  data = quadraticData,\n  span = 0.01,\n  degree = 1)\n\nloessNewData &lt;- data.frame(\n  quadraticX = seq(\n    from = min(quadraticData$quadraticX),\n    to = max(quadraticData$quadraticY),\n    length.out = sampleSize))\n\nquadraticNewData$loessY &lt;- predict(\n  loessFit,\n  newdata = quadraticNewData)\n\nplot(\n  x = quadraticData$quadraticX,\n  y = quadraticData$quadraticY,\n  xlab = \"\",\n  ylab = \"\")\n\nlines(\n  quadraticNewData$quadraticY ~ quadraticNewData$quadraticX,\n  lwd = 2,\n  col = \"blue\")\n\nlines(\n  quadraticNewData$loessY ~ quadraticNewData$quadraticX,\n  lwd = 2,\n  col = \"red\")\n\n\n\n\n\n\nFigure 10.3: Over-fitting Model in Red Relative to the True Distribution of the Data in Blue. From Petersen (2024a) and Petersen (2024b).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-covariates",
    "href": "multiple-regression.html#sec-covariates",
    "title": "10  Multiple Regression",
    "section": "\n10.7 Covariates",
    "text": "10.7 Covariates\nCovariates are variables that you include in the statistical model to try to control for them so you can better isolate the unique contribution of the predictor variable(s) in relation to the outcome variable. Use of covariates examines the association between the predictor variable and the outcome variable when holding people’s level constant on the covariates. Inclusion of confounds as covariates allows potentially gaining a more accurate estimate of the causal effect of the predictor variable on the outcome variable. Ideally, you want to include any and all confounds as covariates. As described in Section 7.3.2.1, confounds are third variables that influence both the predictor variable and the outcome variable and explain their association. Covariates are potentially (but not necessarily) confounds. For instance, you might include the player’s age as a covariate in a model that examines whether a player’s 40-yard dash time at the NFL Combine predicts their fantasy points in their rookie year, but it may not be a confound.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionMulticollinearity",
    "href": "multiple-regression.html#sec-multipleRegressionMulticollinearity",
    "title": "10  Multiple Regression",
    "section": "\n10.8 Multicollinearity",
    "text": "10.8 Multicollinearity\nMulticollinearity occurs when two or more predictor variables in a regression model are highly correlated. The problem of having multiple predictor variables that are highly correlated is that it makes it challenging to estimate the regression coefficients accurately.\nMulticollinearity in multiple regression is depicted conceptually in Figure 10.4.\n\n\n\n\n\nFigure 10.4: Conceptual Depiction of Multicollinearity in Multiple Regression. From Petersen (2024a) and Petersen (2024b).\n\n\nConsider the following example adapted from Petersen (2024b) where you have two predictor variables and one outcome variable, as shown in Table 10.3.\n\n\n\nTable 10.3: Example Data of Predictors (x1 and x2) and Outcome (y) Used for Regression Model.\n\n\n\n\ny\nx1\nx2\n\n\n\n9\n2.0\n4\n\n\n11\n3.0\n6\n\n\n17\n4.0\n8\n\n\n3\n1.0\n2\n\n\n21\n5.0\n10\n\n\n13\n3.5\n7\n\n\n\n\n\n\n\n\nThe second predictor variable is not very good—it is exactly twice the value of the first predictor variable; thus, the two predictor variables are perfectly correlated (i.e., \\(r = 1.0\\)). This means that there are different prediction equation possibilities that are equally good—see Equations in Equation 10.5:\n\\[\n\\begin{aligned}\n  2x_2 &= y \\\\\n  0x_1 + 2x_2 &= y \\\\\n  4x_1 &= y \\\\\n  4x_1 + 0x_2 &= y \\\\\n  2x_1 + 1x_2 &= y \\\\\n  5x_1 - 0.5x_2 &= y \\\\\n  ...\n&= y\n\\end{aligned}\n\\tag{10.5}\\]\nThen, what are the regression coefficients? We do not know what are the correct regression coefficients because each of the possibilities fits the data equally well. Thus, when estimating the regression model, we could obtain arbitrary estimates of the regression coefficients with an enormous standard error around each estimate. In general, multicollinearity increases the uncertainty (i.e., standard errors and confidence intervals) around the parameter estimates. Any predictor variables that have a correlation above ~ \\(r = .30\\) with each other could have an impact on the confidence interval of the regression coefficient. As the correlations among the predictor variables increase, the chance of getting an arbitrary answer increases, sometimes called “bouncing betas.” So, it is important to examine a correlation matrix of the predictor variables before putting them in the same regression model. You can also examine indices such as variance inflation factor (VIF).\nTo address multicollinearity, you can drop a redundant predictor or you can also use principal component analysis or factor analysis of the predictors to reduce the predictors down to a smaller number of meaningful predictors. For a meaningful answer in a regression framework that is precise and confident, you need a low level of intercorrelation among predictors, unless you have a very large sample size.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionOutliers",
    "href": "multiple-regression.html#sec-multipleRegressionOutliers",
    "title": "10  Multiple Regression",
    "section": "\n10.9 Impact of Oultiers",
    "text": "10.9 Impact of Oultiers\nAs with correlation, multiple regression can be strongly impacted by outliers.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-moderatedMultipleRegression",
    "href": "multiple-regression.html#sec-moderatedMultipleRegression",
    "title": "10  Multiple Regression",
    "section": "\n10.10 Moderated Multiple Regression",
    "text": "10.10 Moderated Multiple Regression",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionMediation",
    "href": "multiple-regression.html#sec-multipleRegressionMediation",
    "title": "10  Multiple Regression",
    "section": "\n10.11 Mediation",
    "text": "10.11 Mediation",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionBayesian",
    "href": "multiple-regression.html#sec-multipleRegressionBayesian",
    "title": "10  Multiple Regression",
    "section": "\n10.12 Bayesian Multiple Regression",
    "text": "10.12 Bayesian Multiple Regression",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionConclusion",
    "href": "multiple-regression.html#sec-multipleRegressionConclusion",
    "title": "10  Multiple Regression",
    "section": "\n10.13 Conclusion",
    "text": "10.13 Conclusion\nMultiple regression allows examining the association between multiple predictor variables and one outcome variable. Inclusion of multiple predictors in the model allows for potentially greater predictive accuracy and identification of the extent to which each variable uniquely contributes to the outcome variable. As with correlation, an association does not imply causation. However, identifying associations is important because associations are a necessary (but insufficient) condition for causality. When developing a multiple regression model, it is important to pay attention for potential multicollinearity—it may become difficult to detect a given predictor variable as statistically significant due to the greater uncertainty around the parameter estimates.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "multiple-regression.html#sec-multipleRegressionSessionInfo",
    "href": "multiple-regression.html#sec-multipleRegressionSessionInfo",
    "title": "10  Multiple Regression",
    "section": "\n10.14 Session Info",
    "text": "10.14 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] knitr_1.48        lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1    \n [5] dplyr_1.1.4       purrr_1.0.2       readr_2.1.5       tidyr_1.3.1      \n [9] tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0   petersenlab_1.0.3\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5       xfun_0.46          htmlwidgets_1.6.4  psych_2.4.6.26    \n [5] lattice_0.22-6     tzdb_0.4.0         quadprog_1.5-8     vctrs_0.6.5       \n [9] tools_4.4.1        generics_0.1.3     stats4_4.4.1       parallel_4.4.1    \n[13] fansi_1.0.6        cluster_2.1.6      pkgconfig_2.0.3    data.table_1.15.4 \n[17] checkmate_2.3.1    RColorBrewer_1.1-3 lifecycle_1.0.4    compiler_4.4.1    \n[21] munsell_0.5.1      mnormt_2.1.1       mitools_2.4        htmltools_0.5.8.1 \n[25] yaml_2.3.9         htmlTable_2.4.3    Formula_1.2-5      pillar_1.9.0      \n[29] Hmisc_5.1-3        rpart_4.1.23       nlme_3.1-164       lavaan_0.6-18     \n[33] tidyselect_1.2.1   digest_0.6.36      mvtnorm_1.2-5      stringi_1.8.4     \n[37] reshape2_1.4.4     fastmap_1.2.0      grid_4.4.1         colorspace_2.1-0  \n[41] cli_3.6.3          magrittr_2.0.3     base64enc_0.1-3    utf8_1.2.4        \n[45] pbivnorm_0.6.0     foreign_0.8-86     withr_3.0.0        scales_1.3.0      \n[49] backports_1.5.0    timechange_0.3.0   rmarkdown_2.27     nnet_7.3-19       \n[53] gridExtra_2.3      hms_1.1.3          evaluate_0.24.0    mix_1.0-12        \n[57] viridisLite_0.4.2  rlang_1.1.4        Rcpp_1.0.13        xtable_1.8-4      \n[61] glue_1.7.0         DBI_1.2.3          rstudioapi_0.16.0  jsonlite_1.8.8    \n[65] R6_2.5.1           plyr_1.8.9        \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nPetersen, I. T. (2024a). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "causal-inference.html",
    "href": "causal-inference.html",
    "title": "11  Causal Inference",
    "section": "",
    "text": "11.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceGettingStarted",
    "href": "causal-inference.html#sec-causalInferenceGettingStarted",
    "title": "11  Causal Inference",
    "section": "",
    "text": "11.1.1 Load Packages\n\nCodelibrary(\"dagitty\")\nlibrary(\"ggdag\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-correlationCausality",
    "href": "causal-inference.html#sec-correlationCausality",
    "title": "11  Causal Inference",
    "section": "\n11.2 Correlation Does Not Imply Causation",
    "text": "11.2 Correlation Does Not Imply Causation\nAs described in Section 7.3.2.1, there are several reasons why two variables, X and Y, might be correlated:\n\n\nX causes Y\n\n\nY causes X\n\n\nX and Y are bidirectional: X causes Y and Y causes X\n\na third variable (i.e., confound), Z, influences both X and Y\n\nthe association between X and Y is spurious",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-conditionsForCausality",
    "href": "causal-inference.html#sec-conditionsForCausality",
    "title": "11  Causal Inference",
    "section": "\n11.3 Criteria for Causality",
    "text": "11.3 Criteria for Causality\nHow do we know whether two processes are causally related? There are three criteria for establishing causality (Shadish et al., 2002):\n\nThe cause (e.g., the independent or predictor variable) temporally precedes the effect (i.e., the dependent or outcome variable).\nThe cause is related to (i.e., associated with) the effect.\nThere are no other alternative explanations for the effect apart from the cause.\n\nThe first criterion for establishing causality involves temporal precedence. In order for a cause to influence an effect, the cause must occur before the effect. For instance, if sports drink consumption influences player performance, the sports drink consumption (that is presumed to influence performance) must occur prior to the performance improvement. Establishing the first criterion eliminates the possibility that the association between the purported cause and effect reflects reverse causation. Reverse causation occurs when the purported effect is actually the cause of the purported cause, rather than the other way around. For instance, if sports drink consumption occurs only once, and it occurs only before and not after performance, then we have ruled out the possibility of reverse causation (i.e., that better performance causes players to consume sports drink).\nThe second criterion involves association. The purported cause must be associated with the purported effect. Nevertheless, as the maxim goes, “correlation does not imply causation.” Just because two variables are correlated does not necessarily mean that they are causally related. However, correlation is useful because causality requires that the two processes be correlated. That is, correlation is a necessary but insufficient condition for causality. For instance, if sports drink consumption influences player performance, sports drink consumption must be associated with performance improvement.\nThe third criterion involves ruling out alternative reasons why the purported cause and effect may be related. As noted in Section 11.2, there are four reasons why X may be correlated with Y. If we meet the first criterion of causality, we have removed the possibility that Y causes X (i.e., reverse causality). To meet the third criterion of causality, we need to remove the possibility that the association reflects a third variable (confound) that influences both the cause and effect, and we need to remove the possibility that the association is spurious—the possibility that the association between the purported cause and effect is due to random chance.\nThere are multiple approaches to meeting the third criterion of causality, such as by use of experiments, longitudinal designs, control variables, within-subject designs, and genetically informed designs, as described in Section 11.4.\nIn general, to meet the third criterion of causality, one must consider the counterfactual. A counterfactual is what would have happened in the hypothetical scenario that the cause did not occur [i.e., what would have happened in the absence of the cause; Shadish et al. (2002)]. When engaging in causal inference, it is important to consider what would have happened if the hypothetical cause had actually not occurred. For instance, consider that we conduct an experiment to randomly assign some players to consume a sports drink before a game and other players to drink only water. In this case, our treatment/intervention group is the group of players that consumed a sports drink. The control group is the group players that drank only water. Now, consider that the players in the treatment group outperform the players in the the control group in their football game. In such a study, we observe what did happen when players received a treatment. The counterfactual is knowledge of what would have happened to those same players if they simultaneously had not received treatment (Shadish et al., 2002). The true causal effect, then, is the difference between what did happen and what would have happened. However, we cannot observe a counterfactual. That is, we do not know for sure what would have happened to the players who received treatment if those same players had actually not received treatment. We have a control group, but the control group does not have the same players as the intervention group, and it is impossible for a person to simultaneously receive and not receive treatment.\nSo, our goal in working toward causal inference as scientists is to create reasonable approximations to this impossible counterfactual (Shadish et al., 2002). For instance, if using a between-subject design, we want the two groups to be equivalent in every possible way except whether or not they receive the treatment, so we might stratify each group to be equivalent in terms of age, weight, position, experience, skill, etc. Or, we might test the same people using an A-B-A-B within-subject design. In an A-B-A-B within-subject design, players receive no treatment at baseline (timepoint 1: game 1), receive the treatment at timepoint 2 (game 2), receive no treatment at timepoint 3 (game 3), and receive the treatment at timepoint 4 (game 4). Neither of these approximations is a true counterfactual. In the between-subject design, the players differ between the two groups, so we cannot know how the individuals who received the treatment would have performed if they had actually not received the treatment. In the A-B-A-B within-subject design, the players are the same, but they timepoints that they receive or do not receive the treatment differ, and there can be carryover effects from one condition to the next. For instance, consuming sports drinks before game 2 might also help them be better hydrated in general, including, for subsequent games. Thus, we cannot know how a player would have performed in game 1 with treatment or in game 2 without treatment, etc. Nevertheless, it is important to be aware of the counterfactual and to engage in counterfactual reasoning to consider what would have happened if the supposed cause had not occurred. Considering the counterfactual is important for designing closer approximations to the counterfactual in studies for stronger research designs and stronger causal inference.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-approachesCausalInference",
    "href": "causal-inference.html#sec-approachesCausalInference",
    "title": "11  Causal Inference",
    "section": "\n11.4 Approaches for Causal Inference",
    "text": "11.4 Approaches for Causal Inference\n\n11.4.1 Experimental Designs\nAs described in Section 7.3.1, experimental designs are designs in which participants are randomly assigned to one or more levels of the independent variable to observe its effects on the dependent variable. Experimental designs provide the strongest tests of causality because they can rule out reverse causation and third variables. For instance, by manipulating sports drink consumption before the player performs, they can eliminate the possibility that reverse causation explains the effect of the independent variable on the dependent variable. Second, through randomly assigning players to consume or not consume sports drink, this holds everything else constant (so long as the groups are evenly distributed according to other factors, such as their age, weight, etc.) and thus removes the possibility that third variable confounds explain the effect of the independent variable on the dependent variable.\n\n11.4.2 Quasi-Experimental Designs\nAlthough experimental designs provide the strongest tests of causality, manytimes they are impossible, unethical, or impractical to conduct. For instance, it would likely not be practical to randomly assign National Football League (NFL) players to either consume or not consume sports drink before their games. Players have their pregame rituals and routines and many would likely not agree to participate in such a study. Thus, we often rely on quasi-experimental designs such as natural experiments and observational/correlational designs.\nWe cannot directly test or establish causality from a non-experimental research design. Nevertheless, we can leverage various design features that, in combination with other studies using different research methods, collectively strengthen our ability to make causal inferences. For instance, there are are no experiments in humans showing that smoking causes cancer—randomly assigning people to smoke or not smoke would not be ethical. The causal inference that smoking causes cancer was derived from a combination of experimental studies in rodents and observational studies in humans.\n\n11.4.2.1 Longitudinal Designs\nResearch designs can be compared in terms of their internal validity—the extent to which we can be confident about causal inferences. A cross-sectional association is depicted in Figure 11.1:\n\n\n\n\n\nFigure 11.1: Cross-Sectional Association. T1 = Timepoint 1. From Petersen (2024a) and Petersen (2024b).\n\n\nFor instance, we might observe that sports drink consumptions is concurrently associated with better player performance. Among observational/correlational research designs, cross-sectional designs tend to have the weakest internal validity. For the reasons described in Section 11.2, if we observe a cross-sectional association between X (e.g., sports drink consumption) and Y (e.g., player performance), we have little confidence that X causes Y. As a result, longitudinal designs can be valuable for more closely approximating causality if an experimental designs is not possible. Consider a lagged association that might be observed in a longitudinal design, as in Figure 11.2, which is a slightly better approach than relying on cross-sectional associations:\n\n\n\n\n\nFigure 11.2: Lagged Association. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024a) and Petersen (2024b).\n\n\nFor instance, we might observe that sports drink performance before the game is associated with better player performance during the game. A lagged association has somewhat better internal validity than a cross-sectional association because we have greater evidence of temporal precedence—that the influence of the predictor precedes the outcome because the predictor was assessed before the outcome and it shows a predictive association. However, part of the association between the predictor with later levels of the outcome could be due to prior levels of the outcome that are stable across time. That is, it could be that better player performance leads players to consume more sports drink and that player performance is relatively stable across time. In such a case, it may be observed that sports drink consumption predicts later player performance even though player performance influences sports drink consumption, rather than the other way around Thus, consider an even stronger alternative—a lagged association that controls for prior levels of the outcome, as in Figure 11.3:\n\n\n\n\n\nFigure 11.3: Lagged Association, Controlling for Prior Levels of the Outcome. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024a) and Petersen (2024b).\n\n\nFor instance, we might observe that sports drink performance before the game is associated with better player performance during the game, while controlling for prior player performance. A lagged association controlling for prior levels of the outcome has better internal validity than a lagged association that does not control for prior levels of the outcome. A lagged association that controls for prior levels further reduces the likelihood that the association owes to the reverse direction of effect, because earlier levels of the outcome are controlled. However, consider an even stronger alternative—lagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect, as depicted in Figure 11.4:\n\n\n\n\n\nFigure 11.4: Lagged Association, Controlling for Prior Levels of the Outcome, Simultaneously Testing Both Directions Of Effect. T1 = Timepoint 1. T2 = Timepoint 2. From Petersen (2024a) and Petersen (2024b).\n\n\nLagged associations that control for prior levels of the outcome and that simultaneously test each direction of effect provide the strongest internal validity among observational/correlational designs. Such a design can help better clarify which among the variables is the chicken and the egg—which variable is more likely to be the cause and which is more likely to be the effect. If there are bidirectional effects, such a design can also help clarify the magnitude of each direction of effect. For instance, we can simultaneously evaluate the extent to which sports drink predicts later player performance (while controlling for prior performance) and the reverse—player performance predicting later sports drink consumption (while contorlling for prior sports drink consumption).\n\n11.4.2.2 Within-Subject Analyses\nAnother design feature of longitudinal designs that can lead to greater internal validity is the use of within-subject analyses. Between-subject analyses, might examine, for instanc, whether players who consume more sports drink perform better on average compared to players who consume less sports drink. However, there are other between-person differences that could explain any observed between-subject associations between sports drink consumption and players performance. Another approach could be to apply within-subject analyses. For instance, you could examining whether, within the same individual, if a player consumes a sports drink, do they perform better compared to games in which they did not consume a sports drink. When we control for prior levels of the outcome in the prediction, we are evaluating whether the predictor is associated with witin-person change in the outcome. Predicting within-person change provides stronger evidence consistent with causality because it uses the individual as their own control and controls for many time-invariant confounds (i.e., confounds that do not change across time). However, predicting within-person change does not, by itself, control for time-varying confounds. So, it can also be useful to control for time-varying confounds, such as by use of control variables.\n\n11.4.2.3 Control Variables\nOne of the plausible alternatives to the inference that X causes Y is that there are third variable confounds that influence both X and Y, thus explaining why X and Y are associated, as depicted in Figures 7.3 and 11.10. Thus, another approach that can help increase internal validity is to include plausible confounds as control variables. For instance, if a third variable such as education level might be a confound that influences both sports drink consumption and player performance, you could include education level as a covariate in the model. Inclusion of a covariate attempts to control for the variable by examining the association between the predictor variable and the outcome variable while holding the covariate variables constant. For instance, such a model would examine whether, when accounting for education level, there is an association between sports drink consumption and player performance.\nFailure to control for important third variables can lead to erroneous conclusions, as evidenced by the association depicted in Figure 11.5. In the example, if we did not control for gender, we would infer that there is a positive association between dosage and recovery probability. However, when we examine each men and women separately, we learn that the association between dosage and recovery probability is actually negative within each gender group. Thus, in this case, failure to control for gender would lead to false inferences about the association between dosage and recovery probability.\n\n\n\n\n\nFigure 11.5: Example Where Failing to Control for a Variable (In This Case, Gender) Would Lead to False Inferences. In this example, the association between dosage and recovery probability is positive at the population level, but the association is negative among men and women separately. (Figure reprinted from Kievit et al. (2013), Figure 1, p. 2. Kievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013). Simpson’s paradox in psychological science: a practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513)\n\n\nHowever, it can be problematic to control for variables indiscriminantly. The use of causal diagrams can inform which variables are important to be included as control variables, and—just as important—which variables not to include as control variables, as described in Section 11.5.\n\n11.4.2.4 Genetically Informed Designs\nAnother approach to control for variables is to use genetically informed designs. Genetically informed designs allow controlling for potential genetic effects in order to more closely approximate the contributions of various environmental effects. Genetically informed designs exploit differing degrees of genetic relatedness among participants to capture the extent to which genetic factors may contribute to an outcome. The average percent of DNA shared between people of varying relationships is provided in Table 11.1 (https://isogg.org/wiki/Autosomal_DNA_statistics; archived at https://perma.cc/MK3D-DST8):\n\n\nTable 11.1: Average Percent of Autosomal DNA Shared by Pairs of Relatives by Relationship Type.\n\n\n\n\n\n\n\nRelationship\nAverage Percent of Autosomal DNA Shared by Pairs of Relatives\n\n\n\nMonozygotic (“identical”) twins\n100%\n\n\nDizygotic (“fraternal”) twins\n50%\n\n\nParent/child\n50%\n\n\nFull siblings\n50%\n\n\nGrandparent/grandchild\n25%\n\n\nAunt-or-uncle/niece-or-nephew\n25%\n\n\nHalf-siblings\n25%\n\n\nFirst cousin\n12.5%\n\n\nGreat-grandparent/great-grandchild\n12.5%\n\n\n\n\n\n\nFor instance, researchers may compare monozygotic twins versus dizygotic twins in some outcome—a so-called “twin study”. It is assumed that the trait/outcome is attributable to genetic factors to the extent that the monozygotic twins (who share 100% of their DNA) are more similar in the trait or outcome compared to the dizygotic twins (who share on average 50% of their DNA). Alternatively, researchers could compare full siblings versus half-siblings, or they could compare full siblings versus first cousins.\nGenetically informed designs are not as relevant for fantasy football analytics, but they are useful to present as one of various design features that researchers can draw upon to strengthen their ability to make causal inferences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalDiagrams",
    "href": "causal-inference.html#sec-causalDiagrams",
    "title": "11  Causal Inference",
    "section": "\n11.5 Causal Diagrams",
    "text": "11.5 Causal Diagrams\n\n11.5.1 Overview\nA key tool when describing a research question or hypothesis is to create a conceptual depiction of the hypothesized causal processes. A causal diagram depicts the hypothesized causal processes that link two or more variables. A common form of causal diagrams is the directed acyclic graph (DAG). DAGs provide a helpful tool to communicate about causal questions and help identify how to avoid bias (i.e., over-estimation) in associations between variables due to confounding (i.e., common causes) (Digitale et al., 2022). For instance, from a DAG, it is possible to determine what variables it is important to control for in order to get unbiased estimates of the association between two variables of interest. To create DAGs, you can use the R package dagitty (Textor et al., 2017) or the associated browser-based extension, DAGitty: https://dagitty.net (archived at https://perma.cc/U9BY-VZE2). Examples of various causal diagrams that could explain why X is associated with Y are in Figures 11.6, 11.8 and 11.10.\n\nCodeXCausesY &lt;- dagitty::dagitty(\"dag{\n  X -&gt; Y\n}\")\n\nplot(dagitty::graphLayout(XCausesY))\n\ndagitty::impliedConditionalIndependencies(XCausesY)\n\n\n\n\n\n\nFigure 11.6: Causal Diagram (Directed Acyclic Graph) Depicting X Causing Y.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodeXCausesY_alt &lt;- ggdag::dagify(\n  Y ~ X\n)\n\n#plot(XCausesY_alt) # this creates the same plot as above\nggdag::ggdag(XCausesY_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 11.7: Causal Diagram (Directed Acyclic Graph) Depicting X Causing Y.\n\n\n\n\n\nCodeYCausesX &lt;- dagitty::dagitty(\"dag{\n  Y -&gt; X\n}\")\n\nplot(dagitty::graphLayout(YCausesX))\n\ndagitty::impliedConditionalIndependencies(YCausesX)\n\n\n\n\n\n\nFigure 11.8: Causal Diagram (Directed Acyclic Graph) Depicting Reverse Causation: Y Causing X.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodeYCausesX_alt &lt;- ggdag::dagify(\n  X ~ Y\n)\n\n#plot(YCausesX_alt) # this creates the same plot as above\nggdag::ggdag(YCausesX_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 11.9: Causal Diagram (Directed Acyclic Graph) Depicting Reverse Causation: Y Causing X.\n\n\n\n\n\nCodeZCausesXandY &lt;- dagitty::dagitty(\"dag{\n  Z -&gt; Y\n  Z -&gt; X\n  X &lt;-&gt; Y\n}\")\n\nplot(dagitty::graphLayout(ZCausesXandY))\n\n\n\n\n\n\nFigure 11.10: Causal Diagram (Directed Acyclic Graph) Depicting a Third Variable Confound, Z, Causing X and Y, Thus Explaining Why X and Y are associated.\n\n\n\n\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodeZCausesXandY_alt &lt;- ggdag::dagify(\n  X ~ Z,\n  Y ~ Z,\n  X ~~ Y\n)\n\n#plot(ZCausesXandY_alt) # this creates the same plot as above\nggdag::ggdag(ZCausesXandY_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 11.11: Causal Diagram (Directed Acyclic Graph) Depicting a Third Variable Confound, Z, Causing X and Y, Thus Explaining Why X and Y are associated.\n\n\n\n\nConsider another example in Figure 11.12:\n\nCodemediationDag &lt;- dagitty::dagitty(\"dag{\n  X -&gt; M1\n  X -&gt; M2\n  M1 -&gt; Y\n  M2 -&gt; Y\n  M1 &lt;-&gt; M2\n}\")\n\nplot(dagitty::graphLayout(mediationDag))\n\n\n\n\n\n\nFigure 11.12: Causal Diagram (Directed Acyclic Graph).\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(mediationDag)\n\nX _||_ Y | M1, M2\n\nCodedagitty::adjustmentSets(\n  mediationDag, \n  exposure = \"M1\",\n  outcome = \"Y\",\n  effect = \"total\")\n\n{ M2 }\n\n\nIn this example, X influences Y via M1 and M2 (i.e., multiple mediators), and M1 is also associated with M2. The dagitty::impliedConditionalIndependencies() function identifies variables in the causal diagram that are conditionally independent (i.e., uncorrelated) after controlling for other variables in the model. For this causal diagram, X is conditionally independent with Y because X is independent with Y when controlling for M1 and M2.\nThe dagitty::adjustmentSets() function identifies variables that would be necessary to control for (i.e., to include as covariates) in order to identify an unbiased estimate of the association (whether the total effect, i.e., effect = \"total\"; or the direct effect, i.e., effect = \"direct\") between two variables (exposure and outcome). In this case, to identify the unbiased association between M1 and Y, it is important to control for M2.\nHere is an alternative way of specifying the same diagram (more similar to lavaan syntax):\n\nCodemediationDag_alt &lt;- ggdag::dagify(\n  M1 ~ X,\n  M2 ~ X,\n  Y ~ M1,\n  Y ~ M2,\n  M1 ~~ M2\n)\n\n#plot(mediationDag_alt) # this creates the same plot as above\nggdag::ggdag(mediationDag_alt) + theme_dag_blank()\n\n\n\n\n\n\nFigure 11.13: Causal Diagram (Directed Acyclic Graph).\n\n\n\n\n\n11.5.2 Confounding\nConfounding occurs when two variables—that are both caused by another variable(s)—have a spurious or noncausal association (D’Onofrio et al., 2020). That is, two variables share a common cause, and the common cause leads the variables to be associated even though they are not causally related. The common cause—i.e., the variable that influences the two variables—is known as a confound (or confounder). An example of confounding is depicted in Figure 11.14:\n\nCodeconfounding &lt;- ggdag::confounder_triangle(\n  x = \"Player Endurance\",\n  y = \"Field Goals Made\",\n  z = \"Stadium Altitude\") \n\nconfounding %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 11.14: Causal Diagram (Directed Acyclic Graph) Example of Confounding.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(confounding)\n\nx _||_ y | z\n\n\nThe output indicates that player endurance (X) and field goals made (Y) are conditionally independent when accounting for stadium altitude (Z). Conditional independence refers to two variables being unassociated when controlling for other variables.\n\nCodedagitty::adjustmentSets(\n  confounding, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n{ z }\n\n\nThe output indicates that, to obtain an unbiased estimate of the causal association between two variables, it is necessary to control for any confounding (Lederer et al., 2019). That is, to obtain an unbiased estimate of the causal association between player endurance (X) and field goals made (Y), it is necessary to control for stadium altitude (Z).\n\n11.5.3 Mediation\nMediation can be divided into two types: full and partial. In full mediation, the mediator(s) fully account for the effect of the predictor variable on the outcome variable. In partial mediation, the mediator(s) partially but do not fully account for the effect of the predictor variable on the outcome variable.\n\n11.5.3.1 Full Mediation\nAn example of full mediation is depicted in Figure 11.15:\n\nCodefull_mediation &lt;- ggdag::mediation_triangle(\n  x = \"Coaching Quality\",\n  y = \"Fantasy Points\",\n  m = \"Player Preparation\")\n\nfull_mediation %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 11.15: Causal Diagram (Directed Acyclic Graph) Example of Full Mediation.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(full_mediation)\n\nx _||_ y | m\n\n\nIn full mediation, X and Y are conditionally independent when accounting for the mediator (Z). In this case, coaching quality (X) and fantasy points (Y) are conditionally independent when accounting for player preparation (M). In other words, in this example, player preparation is the mechanism that fully (i.e., 100%) accounts for the effect of coaching quality on players’ fantasy points.\n\nCodedagitty::adjustmentSets(\n  full_mediation, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"direct\")\n\n{ m }\n\n\nThe output indicates that, to obtain an unbiased estimate of the direct causal association between coaching quality (X) and fantasy points (Y) (i.e., the effect that is not mediated through intermediate processes), it is necessary to control for player preparation (M).\n\nCodedagitty::adjustmentSets(\n  full_mediation, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nHowever, to obtain an unbiased estimate of the total causal association between coaching quality (X) and fantasy points (Y) (i.e., including the portion of the effect that is mediated through intermediate processes), it is important not to control for player preparation (M). When the goal is to understand the (total) causal effect of coaching quality (X) and fantasy points (Y), controlling for the mediator (player preparation; M) would be inappropriate because doing so would remove the causal effect, thus artificially reducing the estimate of the association between coaching quality (X) and fantasy points (Y) (Lederer et al., 2019).\n\n11.5.3.2 Partial Mediation\nAn example of partial mediation is depicted in Figure 11.16:\n\nCodepartial_mediation &lt;- ggdag::mediation_triangle(\n  x = \"Coaching Quality\",\n  y = \"Fantasy Points\",\n  m = \"Player Preparation\",\n  x_y_associated = TRUE)\n\npartial_mediation %&gt;%\n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 11.16: Causal Diagram (Directed Acyclic Graph) Example of Partial Mediation.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(partial_mediation)\n\n\nIn partial mediation, X and Y are not conditionally independent when accounting for the mediator (Z). In this case, coaching quality (X) and fantasy points (Y) are still associated when accounting for player preparation (M). In other words, in this example, player preparation is a mechanism that partially but does not fully account for the effect of coaching quality on players’ fantasy points. That is, there are likely other mechanisms, in addition to player preparation, that collectively account for the effect of coaching quality on fantasy points. For instance, coaching quality could also influence player fantasy points through better play-calling.\n\nCodedagitty::adjustmentSets(\n  partial_mediation, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"direct\")\n\n{ m }\n\n\nAs with full mediation, the output indicates that, to obtain an unbiased estimate of the direct causal association between coaching quality (X) and fantasy points (Y) (i.e., the effect that is not mediated through intermediate processes), it is necessary to control for player preparation (M).\n\nCodedagitty::adjustmentSets(\n  partial_mediation, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nHowever, as with full mediation, to obtain an unbiased estimate of the total causal association between coaching quality (X) and fantasy points (Y) (i.e., including the portion of the effect that is mediated through intermediate processes), it is important not to control for player preparation (M). When the goal is to understand the (total) causal effect of coaching quality (X) and fantasy points (Y), controlling for a mediator (player preparation; M) would be inappropriate because doing so would remove the causal effect, thus artificially reducing the estimate of the association between coaching quality (X) and fantasy points (Y) (Lederer et al., 2019).\n\n11.5.4 Collider Bias\nCollision occurs when two variables influence a third variable, the collider (D’Onofrio et al., 2020). That is, a collider is a variable that is caused by two other variables (i.e., a common effect). An example collision is depicted in Figures 11.17 and 11.18:\n\nCodecolliderBias1 &lt;- ggdag::collider_triangle(\n  x = \"Diet\",\n  y = \"Coaching Strategy\",\n  m = \"Injury Status\")\n\ncolliderBias1 %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 11.17: Causal Diagram (Directed Acyclic Graph) Example of a Collision with a Collider (Injury Status).\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(colliderBias1)\n\nx _||_ y\n\n\nIn this example collision, diet (X) and coaching strategy (Y) are independent.\n\nCodedagitty::adjustmentSets(\n  colliderBias1, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nAs the output indicates, we should not control for the collider when examining the association between the two causes of the collider. That is, we should not control for injury status (M) when examining the association between diet (X) and coaching strategy. Controlling for the collider leads to confounding and can artificially induce an association between the two causes of the collider despite no causal association between them (Lederer et al., 2019). Obtaining a distorted or artificial association between two variables due to inappropriately controlling for a collider is known as collider bias.\nConsider another example:\n\nCodecolliderBias2 &lt;- ggdag::collider_triangle(\n  x = \"Coaching Quality\",\n  y = \"Player Preparation\",\n  m = \"Fantasy Points\",\n  x_y_associated = TRUE)\n\ncolliderBias2 %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 11.18: Causal Diagram (Directed Acyclic Graph) Example of Collider Bias.\n\n\n\n\n\nCodedagitty::impliedConditionalIndependencies(colliderBias2)\n\n\nIn this example of collider bias, there are no conditional independencies.\n\nCodedagitty::adjustmentSets(\n  colliderBias2, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nAgain, it would be important not to control for the collider, fantasy points (M), when examining the association between coaching quality (X) and player preparation (Y). In this case, controlling for the collider would remove some of the causal effect of coaching quality on player preparation and could lead to an artificially smaller estimate of the causal effect between coaching quality and player preparation.\n\n11.5.4.1 M-Bias\nCollider bias may also occur when neither variable of interest is a direct cause of the collider (Lederer et al., 2019). M-bias is a form of collider bias that occurs when two variables that are not causally related, A and B, both influence a collider, M, and each (A and B) also influences a separate variable—e.g., A influences X and B influences Y. M-bias is so-named from the M-shape of the DAG. An example of M-bias is depicted in Figure 11.19:\n\nCodemBias &lt;- ggdag::m_bias(\n  x = \"Number of Media Articles About the Team\",\n  y = \"Fantasy Points\",\n  a = \"Team Record\",\n  b = \"Coaching Quality\",\n  m = \"Fan Attendance at Game\")\n\nmBias %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 11.19: Causal Diagram (Directed Acyclic Graph) Example of M-Bias.\n\n\n\n\nIn this example, fan attendance is the collider that is influenced separately by the team record and the coaching quality. This is a fictitious example for purposes of demonstration; in reality, coaching quality influences the team’s record.\n\nCodedagitty::impliedConditionalIndependencies(mBias)\n\na _||_ b\na _||_ y\nb _||_ x\nm _||_ x | a\nm _||_ y | b\nx _||_ y\n\n\nAs the output indicates, there are several conditional independencies.\n\nCodedagitty::adjustmentSets(\n  mBias, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n {}\n\n\nIt is important not to control for the collider (fan attendance). If you control for the collider, you can induce an artificial association between team record and coaching quality. Moreover, because doing so induces an artificial association between team record and coaching quality, it can also induce an artificial association between the effects of team record and coaching quality: number of media articles about the team and fantasy points, respectively. That is, controlling for the collider can lead to an artificial association between X and Y that does not reflect a causal process.\n\n11.5.4.2 Butterfly Bias\nButterfly bias occurs when both confounding and M-bias are present. Butterfly bias (aka bow-tie bias) is so-named from the butterfly shape of the DAG. In butterfly bias, the following criteria are met:\n\nTwo variables (A and B) influence a collider (M).\nThe collider influences two variables, X and Y.\n\nA also influences X.\n\nB also influences Y.\n\nA and B are not causally related.\n\nX and Y are not causally related.\n\nOr, more succinctly:\n\n\nA influences M and X.\n\nB influences M and Y.\n\nM influences X and Y.\n\nIn butterfly bias, the collider (M) is also a confound. That is, a variable is both influenced by two variables and influences two variables. An example of butterfly bias is depicted in Figure 11.20:\n\nCodebutterflyBias &lt;- ggdag::butterfly_bias(\n  x = \"Off-field Behavior\",\n  y = \"Fantasy Points\",\n  a = \"Diet\",\n  b = \"Coaching Quality\",\n  m = \"Mental Health\")\n\nbutterflyBias %&gt;% \n  ggdag(\n    text = FALSE,\n    use_labels = \"label\") + \n  theme_dag_blank()\n\n\n\n\n\n\nFigure 11.20: Causal Diagram (Directed Acyclic Graph) Example of Butterfly Bias.\n\n\n\n\nIn this case, players’ mental health is a collider of their diet and the quality of the coaching they receive. In addition, players’ mental health is a confound of their off-field behavior and fantasy points.\n\nCodedagitty::impliedConditionalIndependencies(butterflyBias)\n\na _||_ b\na _||_ y | b, m\nb _||_ x | a, m\nx _||_ y | b, m\nx _||_ y | a, m\n\n\nAs the output indicates, there are several conditional independencies.\n\nCodedagitty::adjustmentSets(\n  butterflyBias, \n  exposure = \"x\",\n  outcome = \"y\",\n  effect = \"total\")\n\n{ b, m }\n{ a, m }\n\n\nWhen dealing with a collider that is also a confound, controlling for either set, B and M or A and M, will provide an unbiased estimate of the association between X and Y. In this case, controlling for either a) coaching quality and mental health or b) diet and mental health—but not both sets—will yield an unbiased estimate of the association between off-field behavior and fantasy points.\n\n11.5.5 Selection Bias\nSelection bias occurs when the selection of participants or their inclusion in analyses depends on the variables being studied. For instance, if you are conducting a study on the extent to which sports drink consumption influences fantasy points, there would be selection bias if players are less likely to participate in the study if they score fewer fantasy points.\nNow, consider a study in which you conduct a randomized controlled trial (RCT; i.e., an experiment) to evaluate the effect of a new medication on player performance. You randomly assign some players to take the medication and other players to take a placebo. Assume the new medication has side effects and leads many of the players who take it to drop out of the study. This is an example of attrition bias (i.e., systematic attrition). If you were to exclude these individuals from your analysis, it may make it appear that the medication led to better performance, because the players who experienced the side effect (and worse performance) dropped out of the study. Hence, conducting an analysis that excludes these players from the analysis would involve selection bias.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceConclusion",
    "href": "causal-inference.html#sec-causalInferenceConclusion",
    "title": "11  Causal Inference",
    "section": "\n11.6 Conclusion",
    "text": "11.6 Conclusion\nThere are three criteria for establishing causality: 1) the cause precedes the effect. 2) The cause is related to the effect. 3) There are no other alternative explanations for the effect apart from the cause. In general, it is important to be aware of the counterfactual and to consider what would have happened if the supposed cause had not occurred. Various experimental and quasi-experimental designs and approaches can be leveraged to more closely approximate causal inferences. Longitudinal designs, within-subject analyses, inclusion of control variables, and genetically informed designs are all quasi-experimental designs that afford the researcher greater control over some possible third variable confounds. Causal diagrams can be a useful tool for identifying the proper variables to control for (and those not to control for). When confounding exists, it is important to control for the confound(s). It is important not to control for mediators when interested in the total effect of the predictor variable on the outcome variable. When there is a collision, it is important not to control for the collider unless the collider is also a confound.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "causal-inference.html#sec-causalInferenceSessionInfo",
    "href": "causal-inference.html#sec-causalInferenceSessionInfo",
    "title": "11  Causal Inference",
    "section": "\n11.7 Session Info",
    "text": "11.7 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggdag_0.2.12  dagitty_0.3-4\n\nloaded via a namespace (and not attached):\n [1] viridis_0.6.5      utf8_1.2.4         generics_0.1.3     tidyr_1.3.1       \n [5] stringi_1.8.4      digest_0.6.36      magrittr_2.0.3     evaluate_0.24.0   \n [9] grid_4.4.1         fastmap_1.2.0      jsonlite_1.8.8     ggrepel_0.9.5     \n[13] gridExtra_2.3      purrr_1.0.2        fansi_1.0.6        viridisLite_0.4.2 \n[17] scales_1.3.0       tweenr_2.0.3       cli_3.6.3          rlang_1.1.4       \n[21] graphlayouts_1.1.1 polyclip_1.10-6    tidygraph_1.3.1    munsell_0.5.1     \n[25] withr_3.0.0        cachem_1.1.0       yaml_2.3.9         tools_4.4.1       \n[29] memoise_2.0.1      dplyr_1.1.4        colorspace_2.1-0   ggplot2_3.5.1     \n[33] boot_1.3-30        curl_5.2.1         vctrs_0.6.5        R6_2.5.1          \n[37] lifecycle_1.0.4    stringr_1.5.1      V8_4.4.2           htmlwidgets_1.6.4 \n[41] MASS_7.3-60.2      ggraph_2.2.1       pkgconfig_2.0.3    pillar_1.9.0      \n[45] gtable_0.3.5       glue_1.7.0         Rcpp_1.0.13        ggforce_0.4.2     \n[49] xfun_0.46          tibble_3.2.1       tidyselect_1.2.1   knitr_1.48        \n[53] farver_2.1.2       htmltools_0.5.8.1  igraph_2.0.3       labeling_0.4.3    \n[57] rmarkdown_2.27     compiler_4.4.1    \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nD’Onofrio, B. M., Sjölander, A., Lahey, B. B., Lichtenstein, P., & Öberg, A. S. (2020). Accounting for confounding in observational studies. Annual Review of Clinical Psychology, 16(1), 25–48. https://doi.org/10.1146/annurev-clinpsy-032816-045030\n\n\nDigitale, J. C., Martin, J. N., & Glymour, M. M. (2022). Tutorial on directed acyclic graphs. Journal of Clinical Epidemiology, 142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001\n\n\nKievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513\n\n\nLederer, D. J., Bell, S. C., Branson, R. D., Chalmers, J. D., Marshall, R., Maslove, D. M., Ost, D. E., Punjabi, N. M., Schatz, M., Smyth, A. R., Stewart, P. W., Suissa, S., Adjei, A. A., Akdis, C. A., Azoulay, É., Bakker, J., Ballas, Z. K., Bardin, P. G., Barreiro, E., … Vincent, J.-L. (2019). Control of confounding and reporting of results in causal inference studies. Guidance for authors from editors of respiratory, sleep, and critical care journals. Annals of the American Thoracic Society, 16(1), 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nPetersen, I. T. (2024a). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin.\n\n\nTextor, J., Zander, B. van der, Gilthorpe, M. S., Liśkiewicz, M., & Ellison, G. T. (2017). Robust causal inference using directed acyclic graphs: The R package “dagitty”. International Journal of Epidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Causal Inference</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html",
    "href": "cognitive-bias.html",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "",
    "text": "12.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasGettingStarted",
    "href": "cognitive-bias.html#sec-cognitiveBiasGettingStarted",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "",
    "text": "12.1.1 Load Packages\n\nCodelibrary(\"tidyverse\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasOverview",
    "href": "cognitive-bias.html#sec-cognitiveBiasOverview",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "\n12.2 Overview",
    "text": "12.2 Overview\nWhen considering judgment and prediction, it is important to consider psychological concepts, including heuristics and cognitive biases. In the modern world of big data, research and society need people who know how to make sense of the information around us. Given humans’ cognitive biases, it is valuable to leverage more objective approaches than relying on our “gut” and intutition. Statistical approaches can be a more objective way to identify systematic patterns.\nStatistical analysis—and science more generally—is a process to the pursuit of knowledge. An epistemology is an approach to knowledge. Science is perhaps the best approach (epistemology) that society has to approximate truth. Unlike other approaches to knowledge, science relies on empirical evidence and does not give undue weight to anecdotal evidence, intuition, tradition, or authority.\nPer Petersen (2024), here are the characteristics of science that distinguish it from pseudoscience:\n\n\nRisky hypotheses are posed that are falsifiable. The hypotheses can be shown to be wrong.\nFindings can be replicated independently by different research groups and different methods. Evidence converges across studies and methods.\nPotential alternative explanations for findings are specified and examined empirically (with data).\nSteps are taken to guard against the undue influence of personal beliefs and biases.\nThe strength of claims reflects the strength of evidence. Findings and the ability to make judgments or predictions are not overstated. For instance, it is important to present the degree of uncertainty from assessments with error bars or confidence intervals.\nScientifically supported measurement strategies are used based on their psychometrics, including reliability and validity.\n\n\nNevertheless, statistical analysis is not purely objective and is not a panacea. Science is a human enterprise—it is performed by humans each of whom has their own biases. For instance, cognitive biases can influence how people interpret statistics. As a result, the findings from any given study may be incorrect. Thus, it would be imprudent to make decisions based solely on the results of one study. That is why we wait for findings to be independently replicated by different groups of researchers using different methods.\nIf a research team publishes flashy new and exciting findings, other researchers have an incentive to disprove the prior findings. Thus, we have more confidence if findings stand up to scrutiny from independent groups of researchers. We also draw upon meta-analyses—studies of many studies, to summarize the results of many studies and not just the findings from any single study that may not replicate. In this way, we can identify which findings are robust and most likely true versus the findings that fail to replicate. Thus, despite its flaws like any other human enterprise, science is a self-correcting process in which the long arc bends toward truth.\nIn our everyday lives, humans are presented with overwhelming amounts of information. Because human minds cannot parse every piece of information equally, we tend to take mental shortcuts, called heuristics. These mental shortcuts can be helpful. They reduce our mental load and can help us make quick judgments to stay alive or to make complex decisions in the face of uncertainty. However, these mental shortcuts can also lead us astray and to make systematic errors in our judgments and predictions. Cognitive biases are systematic errors in thinking. Fallacies are forms of flawed reasoning.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-heuristics",
    "href": "cognitive-bias.html#sec-heuristics",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "\n12.3 Examples of Heuristics",
    "text": "12.3 Examples of Heuristics\nAs described above, heuristics are mental shortcuts that people use to handle the overwhelming amount of information to process. Three important heuristics used in judgment and prediction in the face of uncertainty include (Tversky & Kahneman, 1974):\n\navailability heuristic\nrepresentativeness heuristic\nanchoring and adjustment heuristic\n\n\n12.3.1 Availability Heuristic\nThe availability heuristic refers to the tendency for a person’s judgments or predictions about the frequency or probability of something to be made based on how readily instances can be brought to mind. For instance, when making fantasy predictions about a player, more recent big performance games may more easily come to mind compared to lower-scoring games and games that occurred longer ago. Thus, a manager may be more inclined to pick players to start who had more recent, stronger performances rather than players who have higher long-term averages.\n\n12.3.2 Representativeness Heuristic\nThe representativess heuristic refers to the tendency for a person’s judgments or predictions about individuals to be made based on how similar the individual is to the person’s existing mental prototypes. For instance, when coming out of college, Tight End Kyle Pitts drew comparisons to the “LeBron James” of Tight Ends (archived at https://perma.cc/JQB5-XPVL). The idea that his athletic profile leads him to be similar to the prototype of LeBron James may have led him to be too highly drafted by fantasy managers in his first seasons.\nHere is a video of Kyle Pitts drawing comparisons to the LeBron James of Tight Ends:\n\nVideo\nVideo of Kyle Pitts drawing comparisons to the LeBron James of Tight Ends. From: https://x.com/GetUpESPN/status/1380165126108672001 (archived at https://perma.cc/JW8E-KV2C)\n\nThe representativeness heuristic has been observed in gambling markets for predicting team wins in the National Football League (NFL) (Woodland & Woodland, 2015) and in decision making in fantasy soccer (Kotrba, 2020).\n\n12.3.3 Anchoring and Adjustment Heuristic\nThe anchoring and adjustment heuristic refers to the tendency for a person’s judgments or predictions to be made with a reference point—an anchor—as a starting point from which they adjust their estimates upward or downward. The anchor is often inaccurate and given too much weight in the person’s calculation, and too little adjustment is made to the anchor. For instance, a manager is trying to predict how many fantasy points a top Running Back may score. The player scored 300 fantasy points last season, but the team added a stronger backup Running Back and changed the Offensive Coordinator to be a more pass-heavy offense. The manager may use 300 fantasy points as an anchor (based on the player’s performance last season), and may adjust downward 15 points to account for the offseason changes. However, it is possible that this downward adjustment is insufficient to account not only for the offseasons changes but also for potential regression effects. Regression effects are discussed further in Section 12.5.2.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiases",
    "href": "cognitive-bias.html#sec-cognitiveBiases",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "\n12.4 Examples of Cognitive Biases",
    "text": "12.4 Examples of Cognitive Biases\nAs described above, cognitive biases are systematic errors in thinking. Cognitive biases are often due to the use of heuristics. Examples of cognitive biases that result from one or more of these heuristics include:\n\noverconfidence bias\nconfirmation bias\nrecency bias\nhindsight bias\nloss aversion bias\nendowment bias\nbandwagon effect bias\nDunning–Kruger effect bias\n\n\n12.4.1 Overconfidence Bias\nIn general, people tend to be overconfident in their judgments and predictions. Overconfidence bias is the tendency for a person to have greater confidence in their abilities (including judgments and predictions) than is objectively warranted. There are three general ways that overconfidence has been identified (Moore & Healy, 2008):\n\n\noverestimation of one’s actual performance\n\noverplacement of one’s performance relative to others\n\noverprecision in one’s beliefs/jugments/predictions\n\nOverestimation involves believing that one will perform better than one actually performs. Overestimation can be identified with a calibration plot of the predicted performance versus actual performance, where the person’s predicted performance is systematically higher (in at least some cases) than their actual performance. Overestimation corresponds to the “overprediction” form of miscalibration.\nOverplacement involves believing that one is better than others or will perform better than others, even when they do not. For instance, it is a common finding that more than half of people believe they are “above average” (i.e., above the median), even though that is statistically impossible. This calls to mind the fictitious Lake Wobegon in the radio show A Praririe Home Companion, “where all the women are strong, all the men are good-looking, and all the children are above average.”\nOverprecision involves expressing excessive certainty regarding the accuracy of one’s beliefs/judgments/predictions. For instance, if when a given meteorologist says it will rain 80% of the time, it actually rains 30% of the time, the meteorologist’s predictions are overprecise. Likewise, if the weather forecast says it will rain 10% of the time and it actually rains 30% of the time, the predictions are also overprecise because the forecaster is expressing stronger confidence than is warranted that it will not rain. Overprecision can be identified with a calibration plot of the predicted probabilities versus the actual probabilities. Overprecision corresponds to the “overextremity” form of miscalibration.\nOverestimation and overprecision are studied in various ways. Typically, people are asked about (a) whether an event will occur or (b) the likelihood that the event will occur, across many events. For the former (approach “a”), people may be asked to make a dichotomous judgment or prediction, by responding to the question: e.g., “Will it rain tomorrow? [YES/NO]”. They will then rate their confidence (as a percentage) in their answer (0–100%). They would make each of these two ratings for each of many events. Then, we can evaluate, for a given respondent, the degree to which the probabilistic estimate of an event reflects the true underlying probability of the event. For instance, for a given respondent (and for respondents in general), for the events when the respondent says they are 80% confident an event (e.g., rain) will occur, does the event actually occur around 80% of the time? For the latter (approach “b”), people may indicate the likelihood that the event will occur, by responding to the question: “How likely is it that it will rain tomorrow? (0–100%)”. Then, we can evaluate, for instance, for the events when the respondent says that an event (e.g., rain) is 80% likely to occur, does the event actually occur around 80% of the time?\nA fantasy manager may be even more likely to exhibit overconfidence if they previously performed well or won their league, for which luck and random chance plays an important role. Indeed, it is estimated that nearly half (~45%) of the variability in fantasy football performance is estimated to be luck [and around 55% due to skill; Getty et al. (2018)]. A manager who won their league in the prior season may believe they will perform better than they actually will (overestimation), will perform better than average (overplacement), and may hold excessive confidence regarding the accuracy of their predictions about which players will perform well or poorly (overprecision). These various types of overconfidence may lead them to draft high-risk players based on gut feeling, neglecting statistical analysis and expert consensus.\nPlayers’ performance in fantasy football, and human behavior more generally, is complex and multiply determined (i.e., is influenced by many factors). Despite the bluster of so-called experts who pretend to know more than they can know, no one can consistently and accurately predict how all players will perform. Remain humble in your predictions; do not be more confident than is warranted. If you approach the task of prediction with humility, you may be more able to be flexible and more willing to consider other players who you can draft for good value.\n\n12.4.2 Confirmation Bias\nConfirmation bias is the tendency for people to search for, interpret, and remember information that confirms one’s beliefs, as opposed to information that might disconfirm one’s beliefs. The result of confirmation bias is that people are unlikely to change their minds about something that they have a pre-existing belief about, because they tend to look only for information that supports their pre-existing beliefs. For instance, if you believe that a particular player is a strong breakout candidate to be a sleeper, you may be more likely to pay attention to evidence that supports that the player will breakout and may be less likely to pay attention to evidence that indicates the player may struggle.\nAs a budding empiricist, you should actively seek out information that challenges or disconfirms your beliefs and work to incorporate it into your beliefs. Do your best to go into observation, data analysis, and data interpretation with an open mind.\n\n12.4.3 Recency Bias\nRecency bias is the tendency to weigh recent events more than earlier ones. For instance, a manager might observe that a Running Back on the waiver wire performed well in the last two games. Recency bias may lead the manager to pick up the player, overvaluing their recent performance. For instance, the manager may not have adequately weighed the player’s overall season performance and the fact that the starting Running Back is returning to the lineup from injury, and that is why the player received more carries in the past two games (i.e., in place of the injured starter).\n\n12.4.4 Hindsight Bias\n\n“Hindsight is 20/20.” – Idiom\n\nHindsight bias is the tendency to perceive that past events were more predictable than they were. People tend to remember the succeess of their predictions and forget the failures of their predictions. For instance, if a third-string Quarterback has a breakout game, a fantasy manager may claim that they “knew it all along” that the player was going to breakout, despite not having picked up the player. That same manager may forget the many other predictions they had that did not come true.\n\n12.4.5 Loss Aversion Bias\nLoss aversion bias is the tendency to avoid losses rather than acquiring equivalent gains. Loss aversion is exemplified when teams play conservatively so as “not to lose” instead of “to win.” In fantasy football, loss aversion may lead managers to start or hold onto underperforming high drafts for too long instead of a starting a more promising player out of fear of losing potential value from their initial investment.\n\n12.4.6 Endowment Bias\nEndowment bias is the tendency to overvalue merely because one owns it. For instance, a manager might overvalue a player they drafted in the first round, refusing to trade them even if they could get a better-performing player in return.\n\n12.4.7 Bandwagon Effect Bias\nThe bandwagon effect bias is the tendency to do or believe things because other people are. It involves social conformity. For instance, consider if a rookie Wide Receiver has a breakout game and he is picked up in many fantasy leagues. A given manager might pick up the player because the player is frequently being picked up in many fantasy leagues, without evaluating whether the player’s success is sustainable.\n\n12.4.8 Dunning–Kruger Effect Bias\n\n“The more you know, the more you know you don’t know.” – Anonymous\n\nThe Dunning–Kruger effect bias is the tendency for people with low ability/competency in a task to overestimate their ability. The Dunning–Kruger effect is depicted in Figures 12.1 and 12.2. For instance, consider a new fantasy manager who experiences some initial wins (often called “beginner’s luck”). They may attribute their successes to their skill rather than to luck. Their overconfidence may lead them to believe they can win the league without much preparation.\n\n\n\n\n\nFigure 12.1: Dunning–Krueger Effect: Confidence as a Function of Competence. Adapted from https://commons.wikimedia.org/wiki/File:Effet_Dunning-Kruger.svg.\n\n\n\n\n\n\n\nFigure 12.2: Dunning–Krueger Effect: Perceived Performance as a Function of Actual Performance. Adapted from https://commons.wikimedia.org/wiki/File:Dunning-kruger_effect_-_percentile.svg.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-fallacies",
    "href": "cognitive-bias.html#sec-fallacies",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "\n12.5 Examples of Fallacies",
    "text": "12.5 Examples of Fallacies\nAs described above, fallacies are mistaken beliefs and flawed reasoning. Fallacies are often due to the use of heuristics and to cognitive biases. Examples of fallacies include:\n\nbase rate fallacy (aka base rate neglect)\nregression fallacy\nsunk cost fallacy\nhot hand fallacy\ngambler’s fallacy\nconditional probability fallacy\n\n\n12.5.1 Base Rate Fallacy\nThe base rate fallacy (aka base rate neglect) is the tendency to ignore information about the general probability of an event in favor of specific information about the event. The base rate is a marginal probability, which is the general probability of an event irrespective of other things. For example, the base rate of work-related injury is the general probability of experiencing a work-related injury, irrespective of other factors (e.g., the type of job, the person’s age, the person’s sex). Among the working population in the U.S., the lifetime prevalence of work-related injuries (i.e., the percent of people who will experience a work-related injury at some point in their lives), is ~35% (https://www.cdc.gov/mmwr/volumes/69/wr/mm6913a1.htm; archived at https://perma.cc/A2L6-WPEH). Thus, the base rate of work-related injuries in the U.S. is ~35%. The probability of work-related injuries is higher for some occupations (e.g., construction) and for some groups (e.g., men, 55–64-year-olds, Black or Multiracial, who are self-employed and have less than high school education) than others. Nevertheless, if we ignore all of the interacting factors, the general probability of work-related injuries is 35%. If we made a prediction that someone would be highly likely (&gt; 90%) to experience a work-related injury because they are male and self-employed, this would be ignoring the relatively lower base rate of work-related injury. Indeed, even men (36.7%) and self-employed individuals (41.2%) have less than a 50% chance of experiencing a work-related injury.\nAs applied to fantasy football, consider that you read about a potential sleeper Wide Receiver who had a stellar performance in a preseason game. If you select this player early on in the draft based on this information, this would be ignoring the general probability that most players who have a strong performance in the preseason do not perform as well in the regular season (i.e., base rate neglect). Performance in the preseason is not strongly predictive of performance in the regular season (https://fivethirtyeight.com/features/the-nfl-preseason-is-not-predictive-but-it-can-often-seem-that-way/; archived at https://perma.cc/FSG2-6AXE).\nMore information on base rates and how to counteract the base rate fallacy in described in Chapter 14.\n\n12.5.2 Regression Fallacy\nThe regression fallacy is the failure to account for the fact that things tend to naturally fluctuate around their mean and that, after an extreme fluctuation, subsequent scores tend to regress (or reverse) to the mean. An example of the regression fallacy is the so-called Sports Illustrated or Madden cover jinx curse. The Sports Illustrated or Madden cover jinx curse is the urban legend that players who appear on the cover of Sports Illustrated (the magazine) or Madden (the video game) will perform poorly. But, such a phenomenon can be more simply explained by regression to the mean (https://www.psychologytoday.com/us/blog/what-the-luck/201610/the-sports-illustrated-cover-jinx; archived at https://perma.cc/CZM9-TVFN). When a player has a superb season, they likely benefited from some degree to good luck, and it is unlikely that they will repeat such a stellar season the following year. Instead, they are likely—at least based on random fluctuation—to regress to their long-term mean.\nApplied to fantasy football, consider that a Quarterback had a 5-touchdown game in Week 1. You are in need of a strong Quarterback, so you drop a solid player to pick him up. However, it is possible that the Quarterback benefited from playing against a week defense in the first game of the season. Future matchups may prove more difficult, and the player is unlikely to sustain such a solid performance consistently throughout the season (i.e., they are likely to regress toward their mean).\n\n12.5.3 Hot Hand Fallacy\nThe “hot hand” is the idea that a player who experiences a successful outcome will have greater chance of success in subsequent attempts. For instance, in basketball, it is widely claimed by coaches, players, and commentators that players who have the hot hand (i.e., who are “on fire”) are more likely to make shots because they made previous shots. Evidence on the hot hand is mixed. Considerable evidence historically has suggested that there is no such thing as a “hot hand” (Avugos et al., 2013; Bar-Eli et al., 2006; Gilovich et al., 1985). Some recent research, however, has suggested that there may be a small hot hand effect in some contexts (Bocskocsky et al., 2014; Miller & Sanjurjo, 2014). However, if any such effect exists, the hot hand may be limited to a small subset of players and the effect size of any hot hand effect appears to be small (Pelechrinis & Winston, 2022).\nIn football, when trying how to distribute the ball among multiple Running Backs, it is not uncommon to hear that a coach wants to give the ball to the Running Back with the “hot hand.” In fantasy football, consider that a player just had a multiple touchdown game. Due to the hot hand fallacy, a manager might continue to start the player because they believe the player is “on fire” and is likely to continue to score at an unsustainable rate.\nIt is important consider whether such a string of strong performances are outliers and if the player may, in future games, regress to the mean. When considering whether strong performances are outliers and may regress to the mean, it is valuable to consider whether the player’s health, skill, or situation has appreciably changed (compared to the player’s earlier, weaker performances). Is the player finally fully healthy? Has the player appreciably improved in some skill that will benefit them in future games? Has the player’s long-term situation improved, such as moving up the depth chart, or receiving more carries/targets that is not tied to a specific opponent or game script? Or, alternatively, do the improvements appear to be driven by transient, game-specific factors, such as a the health of a teammate, the opponents they played, or the game script that ensued? If long-term outlook of the player has appreciably changed due to changes in the fundamentals of a player’s value, such as their health, skill, or situation, it is less likely that such performance improvements will regress over the long run.\n\n12.5.4 Sunk Cost Fallacy\nA sunk cost is a cost (e.g., in money, time, or effort) that has already been incurred and cannot be recovered. For instance, if a person orders an expensive meal at a restaurant, the order is a sunk cost. The sunk cost fallacy is the tendency to continue an endeavor when there is a sunk cost. For instance, when ordering the expensive meal at the restaurant, a person may over-eat so that they feel that they eat their money’s worth of food.\nApplied to fantasy football, consider a situation in which you invest a lot of salary cap or a high draft pick to draft a promising player, but they repeatedly underperform. If you continue to start the player to justify your large investment, instead of benching him in favor of a higher-performing player, you are committing the sunk cost fallacy.\n\n12.5.5 Gambler’s Fallacy\nThe gambler’s fallacy occurs due to an erroneous belief in the law of small numbers. The law of large numbers is a mathematical theorem that the average of a sufficiently large number of independent observations converges to the true value. For instance, if you flip a fair coin 1 million times, it is likely to land heads-up ~50% of the time. The law of small numbers (aka hasty generalization), by contrast, is an erroneous belief that small samples are representative of the populations from which they were drawn. For instance, if you flip a coin 10 times, belief in the law of small numbers would lead one to believe that the coin will flip heads-up exactly 5 times out of 10. However, in reality, the chance is less than 1 in 4 (24.6%) that exactly 5 of 10 coin flips turn up heads, as calculated below and as depicted in Figure 12.3:\n\nCodedbinom(\n  x = 5,     # number of coins that flip heads-up\n  size = 10, # how many times you flip a coin\n  prob = 0.5 # probability of a coin flipping heads-up (i.e., fair coin = 50%)\n)\n\n[1] 0.2460938\n\n\nThe dbinom() function in R provides the density of a binomial distribution. A binomial distribution is the probability of a particular number of successes (e.g., coins flipping heads-up) given a certain number of independent trials.\n\nCodeset.seed(52242)\n\nnumHeads &lt;- rbinom(\n  n = 100000,\n  size = 10,\n  prob = .5\n)\n\nsimulationOfFlipping10Coins &lt;- data.frame(\n  numHeads = numHeads\n)\n\nsimulationOfFlipping10Coins &lt;- simulationOfFlipping10Coins %&gt;% \n  mutate(\n    highlight = ifelse(numHeads == 5, \"yes\", \"no\")\n  )\n\nggplot2::ggplot(\n  data = simulationOfFlipping10Coins,\n  mapping = aes(\n    x = numHeads,\n    fill = highlight)\n) +\n  geom_histogram(\n    color = \"#000000\",\n    bins = 11\n  ) +\n  scale_x_continuous(\n    breaks = 0:10\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"yes\" = \"tomato\",\n      \"no\" = \"gray\")\n  ) +\n  labs(\n    x = \"Number of Coins Flipped Heads (out of 10 Coin Flips)\",\n    y = \"Frequency\",\n    title = \"Histogram of Number of Coins that Flip Heads-Up\\nin a Simulation of 10 Coin Flips\\n(with 100,000 Replications).\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 12.3: Histogram of Number of Coins that Flip Heads-Up in a Simulation of 10 Coin Flips (with 100,000 Replications).\n\n\n\n\nAlthough 5 is the modal count of coins that flip heads-up out of 10 flips (i.e., it was more common than any other number), it is less common than the aggregate probability of flipping any number of heads besides 5. The probability of getting any other number of coin flips turning up heads (other than 5) is:\n\nCodedbinom(\n  x = c(0:4, 6:10),\n  size = 10,\n  prob = 0.5\n) %&gt;% sum()\n\n[1] 0.7539063\n\n\nThe gambler’s fallacy is the erroneous belief that future probabilities are influenced by past events, even when the events are independent. For example, a gambler may pay close attention to a particular slot machine. If the slot machine has not paid out in a while, the gambler may believe that the slot machine is about to pay out soon, and may start putting coins in the slots.\nApplied to fantasy football, consider that a Quarterback has had several lousy games in a row. The gambler’s fallacy might lead a manager to start the player under the belief that the player “is due” for a big game, expecting a strong performance from the player merely because they player has not had a good game in a while.\n\n12.5.6 Conditional Probability Fallacy\nWe describe the conditional probability fallacy in Section 14.3.2 after introducing conditional probability in Section 14.3.1.3.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasConclusion",
    "href": "cognitive-bias.html#sec-cognitiveBiasConclusion",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "\n12.6 Conclusion",
    "text": "12.6 Conclusion\nIn conclusion, there are many heuristics, cognitive bias, and fallacies that people engage in when making judgments and predictions. It is prudent to be aware of these common biases and to work to counteract them. For instance, look for information that challenges or disconfirms your beliefs, and work to incorporate this information into your beliefs. Do your best to pursue observation, data analysis, and data interpretation with an open mind. You never know what important information you might discover if you go in with an open mind. Pay attention to fundamentals of a player’s value, such as their health, skill, or situation when considering whether a player’s performance may regress to the mean. If the strong performances appear to be driven by transient, game-specific factors, such as a the health of a teammate, the opponents they played, or the game script that ensued, future performances may be more likely to regress to the mean. In general, people tend to be overconfident in their predictions. There is considerable luck in fantasy football. Approach the task of prediction with humility; no one is consistently able to accurately predict how well players will perform.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "cognitive-bias.html#sec-cognitiveBiasSessionInfo",
    "href": "cognitive-bias.html#sec-cognitiveBiasSessionInfo",
    "title": "12  Heuristics and Cognitive Biases in Prediction",
    "section": "\n12.7 Session Info",
    "text": "12.7 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [5] purrr_1.0.2     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n [9] ggplot2_3.5.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      jsonlite_1.8.8    compiler_4.4.1    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.9        fastmap_1.2.0     R6_2.5.1         \n [9] labeling_0.4.3    generics_0.1.3    knitr_1.48        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.9.0      tzdb_0.4.0        rlang_1.1.4      \n[17] utf8_1.2.4        stringi_1.8.4     xfun_0.46         timechange_0.3.0 \n[21] cli_3.6.3         withr_3.0.0       magrittr_2.0.3    digest_0.6.36    \n[25] grid_4.4.1        hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5      \n[29] evaluate_0.24.0   glue_1.7.0        farver_2.1.2      fansi_1.0.6      \n[33] colorspace_2.1-0  rmarkdown_2.27    tools_4.4.1       pkgconfig_2.0.3  \n[37] htmltools_0.5.8.1\n\n\n\n\n\n\nAvugos, S., Köppen, J., Czienskowski, U., Raab, M., & Bar-Eli, M. (2013). The “hot hand” reconsidered: A meta-analytic approach. Psychology of Sport and Exercise, 14(1), 21–27. https://doi.org/10.1016/j.psychsport.2012.07.005\n\n\nBar-Eli, M., Avugos, S., & Raab, M. (2006). Twenty years of “hot hand” research: Review and critique. Psychology of Sport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001\n\n\nBocskocsky, A., Ezekowitz, J., & Stein, C. (2014). The hot hand: A new approach to an old “fallacy.” MIT Sloan Sports Analytics Conference.\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nGetty, D., Li, H., Yano, M., Gao, C., & Hosoi, A. E. (2018). Luck and the law: Quantifying chance in fantasy sports and other contests. SIAM Review, 60(4), 869–887. https://doi.org/10.1137/16m1102094\n\n\nGilovich, T., Vallone, R., & Tversky, A. (1985). The hot hand in basketball: On the misperception of random sequences. Cognitive Psychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6\n\n\nKotrba, V. (2020). Heuristics in fantasy sports: Is it profitable to strategize based on favourite of the match? Mind & Society, 19(1), 195–206. https://doi.org/10.1007/s11299-020-00231-7\n\n\nMiller, J. B., & Sanjurjo, A. (2014). A cold shower for the hot hand fallacy. Innocenzo Gasparini Institute for Economic Research. https://repec.unibocconi.it/igier/igi/wp/2014/518.pdf\n\n\nMoore, D. A., & Healy, P. J. (2008). The trouble with overconfidence. Psychological Review, 115(2), 502–517. https://doi.org/10.1037/0033-295X.115.2.502\n\n\nPelechrinis, K., & Winston, W. (2022). The hot hand in the wild. PLOS ONE, 17(1), e0261890. https://doi.org/10.1371/journal.pone.0261890\n\n\nPetersen, I. T. (2024). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131. https://doi.org/10.1126/science.185.4157.1124\n\n\nWoodland, L. M., & Woodland, B. M. (2015). The National Football League season wins total betting market: The impact of heuristics on behavior. Southern Economic Journal, 82(1), 38–54. https://doi.org/10.4284/0038-4038-2013.145",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Heuristics and Cognitive Biases in Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html",
    "href": "actuarial.html",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "",
    "text": "13.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-judgmentVsActuarialGettingStarted",
    "href": "actuarial.html#sec-judgmentVsActuarialGettingStarted",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "",
    "text": "13.1.1 Load Packages",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-approachesToPrediction",
    "href": "actuarial.html#sec-approachesToPrediction",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n13.2 Approaches to Prediction",
    "text": "13.2 Approaches to Prediction\nThere are two primary approaches to prediction: human judgment and the actuarial (i.e., statistical) method.\n\n13.2.1 Human Judgment\nUsing the judgment method of prediction, all gathered information is collected and formulated into a prediction in the person’s mind. The person selects, measures, and combines information and produces projections solely according to their experience and judgment. For instance, a proclaimed “fantasy expert” might use their experience, expertise, and judgment to make a prediction about how each player will perform by using whatever information and data they deem to be important, aggregating all of this information in their mind to make the prediction for each player.\n\n13.2.2 Actuarial/Statistical Method\nIn the actuarial or statistical method of prediction, information is gathered and combined systematically in an evidence-based statistical prediction formula. The method is based on equations and data, so both are needed.\nAn example of a statistical method of prediction is the Violence Risk Appraisal Guide (Rice et al., 2013). The Violence Risk Appraisal Guide is used in an attempt to predict violence and is used for parole decisions. For instance, the equation might be something like Equation 13.1:\n\\[\n\\scriptsize\n\\text{violence risk} = \\beta \\cdot \\text{conduct disorder} + \\beta \\cdot \\text{substance use} + \\beta \\cdot \\text{suspended from school} + \\beta \\cdot \\text{childhood aggression} + ...\n\\tag{13.1}\\]\nThen, based on their score and the established cutoffs, a person is given a “low risk”, “medium risk”, or “high risk” designation.\nAn actuarial formula for projecting a Running Back’s rushing yards might be something like Equation 13.2:\n\\[\n\\scriptsize\n\\text{rushing yards} = \\beta \\cdot \\text{rushing yards last season} + \\beta \\cdot \\text{age} + \\beta \\cdot \\text{injury history} + \\beta \\cdot \\text{strength of offensive line} + ...\n\\tag{13.2}\\]\nThe beta weights in the actuarial model reflect the relative weight to assign each predictor. For instance, in predicting rushing yards, a player’s historical performance is likely the strongest predictor, whereas injury history might be a relatively weaker predictor. Thus, we might give historical performance a beta of 3 and injury history a beta of 1 to give a player’s historical performance three times more weight than the player’s injury history in predicting their rushing yards. For generating the actuarial model, you could obtain the beta weights for each predictor from multiple regression, from machine learning, or from prior research on the relative importance of each predictor.\n\n13.2.3 Combining Human Judgment and Statistical Algorithms\nThere are numerous ways in which humans and statistical algorithms could be involved. On one extreme, humans make all judgments. On the other extreme, although humans may be involved in data collection, a statistical formula makes all decisions based on the input data, consistent with an actuarial approach. However, the human judgment and actuarial approaches can be combined in a hybrid way (Dana & Thomas, 2006). For example, to save time and money, a clinical psychologist might use an actuarial approach in all cases, but might only use a judgment approach when the actuarial approach gives a “positive” test. Or, the clinical psychologist might use both human judgment and an actuarial approach independently to see whether they agree. That is, the clinician may make a prediction based on their judgment and might also generate a prediction from an actuarial approach.\nThe challenge is what to do when the human and the algorithm disagree. Hypothetically, humans reviewing and adjusting the results from the statistical algorithm could lead to more accurate prediction. However, human input also could lead to the possibility or exacerbation of biased predictions. In general, with very few exceptions, actuarial approaches are as accurate or more accurate than “expert” judgment (Ægisdóttir et al., 2006; Baird & Wagner, 2000; Dawes et al., 1989; Grove et al., 2000; Grove & Meehl, 1996). This is also likely true with respect to predicting player performance in sports (Den Hartigh et al., 2018). Moreover, the superiority of actuarial approaches to human judgment tends to hold even when the expert is given more information than the actuarial approach (Dawes et al., 1989). Allowing experts to override actuarial predictions consistently leads to lower predictive accuracy (Garb & Wood, 2019).\nThere is sometimes a misconception that formulas cannot account for qualitative information. However, that is not true. Qualitative information can be scored or coded to be quantified so that it can be included in statistical formulas. For instance, if an expert scout is able to meaningfully assess a player’s cognitive and motivational factors (i.e., the “X factor” or “intangibles”), the scout can score this across multiple players and include these data in the actuarial prediction formula. For instance, the scout could use a rating scale (e.g., 1 = “poor”; 2 = “fair”; 3 = “good”; 4 = “very good”; 5 = “excellent”) to code (i.e., translate) their qualitative judgment into a quantifiable rating that can be integrated with other information in the actuarial formula. That said, the quality of predictions rests on the quality and relevance of the assessment information for the particular prediction decision. If the assessment data are lousy, it is unlikely that a statistical algorithm (or a human for that matter) will make an accurate prediction: “Garbage in, garbage out”. A statistical formula cannot rescue inaccurate assessment data.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-errorsInHumanJudgment",
    "href": "actuarial.html#sec-errorsInHumanJudgment",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n13.3 Errors in Human Judgment",
    "text": "13.3 Errors in Human Judgment\nHuman judgment is naturally subject to errors. Common heuristics, cognitive biases, and fallacies are described in Chapter 12. Below, I describe a few errors to which human judgment seems particularly prone.\nWhen operating freely, clinicians and medical experts (and humans more generally) tend to overestimate exceptions to the established rules (i.e., the broken leg syndrome). Meehl (1957) acknowledged that there may be some situations where it is glaringly obvious that the statistical formula would be incorrect because it fails to account for an important factor. He called these special cases “broken leg” cases, in which the human should deviate from the formula (i.e., broken leg countervailing). The example goes like this:\n\n“If a sociologist were predicting whether Professor X would go to the movies on a certain night, he might have an equation involving age, academic specialty, and introversion score. The equation might yield a probability of .90 that Professor X goes to the movie tonight. But if the family doctor announced that Professor X had just broken his leg, no sensible sociologist would stick with the equation. Why didn’t the factor of ‘broken leg’ appear in the formula? Because broken legs are very rare, and in the sociologist’s entire sample of 500 criterion cases plus 250 cross-validating cases, he did not come upon a single instance of it. He uses the broken leg datum confidently, because ‘broken leg’ is a subclass of a larger class we may crudely denote as ‘relatively immobilizing illness or injury,’ and movie-attending is a subclass of a larger class of ‘actions requiring moderate mobility.’” (Meehl, 1957, pp. 269–270)\n\nHowever, people too often think that cases where they disagree with the statistical algorithm are broken leg cases. People too often think their case is an exception to the rule. As a result, they too often change the result of the statistical algorithm and are more likely to be wrong than right in doing so. Because actuarial methods are based on actual population levels (i.e., base rates), unique exceptions are not overestimated.\nActuarial predictions are perfectly reliable—they will always return the same conclusion given an identical set of data. The human judge is likely to both disagree with others and with themselves given the same set of symptoms.\nThe decision by an expert (all by all humans) is likely to be influenced by past experiences. Actuarial methods are based on objective algorithms, and past personal experience and personal biases do not factor into any decisions. Humans give weight to less relevant information, and often give too much weight to singular variables. Actuarial formulas do a better job of focusing on relevant variables. Computers are good at factoring in base rates. Humans ignore base rates (base rate neglect).\nComputers are better at accurately weighing predictors and calculating unbiased risk estimates. In an actuarial formula, the relevant predictors are weighted according to their predictive power.\nHumans are typically given no feedback on their judgments. To improve accuracy of judgments, it is important for feedback to be clear, consistent, and timely.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-humansVsComputers",
    "href": "actuarial.html#sec-humansVsComputers",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n13.4 Humans Versus Computers",
    "text": "13.4 Humans Versus Computers\n\n13.4.1 Advantages of Computers\nHere are some advantages of computers over humans:\n\nComputers can process lots of information simultaneously. So can humans. But computers can to an even greater degree.\nComputers are faster at making calculations.\nComputations by computers are error-free (as long as the computations are programmed correctly).\nComputers’ judgments will not be biased by fatigue or emotional responses.\nComputers’ judgments will tend not to be biased in the way that humans’ cognitive biases are. Computers are less likely to be overconfident in their judgments.\nComputers can more accurately weight the set of predictors based on large data sets. Humans tend to give too much weight to singular predictors.\n\n13.4.2 Advantages of Humans\nComputers are bad at some things too. Here are some advantages of humans over computers (as of now):\n\nHumans can be better at identifying patterns in data (but also can mistakenly identify patterns where there are none—i.e., illusory correlation).\nHumans can be flexible and take a different approach if a given approach is not working.\nHumans are better at tasks requiring creativity and imagination, such as developing theories that explain phenomena.\nHumans have the ability to reason, which is especially important when dealing with complex, abstract, or open-ended problems, or problems that have not been faced before (or for which we have insufficient data).\nHumans are better able to learn.\nHumans are better at holistic, gestalt processing, including facial and linguistic processing.\n\nThere may be situations in which a human judgment would do better than an actuarial judgment. One situation where human judgment would be important is when no actuarial method exists for the judgment or prediction. For instance, when no actuarial method exists for the diagnosis or disorder (e.g., suicide), it is up to the clinician. However, we could collect data on the outcomes or on clinicians’ judgments to develop an actuarial method that will be more reliable than the clinicians’ judgments. That is, an actuarial method developed based on clinicians’ judgments will be more accurate than clinicians’ judgments. In other words, we do not necessarily need outcome data to develop an actuarial method. We could use the client’s data as predictors of the clinicians’ judgments to develop a structured approach to prediction that weighs factors similarly to clinicians, but with more reliable predictions.\nAnother situation in which human judgment could outperform a statistical algorithm is in true “broken leg” cases, e.g., important and rare events (edge cases) that are not yet accounted for by the algorithm.\nAnother situation in which human judgment could be preferable is if advanced, complex theories exist. Computers have a difficult time adhering to complex theories, so clinicians may be better suited. However, we do not have any of these complex theories in psychology that are accurate. We would need strong theory informed by data regarding causal influences, and accurate measures to assess them. However, no theories in psychology are that good. Nevertheless, predictive accuracy can be improved when considering theory (Garb & Wood, 2019; Silver, 2012).\nIf the prediction requires complex configural relations that a computer will have a difficult time replicating, a clinician’s judgment may be preferred. Although the likelihood that a person can accurately work through these complex relations is theoretically possible, it is highly unlikely. Holistic pattern recognition (such as language and faces) tends to be better by humans than computers. But computers are getting better with holistic pattern recognition through machine learning.\nIn sum, the human seeks to integrate information to make a decision, but is biased.\n\n13.4.3 Comparison of Evidence\nHundreds of studies have examined clinical versus actuarial prediction methods across many disciplines. Findings consistently show that actuarial methods are as accurate or more accurate than human judgment/prediction methods. “There is no controversy in social science that shows such a large body of qualitatively diverse studies coming out so uniformly…as this one” (Meehl, 1986, pp. 373–374).\nActuarial methods are particularly valuable for criterion-referenced assessment tasks, in which the aim is to predict specific events or outcomes (Garb & Wood, 2019). For instance, actuarial methods have shown promise in predicting violence, criminal recidivism, psychosis onset, course of mental disorders, treatment selection, treatment failure, suicide attempts, and suicide (Garb & Wood, 2019).\nMoreover, actuarial methods are explicit; they can be transparent and lead to informed scientific criticism to improve them. By contrast, human judgment methods are not typically transparent; human judgment relies on mental processes that are often difficult to specify.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-whyHumanJudgmentIsMoreWidelyUsed",
    "href": "actuarial.html#sec-whyHumanJudgmentIsMoreWidelyUsed",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n13.5 Why Judgment is More Widely Used Than Statistical Formulas",
    "text": "13.5 Why Judgment is More Widely Used Than Statistical Formulas\nDespite actuarial methods being generally more accurate than human judgment, judgment is much more widely used by clinicians. There are several reasons why actuarial methods have not caught on; one reason is professional traditions. Experts in any field do not like to think that a computer could outperform them. Some practitioners argue that judgment/prediction is an “art form” and that using a statistical formula is treating people like a number. However, using an approach (i.e., human judgment) that systematically leads to less accurate decisions and predictions is an ethical problem.\nSome clinicians do not think that group averages (e.g., in terms of which treatment is most effective) apply to an individual client. This invokes the distinction between nomothetic (group-level) inferences and idiographic (individual-level) inferences. However, the scientific evidence and probability theory strongly indicate that it is better to generalize from group-level evidence than throwing out all the evidence and taking the approach of “anything goes.” Clinicians frequently believe the broken leg fallacy, i.e., thinking that your client is an exception to the algorithmic prediction. In most cases, deviating from the statistical formula will result in less accurate predictions. People tend to overestimate the probability of low base rate conditions and events.\nAnother reason why actuarial methods have not caught on is the belief that receiving a treatment is the only thing that matters. But it is an empirical question which treatment is most effective for whom. What if we could do better? For example, we could potentially use a formula to identify the most effective treatment for a client. Some treatments are no better than placebo; other treatments are actually harmful (Lilienfeld, 2007; Williams et al., 2021).\nAnother reason why judgment methods are more widely used than actuarial methods is that so-called “experts” (and people in general) show overconfidence in their predictions—clinicians, experts, and humans in general think they are more accurate than they actually are. We see this when examining their calibration; their predictions tend to be miscalibrated. For example, things they report with 80% confidence occur less than 80% of the time, an example of overprecision in their predictions. Humans will sometimes be correct by chance, and they tend to mis-attribute that to their skill; humans tend to remember the successes and forget the failures.\nAnother argument against using actuarial methods is that “no methods exist”. In some cases, that is true—actuarial methods do not yet exist for some prediction problems. However, one can always create an algorithm of the experts’ judgments, even if one does not have access to the outcome information. A model of clinicians’ responses tends to be more accurate than clinicians’ judgments themselves because the model gives the same outcome with the same input data—i.e., it is perfectly reliable.\nAnother argument from some clinicians is that, “My job is to understand, not to predict”. But what kind of understanding does not involve predictions? Accurate predictions help in understanding. Knowing how people would perform in different conditions is the same thing as good understanding.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-bestActuarialApproaches",
    "href": "actuarial.html#sec-bestActuarialApproaches",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n13.6 Best Actuarial Approaches to Prediction",
    "text": "13.6 Best Actuarial Approaches to Prediction\nThe best actuarial models tend to be relatively simple (parsimonious), that can account for one or several of the most important predictors and their optimal weightings, and that account for the base rate of the phenomenon. Even unit-weighted formulas (formulas whose predictor variables are equally weighted with a weight of one) can sometimes generalize better to other samples than complex weightings (Garb & Wood, 2019). Differential weightings sometimes capture random variance and over-fit the model, thus leading to predictive accuracy shrinkage in cross-validation samples (Garb & Wood, 2019), as described below. The choice of predictor variables often matters more than their weighting.\nIn general, there is often shrinkage of estimates from training data set to a test data set. Shrinkage is when variables with stronger predictive power in the original data set tend to show somewhat smaller predictive power (smaller regression coefficients) when applied to new groups. Shrinkage reflects a model overfitting (i.e., fitting to error by capitalizing on chance). Shrinkage is especially likely when the original sample is small and/or unrepresentative and the number of variables considered for inclusion is large. Cross-validation with large, representative samples can help evaluate the amount of shrinkage of estimates, particularly for more complex models such as machine learning models (Ursenbach et al., 2019). Ideally, cross-validation would be conducted with a separate sample (external cross-validation) to see the generalizability of estimates. However, you can also do internal cross-validation. For example, you can perform k-fold cross-validation, where you:\n\nsplit the data set into k groups\nfor each unique group:\n\ntake the group as a hold-out data set (also called a test data set)\ntake the remaining groups as a training data set\nfit a model on the training data set and evaluate it on the test data set\nafter all k-folds have been used as the test data set, and all models have been fit, you average the estimates across the models, which presumably yields more robust, generalizable estimates\n\n\n\nAn emerging technique that holds promise for increasing predictive accuracy of actuarial methods is machine learning (Garb & Wood, 2019). However, one challenge of some machine learning techniques is that they are like a “black box” and are not transparent, which raises ethical concerns (Garb & Wood, 2019). machine learning may be most valuable when the data available are complex and there are many predictor variables (Garb & Wood, 2019).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-conclusion-actuarial",
    "href": "actuarial.html#sec-conclusion-actuarial",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n13.7 Conclusion",
    "text": "13.7 Conclusion\nIn general, it is better to develop and use structured, actuarial approaches than informal approaches that rely on human judgment or judgment by “so-called” experts. Actuarial approaches to prediction tend to be as accurate or more accurate than expert judgment. Nevertheless, in many domains, human judgment tends to be much more widely used than actuarial approaches.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "actuarial.html#sec-judgmentVsActuarialSessionInfo",
    "href": "actuarial.html#sec-judgmentVsActuarialSessionInfo",
    "title": "13  Judgment Versus Actuarial Approaches to Prediction",
    "section": "\n13.8 Session Info",
    "text": "13.8 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.1    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.4.1       htmltools_0.5.8.1 rmarkdown_2.27    knitr_1.48       \n [9] jsonlite_1.8.8    xfun_0.46         digest_0.6.36     rlang_1.1.4      \n[13] evaluate_0.24.0  \n\n\n\n\n\n\nÆgisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S., Anderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K., Walker, B. S., Cohen, G., & Rush, J. D. (2006). The meta-analysis of clinical judgment project: Fifty-six years of accumulated research on clinical versus statistical prediction. The Counseling Psychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875\n\n\nBaird, C., & Wagner, D. (2000). The relative validity of actuarial- and consensus-based risk assessment systems. Children and Youth Services Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nDana, J., & Thomas, R. (2006). In defense of clinical judgment … and mechanical prediction. Journal of Behavioral Decision Making, 19(5), 413–428. https://doi.org/10.1002/bdm.537\n\n\nDawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus actuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., & Meijer, R. R. (2018). Selection procedures in sports: Improving predictions of athletes’ future performance. European Journal of Sport Science, 18(9), 1191–1198. https://doi.org/10.1080/17461391.2018.1480662\n\n\nGarb, H. N., & Wood, J. M. (2019). Methodological advances in statistical prediction. Psychological Assessment, 31(12), 1456–1466. https://doi.org/10.1037/pas0000673\n\n\nGrove, W. M., & Meehl, P. E. (1996). Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical–statistical controversy. Psychology, Public Policy, and Law, 2(2), 293–323. https://doi.org/10.1037/1076-8971.2.2.293\n\n\nGrove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C. (2000). Clinical versus mechanical prediction: A meta-analysis. Psychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19\n\n\nLilienfeld, S. O. (2007). Psychological treatments that cause harm. Perspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1986). Causes and effects of my disturbing little book. Journal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6\n\n\nRice, M. E., Harris, G. T., & Lang, C. (2013). Validation of and revision to the VRAG and SORAG: The Violence Risk Appraisal Guide—Revised (VRAG-R). Psychological Assessment, 25(3), 951–965. https://doi.org/10.1037/a0032878\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.\n\n\nUrsenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D., Kosteniuk, J., & Spiteri, R. J. (2019). Scoring algorithms for a computer-based cognitive screening tool: An illustrative example of overfitting machine learning approaches and the impact on estimates of classification accuracy. Psychological Assessment, 31(11), 1377–1382. https://doi.org/10.1037/pas0000764\n\n\nWilliams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., & Sakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific review of evidential value. Clinical Psychology: Science and Practice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Judgment Versus Actuarial Approaches to Prediction</span>"
    ]
  },
  {
    "objectID": "base-rates.html",
    "href": "base-rates.html",
    "title": "14  Base Rates",
    "section": "",
    "text": "14.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesGettingStarted",
    "href": "base-rates.html#sec-baseRatesGettingStarted",
    "title": "14  Base Rates",
    "section": "",
    "text": "14.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesOverview",
    "href": "base-rates.html#sec-baseRatesOverview",
    "title": "14  Base Rates",
    "section": "\n14.2 Overview",
    "text": "14.2 Overview\nPredicting player performance is a complex prediction task. Performance is probabilistically influenced by many processes, including processes internal to the player in addition to external processes. Moreover, people’s performance occurs in the context of a dynamic system with nonlinear, probabilistic, and cascading influences that change across time. The ever-changing system makes behavior challenging to predict. And, similar to chaos theory, one small change in the system can lead to large differences later on. Moreover, there are important factors to keep in mind when making predictions.\nLet’s consider a prediction example, assuming the following probabilities:\n\nThe probability of contracting HIV is .3%\nThe probability of a positive test for HIV is 1%\nThe probability of a positive test if you have HIV is 95%\n\nWhat is the probability of HIV if you have a positive test?\nAs we will see, the probability is: \\(\\frac{95\\% \\times .3\\%}{1\\%} = 28.5\\%\\). So based on the above probabilities, if you have a positive test, the probability that you have HIV is 28.5%. Most people tend to vastly overestimate the likelihood that the person has HIV in this example. Why? Because they do not pay enough attention to the base rate (in this example, the base rate of HIV is .3%).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-probability",
    "href": "base-rates.html#sec-probability",
    "title": "14  Base Rates",
    "section": "\n14.3 Issues Around Probability",
    "text": "14.3 Issues Around Probability\n\n14.3.1 Types of Probabilities\nIt is important to distinguish between different types of probabilities: marginal probabilities, joint probabilities, and conditional probabilities.\n\n14.3.1.1 Base Rate (Marginal Probability)\nThe base rate is a marginal probability, which is the general probability of an event irrespective of other things. For instance, the base rate of HIV is the probability of developing HIV. In the U.S., the prevalence rate of HIV is ~0.4% of the adult population (archived at https://perma.cc/8GE6-GAPC).\nFor instance, we can consider the following marginal probabilities:\n\\(P(C_i)\\) is the probability (i.e., base rate) of a classification, \\(C\\), independent of other things. A base rate is often used as the “prior probability” in a Bayesian model. In our example above, \\(P(C_i)\\) is the base rate (i.e., prevalence) of HIV in the population: \\(P(\\text{HIV}) = .3\\%\\). \\(P(R_i)\\) is the probability (base rate) of a response, \\(R\\), independent of other things. In the example above, \\(P(R_i)\\) is the base rate of a positive test for HIV: \\(P(\\text{positive test}) = 1\\%\\). The base rate of a positive test is known as the positivity rate or selection ratio.\n\n14.3.1.2 Joint Probability\nA joint probability is the probability of two (or more) events occurring simultaneously. For instance, the probability of events \\(A\\) and \\(B\\) both occurring together is \\(P(A, B)\\). A joint probability can be calculated using the marginal probability of each event, as in Equation 14.1:\n\\[\nP(A, B) = P(A) \\cdot P(B)\n\\tag{14.1}\\]\nConversely (and rearranging the terms for the calculation of conditional probability), a joint probability can also be calculated using the conditional probability and marginal probability, as in Equation 14.2:\n\\[\nP(A, B) = P(A | B) \\cdot P(B)\n\\tag{14.2}\\]\n\n14.3.1.3 Conditional Probability\nA conditional probability is the probability of one event occurring given the occurrence of another event. Conditional probabilities are written as: \\(P(A | B)\\). This is read as the probability that event \\(A\\) occurs given that event \\(B\\) occurred. For instance, we can consider the following conditional probabilities:\n\\(P(C | R)\\) is the probability of a classification, \\(C\\), given a response, \\(R\\). In other words, \\(P(C | R)\\) is the probability of having HIV given a positive test: \\(P(\\text{HIV} | \\text{positive test})\\). \\(P(R | C)\\) is the probability of a response, \\(R\\), given a classification, \\(C\\). In the example above, \\(P(R | C)\\) is the probability of having a positive test given that a person has HIV: \\(P(\\text{positive test} | \\text{HIV}) = 95\\%\\).\nA conditional probability can be calculated using the joint probability and marginal probability (base rate), as in Equation 14.3:\n\\[\nP(A, B) = P(A | B) \\cdot P(B)\n\\tag{14.3}\\]\n\n14.3.2 Confusion of the Inverse\nA conditional probability is not the same thing as its reverse (or inverse) conditional probability. Unless the base rate of the two events (\\(C\\) and \\(R\\)) are the same, \\(P(C | R) \\neq P(R | C)\\). However, people frequently make the mistake of thinking that two inverse conditional probabilities are the same. This mistake is known as the “confusion of the inverse”, or the “inverse fallacy”, or the “conditional probability fallacy”. The confusion of inverse probabilities is the logical error of representative thinking that leads people to assume that the probability of \\(C\\) given \\(R\\) is the same as the probability of \\(R\\) given C, even though this is not true. As a few examples to demonstrate the logical fallacy, if 93% of breast cancers occur in high-risk women, this does not mean that 93% of high-risk women will eventually get breast cancer. As another example, if 77% of car accidents take place within 15 miles of a driver’s home, this does not mean that you will get in an accident 77% of times you drive within 15 miles of your home.\nWhich car is the most frequently stolen? It is often the Honda Accord or Honda Civic—probably because they are among the most popular/commonly available cars. The probability that the car is a Honda Accord given that a car was stolen (\\(p(\\text{Honda Accord } | \\text{ Stolen})\\)) is what the media reports and what the police care about. However, that is not what buyers and car insurance companies should care about. Instead, they care about the probability that the car will be stolen given that it is a Honda Accord (\\(p(\\text{Stolen } | \\text{ Honda Accord})\\)).\nApplied to fantasy football, the probability that a given player will be injured given that he is a Running Back (\\(p(\\text{Injured } | \\text{ RB})\\)) is not the same as the probability that a given player is a Running Back given that he is injured (\\(p(\\text{RB } | \\text{ Injured})\\)).\n\n14.3.3 Bayes’ Theorem\nAn alternative way of calculating a conditional probability is using the inverse conditional probability (instead of the joint probability). This is known as Bayes’ theorem. Bayes’ theorem can help us calculate a conditional probability of some classification, \\(C\\), given some response, \\(R\\), if we know the inverse conditional probability and the base rate (marginal probability) of each. Bayes’ theorem is in Equation 14.4:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}\n\\end{aligned}\n\\tag{14.4}\\]\nOr, equivalently (rearranging the terms):\n\\[\n\\begin{aligned}\n  \\frac{P(C | R)}{P(R | C)} = \\frac{P(C_i)}{P(R_i)}\n\\end{aligned}\n\\tag{14.5}\\]\nOr, equivalently (rearranging the terms):\n\\[\n\\begin{aligned}\n  \\frac{P(C | R)}{P(C_i)} = \\frac{P(R | C)}{P(R_i)}\n\\end{aligned}\n\\tag{14.6}\\]\nMore generally, Bayes’ theorem has been described as:\n\\[\n\\begin{aligned}\n  P(H | E) &= \\frac{P(E | H) \\cdot P(H)}{P(E)} \\\\\n  \\text{posterior probability} &= \\frac{\\text{likelihood} \\times \\text{prior probability}}{\\text{model evidence}}\n\\end{aligned}\n\\tag{14.7}\\]\nwhere \\(H\\) is the hypothesis, and \\(E\\) is the evidence—the new information that was not used in computing the prior probability.\nIn Bayesian terms, the posterior probability is the conditional probability of one event occurring given another event—it is the updated probability after the evidence is considered. In this case, the posterior probability is the probability of the classification occurring (\\(C\\)) given the response (\\(R\\)). The likelihood is the inverse conditional probability—the probability of the response (\\(R\\)) occurring given the classification (\\(C\\)). The prior probability is the marginal probability of the event (i.e., the classification) occurring, before we take into account any new information. The model evidence is the marginal probability of the other event occurring—i.e., the marginal probability of seeing the evidence.\nBayes’ theorem provides the foundation for a paradigm of statistics called Bayesian statistics, which (unlike frequentist statistics) does not use p-values.\nIn the HIV example above, we can calculate the conditional probability of HIV given a positive test using three terms: the conditional probability of a positive test given HIV (i.e., the sensitivity of the test), the base rate of HIV, and the base rate of a positive test for HIV. The conditional probability of HIV given a positive test is in Equation 14.8:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{HIV} | \\text{positive test}) &= \\frac{P(\\text{positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{positive test})} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{base rate of HIV}}{\\text{base rate of positive test}} \\\\\n  &= \\frac{95\\% \\times .3\\%}{1\\%} = \\frac{.95 \\times .003}{.01}\\\\\n  &= 28.5\\%\n\\end{aligned}\n\\tag{14.8}\\]\nThe petersenlab package (Petersen, 2024a) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .95,\n  pA = .003,\n  pB = .01)\n\n[1] 0.285\n\n\nThus, assuming the probabilities in the example above, the conditional probability of having HIV if a person has a positive test is 28.5%. Given a positive test, chances are higher than not that the person does not have HIV.\nNow let’s see what happens if the person tests positive a second time. We would revise our “prior probability” for HIV from the general prevalence in the population (0.3%) to be the “posterior probability” of HIV given a first positive test (28.5%). This is known as Bayesian updating. We would also update the “evidence” to be the marginal probability of getting a second positive test.\nIf we do not know a marginal probability (i.e., base rate) of an event (e.g., getting a second positive test), we can calculate a marginal probability with the law of total probability using conditional probabilities and the marginal probability of another event (e.g., having HIV). According to the law of total probability, the probability of getting a positive test is the probability that a person with HIV gets a positive test (i.e., sensitivity) times the base rate of HIV plus the probability that a person without HIV gets a positive test (i.e., false positive rate) times the base rate of not having HIV, as in Equation 14.9:\n\\[\n\\begin{aligned}\nP(\\text{not } C_i) &= 1 - P(C_i) \\\\\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\\n  1\\% &= 95\\% \\times .3\\% + P(R | \\text{not } C) \\times 99.7\\% \\\\\n\\end{aligned}\n\\tag{14.9}\\]\nIn this case, we know the marginal probability (\\(P(R_i)\\)), and we can use that to solve for the unknown conditional probability that reflects the false positive rate (\\(P(R | \\text{not } C)\\)), as in Equation 14.10:\n\\[\n\\scriptsize\n\\begin{aligned}\n  P(R_i) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) && \\\\\n  P(R_i) - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &= P(R | C) \\cdot P(C_i) && \\text{Move } P(R | \\text{not } C) \\text{ to the left side} \\\\\n  - [P(R | \\text{not } C) \\cdot P(\\text{not } C_i)] &= P(R | C) \\cdot P(C_i) - P(R_i) && \\text{Move } P(R_i) \\text{ to the right side} \\\\\n  P(R | \\text{not } C) \\cdot P(\\text{not } C_i) &= P(R_i) - [P(R | C) \\cdot P(C_i)] && \\text{Multiply by } -1 \\\\\n  P(R | \\text{not } C) &= \\frac{P(R_i) - [P(R | C) \\cdot P(C_i)]}{P(\\text{not } C_i)} && \\text{Divide by } P(R | \\text{not } C) \\\\\n  &= \\frac{1\\% - [95\\% \\times .3\\%]}{99.7\\%} = \\frac{.01 - [.95 \\times .003]}{.997}\\\\\n  &= .7171515\\% \\\\\n\\end{aligned}\n\\tag{14.10}\\]\nThe petersenlab package (Petersen, 2024a) contains the pBgivenNotA() function that estimates the probability of one event, \\(B\\), given that another event, \\(A\\), did not occur.\n\nCodepetersenlab::pBgivenNotA(\n  pBgivenA = .95,\n  pA = .003,\n  pB = .01)\n\n[1] 0.007171515\n\n\nWith this conditional probability (\\(P(R | \\text{not } C)\\)), the updated marginal probability of having HIV (\\(P(C_i)\\)), and the updated marginal probability of not having HIV (\\(P(\\text{not } C_i)\\)), we can now calculate an updated estimate of the marginal probability of getting a second positive test. The probability of getting a second positive test is the probability that a person with HIV gets a second positive test (i.e., sensitivity) times the updated probability of HIV plus the probability that a person without HIV gets a second positive test (i.e., false positive rate) times the updated probability of not having HIV, as in Equation 14.11:\n\\[\n\\begin{aligned}\n  P(R_{i}) &= P(R | C) \\cdot P(C_i) + P(R | \\text{not } C) \\cdot P(\\text{not } C_i) \\\\\n  &= 95\\% \\times 28.5\\% + .7171515\\% \\times 71.5\\% = .95 \\times .285 + .007171515 \\times .715 \\\\\n  &= 27.58776\\%\n\\end{aligned}\n\\tag{14.11}\\]\nThe petersenlab package (Petersen, 2024a) contains the pB() function that estimates the marginal probability of one event, \\(B\\).\n\nCodepetersenlab::pB(\n  pBgivenA = .95,\n  pA = .285,\n  pBgivenNotA = .007171515)\n\n[1] 0.2758776\n\n\nWe then substitute the updated marginal probability of HIV (\\(P(C_i)\\)) and the updated marginal probability of getting a second positive test (\\(P(R_i)\\)) into Bayes’ theorem to get the probability that the person has HIV if they have a second positive test (assuming the errors of each test are independent, i.e., uncorrelated), as in Equation 14.12:\n\\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{HIV} | \\text{a second positive test}) &= \\frac{P(\\text{a second positive test} | \\text{HIV}) \\cdot P(\\text{HIV})}{P(\\text{a second positive test})} \\\\\n  &= \\frac{\\text{sensitivity of test} \\times \\text{updated base rate of HIV}}{\\text{updated base rate of positive test}} \\\\\n  &= \\frac{95\\% \\times 28.5\\%}{27.58776\\%} \\\\\n  &= 98.14\\%\n\\end{aligned}\n\\tag{14.12}\\]\nThe petersenlab package (Petersen, 2024a) contains the pAgivenB() function that estimates the probability of one event, \\(A\\), given another event, \\(B\\).\n\nCodepetersenlab::pAgivenB(\n  pBgivenA = .95,\n  pA = .285,\n  pB = .2758776)\n\n[1] 0.9814135\n\n\nThus, a second positive test greatly increases the posterior probability that the person has HIV from 28.5% to over 98%.\nAs seen in the rearranged formula in Equation 14.5, the ratio of the conditional probabilities is equal to the ratio of the base rates. Thus, it is important to consider base rates. People have a strong tendency to ignore (or give insufficient weight to) base rates when making predictions. The failure to consider the base rate when making predictions when given specific information about a case is known as the base rate fallacy or as base rate neglect. For example, people tend to say that the probability of a rare event is more likely than it actually is given specific information.\nAs seen in the rearranged formula in Equation 14.6, the inverse conditional probabilities (\\(P(C | R)\\) and \\(P(R | C)\\)) are not equal unless the base rates of \\(C\\) and \\(R\\) are the same. If the base rates are not equal, we are making at least some prediction errors. If \\(P(C_i) &gt; P(R_i)\\), our predictions must include some false negatives. If \\(P(R_i) &gt; P(C_i)\\), our predictions must include some false positives.\nIn sum, the marginal probability, including the prior probability or base rate, should be weighed heavily in predictions unless there are sufficient data to indicate otherwise, i.e., to update the posterior probability based on new evidence. Bayes’ theorem provides a powerful tool to anchor predictions to the base rate unless sufficient evidence changes the posterior probability (by updating the evidence and prior probability).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRateRookiePerformance",
    "href": "base-rates.html#sec-baseRateRookiePerformance",
    "title": "14  Base Rates",
    "section": "\n14.4 Base Rate of Rookie Performance",
    "text": "14.4 Base Rate of Rookie Performance\n\n14.4.1 Quarterbacks\n\n14.4.2 Running Backs",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-accountForBaseRates",
    "href": "base-rates.html#sec-accountForBaseRates",
    "title": "14  Base Rates",
    "section": "\n14.5 How to Account for Base Rates",
    "text": "14.5 How to Account for Base Rates\nThere are various ways to account for base rates, including the use of actuarial formulas and the use of Bayesian updating.\n\n14.5.1 Actuarial Formula\nOne approach to account for base rates is to use actuarial formulas (rather than human judgment) to make the predictions. Actuarial formulas based on multiple regression or machine learning can account for the base rate of the event.\n\n14.5.2 Bayesian Updating\nAnother approach to account for base rates is to leverage Bayes’ theorem, using Bayesian updating and the probability nomogram. Bayesian updating is a form of anchoring and adjustment; however, unlike the anchoring and adjustment heuristic, it is a systematic approach to anchoring and adjustment that anchors one’s predictions to the base rate, and then adjusts according to new information. That is, we start with a pretest probability (i.e., base rate) and update our predictions based on the extent of new information (i.e., the likelihood ratio).\nTo perform Bayesian updating involves comparing the relative probability of two outcomes, \\(P(C | R)\\) versus \\(P(\\text{not } C | R)\\). If we want to compare the relative probability of two outcomes, we can use the odds form of Bayes’ theorem, as in Equation 14.13: \\[\n\\begin{aligned}\n  P(C | R) &= \\frac{P(R | C) \\cdot P(C_i)}{P(R_i)} \\\\\n  P(\\text{not } C | R) &= \\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)} \\\\\n  \\frac{P(C | R)}{P(\\text{not } C | R)} &= \\frac{\\frac{P(R | C) \\cdot P(C_i)}{P(R_i)}}{\\frac{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)}{P(R_i)}} \\\\\n  &= \\frac{P(R | C) \\cdot P(C_i)}{P(R | \\text{not } C) \\cdot P(\\text{not } C_i)} \\\\\n  &= \\frac{P(C_i)}{P(\\text{not } C_i)} \\times \\frac{P(R | C)}{P(R | \\text{not } C)} \\\\\n  \\text{posterior odds} &= \\text{prior odds} \\times \\text{likelihood ratio}\n\\end{aligned}\n\\tag{14.13}\\]\nAs presented in Equation 14.13, the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. Below, we describe the likelihood ratio.\n\n14.5.2.1 Diagnostic Likelihood Ratio\nA likelihood ratio is the ratio of two probabilities. It can be used to compare the likelihood of two possibilities. The diagnostic likelihood ratio is an index of the predictive validity of an instrument: it is the ratio of the probability that a test result is correct to the probability that the test result is incorrect. The diagnostic likelihood ratio is also called the risk ratio. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio.\n\n14.5.2.1.1 Positive Likelihood Ratio (LR+)\nThe positive likelihood ratio (LR+) compares the true positive rate to the false positive rate. Positive likelihood ratio values range from 1 to infinity. Higher values reflect greater accuracy, because it indicates the degree to which a true positive is more likely than a false positive. The formula for calculating the positive likelihood ratio is in Equation 14.14.\n\\[\n\\begin{aligned}\n  \\text{positive likelihood ratio (LR+)} &= \\frac{\\text{TPR}}{\\text{FPR}} \\\\\n  &= \\frac{P(R|C)}{P(R|\\text{not } C)} \\\\\n  &= \\frac{P(R|C)}{1 - P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{\\text{sensitivity}}{1 - \\text{specificity}}\n\\end{aligned}\n\\tag{14.14}\\]\n\n14.5.2.1.2 Negative Likelihood Ratio (LR−)\nThe negative likelihood ratio (LR−) compares the false negative rate to the true negative rate. Negative likelihood ratio values range from 0 to 1. Smaller values reflect greater accuracy, because it indicates that a false negative is less likely than a true negative. The formula for calculating the negative likelihood ratio is in Equation 14.15.\n\\[\n\\begin{aligned}\n  \\text{negative likelihood ratio } (\\text{LR}-) &= \\frac{\\text{FNR}}{\\text{TNR}} \\\\\n  &= \\frac{P(\\text{not } R|C)}{P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{1 - P(R|C)}{P(\\text{not } R|\\text{not } C)} \\\\\n  &= \\frac{1 - \\text{sensitivity}}{\\text{specificity}}\n\\end{aligned}\n\\tag{14.15}\\]\n\n14.5.2.2 Probability Nomogram\nUsing Bayes’ theorem (described in Section 14.3.3), solving for posttest odds (based on pretest odds and the likelihood ratio, as in Equation 14.13), and converting odds to probabilities, we can use a Fagan probability nomogram to determine the posttest probability following a test result. The calculation of posttest probability is described in INSERT. A probability nomogram is a way of visually applying Bayes’ theorem to determine the posttest probability of having a condition based on the pretest (or prior) probability and likelihood ratio, as depicted in Figure 14.1. To use a probability nomogram, connect the dots from the starting probability (left line) with the likelihood ratio (middle line) to see the updated probability. The updated (posttest) probability is where the connecting line crosses the third, right line.\n\n\n\n\n\nFigure 14.1: Probability Nomogram. (Figure retrieved from https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Fagan_nomogram.svg/945px-Fagan_nomogram.svg.png).\n\n\nFor instance, if the starting probability is 0.5% and the likelihood ratio is 10 (e.g., sensitivity = .90, specificity = .91: \\(\\text{likelihood ratio} = \\frac{\\text{sensitivity}}{1 - \\text{specificity}} = \\frac{.9}{1-.91} = 10\\)) from a positive test (i.e., positive likelihood ratio), the updated probability is less than 5%, as depicted in Figure 14.2. The petersenlab package (Petersen, 2024a) contains the posttestProbability() function that estimates the posttest probability of an event, given the pretest probability and the likelihood ratio, or given the pretest probability and the sensitivity (SN) and specificity (SP) of the test.\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  likelihoodRatio = 10)\n\n[1] 0.04784689\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  SN = .90,\n  SP = .91)\n\n[1] 0.04784689\n\n\nThe function can also estimate the posttest probability of an event given the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):\n\nCodepetersenlab::posttestProbability(\n  TP = 450,\n  TN = 90545,\n  FP = 8955,\n  FN = 50)\n\n[1] 0.04784689\n\n\nWe discuss true positives (TP), true negatives (TN), false positives (FP), false negatives (FN), sensitivity (SN), and specificity (SP) in INSERT.\nIf the starting probability is 0.5% and the likelihood ratio is 0.11 from a negative test (i.e., negative likelihood ratio), the updated probability is nearly indistinguishable from zero (0.05%).\n\nCodepetersenlab::posttestProbability(\n  pretestProb = .005,\n  likelihoodRatio = 0.11)\n\n[1] 0.0005524584\n\n\n\n\n\n\n\nFigure 14.2: Probability Nomogram Example. (Figure adapted from https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Fagan_nomogram.svg/945px-Fagan_nomogram.svg.png. Also provided in: Petersen (2024b) and Petersen (2024c).)\n\n\nA probability nomogram calculator can be found at the following link: http://araw.mede.uic.edu/cgi-bin/testcalc.pl (archived at https://perma.cc/X8TF-7YBX). The petersenlab package (Petersen, 2024a) contains the nomogrammer() function that creates a nomogram plot using the positive and negative likelihood ratio or using the sensitivity (SN) and specificity (SP) of the test, as adapted from Adam Chekroud (https://github.com/achekroud/nomogrammer):\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .005,\n  SN = 0.90,\n  SP = 0.91)\n\n\n\n\n\n\nCodepetersenlab::nomogrammer(\n  pretestProb = .005,\n  PLR = 10,\n  NLR = 0.11)\n\n\n\n\n\n\n\nThe blue line indicates the posterior probability of the condition given a positive test. The pink line indicates the posterior probability of the condition given a negative test.\nThe function can also create a nomogram plot using the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN):\n\nCodepetersenlab::nomogrammer(\n  TP = 450,\n  TN = 90545,\n  FP = 8955,\n  FN = 50)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesConclusion",
    "href": "base-rates.html#sec-baseRatesConclusion",
    "title": "14  Base Rates",
    "section": "\n14.6 Conclusion",
    "text": "14.6 Conclusion\nFantasy performance—and behavior more generally—is challenging to predict. People commonly demonstrate biases and fallacies when making predictions. People tend to ignore base rates (base rate fallacy) when making predictions. They also tend to confuse inverse conditional probabilities (conditional probability fallacy). Bayes’ theorem provides a way to convert from one conditional probability to its inverse conditional probability using the base rate of each event. There are various ways to account for base rates for more accurate predictions, including through the use of actuarial formulas and Bayesian updating. Bayesian updating uses Bayes’ theorem to calculate a posttest probability from a pretest probability and a test result (likelihood ratio). The probability nomogram is a visual approach to Bayesian updating.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "base-rates.html#sec-baseRatesSessionInfo",
    "href": "base-rates.html#sec-baseRatesSessionInfo",
    "title": "14  Base Rates",
    "section": "\n14.7 Session Info",
    "text": "14.7 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] petersenlab_1.0.3\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     lattice_0.22-6     stringi_1.8.4     \n [5] digest_0.6.36      magrittr_2.0.3     evaluate_0.24.0    grid_4.4.1        \n [9] RColorBrewer_1.1-3 mvtnorm_1.2-5      fastmap_1.2.0      plyr_1.8.9        \n[13] jsonlite_1.8.8     nnet_7.3-19        backports_1.5.0    DBI_1.2.3         \n[17] Formula_1.2-5      gridExtra_2.3      fansi_1.0.6        viridisLite_0.4.2 \n[21] scales_1.3.0       pbivnorm_0.6.0     mnormt_2.1.1       cli_3.6.3         \n[25] mitools_2.4        rlang_1.1.4        munsell_0.5.1      Hmisc_5.1-3       \n[29] withr_3.0.0        base64enc_0.1-3    mix_1.0-12         parallel_4.4.1    \n[33] tools_4.4.1        reshape2_1.4.4     checkmate_2.3.1    htmlTable_2.4.3   \n[37] dplyr_1.1.4        colorspace_2.1-0   ggplot2_3.5.1      vctrs_0.6.5       \n[41] R6_2.5.1           rpart_4.1.23       stats4_4.4.1       lifecycle_1.0.4   \n[45] stringr_1.5.1      htmlwidgets_1.6.4  psych_2.4.6.26     foreign_0.8-86    \n[49] cluster_2.1.6      pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.5      \n[53] Rcpp_1.0.13        glue_1.7.0         data.table_1.15.4  xfun_0.46         \n[57] tibble_3.2.1       tidyselect_1.2.1   rstudioapi_0.16.0  knitr_1.48        \n[61] farver_2.1.2       xtable_1.8-4       nlme_3.1-164       htmltools_0.5.8.1 \n[65] labeling_0.4.3     rmarkdown_2.27     lavaan_0.6-18      compiler_4.4.1    \n[69] quadprog_1.5-8    \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nPetersen, I. T. (2024a). petersenlab: A collection of R functions by the Petersen Lab. https://github.com/DevPsyLab/petersenlab\n\n\nPetersen, I. T. (2024c). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Base Rates</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html",
    "href": "evaluating-prediction-accuracy.html",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "",
    "text": "15.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyGettingStarted",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyGettingStarted",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "",
    "text": "15.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"tidyverse\")\nlibrary(\"pROC\")\nlibrary(\"magrittr\")\nlibrary(\"viridis\")",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyOverview",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyOverview",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.2 Overview",
    "text": "15.2 Overview\nPredictions can come in different types. Some predictions involve categorical data, whereas other predictions involve continuous data. When dealing with a dichotomous (nominal data that are binary) predictor and outcome variable (or continuous data that have been dichotomized using a cutoff), we can evaluate predictions using a 2x2 table known as a confusion matrix (see INSERT), or with logistic regression models. When dealing with a continuous outcome variable (e.g., ordinal, interval, or ratio data), we can evaluate predictions using multiple regression or similar variants such as structural equation modeling and mixed models.\nIn fantasy football, we most commonly predict continuous outcome variables (e.g., fantasy points, rushing yards). Nevertheless, it is also important to understand principles in the prediction of categorical outcomes variables.\nIn any domain, it is important to evaluate the accuracy of predictions, so we can know how (in)accurate we are, and we can strive to continually improve our predictions. Fantasy performance—and human behavior more general—is incredibly challenging to predict. In fantasy football, there is considerable luck/chance/randomness. There are relatively few (i.e. 17) games, and there is a sizeable injury risk for each player in a given game. These and other factors combine to render fantasy football predictions not highly accurate. But, first, let’s learn about the various ways we can evaluate the accuracy of predictions.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-accuracyTypes",
    "href": "evaluating-prediction-accuracy.html#sec-accuracyTypes",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.3 Types of Accuracy",
    "text": "15.3 Types of Accuracy\nThere are two primary dimensions of accuracy: (1) discrimination and (2) calibration. Discrimination and calibration are distinct forms of accuracy. Just because predictions are high in one form of accuracy does not mean that they will be high in the other form of accuracy. As described by Lindhiem et al. (2020), predictions can follow any of the following configurations (and anywhere in between):\n\nhigh discrimination, high calibration\n\nhigh discrimination, low calibration\n\nlow discrimination, high calibration\n\nlow discrimination, low calibration\n\n\nSome general indexes of accuracy combine discrimination and calibration, as described in Section 15.3.3.\nIn addition, accuracy indices can be threshold-dependent or -independent and can be scale-dependent or -independent. Threshold-dependent accuracy indices differ based on the cutoff (i.e., threshold), whereas threshold-independent accuracy indices do not. Thus, raising or lowering the cutoff will change threshold-dependent accuracy indices. Scale-dependent accuracy indices depend on the metric/scale of the data, whereas scale-independent accuracy indices do not. Thus, scale-dependent accuracy indices cannot be directly compared when using measures of differing scales, whereas scale-independent accuracy indices can be compared across data of differing scales.\n\n15.3.1 Discrimination\nWhen dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity, area under the ROC curve) are described in INSERT.\n\n15.3.2 Calibration\nWhen dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by Lindhiem et al. (2020).\nCalibration is relevant to all kinds of predictions, including weather forecasts. For instance, on the days that the meteorologist says there is a 60% chance of rain, it should rain about 60% of the time. Calibration is also important for fantasy football predictions. When projections state that a group of players is each expected to score 200 points, their projections would be miscalibrated if those players scored only 150 points on average.\nThere are four general patterns of miscalibration: overextremity, underextremity, overprediction, and underprediction (see Figure 15.7). Overextremity exists when the predicted probabilites are too close to the extremes (zero or one). Underextremity exists when the predicted probabilities are too far away from the extremes. Overprediction exists when the predicted probabilities are consistently greater than the observed probabilities. Underprediction exists when the predicted probabilities are consistently less than the observed probabilities. For a more thorough description of these types of miscalibration, see Lindhiem et al. (2020).\nIndices for evaluating calibration are described in Section 15.7.3.\n\n15.3.3 General Accuracy\nGeneral accuracy indices combine estimates of discrimination and calibration.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionCategorical",
    "href": "evaluating-prediction-accuracy.html#sec-predictionCategorical",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.4 Prediction of Categorical Outcomes",
    "text": "15.4 Prediction of Categorical Outcomes\nTo evaluate the accuracy of our predictions for categorical outcome variables (e.g., binary, dichotomous, or nominal data), we can use either threshold-dependent or threshold-independent accuracy indices.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionContinuous",
    "href": "evaluating-prediction-accuracy.html#sec-predictionContinuous",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.5 Prediction of Continuous Outcomes",
    "text": "15.5 Prediction of Continuous Outcomes\nTo evaluate the accuracy of our predictions for continuous outcome variables (e.g., ordinal, interval, or ratio data), the outcome variable does not have cutoffs, so we would use threshold-independent accuracy indices.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-thresholdDependentAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-thresholdDependentAccuracy",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.6 Threshold-Dependent Accuracy Indices",
    "text": "15.6 Threshold-Dependent Accuracy Indices\n\n15.6.1 Decision Outcomes\nTo consider how we can evaluate the accuracy of predictions for a categorical outcome, consider an example adapted from Meehl & Rosen (1955). The military conducts a test of its prospective members to screen out applicants who would likely fail basic training. To evaluate the accuracy of our predictions using the test, we can examine a confusion matrix. A confusion matrix is a matrix that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes): true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\nWhen discussing the four decision outcomes, “true” means an accurate judgment, whereas “false” means an inaccurate judgment. “Positive” means that the judgment was that the person has the characteristic of interest, whereas “negative” means that the judgment was that the person does not have the characteristic of interest. A true positive is a correct judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually have (or will have) the characteristic. A true negative is a correct judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false positive is an incorrect judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A false negative is an incorrect judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do have (or will have) the characteristic.\nAn example of a confusion matrix is in INSERT.\nWith the information in the confusion matrix, we can calculate the marginal sums and the proportion of people in each cell (in parentheses), as depicted in INSERT.\nThat is, we can sum across the rows and columns to identify how many people actually showed poor adjustment (\\(n = 100\\)) versus good adjustment (\\(n = 1,900\\)), and how many people were selected to reject (\\(n = 508\\)) versus retain (\\(n = 1,492\\)). If we sum the column of predicted marginal sums (\\(508 + 1,492\\)) or the row of actual marginal sums (\\(100 + 1,900\\)), we get the total number of people (\\(N = 2,000\\)).\nBased on the marginal sums, we can compute the marginal probabilities, as depicted in INSERT.\nThe marginal probability of the person having the characteristic of interest (i.e., showing poor adjustment) is called the base rate (BR). That is, the base rate is the proportion of people who have the characteristic. It is calculated by dividing the number of people with poor adjustment (\\(n = 100\\)) by the total number of people (\\(N = 2,000\\)): \\(BR = \\frac{FN + TP}{N}\\). Here, the base rate reflects the prevalence of poor adjustment. In this case, the base rate is .05, so there is a 5% chance that an applicant will be poorly adjusted. The marginal probability of good adjustment is equal to 1 minus the base rate of poor adjustment.\nThe marginal probability of predicting that a person has the characteristic (i.e., rejecting a person) is called the selection ratio (SR). The selection ratio is the proportion of people who will be selected (in this case, rejected rather than retained); i.e., the proportion of people who are identified as having the characteristic. The selection ratio is calculated by dividing the number of people selected to reject (\\(n = 508\\)) by the total number of people (\\(N = 2,000\\)): \\(SR = \\frac{TP + FP}{N}\\). In this case, the selection ratio is .25, so 25% of people are rejected. The marginal probability of not selecting someone to reject (i.e., the marginal probability of retaining) is equal to 1 minus the selection ratio.\nThe selection ratio might be something that the test dictates according to its cutoff score. Or, the selection ratio might be imposed by external factors that place limits on how many people you can assign a positive test value. For instance, when deciding whether to treat a client, the selection ratio may depend on how many therapists are available and how many cases can be treated.\n\n15.6.2 Percent Accuracy\nBased on the confusion matrix, we can calculate the prediction accuracy based on the percent accuracy of the predictions. The percent accuracy is the number of correct predictions divided by the total number of predictions, and multiplied by 100. In the context of a confusion matrix, this is calculated as: \\(100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\\). In this case, our percent accuracy was 78%—that is, 78% of our predictions were accurate, and 22% of our predictions were inaccurate.\n\n15.6.3 Percent Accuracy by Chance\n78% sounds pretty accurate. And it is much higher than 50%, so we are doing a pretty good job, right? Well, it is important to compare our accuracy to what accuracy we would expect to get by chance alone, if predictions were made by a random process rather than using a test’s scores. Our selection ratio was 25.4%. How accurate would we be if we randomly selected 25.4% of people to reject? To determine what accuracy we could get by chance alone given the selection ratio and the base rate, we can calculate the chance probability of true positives and the chance probability of true negatives. The probability of a given cell in the confusion matrix is a joint probability—the probability of two events occurring simultaneously. To calculate a joint probability, we multiply the probability of each event.\nSo, to get the chance expectancies of true positives, we would multiply the respective marginal probabilities, as in Equation 15.1:\n\\[\n\\begin{aligned}\n  P(TP) &= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\\n   &= BR \\times SR \\\\\n  &= .05 \\times .254 \\\\\n  &= .0127\n\\end{aligned}\n\\tag{15.1}\\]\nTo get the chance expectancies of true negatives, we would multiply the respective marginal probabilities, as in Equation 15.2:\n\\[\n\\begin{aligned}\n  P(TN) &= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\\n   &= (1 - BR) \\times (1 - SR) \\\\\n  &= .95 \\times .746 \\\\\n  &= .7087\n\\end{aligned}\n\\tag{15.2}\\]\nTo get the percent accuracy by chance, we sum the chance expectancies for the correct predictions (TP and TN): \\(.0127 + .7087 = .7214\\). Thus, the percent accuracy you can get by chance alone is 72%. This is because most of our predictions are to retain people, and the base rate of poor adjustment is quite low (.05). Our measure with 78% accuracy provides only a 6% increment in correct predictions. Thus, you cannot judge how good your judgment or prediction is until you know how you would do by random chance.\nThe chance expectancies for each cell of the confusion matrix are in INSERT\n\n15.6.4 Predicting from the Base Rate\nNow, let us consider how well you would do if you were to predict from the base rate. Predicting from the base rate is also called “betting from the base rate”, and it involves setting the selection ratio by taking advantage of the base rate so that you go with the most likely outcome in every prediction. Because the base rate is quite low (.05), we could predict from the base rate by selecting no one to reject (i.e., setting the selection ratio at zero). Our percent accuracy by chance if we predict from the base rate would be calculated by multiplying the marginal probabilities, as we did above, but with a new selection ratio, as in Equation 15.3:\n\\[\n\\begin{aligned}\n  P(TP) &= P(\\text{Poor adjustment}) \\times P(\\text{Reject})\\\\\n   &= BR \\times SR \\\\\n  &= .05 \\times 0 \\\\\n  &= 0 \\\\ \\\\\n  P(TN) &= P(\\text{Good adjustment}) \\times P(\\text{Retain})\\\\\n   &= (1 - BR) \\times (1 - SR) \\\\\n  &= .95 \\times 1 \\\\\n  &= .95\n\\end{aligned}\n\\tag{15.3}\\]\nWe sum the chance expectancies for the correct predictions (TP and TN): \\(0 + .95 = .95\\). Thus, our percent accuracy by predicting from the base rate is 95%. This is damning to our measure because it is a much higher accuracy than the accuracy of our measure. That is, we can be much more accurate than our measure simply by predicting from the base rate and selecting no one to reject.\nGoing with the most likely outcome in every prediction (predicting from the base rate) can be highly accurate (in terms of percent accuracy) as noted by Meehl & Rosen (1955), especially when the base rate is very low or very high. This should serve as an important reminder that we need to compare the accuracy of our measures to the accuracy by (1) random chance and (2) predicting from the base rate. There are several important implications of the impact of base rates on prediction accuracy. One implication is that using the same test in different settings with different base rates will markedly change the accuracy of the test. Oftentimes, using a test will actually decrease the predictive accuracy when the base rate deviates greatly from .50. But percent accuracy is not everything. Percent accuracy treats different kinds of errors as if they are equally important. However, the value we place on different kinds of errors may be different, as described next.\n\n15.6.5 Different Kinds of Errors Have Different Costs\nSome errors have a high cost, and some errors have a low cost. Among the four decision outcomes, there are two types of errors: false positives and false negatives. The extent to which false positives and false negatives are costly depends on the prediction problem. So, even though you can often be most accurate by going with the base rate, it may be advantageous to use a screening instrument despite lower overall accuracy because of the huge difference in costs of false positives versus false negatives in some cases.\nConsider the example of a screening instrument for HIV. False positives would be cases where we said that someone is at high risk of HIV when they are not, whereas false negatives are cases where we said that someone is not at high risk when they actually are. The costs of false positives include a shortage of blood, some follow-up testing, and potentially some anxiety, but that is about it. The costs of false negatives may be people getting HIV. In this case, the costs of false negatives greatly outweigh the costs of false positives, so we use a screening instrument to try to identify the cases at high risk for HIV because of the important consequences of failing to do so, even though using the screening instrument will lower our overall accuracy level.\nAnother example is when the Central Intelligence Agency (CIA) used a screen for protective typists during wartime to try to detect spies. False positives would be cases where the CIA believes that a person is a spy when they are not, and the CIA does not hire them. False negatives would be cases where the CIA believes that a person is not a spy when they actually are, and the CIA hires them. In this case, a false positive would be fine, but a false negative would be really bad.\nHow you weigh the costs of different errors depends considerably on the domain and context. Possible costs of false positives to society include: unnecessary and costly treatment with side effects and sending an innocent person to jail (despite our presumption of innocence in the United States criminal justice system that a person is innocent until proven guilty). Possible costs of false negatives to society include: setting a guilty person free, failing to detect a bomb or tumor, and preventing someone from getting treatment who needs it.\nThe differential costs of different errors also depend on how much flexibility you have in the selection ratio in being able to set a stringent versus loose selection ratio. Consider if there is a high cost of getting rid of people during the selection process. For example, if you must hire 100 people and only 100 people apply for the position, you cannot lose people, so you need to hire even high-risk people. However, if you do not need to hire many people, then you can hire more conservatively.\nAny time the selection ratio differs from the base rate, you will make errors. For example, if you reject 25% of applicants, and the base rate of poor adjustment is 5%, then you are making errors of over-rejecting (false positives). By contrast, if you reject 1% of applicants and the base rate of poor adjustment is 5%, then you are making errors of under-rejecting or over-accepting (false negatives).\nA low base rate makes it harder to make predictions, and tends to lead to less accurate predictions. For instance, it is very challenging to predict low base rate behaviors, including suicide (Kessler et al., 2020). For this reason, it is likely much more challenging to predict touchdowns—which happen relatively less often—than it is to predict passing/rushing/receiving yards—which are more frequent and continuously distributed.\n[EVALUATE EMPIRICALLY]\n\n15.6.6 Sensitivity, Specificity, PPV, and NPV\nAs described earlier, percent accuracy is not the only important aspect of accuracy. Percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the condition (if the base rate is low) or that everyone has the condition (if the base rate is high). Thus, it is also important to consider other aspects of accuracy, including sensitivity (SN), specificity (SP), positive predictive value (PPV), and negative predictive value (NPV). We want our predictions to be sensitive to be able to detect the characteristic but also to be specific so that we classify only people actually with the characteristic as having the characteristic.\nLet us return to the confusion matrix in INSERT. If we know the frequency of each of the four predicted-actual combinations of the confusion matrix (TP, TN, FP, FN), we can calculate sensitivity, specificity, PPV, and NPV.\nSensitivity is the proportion of those with the characteristic (\\(\\text{TP} + \\text{FN}\\)) that we identified with our measure (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{86}{86 + 14} = .86\\). Specificity is the proportion of those who do not have the characteristic (\\(\\text{TN} + \\text{FP}\\)) that we correctly classify as not having the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{1,478}{1,478 + 422} = .78\\). PPV is the proportion of those who we classify as having the characteristic (\\(\\text{TP} + \\text{FP}\\)) who actually have the characteristic (\\(\\text{TP}\\)): \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{86}{86 + 422} = .17\\). NPV is the proportion of those we classify as not having the characteristic (\\(\\text{TN} + \\text{FN}\\)) who actually do not have the characteristic (\\(\\text{TN}\\)): \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{1,478}{1,478 + 14} = .99\\).\nSensitivity, specificity, PPV, and NPV are proportions, and their values therefore range from 0 to 1, where higher values reflect greater accuracy. With sensitivity, specificity, PPV, and NPV, we have a good snapshot of how accurate the measure is at a given cutoff. In our case, our measure is good at finding whom to reject (high sensitivity), but it is rejecting too many people who do not need to be rejected (lower PPV due to many FPs). Most people whom we classify as having the characteristic do not actually have the characteristic. However, the fact that we are over-rejecting could be okay depending on our goals, for instance, if we do not care about over-dropping (i.e., the PPV being low).\n\n15.6.6.1 Some Accuracy Estimates Depend on the Cutoff\nSensitivity, specificity, PPV, and NPV differ based on the cutoff (i.e., threshold) for classification. Consider the following example. Aliens visit Earth, and they develop a test to determine whether a berry is edible or inedible.\nFigure 15.1 depicts the distributions of scores by berry type. Note how there are clearly two distinct distributions. However, the distributions overlap to some degree. Thus, any cutoff will have at least some inaccurate classifications. The extent of overlap of the distributions reflects the amount of measurement error of the measure with respect to the characteristic of interest.\n\nCode#No Cutoff\nsampleSize &lt;- 1000\n\nedibleScores &lt;- rnorm(sampleSize, 50, 15)\ninedibleScores &lt;- rnorm(sampleSize, 100, 15)\n\nedibleData &lt;- data.frame(\n  score = c(\n    edibleScores,\n    inedibleScores),\n  type = c(\n    rep(\"edible\", sampleSize),\n    rep(\"inedible\", sampleSize)))\n\ncutoff &lt;- 75\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = edibleData,\n  aes(\n    x = score,\n    ymin = 0,\n    fill = type)) +\n  geom_density(alpha = .5) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(2)[1],\n      viridis::viridis(2)[2])) +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 15.1: Distribution of Test Scores by Berry Type.\n\n\n\n\nFigure 15.2 depicts the distributions of scores by berry type with a cutoff. The red line indicates the cutoff—the level above which berries are classified by the test as inedible. There are errors on each side of the cutoff. Below the cutoff, there are some false negatives (blue): inedible berries that are inaccurately classified as edible. Above the cutoff, there are some false positives (green): edible berries that are inaccurately classified as inedible. Costs of false negatives could include sickness or death from eating the inedible berries. Costs of false positives could include taking longer to find food, finding insufficient food, and starvation.\n\nCode#Standard Cutoff\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 15.2: Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nBased on our assessment goals, we might use a different selection ratio by changing the cutoff. Figure 15.3 depicts the distributions of scores by berry type when we raise the cutoff. There are now more false negatives (blue) and fewer false positives (green). If we raise the cutoff (to be more conservative), the number of false negatives increases and the number of false positives decreases. Consequently, as the cutoff increases, sensitivity and NPV decrease (because we have more false negatives), whereas specificity and PPV increase (because we have fewer false positives). A higher cutoff could be optimal if the costs of false positives are considered greater than the costs of false negatives. For instance, if the aliens cannot risk eating the inedible berries because the berries are fatal, and there are sufficient edible berries that can be found to feed the alien colony.\n\nCode#Raise the cutoff\ncutoff &lt;- 85\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 15.3: Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nFigure 15.4 depicts the distributions of scores by berry type when we lower the cutoff. There are now fewer false negatives (blue) and more false positives (green). If we lower the cutoff (to be more liberal), the number of false negatives decreases and the number of false positives increases. Consequently, as the cutoff decreases, sensitivity and NPV increase (because we have fewer false negatives), whereas specificity and PPV decrease (because we have more false positives). A lower cutoff could be optimal if the costs of false negatives are considered greater than the costs of false positives. For instance, if the aliens cannot risk missing edible berries because they are in short supply relative to the size of the alien colony, and eating the inedible berries would, at worst, lead to minor, temporary discomfort.\n\nCode#Lower the cutoff\ncutoff &lt;- 65\n\nhist_edible &lt;- density(\n  edibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &gt;= cutoff)\n\nhist_edible$type[hist_edible$area == TRUE] &lt;- \"edible_FP\"\nhist_edible$type[hist_edible$area == FALSE] &lt;- \"edible_TN\"\n\nhist_inedible &lt;- density(\n  inedibleScores,\n  from = 0,\n  to = 150) %$% # exposition pipe magrittr::`%$%`\n  data.frame(\n    x = x,\n    y = y) %&gt;%\n  mutate(area = x &lt; cutoff)\n\nhist_inedible$type[hist_inedible$area == TRUE] &lt;- \"inedible_FN\"\nhist_inedible$type[hist_inedible$area == FALSE] &lt;- \"inedible_TP\"\n\ndensity_data &lt;- bind_rows(\n  hist_edible,\n  hist_inedible)\n\ndensity_data$type &lt;- factor(\n  density_data$type,\n  levels = c(\n    \"edible_TN\",\n    \"inedible_TP\",\n    \"edible_FP\",\n    \"inedible_FN\"))\n\nggplot(\n  data = density_data,\n  aes(\n    x = x,\n    ymin = 0,\n    ymax = y,\n    fill = type)) +\n  geom_ribbon(alpha = 1) +\n  scale_fill_manual(\n    name = \"Berry Type\",\n    values = c(\n      viridis::viridis(4)[4],\n      viridis::viridis(4)[1],\n      viridis::viridis(4)[3],\n      viridis::viridis(4)[2]),\n    breaks = c(\"edible_TN\",\"inedible_TP\",\"edible_FP\",\"inedible_FN\"),\n    labels = c(\"Edible: TN\",\"Inedible: TP\",\"Edible: FP\",\"Inedible: FN\")) +\n  geom_line(aes(y = y)) +\n  geom_vline(\n    xintercept = cutoff,\n    color = \"red\",\n    linewidth = 2) +\n  scale_x_continuous(name = \"score\") +\n  scale_y_continuous(name = \"Frequency\") +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank())\n\n\n\n\n\n\nFigure 15.4: Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.\n\n\n\n\nIn sum, sensitivity and specificity differ based on the cutoff for classification. If we raise the cutoff, sensitivity and PPV increase (due to fewer false positives), whereas sensitivity and NPV decrease (due to more false negatives). If we lower the cutoff, sensitivity and NPV increase (due to fewer false negatives), whereas specificity and PPV decrease (due to more false positives). Thus, the optimal cutoff depends on how costly each type of error is: false negatives and false positives. If false negatives are more costly than false positives, we would set a low cutoff. If false positives are more costly than false negatives, we would set a high cutoff.\n\n15.6.7 Signal Detection Theory\nSignal detection theory (SDT) is a probability-based theory for the detection of a given stimulus (signal) from a stimulus set that includes non-target stimuli (noise). SDT arose through the development of radar (RAdio Detection And Ranging) and sonar (SOund Navigation And Ranging) in World War II based on research on sensory-perception research. The military wanted to determine which objects on radar/sonar were enemy aircraft/submarines, and which were noise (e.g., different object in the environment or even just the weather itself). SDT allowed determining how many errors operators made (how accurate they were) and decomposing errors into different kinds of errors. SDT distinguishes between sensitivity and bias. In SDT, sensitivity (or discriminability) is how well an assessment distinguishes between a target stimulus and non-target stimuli (i.e., how well the assessment detects the target stimulus amid non-target stimuli). Bias is the extent to which the probability of a selection decision from the assessment is higher or lower than the true rate of the target stimulus.\nSome radar/sonar operators were not as sensitive to the differences between signal and noise, due to factors such as age, ability to distinguish gradations of a signal, etc. People who showed low sensitivity (i.e., who were not as successful at distinguishing between signal and noise) were screened out because the military perceived sensitivity as a skill that was not easily taught. By contrast, other operators could distinguish signal from noise, but their threshold was too low or high—they could take in information, but their decisions tended to be wrong due to systematic bias or poor calibration. That is, they systematically over-rejected or under-rejected stimuli. Over-rejecting leads to many false negatives (i.e., saying that a stimulus is safe when it is not). Under-rejecting leads to many false positives (i.e., saying that a stimulus is harmful when it is not). A person who showed good sensitivity but systematic bias was considered more teach-able than a person who showed low sensitivity. Thus, radar and sonar operators were selected based on their sensitivity to distinguish signal from noise, and then were trained to improve the calibration so they reduce their systematic bias and do not systematically over- or under-reject.\nAlthough SDT was originally developed for use in World War II, it now plays an important role in many areas of science and medicine. A medical application of SDT is tumor detection in radiology. Another application of SDT in society is using x-ray to detect bombs or other weapons. An example of applying SDT to fantasy football could be in the prediction (and evaluation) of whether or not a player scores a touchdown in a game.\nSDT metrics of sensitivity include \\(d'\\) (“\\(d\\)-prime”), \\(A\\) (or \\(A'\\)), and the area under the receiver operating characteristic (ROC) curve. SDT metrics of bias include \\(\\beta\\) (beta), \\(c\\), and \\(b\\).\n\n15.6.7.1 Receiver Operating Characteristic (ROC) Curve\nThe x-axis of the ROC curve is the false alarm rate or false positive rate (\\(1 -\\) specificity). The y-axis is the hit rate or true positive rate (sensitivity). We can trace the ROC curve as the combination between sensitivity and specificity at every possible cutoff. At a cutoff of zero (top right of ROC curve), we calculate sensitivity (1.0) and specificity (0) and plot it. At a cutoff of zero, the assessment tells us to make an action for every stimulus (i.e., it is the most liberal). We then gradually increase the cutoff, and plot sensitivity and specificity at each cutoff. As the cutoff increases, sensitivity decreases and specificity increases. We end at the highest possible cutoff, where the sensitivity is 0 and the specificity is 1.0 (i.e., we never make an action; i.e., it is the most conservative). Each point on the ROC curve corresponds to a pair of hit and false alarm rates (sensitivity and specificity) resulting from a specific cutoff value. Then, we can draw lines or a curve to connect the points.\nINSERT depicts an empirical ROC plot where lines are drawn to connect the hit and false alarm rates.\nINSERT depicts an ROC curve where a smoothed and fitted curve is drawn to connect the hit and false alarm rates.\n\n15.6.7.1.1 Area Under the ROC Curve\nROC methods can be used to compare and compute the discriminative power of measurement devices free from the influence of selection ratios, base rates, and costs and benefits. An ROC analysis yields a quantitative index of how well an index predicts a signal of interest or can discriminate between different signals. ROC analysis can help tell us how often our assessment would be correct. If we randomly pick two observations, and we were right once and wrong once, we were 50% accurate. But this would be a useless measure because it reflects chance responding.\nThe geometrical area under the ROC curve reflects the discriminative accuracy of the measure. The index is called the area under the curve (AUC) of an ROC curve. AUC quantifies the discriminative power of an assessment. AUC is the probability that a randomly selected target and a randomly selected non-target is ranked correctly by the assessment method. AUC values range from 0.0 to 1.0, where chance accuracy is 0.5 as indicated by diagonal line in the ROC curve. That is, a measure can be useful to the extent that its ROC curve is above the diagonal line (i.e., its discriminative accuracy is above chance).\nAUC is a threshold-independent accuracy index that applies across all possible cutoff values.\nFigure 15.5 depicts ROC curves with a range of AUC values.\n\nCodeset.seed(52242)\n\nauc60 &lt;- petersenlab::simulateAUC(.60, 50000)\nauc70 &lt;- petersenlab::simulateAUC(.70, 50000)\nauc80 &lt;- petersenlab::simulateAUC(.80, 50000)\nauc90 &lt;- petersenlab::simulateAUC(.90, 50000)\nauc95 &lt;- petersenlab::simulateAUC(.95, 50000)\nauc99 &lt;- petersenlab::simulateAUC(.99, 50000)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc60,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .52,\n  print.auc.y = .61,\n  print.auc.pattern = \"%.2f\")\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc70,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .6,\n  print.auc.y = .67,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc80,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .695,\n  print.auc.y = .735,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc90,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .805,\n  print.auc.y = .815,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc95,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .875,\n  print.auc.y = .865,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\nplot(\n  pROC::roc(\n    y ~ x,\n    auc99,\n    smooth = TRUE),\n  legacy.axes = TRUE,\n  print.auc = TRUE,\n  print.auc.x = .94,\n  print.auc.y = .94,\n  print.auc.pattern = \"%.2f\",\n  add = TRUE)\n\n\n\n\n\n\nFigure 15.5: Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures.\n\n\n\n\nAs an example, given an AUC of .75, this says that the overall score of an individual who has the characteristic in question will be higher 75% of the time than the overall score of an individual who does not have the characteristic. In lay terms, AUC provides the probability that we will classify correctly based on our instrument if we were to randomly pick one good and one bad outcome. AUC is a stronger index of accuracy than percent accuracy, because you can have high percent accuracy just by going with the base rate. AUC tells us how much better than chance a measure is at discriminating outcomes. AUC is useful as a measure of general discriminative accuracy, and it tells us how accurate a measure is at all possible cutoffs. Knowing the accuracy of a measure at all possible cutoffs can be helpful for selecting the optimal cutoff, given the goals of the assessment. In reality, however, we may not be interested in all cutoffs because not all errors are equal in their costs.\nIf we lower the base rate, we would need a larger sample to get enough people to classify into each group. SDT/ROC methods are traditionally about dichotomous decisions (yes/no), not graded judgments. SDT/ROC methods can get messy with ordinal data that are more graded because you would have an AUC curve for each ordinal grouping.\n\n15.6.8 Accuracy Indices\nThere are various accuracy indices we can use to evaluate the accuracy of predictions for categorical outcome variables. We have already described several accuracy indices, including percent accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and area under the ROC curve. We describe these and other indices in greater detail below.\nThe petersenlab package (Petersen, 2024a) contains the accuracyAtCutoff() function that computes many accuracy indices for the prediction of categorical outcome variables.\n\nCode#petersenlab::accuracyAtCutoff()\n\n\nThe petersenlab package (Petersen, 2024a) contains the accuracyAtEachCutoff() function that computes many accuracy indices for the prediction of categorical outcome variables at each possible cutoff.\n\nCode#petersenlab::accuracyAtEachCutoff()\n\n\nThere are also test calculators available online:\n\nhttp://araw.mede.uic.edu/cgi-bin/testcalc.pl\nhttps://dlrs.shinyapps.io/shinyDLRs\n\n\n15.6.8.1 Confusion Matrix aka 2x2 Accuracy Table aka Cross-Tabulation aka Contingency Table\nA confusion matrix (aka 2x2 accuracy table, cross-tabulation table, or contigency table) is a matrix for categorical data that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes). In such a case, the confusion matrix provides a tabular count of each type of accurate cases (true positives and true negatives) versus the number of each type of error (false positives and false negatives), as shown in INSERT. An example of a confusion matrix is in INSERT.\n\n15.6.8.1.1 Number\n\nCode#table(mydata$diagnosisFactor, mydata$diseaseFactor)\n\n\n\n15.6.8.1.2 Number with margins added\n\nCode#addmargins(table(mydata$diagnosisFactor, mydata$diseaseFactor))\n\n\n\n15.6.8.1.3 Proportions\n\nCode#prop.table(table(mydata$diagnosisFactor, mydata$diseaseFactor))\n\n\n\n15.6.8.1.4 Proportions with margins added\n\nCode#addmargins(prop.table(table(mydata$diagnosisFactor, mydata$diseaseFactor)))\n\n\n\n15.6.8.2 True Positives (TP)\nTrue positives (TPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is correct—that is, the test says that a classification is present, and the classification is present. True positives are also called valid positives (VPs) or hits. Higher values reflect greater accuracy. The formula for true positives is in Equation 15.4:\n\\[\n\\begin{aligned}\n  \\text{TP} &= \\text{BR} \\times \\text{SR} \\times N\n\\end{aligned}\n\\tag{15.4}\\]\n\n15.6.8.3 True Negatives (TN)\nTrue negatives (TNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is correct—that is, the test says that a classification is not present, and the classification is actually not present. True negatives are also called valid negatives (VNs) or correct rejections. Higher values reflect greater accuracy. The formula for true negatives is in Equation 15.5:\n\\[\n\\begin{aligned}\n  \\text{TN} &= (1 - \\text{BR}) \\times (1 - \\text{SR}) \\times N\n\\end{aligned}\n\\tag{15.5}\\]\n\n15.6.8.4 False Positives (FP)\nFalse positives (FPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is incorrect—that is, the test says that a classification is present, and the classification is not present. False positives are also called false alarms (FAs). Lower values reflect greater accuracy. The formula for false positives is in Equation Equation 15.6:\n\\[\n\\begin{aligned}\n  \\text{FP} &= (1 - \\text{BR}) \\times \\text{SR} \\times N\n\\end{aligned}\n\\tag{15.6}\\]\n\n15.6.8.5 False Negatives (FN)\nFalse negatives (FNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is incorrect—that is, the test says that a classification is not present, and the classification is present. False negatives are also called misses. Lower values reflect greater accuracy. The formula for false negatives is in Equation 15.7:\n\\[\n\\begin{aligned}\n  \\text{FN} &= \\text{BR} \\times (1 - \\text{SR}) \\times N\n\\end{aligned}\n\\tag{15.7}\\]\n\n15.6.8.6 Selection Ratio (SR)\nThe selection ratio (SR) is the marginal probability of selection, independent of other things: \\(P(R_i)\\). It is not an index of accuracy, per se. In medicine, the selection ratio is the proportion of people who test positive for the disease. In fantasy football, the selection ratio is the proportion of players who you predict will show a given outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the selection ratio is the proportion of players who you predict will score a touchdown. The formula for calculating the selection ratio is in Equation 15.8.\n\\[\n\\begin{aligned}\n  \\text{SR} &= P(R_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FP}}{N}\n\\end{aligned}\n\\tag{15.8}\\]\n\n15.6.8.7 Base Rate (BR)\nThe base rate (BR) of a classification is its marginal probability, independent of other things: \\(P(C_i)\\). It is not an index of accuracy, per se. In medicine, the base rate of a disease is its prevalence in the population, as in Equation 15.9. Without additional information, the base rate is used as the initial pretest probability. In fantasy football, the base rate is the proportion of players who actually show the particular outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the base rate is the proportion of players who actually score a touchdown in the game. The formula for calculating the selection ratio is in Equation 15.9.\n\\[\n\\begin{aligned}\n  \\text{BR} &= P(C_i) \\\\\n  &= \\frac{\\text{TP} + \\text{FN}}{N}\n\\end{aligned}\n\\tag{15.9}\\]\n\n15.6.8.8 Pretest Odds\nThe pretest odds of a classification can be estimated using the pretest probability (i.e., base rate). To convert a probability to odds, divide the probability by one minus that probability, as in Equation 15.10.\n\\[\n\\begin{aligned}\n  \\text{pretest odds} &= \\frac{\\text{pretest probability}}{1 - \\text{pretest probability}} \\\\\n\\end{aligned}\n\\tag{15.10}\\]\n\n15.6.8.9 Percent Accuracy\nPercent Accuracy is also called overall accuracy. Higher values reflect greater accuracy. The formula for percent accuracy is in Equation 15.11. Percent accuracy has several problems. First, it treats all errors (FP and FN) as equally important. However, in practice, it is rarely the case that false positives and false negatives are equally important. Second, percent accuracy can be misleading because it is highly influenced by base rates. You can have a high percent accuracy by predicting from the base rate and saying that no one has the characteristic (if the base rate is low) or that everyone has the characteristic (if the base rate is high). Thus, it is also important to consider other aspects of accuracy.\n\\[\n\\text{Percent Accuracy} = 100\\% \\times \\frac{\\text{TP} + \\text{TN}}{N}\n\\tag{15.11}\\]\n\n15.6.8.10 Percent Accuracy by Chance\nThe formula for calculating percent accuracy by chance is in Equation 15.12.\n\\[\n\\begin{aligned}\n  \\text{Percent Accuracy by Chance} &= 100\\% \\times [P(\\text{TP}) + P(\\text{TN})] \\\\\n  &= 100\\% \\times \\{(\\text{BR} \\times {\\text{SR}}) + [(1 - \\text{BR}) \\times (1 - \\text{SR})]\\}\n\\end{aligned}\n\\tag{15.12}\\]\n\n15.6.8.11 Percent Accuracy Predicting from the Base Rate\nPredicting from the base rate is going with the most likely outcome in every prediction. If the base rate is less than .50, it would involve predicting that the condition is absent for every case. If the base rate is .50 or above, it would involve predicting that the condition is present for every case. Predicting from the base rate is a special case of percent accuracy by chance when the selection ratio is set to either one (if the base rate \\(\\geq\\) .5) or zero (if the base rate &lt; .5).\n\n15.6.8.12 Relative Improvement Over Chance (RIOC)\nRelative improvement over chance (RIOC) is a prediction’s improvement over chance as a proportion of the maximum possible improvement over chance, as described by Farrington & Loeber (1989). Higher values reflect greater accuracy. The formula for calculating RIOC is in Equation 15.13.\n\\[\n\\begin{aligned}\n  \\text{relative improvement over chance (RIOC)} &= \\frac{\\text{total correct} - \\text{chance correct}}{\\text{maximum correct} - \\text{chance correct}} \\\\\n\\end{aligned}\n\\tag{15.13}\\]\n\n15.6.8.13 Relative Improvement Over Predicting from the Base Rate\nRelative improvement over predicting from the base rate is a prediction’s improvement over predicting from the base rate as a proportion of the maximum possible improvement over predicting from the base rate. Higher values reflect greater accuracy. The formula for calculating relative improvement over predicting from the base rate is in Equation 15.14.\n\\[\n\\scriptsize\n\\begin{aligned}\n  \\text{relative improvement over predicting from base rate} &= \\frac{\\text{total correct} - \\text{correct by predicting from base rate}}{\\text{maximum correct} - \\text{correct by predicting from base rate}} \\\\\n\\end{aligned}\n\\tag{15.14}\\]\n\n15.6.8.14 Sensitivity (SN)\nSensitivity (SN) is also called true positive rate (TPR), hit rate (HR), or recall. Sensitivity is the conditional probability of a positive test given that the person has the condition: \\(P(R|C)\\). Higher values reflect greater accuracy. The formula for calculating sensitivity is in Equation 15.15. As described in Section Section 15.6.6.1, as the cutoff increases (becomes more conservative), sensitivity decreases. As the cutoff decreases, sensitivity increases.\n\\[\n\\begin{aligned}\n  \\text{sensitivity (SN)} &= P(R|C) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{TP}}{N \\times \\text{BR}} = 1 - \\text{FNR}\n\\end{aligned}\n\\tag{15.15}\\]\n\n15.6.8.15 Specificity (SP)\nSpecificity (SP) is also called true negative rate (TNR) or selectivity. Specificity is the conditional probability of a negative test given that the person does not have the condition: \\(P(\\text{not } R|\\text{not } C)\\). Higher values reflect greater accuracy. The formula for calculating specificity is in Equation 15.16. As described in Section Section 15.6.6.1, as the cutoff increases (becomes more conservative), specificity increases. As the cutoff decreases, specificity decreases.\n\\[\n\\begin{aligned}\n  \\text{specificity (SP)} &= P(\\text{not } R|\\text{not } C) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{\\text{TN}}{N (1 - \\text{BR})} = 1 - \\text{FPR}\n\\end{aligned}\n\\tag{15.16}\\]\n\n15.6.8.16 False Negative Rate (FNR)\nThe false negative rate (FNR) is also called the miss rate. The false negative rate is the conditional probability of a negative test given that the person has the condition: \\(P(\\text{not } R|C)\\). Lower values reflect greater accuracy. The formula for calculating false negative rate is in Equation 15.17.\n\\[\n\\begin{aligned}\n  \\text{false negative rate (FNR)} &= P(\\text{not } R|C) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TP}} = \\frac{\\text{FN}}{N \\times \\text{BR}} = 1 - \\text{TPR}\n\\end{aligned}\n\\tag{15.17}\\]\n\n15.6.8.17 False Positive Rate (FPR)\nThe false positive rate (FPR) is also called the false alarm rate (FAR) or fall-out. The false positive rate is the conditional probability of a positive test given that the person does not have the condition: \\(P(R|\\text{not } C)\\). Lower values reflect greater accuracy. The formula for calculating false positive rate is in Equation 15.18:\n\\[\n\\begin{aligned}\n  \\text{false positive rate (FPR)} &= P(R|\\text{not } C) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} = \\frac{\\text{FP}}{N (1 - \\text{BR})} = 1 - \\text{TNR}\n\\end{aligned}\n\\tag{15.18}\\]\n\n15.6.8.18 Positive Predictive Value (PPV)\nThe positive predictive value (PPV) is also called the positive predictive power (PPP) or precision. Many people confuse sensitivity (\\(P(R|C)\\)) with its inverse conditional probability, PPV (\\(P(C|R)\\)). PPV is the conditional probability of having the condition given a positive test: \\(P(C|R)\\). Higher values reflect greater accuracy. The formula for calculating positive predictive value is in Equation 15.19.\nPPV can be low even when sensitivity is high because it depends not only on sensitivity, but also on specificity and the base rate. Because PPV depends on the base rate, PPV is not an intrinsic property of a measure. The same measure will have a different PPV in different contexts with different base rates (Treat & Viken, 2023). As described in Section Section 15.6.6.1, as the base rate increases, PPV increases. As the base rate decreases, PPV decreases. PPV also differs as a function of the cutoff. As described in Section Section 15.6.6.1, as the cutoff increases (becomes more conservative), PPV increases. As the cutoff decreases (becomes more liberal), PPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{positive predictive value (PPV)} &= P(C|R) \\\\\n  &= \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{\\text{TP}}{N \\times \\text{SR}}\\\\\n  &= \\frac{\\text{sensitivity} \\times {\\text{BR}}}{\\text{sensitivity} \\times {\\text{BR}} + [(1 - \\text{specificity}) \\times (1 - \\text{BR})]}\n\\end{aligned}\n\\tag{15.19}\\]\n\n15.6.8.19 Negative Predictive Value (NPV)\nThe negative predictive value (NPV) is also called the negative predictive power (NPP). Many people confuse specificity (\\(P(\\text{not } R|\\text{not } C)\\)) with its inverse conditional probability, NPV (\\(P(\\text{not } C| \\text{not } R)\\)). NPV is the conditional probability of not having the condition given a negative test: \\(P(\\text{not } C| \\text{not } R)\\). Higher values reflect greater accuracy. The formula for calculating negative predictive value is in Equation 15.20.\nNPV can be low even when specificity is high because it depends not only on specificity, but also on sensitivity and the base rate. Because NPV depends on the base rate, NPV is not an intrinsic property of a measure. The same measure will have a different NPV in different contexts with different base rates (Treat & Viken, 2023). As described in Section Section 15.6.6.1, as the base rate increases, NPV decreases. As the base rate decreases, NPV increases. NPV also differs as a function of the cutoff. As described in Section Section 15.6.6.1, as the cutoff increases (becomes more conservative), NPV decreases. As the cutoff decreases (becomes more liberal), NPV decreases.\n\\[\n\\small\n\\begin{aligned}\n  \\text{negative predictive value (NPV)} &= P(\\text{not } C|\\text{not } R) \\\\\n  &= \\frac{\\text{TN}}{\\text{TN} + \\text{FN}} = \\frac{\\text{TN}}{N(\\text{1 - SR})}\\\\\n  &= \\frac{\\text{specificity} \\times (1-{\\text{BR}})}{\\text{specificity} \\times (1-{\\text{BR}}) + [(1 - \\text{sensitivity}) \\times \\text{BR})]}\n\\end{aligned}\n\\tag{15.20}\\]\n\n15.6.8.20 False Discovery Rate (FDR)\nMany people confuse the false positive rate (\\(P(R|\\text{not } C)\\)) with its inverse conditional probability, the false discovery rate (\\(P(\\text{not } C| R)\\)). The false discovery rate (FDR) is the conditional probability of not having the condition given a positive test: \\(P(\\text{not } C| R)\\). Lower values reflect greater accuracy. The formula for calculating false discovery rate is in Equation 15.21.\n\\[\n\\begin{aligned}\n  \\text{false discovery rate (FDR)} &= P(\\text{not } C|R) \\\\\n  &= \\frac{\\text{FP}}{\\text{FP} + \\text{TP}} = 1 - \\text{PPV}\n\\end{aligned}\n\\tag{15.21}\\]\n\n15.6.8.21 False Omission Rate (FOR)\nMany people confuse the false negative rate (\\(P(\\text{not } R|C)\\)) with its inverse conditional probability, the false omission rate (\\(P(C|\\text{not } R)\\)). The false omission rate (FOR) is the conditional probability of having the condition given a negative test: \\(P(C|\\text{not } R)\\). Lower values reflect greater accuracy. The formula for calculating false omission rate is in Section 15.6.8.21.\n\\[\n\\begin{aligned}\n  \\text{false omission rate (FOR)} &= P(C|\\text{not } R) \\\\\n  &= \\frac{\\text{FN}}{\\text{FN} + \\text{TN}} = 1 - \\text{NPV}\n\\end{aligned}\n\\] {#sec-falseOmissionRate}\n\n15.6.8.22 Youden’s J Statistic\nYouden’s J statistic is also called Youden’s Index or informedness. Youden’s J statistic is the sum of sensitivity and specificity (and subtracting one). Higher values reflect greater accuracy. The formula for calculating Youden’s J statistic is in Equation 15.22.\n\\[\n\\begin{aligned}\n  \\text{Youden's J statistic} &= \\text{sensitivity} + \\text{specificity} - 1\n\\end{aligned}\n\\tag{15.22}\\]\n\n15.6.8.23 Balanced Accuracy\nBalanced accuracy is the average of sensitivity and specificity. Higher values reflect greater accuracy. The formula for calculating balanced accuracy is in Equation 15.23.\n\\[\n\\begin{aligned}\n  \\text{balanced accuracy} &= \\frac{\\text{sensitivity} + \\text{specificity}}{2}\n\\end{aligned}\n\\tag{15.23}\\]\n\n15.6.8.24 F-Score\nThe F-score combines precision (positive predictive value) and recall (sensitivity), where \\(\\beta\\) indicates how many times more important sensitivity is than the positive predictive value. If sensitivity and the positive predictive value are equally important, \\(\\beta = 1\\), and the F-score is called the \\(F_1\\) score. Higher values reflect greater accuracy. The formula for calculating the F-score is in Equation 15.24.\n\\[\n\\begin{aligned}\n  F_\\beta &= (1 + \\beta^2) \\cdot \\frac{\\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\beta^2 \\cdot \\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{(1 + \\beta^2) \\cdot \\text{TP}}{(1 + \\beta^2) \\cdot \\text{TP} + \\beta^2 \\cdot \\text{FN} + \\text{FP}}\n\\end{aligned}\n\\tag{15.24}\\]\nThe formula for calculating the \\(F_1\\) score is in Equation 15.25.\n\\[\n\\begin{aligned}\n  F_1 &= \\frac{2 \\cdot \\text{positive predictive value} \\cdot \\text{sensitivity}}{(\\text{positive predictive value}) + \\text{sensitivity}} \\\\\n  &= \\frac{2 \\cdot \\text{TP}}{2 \\cdot \\text{TP} + \\text{FN} + \\text{FP}}\n\\end{aligned}\n\\tag{15.25}\\]\n\n15.6.8.25 Matthews Correlation Coefficient (MCC)\nThe Matthews correlation coefficient (MCC) is also called the phi coefficient. It is a correlation coefficient between predicted and observed values from a binary classification. Higher values reflect greater accuracy. The formula for calculating the MCC is in Equation 15.26.\n\\[\n\\begin{aligned}\n  \\text{MCC} &= \\frac{\\text{TP} \\times \\text{TN} - \\text{FP} \\times \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n\\end{aligned}\n\\tag{15.26}\\]\n\n15.6.8.26 Diagnostic Odds Ratio\nThe diagnostic odds ratio is the odds of a positive test among people with the condition relative to the odds of a positive test among people without the condition. Higher values reflect greater accuracy. The formula for calculating the diagnostic odds ratio is in Equation 15.27. If the predictor is bad, the diagnostic odds ratio could be less than one, and values can go up from there. If the diagnostic odds ratio is greater than 2, we take the odds ratio seriously because we are twice as likely to predict accurately than inaccurately. However, the diagnostic odds ratio ignores/hides base rates. When interpreting the diagnostic odds ratio, it is important to keep in mind the practical significance, because otherwise it is not very meaningful. Consider a risk factor that has a diagnostic odds ratio of 3 for tuberculosis, i.e., it puts you at 3 times as likely to develop tuberculosis. The prevalence of tuberculosis is relatively low. Assuming the prevalence of tuberculosis is less than 1/10th of 1%, your risk of developing tuberculosis is still very low even if the risk factor (with a diagnostic odds ratio of 3) is present.\n\\[\n\\begin{aligned}\n  \\text{diagnostic odds ratio} &= \\frac{\\text{TP} \\times \\text{TN}}{\\text{FP} \\times \\text{FN}} \\\\\n  &= \\frac{\\text{sensitivity} \\times \\text{specificity}}{(1 - \\text{sensitivity}) \\times (1 - \\text{specificity})} \\\\\n  &= \\frac{\\text{PPV} \\times \\text{NPV}}{(1 - \\text{PPV}) \\times (1 - \\text{NPV})} \\\\\n  &= \\frac{\\text{LR+}}{\\text{LR}-}\n\\end{aligned}\n\\tag{15.27}\\]\n\n15.6.8.27 Diagnostic Likelihood Ratio\nThe diagnostic likelihood ratio is described in Section 14.5.2.1. There are two types of diagnostic likelihood ratios: the positive likelihood ratio and the negative likelihood ratio.\n\n15.6.8.27.1 Positive Likelihood Ratio (LR+)\nThe positive likelihood ratio (LR+) is described in Section 14.5.2.1.1. The formula for calculating the positive likelihood ratio is in Equation 14.14.\n\n15.6.8.27.2 Negative Likelihood Ratio (LR−)\nThe negative likelihood ratio (LR−) is described in Section 14.5.2.1.2. The formula for calculating the negative likelihood ratio is in Equation 14.14.\n\n15.6.8.28 Posttest Odds\nAs presented in Equation 14.13, the posttest (or posterior) odds are equal to the pretest odds multiplied by the likelihood ratio. The posttest odds and posttest probability can be useful to calculate when the pretest probability is different from the pretest probability (or prevalence) of the classification. For instance, you might use a different pretest probability if a test result is already known and you want to know the updated posttest probability after conducting a second test. The formula for calculating posttest odds is in Equation 15.28.\n\\[\n\\begin{aligned}\n  \\text{posttest odds} &= \\text{pretest odds} \\times \\text{likelihood ratio} \\\\\n\\end{aligned}\n\\tag{15.28}\\]\nFor calculating the posttest odds of a true positive compared to a false positive, we use the positive likelihood ratio below. We would use the negative likelihood ratio if we wanted to calculate the posttest odds of a false negative compared to a true negative.\n\n15.6.8.29 Posttest Probability\nThe posttest probability is the probability of having the characteristic given a test result. When the base rate is used as the pretest probability, the posttest probability given a positive test is equal to positive predictive value. To convert odds to a probability, divide the odds by one plus the odds, as is in Equation 15.29.\n\\[\n\\begin{aligned}\n  \\text{posttest probability} &= \\frac{\\text{posttest odds}}{1 + \\text{posttest odds}}\n\\end{aligned}\n\\tag{15.29}\\]\n\n15.6.8.30 Mean Difference Between Predicted and Observed Values\nThe mean difference between predicted values versus observed values at a given cutoff is an index of miscalibration of predictions at that cutoff. It is called “calibration-in-the-small” (as opposed to calibration-in-the-large, which spans all cutoffs). Values closer to zero reflect greater accuracy. Values above zero indicate that the predicted values are, on average, greater than the observed values. Values below zero indicate that the observed values are, on average, greater than the predicted values.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-thresholdIndependentAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-thresholdIndependentAccuracy",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.7 Threshold-Independent Accuracy Indices",
    "text": "15.7 Threshold-Independent Accuracy Indices\nThis section describes threshold-independent indexes of accuracy. That is, each index of accuracy described in this section provides a single numerical index of accuracy that aggregates the accuracy across all possible cutoffs. The petersenlab package (Petersen, 2024a) contains the accuracyOverall() function that computes many threshold-independent accuracy indices.\n\n15.7.1 General Prediction Accuracy\nThere are many metrics of general prediction accuracy. When thinking about which metric(s) may be best for a given problem, it is important to consider the purpose of the assessment. The estimates of general prediction accuracy are separated below into scale-dependent and scale-independent accuracy estimates.\n\n15.7.1.1 Scale-Dependent Accuracy Estimates\nThe estimates of prediction accuracy described in this section are scale-dependent. These accuracy estimates depend on the unit of measurement and therefore cannot be compared across measures with different scales or across data sets.\n\n15.7.1.1.1 Mean Error\nHere, “error” (\\(e\\)) is the difference between the predicted and observed value for a given individual (\\(i\\)). Mean error (ME; also known as bias) is the mean difference between the predicted and observed values across individuals (\\(i\\)), that is, the mean of the errors across individuals (\\(e_i\\)). Values closer to zero reflect greater accuracy. If mean error is above zero, it indicates that predicted values are, on average, greater than observed values (i.e., overestimating errors). If mean error is below zero, it indicates that predicted values are, on average, less than observed values (i.e., underestimating errors). If both over-estimating and under-estimating errors are present, however, they can cancel each other out. As a result, even with a mean error of zero, there can still be considerable error present. Thus, although mean error can be helpful for examining whether predictions systematically under- or over-estimate the actual scores, other forms of accuracy are necessary to examine the extent of error. The formula for mean error is in Equation 15.30:\n\\[\n\\begin{aligned}\n  \\text{mean error} &= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)}{n} \\\\\n  &= \\text{mean}(e_i)\n\\end{aligned}\n\\tag{15.30}\\]\n\n15.7.1.1.2 Mean Absolute Error (MAE)\nMean absolute error (MAE) is the mean of the absolute value of differences between the predicted and observed values across individuals, that is, the mean of the absolute value of errors. Smaller MAE values (closer to zero) reflect greater accuracy. MAE is preferred over root mean squared error (RMSE) when you want to give equal weight to all errors and when the outliers have considerable impact. The formula for MAE is in Equation 15.31:\n\\[\n\\begin{aligned}\n  \\text{mean absolute error (MAE)} &= \\frac{\\sum\\limits_{i = 1}^n|\\text{predicted}_i - \\text{observed}_i|}{n} \\\\\n  &= \\text{mean}(|e_i|)\n\\end{aligned}\n\\tag{15.31}\\]\n\n15.7.1.1.3 Mean Squared Error (MSE)\nMean squared error (MSE) is the mean of the square of the differences between the predicted and observed values across individuals, that is, the mean of the squared value of errors. Smaller MSE values (closer to zero) reflect greater accuracy. MSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, MSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for MSE is in Equation 15.32:\n\\[\n\\begin{aligned}\n  \\text{mean squared error (MSE)} &= \\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n} \\\\\n  &= \\text{mean}(e_i^2)\n\\end{aligned}\n\\tag{15.32}\\]\n\n15.7.1.1.4 Root Mean Squared Error (RMSE)\nRoot mean squared error (RMSE) is the square root of the mean of the square of the differences between the predicted and observed values across individuals, that is, the root mean squared value of errors. Smaller RMSE values (closer to zero) reflect greater accuracy. RMSE penalizes larger errors more heavily than smaller errors (unlike MAE). However, RMSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for RMSE is in Equation 15.33:\n\\[\n\\begin{aligned}\n  \\text{root mean squared error (RMSE)} &= \\sqrt{\\frac{\\sum\\limits_{i = 1}^n(\\text{predicted}_i - \\text{observed}_i)^2}{n}} \\\\\n  &= \\sqrt{\\text{mean}(e_i^2)}\n\\end{aligned}\n\\tag{15.33}\\]\n\n15.7.1.2 Scale-Independent Accuracy Estimates\nThe estimates of prediction accuracy described in this section are intended to be scale-independent (unit-free) so the accuracy estimates can be compared across measures with different scales or across data sets (Hyndman & Athanasopoulos, 2021).\n\n15.7.1.2.1 Mean Percentage Error (MPE)\nMean percentage error (MPE) values closer to zero reflect greater accuracy. The formula for percentage error is in Equation 15.34:\n\\[\n\\begin{aligned}\n  \\text{percentage error }(p_i) = \\frac{100\\% \\times (\\text{observed}_i - \\text{predicted}_i)}{\\text{observed}_i}\n\\end{aligned}\n\\tag{15.34}\\]\nWe then take the mean of the percentage errors to get MPE. The formula for MPE is in Equation 15.35:\n\\[\n\\begin{aligned}\n  \\text{mean percentage error (MPE)} &= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i} \\\\\n  &= \\text{mean(percentage error)} \\\\\n  &= \\text{mean}(p_i)\n\\end{aligned}\n\\tag{15.35}\\]\nNote: MPE is undefined when one or more of the observed values equals zero, due to division by zero. The accuracyOverall() function of the petersenlab package (Petersen, 2024a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n15.7.1.2.2 Mean Absolute Percentage Error (MAPE)\nSmaller mean absolute percentage error (MAPE) values (closer to zero) reflect greater accuracy. The formula for MAPE is in Equation 15.36. MAPE is asymmetric because it overweights underestimates and underweights overestimates. MAPE can be preferable to symmetric mean absolute percentage error (sMAPE) if there are no observed values of zero and if you want to emphasize the importance of underestimates (relative to overestimates).\n\\[\n\\begin{aligned}\n  \\text{mean absolute percentage error (MAPE)} &= \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\Bigg|\\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{observed}_i}\\Bigg| \\\\\n  &= \\text{mean(|percentage error|)} \\\\\n  &= \\text{mean}(|p_i|)\n\\end{aligned}\n\\tag{15.36}\\]\nNote: MAPE is undefined when one or more of the observed values equals zero, due to division by zero. The accuracyOverall() function of the petersenlab package (Petersen, 2024a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n15.7.1.2.3 Symmetric Mean Absolute Percentage Error (sMAPE)\nUnlike MAPE, symmetric mean absolute percentage error (sMAPE) is symmetric because it equally weights underestimates and overestimates. Smaller sMAPE values (closer to zero) reflect greater accuracy. The formula for sMAPE is in Equation 15.37:\n\\[\n\\small\n\\begin{aligned}\n  \\text{symmetric mean absolute percentage error (sMAPE)} = \\frac{100\\%}{n} \\sum\\limits_{i = 1}^n \\frac{|\\text{predicted}_i - \\text{observed}_i|}{|\\text{predicted}_i| + |\\text{observed}_i|}\n\\end{aligned}\n\\tag{15.37}\\]\nNote: sMAPE is undefined when one or more of the individuals has a prediction–observed combination such that the sum of the absolute value of the predicted value and the absolute value of the observed value equals zero (\\(|\\text{predicted}_i| + |\\text{observed}_i|\\)), due to division by zero. The accuracyOverall() function of the petersenlab package (Petersen, 2024a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n15.7.1.2.4 Mean Absolute Scaled Error (MASE)\nMean absolute scaled error (MASE) is described by (Hyndman & Athanasopoulos, 2021). Values closer to zero reflect greater accuracy.\nThe adapted formula for MASE with non-time series data is described here (https://stats.stackexchange.com/a/108963/20338) (archived at https://perma.cc/G469-8NAJ). Scaled errors are calculated using Equation 15.38:\n\\[\n\\begin{aligned}\n  \\text{scaled error}(q_i) &= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\text{scaling factor}} \\\\\n  &= \\frac{\\text{observed}_i - \\text{predicted}_i}{\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|}\n\\end{aligned}\n\\tag{15.38}\\]\nThen, we calculate the mean of the absolute value of the scaled errors to get MASE, as in Equation 15.39:\n\\[\n\\begin{aligned}\n  \\text{mean absolute scaled error (MASE)} &= \\frac{1}{n} \\sum\\limits_{i = 1}^n |q_i| \\\\\n  &= \\text{mean(|scaled error|)} \\\\\n  &= \\text{mean}(|q_i|)\n\\end{aligned}\n\\tag{15.39}\\]\nNote: MASE is undefined when the scaling factor is zero, due to division by zero. With non-time series data, the scaling factor is the average of the absolute value of individuals’ observed scores minus the average observed score (\\(\\frac{1}{n} \\sum\\limits_{i = 1}^n |\\text{observed}_i - \\overline{\\text{observed}}|\\)).\n\n15.7.1.2.5 Root Mean Squared Log Error (RMSLE)\nThe squared log of the accuracy ratio is described by Tofallis (2015). The accuracy ratio is in Equation 15.40:\n\\[\n\\begin{aligned}\n  \\text{accuracy ratio} &= \\frac{\\text{predicted}_i}{\\text{observed}_i}\n\\end{aligned}\n\\tag{15.40}\\]\nHowever, the accuracy ratio is undefined with observed or predicted values of zero, so it is common to modify it by adding 1 to the predictor and denominator, as in Equation 15.41:\n\\[\n\\begin{aligned}\n  \\text{accuracy ratio} &= \\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\n\\end{aligned}\n\\tag{15.41}\\]\nSquaring the log values keeps the values positive, such that smaller values (values closer to zero) reflect greater accuracy. Then we take the mean of the squared log values, which keeps the values positive, and calculate the square root of the mean squared log values to put them back on the (pre-squared) log metric. This is known as the root mean squared log error (RMSLE). Division inside the log is equal to subtraction outside the log. So, the formula can be reformulated with the subtraction of two logs, as in Equation 15.42:\n\\[\n\\scriptsize\n\\begin{aligned}\n  \\text{root mean squared log error (RMSLE)} &= \\sqrt{\\sum\\limits_{i = 1}^n log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2} \\\\\n  &= \\sqrt{\\text{mean}\\Bigg[log\\bigg(\\frac{\\text{predicted}_i + 1}{\\text{observed}_i + 1}\\bigg)^2\\Bigg]} \\\\\n  &= \\sqrt{\\text{mean}\\big[log(\\text{accuracy ratio})^2\\big]} = \\sqrt{\\text{mean}\\Big\\{\\big[log(\\text{predicted}_i + 1) - log(\\text{actual}_i + 1)\\big]^2\\Big\\}}\n\\end{aligned}\n\\tag{15.42}\\]\nRMSLE can be preferable when the scores have a wide range of values and are skewed. RMSLE can help to reduce the impact of outliers. RMSLE gives more weight to smaller errors in the prediction of small observed values, while also penalizing larger errors in the prediction of larger observed values. It overweights underestimates and underweights overestimates.\nThere are other variations of prediction accuracy metrics that use the log of the accuracy ratio. One variation makes it similar to median symmetric percentage error (Morley et al., 2018).\nNote: Root mean squared log error is undefined when one or more predicted values or actual values equals −1. When predicted or actual values are -1, this leads to \\(log(0)\\), which is undefined. The accuracyOverall() function of the petersenlab package (Petersen, 2024a) provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.\n\n15.7.1.2.6 Coefficient of Determination (\\(R^2\\))\nThe coefficient of determination (\\(R^2\\)) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions: \\(R^2 = \\frac{\\text{variance explained in }Y}{\\text{total variance in }Y}\\). Larger values indicate greater accuracy.\n\\(R^2\\) is commonly estimated in multiple regression, in which multiple predictors are allowed to predict one outcome.\n\n15.7.1.2.6.1 Adjusted \\(R^2\\) (\\(R^2_{adj}\\))\nAdjusted \\(R^2\\) is similar to the coefficient of determination, but it accounts for the number of predictors included in the regression model to penalize overfitting. Adjusted \\(R^2\\) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictors in the model. Larger values indicate greater accuracy. The formula for adjusted \\(R^2\\) is in Equation 10.4. Adjusted \\(R^2\\) is described further in Section 10.5.\n\n15.7.1.2.6.2 Predictive \\(R^2\\)\n\nPredictive \\(R^2\\) is described here: https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/ (archived at https://perma.cc/BK8J-HFUK). Predictive \\(R^2\\) penalizes overfitting, unlike traditional \\(R^2\\). Larger values indicate greater accuracy.\n\n15.7.2 Discrimination\nWhen dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Threshold-dependent aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity) are described in Section 15.6.\n\n15.7.2.1 Area under the ROC curve (AUC)\nThe area under the ROC curve (AUC) is a general index of discrimination accuracy for a categorical outcome. It is also called the concordance (\\(c\\)) statistic. Larger values reflect greater discrimination accuracy. AUC was estimated using the pROC package (Robin et al., 2023).\n\n15.7.2.2 Effect Size (\\(\\beta\\)) of Regression\nThe effect size of a predictor, i.e., the standardized regression coefficient is called a beta (\\(\\beta\\)) coefficient, is a general index of discrimination accuracy for a continuous outcome. Larger values reflect greater accuracy. We can obtain standardized regression coefficients by standardizing the predictors and outcome using the scale() function in R.\n\n15.7.3 Calibration\nWhen dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by Lindhiem et al. (2020). Calibration can be examined in several ways, including Spiegelhalter’s \\(z\\) (see Section 15.7.3.2), and the mean difference between predicted and observed values at different binned thresholds as depicted graphically with a calibration plot (see Figure 15.7).\n\n15.7.3.1 Calibration Plot\nCalibration plots can be helpful for identifying miscalibration. A calibration plot depicts the predicted probability of an event on the x-axis, and the actual (observed) probability of the event on the y-axis. The predictions are binned into a certain number of groups (commonly 10). The diagonal line reflects predictions that are perfectly calibrated. To the extent that predictions deviate from the diagonal line, the predictions are miscalibrated.\nWell-calibrated predictions are depicted in Figure 15.6:\n\nCode# Specify data\nexamplePredictionsWellCalibrated &lt;- seq(from = 0, to = 1, by = .1)\nexampleOutcomesWellCalibrated &lt;- seq(from = 0, to = 1, by = .1)\n\n# Plot\nplot(\n  examplePredictionsWellCalibrated,\n  exampleOutcomesWellCalibrated,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  type = \"n\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\npoints(\n  examplePredictionsWellCalibrated,\n  exampleOutcomesWellCalibrated,\n  cex = 1.5,\n  col = \"#e41a1c\",\n  lwd = 2,\n  type = \"p\")\n\n\n\n\n\n\nFigure 15.6: Predictions that are Well-Calibrated. That is, the predicted values are close to the observed values.\n\n\n\n\nThe various types of general miscalibration are depicted in Figure 15.7:\n\nCode# Specify data\nexamplePredictions &lt;- seq(from = 0, to = 1, by = .1)\nexampleOutcomes &lt;- c(0, .15, .3, .4, .45, .5, .55, .6, .7, .85, 1)\n\noverPrediction &lt;- c(0, .02, .05, .1, .15, .2, .3, .4, .5, .7, 1)\nunderPrediction &lt;- c(0, .3, .5, .6, .7, .8, .85, .9, .95, .98, 1)\noverExtremity &lt;- c(0, .3, .38, .42, .47, .5, .53, .58, .62, .7, 1)\nunderExtremity &lt;- c(0, .05, .08, .11, .2, .5, .8, .89, .92, .95, 1)\n\n# Plot\npar(\n  mfrow = c(2,2),\n  mar = c(5,4,1,1) + 0.1) #margins: bottom, left, top, right\n\nplot(\n  examplePredictions,\n  overExtremity,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Overextremity\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  underExtremity,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Underextremity\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  overPrediction,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Overprediction\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\nplot(\n  examplePredictions,\n  underPrediction,\n  xlim = c(0,1),\n  ylim = c(0,1),\n  main = \"Underprediction\",\n  xlab = \"Predicted Probability\",\n  ylab = \"Observed Proportion\",\n  bty = \"l\",\n  cex = 1.5,\n  col = \"#e41a1c\",\n  type = \"o\")\n\nlines(\n  c(0,1),\n  c(0,1),\n  lwd = 2,\n  col = \"#377eb8\")\n\n\n\n\n\n\nFigure 15.7: Types of Miscalibration. From Petersen (2024b) and Petersen (2024c).\n\n\n\n\nHowever, predictions could also be miscalibrated in more specific ways. For instance, predictions could be well-calibrated at all predicted probabilities except for a given predicted probability (e.g., 20%). Or, the predictions could be miscalibrated but not systematically over- or underpredicted. Thus, it is important to evaluate a calibration plot to evaluate the extent to which the predictions are miscalibrated and the pattern of that miscalibration.\n\n15.7.3.2 Spiegelhalter’s z\n\nSpiegelhalter’s z was calculated using the rms package (Harrell, Jr., 2024). Smaller z values (and larger associated p-values) reflect greater calibration accuracy. A statistically significant Spiegelhalter’s z (p &lt; .05) indicates a significant degree of miscalibration.\n\n15.7.3.3 Calibration for predicting a continuous outcome\nWhen predicting a continuous outcome, calibration of the predicted values in relation to the outcome values can be examined in multiple ways including:\n\nin a calibration plot, the extent to which the intercept is near zero and the slope is near one\nin a calibration plot, the extent to which the 95% confidence interval of the observed value, across all values of the predicted values, includes the diagonal reference line with an intercept of zero and a slope of one\nmean error\nmean absolute error\nmean squared error\nroot mean squared error\n\nWith a plot of the predictions on the x-axis, and the outcomes on the y-axis (i.e., a calibration plot), calibration can be examined graphically as the extent to which the best-fit regression line has an intercept (alpha) close to zero and a slope (beta) close to one (Stevens & Poppe, 2020; Steyerberg & Vergouwe, 2014). The intercept is also called “calibration-in-the-large”, whereas “calibration-in-the-small” refers to the extent to which the predicted values match the observed values at a specific predicted value (e.g., when the weather forecaster says that there is a 10% chance of rain, does it actually rain 10% of the time?). For predictions to be well calibrated, the intercept should be close to zero and the slope should be close to one. If the slope is close to one but the intercept is not close to zero (or the intercept is close to zero but the slope is not close to one), the predictions would not be considered well calibrated. The 95% confidence interval of the observed value, across all values of the predicted values, should include the diagonal reference line whose intercept is zero and whose slope is one.\nFor instance, based on the intercept and slope of the calibration plot in Figure INSERT, the predictions are not well calibrated, despite having a slope near one, because the 95% confidence interval of the intercept does not include zero. The best-fit line is the yellow line. The intercept from the best-fit line is positive, as shown in the regression equation. This is a case of underprediction, where the predicted values are consistently less than the observed values. The confidence interval of the observed value (i.e., the purple band) is the interval within which we have 95% confidence that the true observed value would lie for a given predicted value, based on the model The 95% prediction interval of the observed value (i.e., the dashed red lines) is the interval within which we would expect that 95% of future observations would lie for a given predicted value. The black diagonal line indicates the reference line with an intercept of zero and a slope of one. The predictions would be significantly miscalibrated at a given level of the predicted values if the 95% confidence interval of the observed value does not include the reference line at that level of the predicted value. In this case, the 95% confidence interval of the observed value does not include the reference line (i.e., the actual observed value) at lower levels of the predicted values, so the predictions are miscalibrated lower levels of the predicted values.\nGold-standard recommendations include examining the predicted values in relation to the observed values using locally estimated scatterplot smoothing (LOESS) (Austin & Steyerberg, 2014), such as in Figure INSERT. We can examine whether the LOESS-based 95% confidence interval of the observed value at every level of the predicted values includes the diagonal reference line (i.e., the actual observed value). In this case, the 95% confidence interval of the observed value does not include the reference line at lower levels of the predicted values, so the predictions are miscalibrated at lower levels of the predicted values.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-integratingAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-integratingAccuracy",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.8 Integrating the Accuracy Indices",
    "text": "15.8 Integrating the Accuracy Indices\nAfter computing the accuracy indices of discrimination and (2) calibration, it is then the task to integrate the indices to determine (a) which are the most accurate predictions for the given goals, and (b) whether additional improvements and refinements to the predictions need to be made. Each of the accuracy indices is computed differently and thus reward (and penalize) predictive (in)accuracy differently. Sometimes, the the accuracy indices will paint a consistent picture regarding which predictions are the most accurate. Other times, the accuracy indices may disagree about which predictions are most accurate.\nIn fantasy football, when evaluating the accuracy of seasonal projections, we care most about accurately distinguishing between higher levels of points (e.g., 200 vs 150) as opposed to lower levels of points (e.g., 0 vs 10). Thus, it can be helpful to punish larger errors more heavily than smaller errors, as RMSE (unlike MAE).\nThus, we would emphasize the following metrics:\n\n\ndiscrimination:\n\nadjusted \\(R^2\\)\n\n\n\ncalibration:\n\ncalibration plot\n\n\n\ngeneral accuracy:\n\nRMSE\n\n\n\nIf you focus on only one accuracy index, RMSE would be a good choice. However, I would also examine a calibration plot to evaluate whether predictions are poorly calibrated at higher levels of points. I would also examine ME—not to compare the accuracy of various predictions per se—but to determine whether predictions are systematically under- or overestimating actual points. If so, predictions may be able to be refined by adding or subtracting a constant to the predictions; however, this could worsen other accuracy indices, so it is important to conduct an iterative process of modifying then evaluating, then further modifiying and evaluating, etc. It may also be valuable to evaluate the accuracy of various subsets of the predictions. For instance, you might examine the predictive accuracy of players whose project points are greater than 100, to evaluate the accuracy of predictions specifically to distinguish between players at higher levels of points.\nIf we are making predictions about a categorical variable, we would emphasize the following metrics:\n\n\ndiscrimination:\n\narea under the receiver operating curve\nand, secondarily—depending on the particulary cutoff and the relative costs of false positives versus false negatives:\n\nsensitivity\nspecificicity\npositive predictive value\nnegative predictive value\n\n\n\n\n\ncalibration:\n\ncalibration plot\nSpiegelhalter’s z\nMean difference between observed and predicted values",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-theoryVsEmpiricism",
    "href": "evaluating-prediction-accuracy.html#sec-theoryVsEmpiricism",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.9 Theory Versus Empiricism",
    "text": "15.9 Theory Versus Empiricism\nOne question that inevitably arises when making predictions is the extent to which one should leverage theory versus empiricism. Theory involves conceptual claims of understanding how the causal system works (i.e., what influences what). For example, use of theory in prediction might involve specification of the causal system that influences player performance, measurement of those factors, and the integration of that information to make a prediction. Empiricism involves “letting the data speak for themselves” and is an atheoretical approach. For example, empiricism might involve examining how thousands of variables are associated with the criterion of interest (e.g., fantasy points) and developing the best-fitting model based on those thousands of predictor variables.\nAlthough the atheoretical approach can perform reasonably well, it can be improved by making better use of theory. An empirical result (e.g., a correlation) might not necessarily have a lot of meaning associated with it. As the maxim goes, correlation does not imply causation. Moreover, empiricism can lead to overfitting. So, empiricism is often not enough.\nAs Silver (2012) notes, “The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.” (p. 9). If we understand the variables in the system and how they influence each other, we can predict things more accurately than predicting for the sake of predicting. For instance, we have made great strides in the last decades when it comes to more accurate weather forecasts (archived at https://perma.cc/PF8P-BT3D), including extreme weather events like hurricanes. These great strides have more to do with a better causal understanding of the weather system and the ability to conduct simulations of the atmosphere than merely because of big data (Silver, 2012). By contrast, other events are still incredibly difficult to predict, including earthquakes, in large part because we do not have a strong understanding of the system (and because we do not have ways of precisely measuring those causes because they occur at a depth below which we are realistically able to drill) (Silver, 2012).\nAt the same time, in the social and behavioral sciences, our theories of the causal processes that influence outcomes are not yet very strong. Indeed, I have misgivings calling them theories because they do not meet the traditional scientific standard for a theory. A scientific theory is an explanation of the natural world that is testable and falsifiable, and that has withstood rigorous scientific testing and scrutiny. In psychology (and other areas of social and behavioral sciences), our “theories” are more like conceptual frameworks. And these conceptual frameworks are often vague, do not make specific predictions of effects and noneffects, and do not hold up consistently when rigorously tested. As described by Meehl (1978):\n\nI consider it unnecessary to persuade you that most so-called “theories” in the soft areas of psychology (clinical, counseling, social, personality, community, and school psychology) are scientifically unimpressive and technologically worthless … Perhaps the easiest way to convince yourself is by scanning the literature of soft psychology over the last 30 years and noticing what happens to theories. Most of them suffer the fate that General MacArthur ascribed to old generals—They never die, they just slowly fade away. In the developed sciences, theories tend either to become widely accepted and built into the larger edifice of well-tested human knowledge or else they suffer destruction in the face of recalcitrant facts and are abandoned, perhaps regretfully as a “nice try.” But in fields like personology and social psychology, this seems not to happen. There is a period of enthusiasm about a new theory, a period of attempted application to several fact domains, a period of disillusionment as the negative data come in, a growing bafflement about inconsistent and unreplicable empirical results, multiple resort to ad hoc excuses, and then finally people just sort of lose interest in the thing and pursue other endeavors. (pp. 806–807).\n\nEven if we had strong theoretical understanding of the causal system that influences behavior, we would likely still have difficulty making accurate predictions because the field has largely relied on relatively crude instruments. According to one philosophical perspective known as LaPlace’s demon, if we were able to know the exact conditions of everything in the universe, we would be able to know how the conditions would be in the future. This is an example of scientific determinism, where if you know the initial conditions, you also know the future. Other perspectives, such as quantum mechanics and chaos theory, would say that, even if we knew the initial conditions with 100% certainty, there would still be uncertainty in our understanding of the future. But assume, for a moment, that LaPlace’s demon is true. A challenge in the social and behavioral sciences is that we have a relatively poor understanding of the initial conditions of the universe. Thus, our predictions would necessarily be probabilistic, similar to weather forecasts. Despite having a strong understanding of how weather systems behave, we have imperfect understanding of the initial conditions (e.g., the position and movement of all molecules) (Silver, 2012).\nTheories tend to make grand conceptual claims that one observed variable influences another observed variable through a complex chain of intervening processes that are unobservable. Empiricism provides rich lower-level information, but lacks the broader picture. So, it seems, that we need both theory and empiricism. Theory and empiricism can—and should—inform each other.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-testBias",
    "href": "evaluating-prediction-accuracy.html#sec-testBias",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.10 Test Bias",
    "text": "15.10 Test Bias\nTest bias refers to systematic error (in measurement, prediction, etc.) as a function of group membership that leads the same score to have different meaning for different groups. For instance, if the Wonderlic Contemporary Cognitive Ability Test is a strong predictor of performance for Quarterbacks but not for Running Backs, the test is biased. Test bias, including how to identify and address it, is described in Petersen (2024c).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-waysToImprovePredictionAccuracy",
    "href": "evaluating-prediction-accuracy.html#sec-waysToImprovePredictionAccuracy",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.11 Ways to Improve Prediction Accuracy",
    "text": "15.11 Ways to Improve Prediction Accuracy\nOn the whole, experts’ predictions are inaccurate. Experts’ predictions from many different domains tend to be inaccurate, including political scientists (Tetlock, 2017), physicians (Koehler et al., 2002), clinical psychologists (Oskamp, 1965), stock market traders and corporate financial officers (Skala, 2008), seismologists’ predictions of earthquakes (Hough, 2016), economists’ predictions about the economy (Makridakis et al., 2009), lawyers (Koehler et al., 2002), and business managers (Russo & Schoemaker, 1992). The most common pattern of experts’ predictions is that they show overextremity, that is, their predictions have probability judgments that tend to be too extreme, as described in Section Section 15.3.2. Overextremity of experts’ predictions reflects the overprecision type of overconfidence bias. The degree of confidence of a person’s predictions is often not a good indicator of the accuracy of their predictions [and confidence and prediction accuracy are sometimes inversely associated; Silver (2012)]. Heuristics such as the anchoring and adjustment heuristic, cognitive biases such as confirmation bias (Hoch, 1985; Koriat et al., 1980), fallacies such as the base rate fallacy (Eddy, 1982; Koehler et al., 2002) could contribute to overconfidence of predictions. Poorly calibrated predictions are especially likely when the base rate is very low (e.g., suicide) or when the base rate is very high (Koehler et al., 2002).\nNevertheless, there are some domains that have shown greater predictive accuracy, from which we may learn what practices may lead to greater accuracy. For instance, experts have shown stronger predictive accuracy in weather forecasting (Murphy & Winkler, 1984), horse race betting (Johnson & Bruce, 2001), and playing the card game of bridge (Keren, 1987), but see Koehler et al. (2002) for exceptions.\nHere are some potential ways to improve the accuracy (and honesty) of predictions and judgments:\n\nProvide appropriate anchoring of your predictions to the base rate of the phenomenon you are predicting. To the extent that the base rate of the event you are predicting is low, more extreme evidence should be necessary to consistently and accurately predict that the event will occur. Applying actuarial formulas and Bayes’ theorem can help you appropriately weigh the base rate and evidence.\nInclude multiple predictors, ideally from different measures and measurement methods. Include the predictors with the strongest validity based on theory of the causal process and based on criterion-related validity.\nWhen possible, aggregate multiple perspectives of predictions, especially predictions made independently (from different people/methods/etc.). The “wisdom of the crowd” is often more accurate than individuals’ predictions, including predictions by so-called “experts” (Silver, 2012).\nA goal of prediction is to capture as much signal as possible and as little noise (error) as possible (Silver, 2012). Parsimony (i.e., not having too many predictors) can help reduce the amount of error variance captured by the prediction model. However, to accurately model complex systems like human behavior, complex models may be necessary. However, strong theory of the causal processes and dynamics may be necessary to develop accurate complex models.\nAlthough incorporating theory can be helpful, provide more weight to empiricism than to theory, until our theories and measures are stronger. Ideally, we would use theory to design a model that mirrors the causal system, with accurate measures of each process in the system, so we could make accurate predictions. However, as described in Section 15.9, our psychological theories of the causal processes that influence behavior are not yet very strong. Until we have stronger theories that specify the causal process for a given outcome, and until we have accurate measures of those causal processes, actuarial approaches are likely to be most accurate, as discussed in Chapter 13. At the same time, keep in mind that measures involving human behavior, and their resulting data, are often noisy. As a result, theoretically (conceptually) informed empirical approaches may lead to more accuracy than empiricism alone.\nUse an empirically validated and cross-validated statistical algorithm to combine information from the predictors in a formalized way. Give each predictor appropriate weight in the statistical algorithm, according to its strength of association with the outcome. Use measures with strong reliability and validity for assessing these processes to be used in the algorithm. Cross-validation will help reduce the likelihood that your model is fitting to noise and will maximize the likelihood that the model predicts accurately when applied to new data (i.e., the model’s predictions accurately generalize), as described in Section 13.6.\nWhen presenting your predictions, acknowledge what you do not know.\nExpress your predictions in terms of probabilistic estimates and present the uncertainty in your predictions with confidence intervals [even though bolder, more extreme predictions tend to receive stronger television ratings; Silver (2012)].\nQualify your predictions by identifying and noting counter-examples that would not be well fit by your prediction model, such as extreme cases, edge cases, and “broken leg” (Meehl, 1957) cases.\nProvide clear, consistent, and timely feedback on the outcomes of the predictions to the people making the predictions (Bolger & Önkal-Atay, 2004).\nBe self-critical about your predictions. Update your judgments based on their accuracy, rather than trying to confirm your beliefs (Atanasov et al., 2020).\nIn addition to considering the accuracy of the prediction, consider the quality of the prediction process, especially when random chance is involved to a degree, such as in poker and fantasy football (Silver, 2012).\nWork to identify and mitigate potential blindspots; be aware of cognitive biases and fallacies, such as confirmation bias and the base rate fallacy.\nEvaluate for the possibility of test bias. Correct for any test bias.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracyConclusion",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracyConclusion",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.12 Conclusion",
    "text": "15.12 Conclusion\nWhen the base rate of a behavior is very low or very high, you can be highly accurate in predicting the behavior by predicting from the base rate. Thus, you cannot judge how accurate your prediction is until you know how accurate your predictions would be by random chance. Moreover, maximizing percent accuracy may not be the ultimate goal because different errors have different costs. Though there are many indices of accuracy, there are two general types of accuracy: discrimination and calibration. Discrimination accuracy is frequently evaluated with the area under the receiver operating characteristic curve, or with sensitivity and specificity, or with standardized regression coefficients or the coefficient of determination. Calibration accuracy is frequently evaluated graphically and with various indices. Sensitivity and specificity depend on the cutoff. It is important to evaluate both discrimination and calibration when evaluating prediction accuracy.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "evaluating-prediction-accuracy.html#sec-predictionAccuracySessionInfo",
    "href": "evaluating-prediction-accuracy.html#sec-predictionAccuracySessionInfo",
    "title": "15  Evaluation of Prediction/Forecasting Accuracy",
    "section": "\n15.13 Session Info",
    "text": "15.13 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] viridis_0.6.5     viridisLite_0.4.2 magrittr_2.0.3    pROC_1.18.5      \n [5] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [9] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n[13] ggplot2_3.5.1     tidyverse_2.0.0   petersenlab_1.0.3\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5       xfun_0.46          htmlwidgets_1.6.4  psych_2.4.6.26    \n [5] lattice_0.22-6     tzdb_0.4.0         quadprog_1.5-8     vctrs_0.6.5       \n [9] tools_4.4.1        generics_0.1.3     stats4_4.4.1       parallel_4.4.1    \n[13] fansi_1.0.6        cluster_2.1.6      pkgconfig_2.0.3    data.table_1.15.4 \n[17] checkmate_2.3.1    RColorBrewer_1.1-3 lifecycle_1.0.4    farver_2.1.2      \n[21] compiler_4.4.1     munsell_0.5.1      mnormt_2.1.1       mitools_2.4       \n[25] htmltools_0.5.8.1  yaml_2.3.9         htmlTable_2.4.3    Formula_1.2-5     \n[29] pillar_1.9.0       Hmisc_5.1-3        rpart_4.1.23       nlme_3.1-164      \n[33] lavaan_0.6-18      tidyselect_1.2.1   digest_0.6.36      mvtnorm_1.2-5     \n[37] stringi_1.8.4      reshape2_1.4.4     labeling_0.4.3     fastmap_1.2.0     \n[41] grid_4.4.1         colorspace_2.1-0   cli_3.6.3          base64enc_0.1-3   \n[45] utf8_1.2.4         pbivnorm_0.6.0     foreign_0.8-86     withr_3.0.0       \n[49] scales_1.3.0       backports_1.5.0    timechange_0.3.0   rmarkdown_2.27    \n[53] nnet_7.3-19        gridExtra_2.3      hms_1.1.3          evaluate_0.24.0   \n[57] knitr_1.48         mix_1.0-12         rlang_1.1.4        Rcpp_1.0.13       \n[61] xtable_1.8-4       glue_1.7.0         DBI_1.2.3          rstudioapi_0.16.0 \n[65] jsonlite_1.8.8     R6_2.5.1           plyr_1.8.9        \n\n\n\n\n\n\nAtanasov, P., Witkowski, J., Ungar, L., Mellers, B., & Tetlock, P. (2020). Small steps to accuracy: Incremental belief updaters are better forecasters. Organizational Behavior and Human Decision Processes, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001\n\n\nAustin, P. C., & Steyerberg, E. W. (2014). Graphical assessment of internal and external calibration of logistic regression models by using loess smoothers. Statistics in Medicine, 33(3), 517–535. https://doi.org/10.1002/sim.5941\n\n\nBolger, F., & Önkal-Atay, D. (2004). The effects of feedback on judgmental interval predictions. International Journal of Forecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.\n\n\nEddy, D. M. (1982). Probabilistic reasoning in clinical medicine: Problems and opportunities. In D. Kahneman, P. Slovic, & A. Tversky (Eds.), Judgment under uncertainty: Heuristics and biases (pp. 249–267). Cambridge University Press.\n\n\nFarrington, D. P., & Loeber, R. (1989). Relative improvement over chance (RIOC) and phi as measures of predictive efficiency and strength of association in 2×2 tables. Journal of Quantitative Criminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737\n\n\nHarrell, Jr., F. E. (2024). rms: Regression modeling strategies. https://hbiostat.org/R/rms/\n\n\nHoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting personal events. Journal of Experimental Psychology: Learning, Memory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719\n\n\nHough, S. E. (2016). Predicting the unpredictable: The tumultuous science of earthquake prediction. Princeton University Press.\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nJohnson, J. E. V., & Bruce, A. C. (2001). Calibration of subjective probability judgments in a naturalistic setting. Organizational Behavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949\n\n\nKeren, G. (1987). Facing uncertainty in the game of bridge: A calibration study. Organizational Behavior and Human Decision Processes, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1\n\n\nKessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., & Zubizarreta, J. R. (2020). Suicide prediction models: A critical review of recent research with recommendations for the way forward. Molecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0\n\n\nKoehler, D. J., Brenner, L., & Griffin, D. (2002). The calibration of expert judgment: Heuristics and biases beyond the laboratory. In T. Gilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics and biases: The psychology of intuitive judgment. Cambridge University Press.\n\n\nKoriat, A., Lichtenstein, S., & Fischhoff, B. (1980). Reasons for confidence. Journal of Experimental Psychology: Human Learning and Memory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107\n\n\nLindhiem, O., Petersen, I. T., Mentch, L. K., & Youngstrom, E. A. (2020). The importance of calibration in clinical psychology. Assessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055\n\n\nMakridakis, S., Hogarth, R. M., & Gaba, A. (2009). Forecasting and uncertainty in the economic and business world. International Journal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula? Journal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. Journal of Consulting and Clinical Psychology, 46(4), 806–834. https://doi.org/10.1037/0022-006x.46.4.806\n\n\nMeehl, P. E., & Rosen, A. (1955). Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores. Psychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070\n\n\nMorley, S. K., Brito, T. V., & Welling, D. T. (2018). Measures of model performance based on the log accuracy ratio. Space Weather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669\n\n\nMurphy, A. H., & Winkler, R. L. (1984). Probability forecasting in meterology. Journal of the American Statistical Association, 79(387), 489–500. https://doi.org/10.2307/2288395\n\n\nOskamp, S. (1965). Overconfidence in case-study judgments. Journal of Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125\n\n\nPetersen, I. T. (2024a). petersenlab: A collection of R functions by the Petersen Lab. https://github.com/DevPsyLab/petersenlab\n\n\nPetersen, I. T. (2024c). Principles of psychological assessment: With applied examples in R. University of Iowa Libraries. https://doi.org/10.25820/work.007199\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment: With applied examples in R. Chapman and Hall/CRC. https://doi.org/10.1201/9781003357421\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & Müller, M. (2023). pROC: Display and analyze ROC curves. https://xrobin.github.io/pROC/\n\n\nRusso, J. E., & Schoemaker, P. J. (1992). Managing overconfidence. Sloan Management Review, 33(2), 7.\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions fail–but some don’t. Penguin.\n\n\nSkala, D. (2008). Overconfidence in psychology and finance–an interdisciplinary literature review. Bank i Kredyt, 4, 33–50.\n\n\nStevens, R. J., & Poppe, K. K. (2020). Validation of clinical prediction models: What does the “calibration slope” really measure? Journal of Clinical Epidemiology, 118, 93–99. https://doi.org/10.1016/j.jclinepi.2019.09.016\n\n\nSteyerberg, E. W., & Vergouwe, Y. (2014). Towards better clinical prediction models: Seven steps for development and an ABCD for validation. European Heart Journal, 35(29), 1925–1931. https://doi.org/10.1093/eurheartj/ehu207\n\n\nTetlock, P. E. (2017). Expert political judgment: How good is it? How can we know? - New edition. Princeton University Press.\n\n\nTofallis, C. (2015). A better measure of relative prediction accuracy for model selection and model estimation. Journal of the Operational Research Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103\n\n\nTreat, T. A., & Viken, R. J. (2023). Measuring test performance with signal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M. McMullen, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA handbook of research methods in psychology: Foundations, planning, measures, and psychometrics (2nd ed., Vol. 1, pp. 837–858). American Psychological Association.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation of Prediction/Forecasting Accuracy</span>"
    ]
  },
  {
    "objectID": "mythbusters.html",
    "href": "mythbusters.html",
    "title": "16  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "",
    "text": "16.1 Getting Started",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersGettingStarted",
    "href": "mythbusters.html#sec-mythbustersGettingStarted",
    "title": "16  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "",
    "text": "16.1.1 Load Packages\n\nCodelibrary(\"petersenlab\")\nlibrary(\"lme4\")\nlibrary(\"lmerTest\")\nlibrary(\"MuMIn\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")\n\n\n\n16.1.2 Specify Package Options\n\nCodeemm_options(lmerTest.limit = 100000)\nemm_options(pbkrtest.limit = 100000)\n\n\n\n16.1.3 Load Data\n\nCodeload(file = \"./data/nfl_playerContracts.RData\")\nload(file = \"./data/player_stats_weekly.RData\")\nload(file = \"./data/player_stats_seasonal.RData\")\nload(file = \"./data/nfl_espnQBR.RData\")\n\n\nWe created the player_stats_weekly.RData and player_stats_seasonal.RData objects in Section 3.21.3.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-contractYear",
    "href": "mythbusters.html#sec-contractYear",
    "title": "16  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n16.2 Do Players Perform Better in their Contract Year?",
    "text": "16.2 Do Players Perform Better in their Contract Year?\nConsiderable speculation exists regarding whether players perform better in their last year of their contract (i.e., their “contract year”). Fantasy football talking heads and commentators frequently discuss the benefit of selecting players who are in their contract year, because it supposedly means that player has more motivation to perform well so they get a new contract and get paid more. To our knowledge, no peer-reviewed studies have examined this question for football players. One study found that National Basketball Association (NBA) players improved in field goal percentage, points, and player efficiency rating (but not other statistics: rebounds, assists, steals, or blocks) from their pre-contract year to their contract year, and that Major League Baseball (MLB) players improved in runs batted in (RBIs; but not other statistics: batting average, slugging percentage, on base percentage, home runs, fielding percentage) from their pre-contract year to their contract year [White2014a]. Other casual analyses have been examined contract-year performance of National Football League (NFL) players, including articles in 2012 (archived here) and 2022 (archived here).\nLet’s examine the question empirically. In order to do that, we have to make some assumptions/constraints. In this example, we will make the following constraints:\n\nWe will determine a player’s contract year programmatically based on the year the contract was signed. For instance, if a player signed a 3-year contract in 2015, their contract would expire in 2018, and thus their contract year would be 2017. Note: this is a coarse way of determining a player’s contract year because it could depend on when during the year the player’s contract is signed. If we were submitting this analysis as a paper to a scientific journal, it would be important to verify each player’s contract year.\nWe will examine performance in all seasons since 2011, beginning when most data for player contracts are available.\nFor maximum statistical power to detect an effect if a contract year effect exists, we will examine all seasons for a player (since 2011), not just their contract year and their pre-contract year.\nTo ensure a more fair, apples-to-apples comparison of the games in which players played, we will examine per-game performance (except for yards per carry, which is based on \\(\\frac{\\text{rushing yards}}{\\text{carries}}\\) from the entire season).\nWe will examine regular season games only (no postseason).\nTo ensure we do not make generalization about a player’s performance in a season from a small sample, the player has to play at least 5 games in a given season for that player–season combination to be included in analysis.\n\nFor analysis, the same player contributes multiple observations of performance (i.e., multiple seasons) due to the longitudinal nature of the data. Inclusion of multiple data points from the same player would violate the assumption of multiple regression that all observations are independent. Thus, we use mixed-effects models that allow nonindependent observations. In our mixed-effects models, we include a random intercept for each player, to allow our model to account for players’ differing level of performance. We examine two mixed-effects models for each outcome variable: one model that accounts for the effects of age and experience, and one model that does not.\nThe model that does not account for the effects of age and experience includes:\n\nrandom intercepts to allow the model to estimate a different starting point for each player\na fixed effect for whether the player is in a contract year\n\nThe model that accounts for the effects of age and experience includes:\n\nrandom intercepts to allow the model to estimate a different starting point for each player\nrandom linear slopes (i.e., random effect of linear age) to allow the model to estimate a different form of change for each player\na fixed quadratic effect of age to allow for curvilinear effects\na fixed effect of experience\na fixed effect for whether the player is in a contract year\n\n\nCode# Subset to remove players without a year signed\nnfl_playerContracts_subset &lt;- nfl_playerContracts %&gt;% \n  dplyr::filter(!is.na(year_signed) & year_signed != 0)\n\n# Determine the contract year for a given contract\nnfl_playerContracts_subset$contractYear &lt;- nfl_playerContracts_subset$year_signed + nfl_playerContracts_subset$years - 1\n\n# Arrange contracts by player and year_signed\nnfl_playerContracts_subset &lt;- nfl_playerContracts_subset %&gt;%\n  dplyr::group_by(player, position) %&gt;% \n  dplyr::arrange(player, position, -year_signed) %&gt;% \n  dplyr::ungroup()\n\n# Determine if the player played in the original contract year\nnfl_playerContracts_subset &lt;- nfl_playerContracts_subset %&gt;%\n  dplyr::group_by(player, position) %&gt;%\n  dplyr::mutate(\n    next_contract_start = lag(year_signed)) %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::mutate(\n    played_in_contract_year = ifelse(\n      is.na(next_contract_start) | contractYear &lt; next_contract_start,\n      TRUE,\n      FALSE))\n\n# Check individual players\n#nfl_playerContracts_subset %&gt;% \n#  dplyr::filter(player == \"Aaron Rodgers\") %&gt;% \n#  dplyr::select(player:years, contractYear, next_contract_start, played_in_contract_year)\n#\n#nfl_playerContracts_subset %&gt;% \n#  dplyr::filter(player %in% c(\"Jared Allen\", \"Aaron Rodgers\")) %&gt;% \n#  dplyr::select(player:years, contractYear, next_contract_start, played_in_contract_year)\n\n# Subset data\nnfl_playerContractYears &lt;- nfl_playerContracts_subset %&gt;% \n  dplyr::filter(played_in_contract_year == TRUE) %&gt;% \n  dplyr::filter(position %in% c(\"QB\",\"RB\",\"WR\",\"TE\")) %&gt;% \n  dplyr::select(player, position, team, contractYear) %&gt;% \n  dplyr::mutate(playerMerge = toupper(str_replace_all(player, \"[[:punct:]]\", \"\"))) %&gt;% \n  dplyr::rename(season = contractYear) %&gt;% \n  dplyr::mutate(contractYear = 1)\n\n# Merge with weekly and seasonal stats data\nplayer_stats_weekly_offense &lt;- player_stats_weekly_offense %&gt;% \n  dplyr::mutate(playerMerge = toupper(str_replace_all(player_display_name, \"[[:punct:]]\", \"\")))\n#nfl_actualStats_offense_seasonal &lt;- nfl_actualStats_offense_seasonal %&gt;% \n#  mutate(playerMerge = toupper(str_replace_all(player_display_name, \"[[:punct:]]\", \"\")))\n\nplayer_statsContracts_offense_weekly &lt;- dplyr::full_join(\n  player_stats_weekly_offense,\n  nfl_playerContractYears,\n  by = c(\"playerMerge\", \"position_group\" = \"position\", \"season\")\n) %&gt;% \n  dplyr::filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\n#player_statsContracts_offense_seasonal &lt;- full_join(\n#  player_stats_seasonal_offense,\n#  nfl_playerContractYears,\n#  by = c(\"playerMerge\", \"position_group\" = \"position\", \"season\")\n#) %&gt;% \n#  filter(position_group %in% c(\"QB\",\"RB\",\"WR\",\"TE\"))\n\nplayer_statsContracts_offense_weekly$contractYear[which(is.na(player_statsContracts_offense_weekly$contractYear))] &lt;- 0\n#player_statsContracts_offense_seasonal$contractYear[which(is.na(player_statsContracts_offense_seasonal$contractYear))] &lt;- 0\n\n#player_statsContracts_offense_weekly$contractYear &lt;- factor(\n#  player_statsContracts_offense_weekly$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\n#player_statsContracts_offense_seasonal$contractYear &lt;- factor(\n#  player_statsContracts_offense_seasonal$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\nplayer_statsContracts_offense_weekly &lt;- player_statsContracts_offense_weekly %&gt;% \n  dplyr::arrange(playerMerge, season, season_type, week)\n\n#player_statsContracts_offense_seasonal &lt;- player_statsContracts_offense_seasonal %&gt;% \n#  arrange(playerMerge, season)\n\nplayer_statsContractsSubset_offense_weekly &lt;- player_statsContracts_offense_weekly %&gt;% \n  dplyr::filter(season_type == \"REG\")\n\n#table(nfl_playerContracts$year_signed) # most contract data is available beginning in 2011\n\n# Calculate Per Game Totals\nplayer_statsContracts_seasonal &lt;- player_statsContractsSubset_offense_weekly %&gt;% \n  dplyr::group_by(player_id, season) %&gt;% \n  dplyr::summarise(\n    player_display_name = petersenlab::Mode(player_display_name),\n    position_group = petersenlab::Mode(position_group),\n    age = min(age, na.rm = TRUE),\n    years_of_experience = min(years_of_experience, na.rm = TRUE),\n    rushing_yards = sum(rushing_yards, na.rm = TRUE), # season total\n    carries = sum(carries, na.rm = TRUE), # season total\n    rushing_epa = mean(rushing_epa, na.rm = TRUE),\n    receiving_yards = mean(receiving_yards, na.rm = TRUE),\n    receiving_epa = mean(receiving_epa, na.rm = TRUE),\n    contractYear = mean(contractYear, na.rm = TRUE),\n    games = n(),\n    .groups = \"drop_last\"\n  ) %&gt;% \n  dplyr::mutate(\n    player_id = as.factor(player_id),\n    ypc = rushing_yards / carries,\n    contractYear = factor(\n      contractYear,\n      levels = c(0, 1),\n      labels = c(\"no\", \"yes\")\n    ))\n\nplayer_statsContracts_seasonal$age[which(is.infinite((player_statsContracts_seasonal$age)))] &lt;- NA\nplayer_statsContracts_seasonal$years_of_experience[which(is.infinite((player_statsContracts_seasonal$years_of_experience)))] &lt;- NA\n\nplayer_statsContracts_seasonal$ageCentered20 &lt;- player_statsContracts_seasonal$age - 20\nplayer_statsContracts_seasonal$ageCentered20Quadratic &lt;- player_statsContracts_seasonal$ageCentered20 ^ 2\n\n# Merge with seasonal fantasy points data\n\n\n\n16.2.1 QB\nFirst, we prepare the data by merging and performing additional processing:\n\nCode# Merge with QBR data\nnfl_espnQBR$playerMerge &lt;- paste(nfl_espnQBR$name_first, nfl_espnQBR$name_last, sep = \" \") %&gt;% \n  stringr::str_replace_all(., \"[[:punct:]]\", \"\") %&gt;% \n  toupper()\n\nnfl_contractYearQBR_weekly &lt;- nfl_playerContractYears %&gt;% \n  dplyr::filter(position == \"QB\") %&gt;% \n  dplyr::full_join(\n    .,\n    nfl_espnQBR,\n    by = c(\"playerMerge\",\"team\",\"season\")\n  )\n\nnfl_contractYearQBR_weekly$contractYear[which(is.na(nfl_contractYearQBR_weekly$contractYear))] &lt;- 0\n#nfl_contractYearQBR_weekly$contractYear &lt;- factor(\n#  nfl_contractYearQBR_weekly$contractYear,\n#  levels = c(0, 1),\n#  labels = c(\"no\", \"yes\"))\n\nnfl_contractYearQBR_weekly &lt;- nfl_contractYearQBR_weekly %&gt;% \n  dplyr::arrange(playerMerge, season, season_type, game_week)\n\nnfl_contractYearQBRsubset_weekly &lt;- nfl_contractYearQBR_weekly %&gt;% \n  dplyr::filter(\n    season_type == \"Regular\",\n    game_week != \"Season Total\") %&gt;% \n  dplyr::mutate(game_week = as.numeric(game_week)) %&gt;% \n  dplyr::arrange(playerMerge, season, season_type, game_week)\n\n# Merge with age and experience\nnfl_contractYearQBRsubset_weekly &lt;- player_statsContractsSubset_offense_weekly %&gt;% \n  dplyr::filter(position == \"QB\") %&gt;% \n  dplyr::select(playerMerge, season, week, age, years_of_experience) %&gt;% \n  full_join(\n    nfl_contractYearQBRsubset_weekly,\n    by = c(\"playerMerge\",\"season\", c(\"week\" = \"game_week\"))\n  ) %&gt;% \n  arrange(player_id, season, week)\n\n#hist(nfl_contractYearQBRsubset_weekly$qb_plays) # players have at least 20 dropbacks per game\n\n# Calculate Per Game Totals\nnfl_contractYearQBR_seasonal &lt;- nfl_contractYearQBRsubset_weekly %&gt;% \n  dplyr::group_by(playerMerge, season) %&gt;% \n  dplyr::summarise(\n    age = min(age, na.rm = TRUE),\n    years_of_experience = min(years_of_experience, na.rm = TRUE),\n    qbr = mean(qbr_total, na.rm = TRUE),\n    pts_added = mean(pts_added, na.rm = TRUE),\n    epa_pass = mean(pass, na.rm = TRUE),\n    qb_plays = sum(qb_plays, na.rm = TRUE), # season total\n    contractYear = mean(contractYear, na.rm = TRUE),\n    games = n(),\n    .groups = \"drop_last\"\n  ) %&gt;% \n  dplyr::mutate(\n    contractYear = factor(\n      contractYear,\n      levels = c(0, 1),\n      labels = c(\"no\", \"yes\")\n    ))\n\nnfl_contractYearQBR_seasonal$age[which(is.infinite((nfl_contractYearQBR_seasonal$age)))] &lt;- NA\nnfl_contractYearQBR_seasonal$years_of_experience[which(is.infinite((nfl_contractYearQBR_seasonal$years_of_experience)))] &lt;- NA\n\nnfl_contractYearQBR_seasonal$ageCentered20 &lt;- nfl_contractYearQBR_seasonal$age - 20\nnfl_contractYearQBR_seasonal$ageCentered20Quadratic &lt;- nfl_contractYearQBR_seasonal$ageCentered20 ^ 2\n\nnfl_contractYearQBR_seasonal &lt;- nfl_contractYearQBR_seasonal %&gt;% \n  group_by(playerMerge) %&gt;%\n  mutate(player_id = as.factor(as.character(cur_group_id())))\n\nnfl_contractYearQBRsubset_seasonal &lt;- nfl_contractYearQBR_seasonal %&gt;% \n  dplyr::filter(\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\nThen, we analyze the data. Below is a mixed model that examines whether a player has a higher QBR per game when they are in a contract year compared to when they are not in a contract year.\n\nCodemixedModel_qbr &lt;- lmerTest::lmer(\n  qbr ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_qbr)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: qbr ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 8905.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2329 -0.5456  0.0896  0.5713  3.2440 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 111.6    10.56   \n Residual              198.5    14.09   \nNumber of obs: 1063, groups:  player_id, 253\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)      44.3390     0.8730 229.8416  50.789   &lt;2e-16 ***\ncontractYearyes  -0.3597     1.2223 953.3947  -0.294    0.769    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.237\n\nCodeMuMIn::r.squaredGLMM(mixedModel_qbr)\n\n            R2m       R2c\n[1,] 6.1808e-05 0.3598974\n\nCodeemmeans::emmeans(mixedModel_qbr, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             44.3 0.874 261     42.6     46.1\n yes            44.0 1.324 772     41.4     46.6\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_qbr &lt;- lmerTest::lmer(\n  qbr ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_qbr)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: qbr ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 6406.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5959 -0.5220  0.0761  0.5392  3.3949 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   112.4476 10.6041       \n           ageCentered20   0.6704  0.8188  -0.33\n Residual                166.1659 12.8905       \nNumber of obs: 779, groups:  player_id, 141\n\nFixed effects:\n                        Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)             40.33207    2.55392 142.62136  15.792  &lt; 2e-16 ***\ncontractYearyes         -0.07791    1.26592 708.57782  -0.062  0.95094    \nageCentered20            2.32928    0.74642 243.89521   3.121  0.00202 ** \nageCentered20Quadratic  -0.08397    0.02614  93.33416  -3.213  0.00180 ** \nyears_of_experience     -0.97172    0.66566 213.47863  -1.460  0.14582    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.030                     \nageCentrd20 -0.760 -0.072              \nagCntrd20Qd  0.716  0.024 -0.541       \nyrs_f_xprnc  0.205 -0.011 -0.706 -0.161\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_qbr)\n\n            R2m       R2c\n[1,] 0.02628777 0.4456601\n\nCodeemmeans::emmeans(mixedModelAge_qbr, \"contractYear\")\n\n contractYear emmean   SE  df lower.CL upper.CL\n no             46.0 1.18 137     43.7     48.4\n yes            45.9 1.43 311     43.1     48.8\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_ptsAdded &lt;- lmerTest::lmer(\n  pts_added ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_ptsAdded)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pts_added ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4855.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6918 -0.4922  0.0889  0.5347  4.4230 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.570    1.603   \n Residual              4.331    2.081   \nNumber of obs: 1063, groups:  player_id, 253\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)      -0.8463     0.1311 218.4369  -6.453 6.97e-10 ***\ncontractYearyes  -0.1217     0.1808 942.5385  -0.673    0.501    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.233\n\nCodeMuMIn::r.squaredGLMM(mixedModel_ptsAdded)\n\n              R2m       R2c\n[1,] 0.0003179454 0.3725808\n\nCodeemmeans::emmeans(mixedModel_ptsAdded, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no           -0.846 0.131 261    -1.10   -0.588\n yes          -0.968 0.197 765    -1.36   -0.581\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_ptsAdded &lt;- lmerTest::lmer(\n  pts_added ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_ptsAdded)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pts_added ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 3512.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.9208 -0.4965  0.0658  0.5031  4.4495 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   2.59159  1.6098        \n           ageCentered20 0.01027  0.1014   -0.34\n Residual                4.03620  2.0090        \nNumber of obs: 779, groups:  player_id, 141\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             -1.387551   0.390885 134.548792  -3.550 0.000532 ***\ncontractYearyes         -0.149952   0.195764 715.811059  -0.766 0.443939    \nageCentered20            0.296921   0.113148 235.035786   2.624 0.009255 ** \nageCentered20Quadratic  -0.011916   0.003933  68.209324  -3.029 0.003458 ** \nyears_of_experience     -0.084377   0.100309 201.954944  -0.841 0.401250    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.030                     \nageCentrd20 -0.762 -0.074              \nagCntrd20Qd  0.720  0.032 -0.548       \nyrs_f_xprnc  0.206 -0.012 -0.704 -0.161\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_ptsAdded)\n\n            R2m       R2c\n[1,] 0.01756871 0.4017384\n\nCodeemmeans::emmeans(mixedModelAge_ptsAdded, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no           -0.511 0.174 138   -0.855   -0.167\n yes          -0.661 0.215 336   -1.085   -0.238\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_epaPass &lt;- lmerTest::lmer(\n  epa_pass ~ contractYear + (1 | player_id),\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: epa_pass ~ contractYear + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4535.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0103 -0.5190  0.0374  0.5646  4.3622 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 2.462    1.569   \n Residual              3.053    1.747   \nNumber of obs: 1063, groups:  player_id, 253\n\nFixed effects:\n                Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)       1.0830     0.1219 238.3991   8.885   &lt;2e-16 ***\ncontractYearyes   0.3796     0.1532 930.4435   2.478   0.0134 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.211\n\nCodeMuMIn::r.squaredGLMM(mixedModel_epaPass)\n\n             R2m       R2c\n[1,] 0.003854912 0.4485124\n\nCodeemmeans::emmeans(mixedModel_epaPass, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             1.08 0.122 261    0.843     1.32\n yes            1.46 0.175 718    1.120     1.81\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaPass &lt;- lmerTest::lmer(\n  epa_pass ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 | player_id), # removed random slopes to address convergence issue\n  data = nfl_contractYearQBR_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaPass)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: epa_pass ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 | player_id)\n   Data: nfl_contractYearQBR_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 3220.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4966 -0.5284  0.0766  0.5353  4.4719 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 1.812    1.346   \n Residual              2.791    1.671   \nNumber of obs: 779, groups:  player_id, 141\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              0.362105   0.312642 649.706653   1.158 0.247203    \ncontractYearyes          0.036609   0.161602 732.113525   0.227 0.820845    \nageCentered20            0.305426   0.092038 452.862249   3.318 0.000978 ***\nageCentered20Quadratic  -0.004981   0.002900 725.381690  -1.718 0.086310 .  \nyears_of_experience     -0.105398   0.084021 259.426955  -1.254 0.210820    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.033                     \nageCentrd20 -0.745 -0.077              \nagCntrd20Qd  0.701  0.046 -0.506       \nyrs_f_xprnc  0.245 -0.014 -0.743 -0.165\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_epaPass)\n\n            R2m       R2c\n[1,] 0.05488804 0.4269798\n\nCodeemmeans::emmeans(mixedModelAge_epaPass, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             1.82 0.144 154     1.54     2.11\n yes            1.86 0.179 349     1.51     2.21\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCode# Placeholder for model predicting fantasy points\n\n\n\n16.2.2 RB\n\nCodeplayer_statsContractsRB_seasonal &lt;- player_statsContracts_seasonal %&gt;% \n  dplyr::filter(\n    position_group == \"RB\",\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\n\nCodemixedModel_ypc &lt;- lmerTest::lmer(\n  ypc ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_ypc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ypc ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4820.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8762 -0.4619  0.0100  0.4828  6.4883 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.3908   0.6252  \n Residual              1.1421   1.0687  \nNumber of obs: 1512, groups:  player_id, 482\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)     3.908e+00  4.472e-02 4.802e+02   87.39   &lt;2e-16 ***\ncontractYearyes 6.388e-02  7.022e-02 1.422e+03    0.91    0.363    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.345\n\nCodeMuMIn::r.squaredGLMM(mixedModel_ypc)\n\n              R2m       R2c\n[1,] 0.0004974734 0.2553257\n\nCodeemmeans::emmeans(mixedModel_ypc, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no             3.91 0.0447  552     3.82     4.00\n yes            3.97 0.0691 1163     3.84     4.11\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_ypc &lt;- lmerTest::lmer(\n  ypc ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_ypc)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ypc ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 3788.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9726 -0.4487  0.0059  0.4747  6.3513 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   1.156674 1.07549       \n           ageCentered20 0.009793 0.09896  -0.82\n Residual                1.016612 1.00827       \nNumber of obs: 1200, groups:  player_id, 297\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             4.306e+00  1.806e-01  3.655e+02  23.850   &lt;2e-16 ***\ncontractYearyes         1.989e-01  7.741e-02  1.051e+03   2.570   0.0103 *  \nageCentered20          -7.557e-02  6.164e-02  3.555e+02  -1.226   0.2211    \nageCentered20Quadratic -1.408e-03  3.858e-03  1.550e+02  -0.365   0.7157    \nyears_of_experience     1.900e-02  3.935e-02  2.313e+02   0.483   0.6298    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.161                     \nageCentrd20 -0.870 -0.202              \nagCntrd20Qd  0.763  0.136 -0.778       \nyrs_f_xprnc  0.175 -0.001 -0.493 -0.093\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_ypc)\n\n            R2m       R2c\n[1,] 0.02640195 0.3663571\n\nCodeemmeans::emmeans(mixedModelAge_ypc, \"contractYear\")\n\n contractYear emmean     SE  df lower.CL upper.CL\n no             3.88 0.0574 347     3.77     3.99\n yes            4.08 0.0765 756     3.93     4.23\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_epaRush &lt;- lmerTest::lmer(\n  rushing_epa ~ contractYear + (1 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: rushing_epa ~ contractYear + (1 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 4286.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6917 -0.5183  0.0880  0.6130  3.4724 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.1027   0.3205  \n Residual              0.9050   0.9513  \nNumber of obs: 1512, groups:  player_id, 482\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       -0.65702    0.03288  571.11887 -19.980   &lt;2e-16 ***\ncontractYearyes    0.04836    0.05962 1508.56556   0.811    0.417    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.424\n\nCodeMuMIn::r.squaredGLMM(mixedModel_epaRush)\n\n              R2m       R2c\n[1,] 0.0004336933 0.1023161\n\nCodeemmeans::emmeans(mixedModel_epaRush, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no           -0.657 0.0329  568   -0.722   -0.592\n yes          -0.609 0.0546 1071   -0.716   -0.501\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaRush &lt;- lmerTest::lmer(\n  rushing_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsRB_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaRush)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: rushing_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsRB_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 3394.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8114 -0.4930  0.0664  0.6129  2.8627 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.158418 0.39802       \n           ageCentered20 0.001865 0.04319  -0.58\n Residual                0.872915 0.93430       \nNumber of obs: 1200, groups:  player_id, 297\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)            -7.358e-01  1.364e-01  2.313e+02  -5.394  1.7e-07 ***\ncontractYearyes         8.520e-02  6.750e-02  1.095e+03   1.262   0.2072    \nageCentered20           8.437e-02  4.648e-02  2.363e+02   1.815   0.0707 .  \nageCentered20Quadratic -4.352e-03  3.138e-03  1.085e+02  -1.387   0.1684    \nyears_of_experience    -5.794e-02  2.706e-02  2.894e+02  -2.141   0.0331 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.141                     \nageCentrd20 -0.889 -0.201              \nagCntrd20Qd  0.811  0.120 -0.824       \nyrs_f_xprnc  0.054  0.002 -0.364 -0.153\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_epaRush)\n\n            R2m       R2c\n[1,] 0.01374299 0.1319981\n\nCodeemmeans::emmeans(mixedModelAge_epaRush, \"contractYear\")\n\n contractYear emmean     SE  df lower.CL upper.CL\n no           -0.662 0.0400 342   -0.741   -0.584\n yes          -0.577 0.0593 809   -0.694   -0.461\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCode# Placeholder for model predicting fantasy points\n\n\n\n16.2.3 WR/TE\n\nCodeplayer_statsContractsWRTE_seasonal &lt;- player_statsContracts_seasonal %&gt;% \n  dplyr::filter(\n    position_group %in% c(\"WR\",\"TE\"),\n    games &gt;= 5, # keep only player-season combinations in which QBs played at least 5 games\n    season &gt;= 2011) # keep only seasons since 2011 (when most contract data are available)\n\n\n\nCodemixedModel_receivingYards &lt;- lmerTest::lmer(\n  receiving_yards ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_receivingYards)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: receiving_yards ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 27097.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.7809 -0.5650 -0.0858  0.5273  4.5022 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 234.0    15.3    \n Residual              190.3    13.8    \nNumber of obs: 3180, groups:  player_id, 937\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)       29.1891     0.6010 1141.2390  48.567  &lt; 2e-16 ***\ncontractYearyes   -3.6373     0.6368 2749.5592  -5.712 1.24e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.247\n\nCodeMuMIn::r.squaredGLMM(mixedModel_receivingYards)\n\n             R2m       R2c\n[1,] 0.006102263 0.5542387\n\nCodeemmeans::emmeans(mixedModel_receivingYards, \"contractYear\")\n\n contractYear emmean    SE   df lower.CL upper.CL\n no             29.2 0.601 1027     28.0     30.4\n yes            25.6 0.760 1971     24.1     27.0\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_receivingYards &lt;- lmerTest::lmer(\n  receiving_yards ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_receivingYards)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nreceiving_yards ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 21882.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.8755 -0.5621 -0.0763  0.5153  3.7832 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   519.305  22.788        \n           ageCentered20   6.608   2.571   -0.69\n Residual                144.036  12.001        \nNumber of obs: 2605, groups:  player_id, 611\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)              16.08848    1.95634  927.68747   8.224 6.62e-16 ***\ncontractYearyes          -3.52243    0.65104 2107.73250  -5.410 7.00e-08 ***\nageCentered20             4.45519    0.67288 1401.41824   6.621 5.07e-11 ***\nageCentered20Quadratic   -0.48965    0.03347 1099.43200 -14.631  &lt; 2e-16 ***\nyears_of_experience       1.91834    0.52570  819.74997   3.649  0.00028 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.104                     \nageCentrd20 -0.827 -0.153              \nagCntrd20Qd  0.665  0.064 -0.631       \nyrs_f_xprnc  0.325  0.050 -0.688 -0.054\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_receivingYards)\n\n           R2m      R2c\n[1,] 0.1130403 0.732794\n\nCodeemmeans::emmeans(mixedModelAge_receivingYards, \"contractYear\")\n\n contractYear emmean    SE  df lower.CL upper.CL\n no             29.1 0.810 701     27.5     30.7\n yes            25.6 0.885 991     23.8     27.3\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCodemixedModel_epaReceiving &lt;- lmerTest::lmer(\n  receiving_epa ~ contractYear + (1 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModel_epaReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: receiving_epa ~ contractYear + (1 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 10539.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1665 -0.5768 -0.0618  0.5286  3.9525 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n player_id (Intercept) 0.5242   0.724   \n Residual              1.2754   1.129   \nNumber of obs: 3178, groups:  player_id, 937\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)        0.71050    0.03523 1259.96934  20.165   &lt;2e-16 ***\ncontractYearyes   -0.10914    0.04987 3029.49236  -2.189   0.0287 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncontrctYrys -0.347\n\nCodeMuMIn::r.squaredGLMM(mixedModel_epaReceiving)\n\n             R2m       R2c\n[1,] 0.001302291 0.2922122\n\nCodeemmeans::emmeans(mixedModel_epaReceiving, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no            0.710 0.0352 1082    0.641     0.78\n yes           0.601 0.0501 2218    0.503     0.70\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\nCodemixedModelAge_epaReceiving &lt;- lmerTest::lmer(\n  receiving_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic + years_of_experience + (1 + ageCentered20 | player_id),\n  data = player_statsContractsWRTE_seasonal,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\n\nsummary(mixedModelAge_epaReceiving)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nreceiving_epa ~ contractYear + ageCentered20 + ageCentered20Quadratic +  \n    years_of_experience + (1 + ageCentered20 | player_id)\n   Data: player_statsContractsWRTE_seasonal\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 8665.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2118 -0.5745 -0.0439  0.5393  3.7526 \n\nRandom effects:\n Groups    Name          Variance Std.Dev. Corr \n player_id (Intercept)   0.914001 0.95603       \n           ageCentered20 0.007514 0.08668  -0.62\n Residual                1.244769 1.11569       \nNumber of obs: 2604, groups:  player_id, 611\n\nFixed effects:\n                         Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)             2.925e-01  1.287e-01  7.271e+02   2.272  0.02338 *  \ncontractYearyes        -1.818e-01  5.583e-02  2.424e+03  -3.256  0.00114 ** \nageCentered20           1.815e-01  4.266e-02  8.762e+02   4.254 2.33e-05 ***\nageCentered20Quadratic -1.365e-02  2.346e-03  3.653e+02  -5.818 1.30e-08 ***\nyears_of_experience     1.007e-02  2.958e-02  7.074e+02   0.340  0.73359    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cntrcY agCn20 agC20Q\ncontrctYrys  0.115                     \nageCentrd20 -0.856 -0.194              \nagCntrd20Qd  0.756  0.093 -0.727       \nyrs_f_xprnc  0.204  0.067 -0.558 -0.099\n\nCodeMuMIn::r.squaredGLMM(mixedModelAge_epaReceiving)\n\n            R2m       R2c\n[1,] 0.02030355 0.3491276\n\nCodeemmeans::emmeans(mixedModelAge_epaReceiving, \"contractYear\")\n\n contractYear emmean     SE   df lower.CL upper.CL\n no            0.820 0.0445  700    0.733    0.907\n yes           0.638 0.0554 1443    0.530    0.747\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\n\nCode# Placeholder for model predicting fantasy points\n\n\n\n16.2.4 QB/RB/WR/TE\n\nCode# Placeholder for model predicting fantasy points\n# Include player position as a covariate",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersConclusion",
    "href": "mythbusters.html#sec-mythbustersConclusion",
    "title": "16  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n16.3 Conclusion",
    "text": "16.3 Conclusion",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "mythbusters.html#sec-mythbustersSessionInfo",
    "href": "mythbusters.html#sec-mythbustersSessionInfo",
    "title": "16  Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test",
    "section": "\n16.4 Session Info",
    "text": "16.4 Session Info\n\nCodesessionInfo()\n\nR version 4.4.1 (2024-06-14)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      \n [5] purrr_1.0.2       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     \n [9] ggplot2_3.5.1     tidyverse_2.0.0   emmeans_1.10.3    MuMIn_1.48.4     \n[13] lmerTest_3.1-3    lme4_1.1-35.5     Matrix_1.7-0      petersenlab_1.0.3\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    psych_2.4.6.26      viridisLite_0.4.2  \n [4] fastmap_1.2.0       digest_0.6.36       rpart_4.1.23       \n [7] timechange_0.3.0    estimability_1.5.1  lifecycle_1.0.4    \n[10] cluster_2.1.6       magrittr_2.0.3      compiler_4.4.1     \n[13] rlang_1.1.4         Hmisc_5.1-3         tools_4.4.1        \n[16] utf8_1.2.4          yaml_2.3.9          data.table_1.15.4  \n[19] knitr_1.48          htmlwidgets_1.6.4   mnormt_2.1.1       \n[22] plyr_1.8.9          RColorBrewer_1.1-3  foreign_0.8-86     \n[25] withr_3.0.0         numDeriv_2016.8-1.1 nnet_7.3-19        \n[28] grid_4.4.1          stats4_4.4.1        fansi_1.0.6        \n[31] lavaan_0.6-18       xtable_1.8-4        colorspace_2.1-0   \n[34] scales_1.3.0        MASS_7.3-60.2       cli_3.6.3          \n[37] mvtnorm_1.2-5       rmarkdown_2.27      generics_0.1.3     \n[40] rstudioapi_0.16.0   tzdb_0.4.0          reshape2_1.4.4     \n[43] minqa_1.2.7         DBI_1.2.3           splines_4.4.1      \n[46] parallel_4.4.1      base64enc_0.1-3     mitools_2.4        \n[49] vctrs_0.6.5         boot_1.3-30         jsonlite_1.8.8     \n[52] hms_1.1.3           pbkrtest_0.5.3      Formula_1.2-5      \n[55] htmlTable_2.4.3     glue_1.7.0          nloptr_2.1.1       \n[58] stringi_1.8.4       gtable_0.3.5        quadprog_1.5-8     \n[61] munsell_0.5.1       pillar_1.9.0        htmltools_0.5.8.1  \n[64] R6_2.5.1            mix_1.0-12          evaluate_0.24.0    \n[67] pbivnorm_0.6.0      lattice_0.22-6      backports_1.5.0    \n[70] broom_1.0.6         Rcpp_1.0.13         coda_0.19-4.1      \n[73] gridExtra_2.3       nlme_3.1-164        checkmate_2.3.1    \n[76] xfun_0.46           pkgconfig_2.0.3    \n\n\n\n\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for windows. Wiley-Blackwell.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "I need your help!\n    \n    I want your feedback to make the book better for you and other readers. If you find typos, errors, or places where the text may be improved, please let me know.\n    The best ways to provide feedback are by GitHub or hypothes.is annotations.\n    \n\n    \n      \n      Opening an issue or submitting a pull request on GitHub: https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook\n    \n    \n      \n      Adding an annotation using hypothes.is.\n      To add an annotation, select some text and then click the\n      \n      \n    \n    \n      symbol on the pop-up menu.\n      To see the annotations of others, click the\n      \n\n      symbol in the upper right-hand corner of the page.\n    \n  \n\n\n\n\n\n\n\nÆgisdóttir, S., White, M. J., Spengler, P. M., Maugherman, A. S.,\nAnderson, L. A., Cook, R. S., Nichols, C. N., Lampropoulos, G. K.,\nWalker, B. S., Cohen, G., & Rush, J. D. (2006). The meta-analysis of\nclinical judgment project: Fifty-six years of accumulated research on\nclinical versus statistical prediction. The Counseling\nPsychologist, 34(3), 341–382. https://doi.org/10.1177/0011000005285875\n\n\nAndersen, D., Petersen, I. T., & Tungate, A. (2024). ffanalytics: Scrape data for fantasy\nfootball. https://github.com/FantasyFootballAnalytics/ffanalytics\n\n\nAtanasov, P., Witkowski, J., Ungar, L., Mellers, B., & Tetlock, P.\n(2020). Small steps to accuracy: Incremental belief updaters are better\nforecasters. Organizational Behavior and Human Decision\nProcesses, 160, 19–35. https://doi.org/10.1016/j.obhdp.2020.02.001\n\n\nAustin, P. C., & Steyerberg, E. W. (2014). Graphical assessment of\ninternal and external calibration of logistic regression models by using\nloess smoothers. Statistics in Medicine, 33(3),\n517–535. https://doi.org/10.1002/sim.5941\n\n\nAvugos, S., Köppen, J., Czienskowski, U., Raab, M., & Bar-Eli, M.\n(2013). The “hot hand” reconsidered: A meta-analytic\napproach. Psychology of Sport and Exercise, 14(1),\n21–27. https://doi.org/10.1016/j.psychsport.2012.07.005\n\n\nBaird, C., & Wagner, D. (2000). The relative validity of actuarial-\nand consensus-based risk assessment systems. Children and Youth\nServices Review, 22(11), 839–871. https://doi.org/10.1016/S0190-7409(00)00122-5\n\n\nBar-Eli, M., Avugos, S., & Raab, M. (2006). Twenty years of\n“hot hand” research: Review and critique. Psychology of\nSport and Exercise, 7(6), 525–553. https://doi.org/10.1016/j.psychsport.2006.03.001\n\n\nBocskocsky, A., Ezekowitz, J., & Stein, C. (2014). The hot hand: A\nnew approach to an old “fallacy.” MIT Sloan Sports\nAnalytics Conference.\n\n\nBolger, F., & Önkal-Atay, D. (2004). The effects of feedback on\njudgmental interval predictions. International Journal of\nForecasting, 20(1), 29–39. https://doi.org/10.1016/S0169-2070(03)00009-8\n\n\nChatterjee, S. (2021). A new coefficient of correlation. Journal of\nthe American Statistical Association, 116(536), 2009–2022.\nhttps://doi.org/10.1080/01621459.2020.1758115\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral\nsciences (2nd ed.). Lawrence Erlbaum Associates, Publishers. https://doi.org/10.4324/9780203771587\n\n\nCongelio, B. J. (2023). Introduction to NFL analytics\nwith R. CRC Press. https://bradcongelio.com/nfl-analytics-with-r-book\n\n\nCorston, R., & Colman, A. M. (2000). A crash course in SPSS for\nwindows. Wiley-Blackwell.\n\n\nD’Onofrio, B. M., Sjölander, A., Lahey, B. B., Lichtenstein, P., &\nÖberg, A. S. (2020). Accounting for confounding in observational\nstudies. Annual Review of Clinical Psychology, 16(1),\n25–48. https://doi.org/10.1146/annurev-clinpsy-032816-045030\n\n\nDana, J., & Thomas, R. (2006). In defense of clinical judgment … and\nmechanical prediction. Journal of Behavioral Decision Making,\n19(5), 413–428. https://doi.org/10.1002/bdm.537\n\n\nDawes, R. M., Faust, D., & Meehl, P. E. (1989). Clinical versus\nactuarial judgment. Science, 243(4899), 1668–1674. https://doi.org/10.1126/science.2648573\n\n\nDen Hartigh, R. J. R., Niessen, A. S. M., Frencken, W. G. P., &\nMeijer, R. R. (2018). Selection procedures in sports:\nImproving predictions of athletes’ future performance.\nEuropean Journal of Sport Science, 18(9), 1191–1198.\nhttps://doi.org/10.1080/17461391.2018.1480662\n\n\nDigitale, J. C., Martin, J. N., & Glymour, M. M. (2022). Tutorial on\ndirected acyclic graphs. Journal of Clinical Epidemiology,\n142, 264–267. https://doi.org/10.1016/j.jclinepi.2021.08.001\n\n\nEddy, D. M. (1982). Probabilistic reasoning in clinical medicine:\nProblems and opportunities. In D. Kahneman, P. Slovic, & A. Tversky\n(Eds.), Judgment under uncertainty: Heuristics and biases (pp.\n249–267). Cambridge University Press.\n\n\nFarrington, D. P., & Loeber, R. (1989). Relative improvement over\nchance (RIOC) and phi as measures of predictive efficiency\nand strength of association in 2×2 tables. Journal of Quantitative\nCriminology, 5(3), 201–213. https://doi.org/10.1007/BF01062737\n\n\nGarb, H. N., & Wood, J. M. (2019). Methodological advances in\nstatistical prediction. Psychological Assessment,\n31(12), 1456–1466. https://doi.org/10.1037/pas0000673\n\n\nGetty, D., Li, H., Yano, M., Gao, C., & Hosoi, A. E. (2018). Luck\nand the law: Quantifying chance in fantasy sports and other contests.\nSIAM Review, 60(4), 869–887. https://doi.org/10.1137/16m1102094\n\n\nGilovich, T., Vallone, R., & Tversky, A. (1985). The hot hand in\nbasketball: On the misperception of random sequences. Cognitive\nPsychology, 17(3), 295–314. https://doi.org/10.1016/0010-0285(85)90010-6\n\n\nGoodman, S. (2008). A dirty dozen: Twelve p-value\nmisconceptions. Seminars in Hematology, 45(3),\n135–140. https://doi.org/10.1053/j.seminhematol.2008.04.003\n\n\nGrove, W. M., & Meehl, P. E. (1996). Comparative efficiency of\ninformal (subjective, impressionistic) and formal (mechanical,\nalgorithmic) prediction procedures: The clinical–statistical\ncontroversy. Psychology, Public Policy, and Law, 2(2),\n293–323. https://doi.org/10.1037/1076-8971.2.2.293\n\n\nGrove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., & Nelson, C.\n(2000). Clinical versus mechanical prediction: A meta-analysis.\nPsychological Assessment, 12(1), 19–30. https://doi.org/10.1037/1040-3590.12.1.19\n\n\nHarrell, Jr., F. E. (2024). rms:\nRegression modeling strategies. https://hbiostat.org/R/rms/\n\n\nHoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting\npersonal events. Journal of Experimental Psychology: Learning,\nMemory, and Cognition, 11(4), 719–731. https://doi.org/10.1037/0278-7393.11.1-4.719\n\n\nHough, S. E. (2016). Predicting the unpredictable: The tumultuous\nscience of earthquake prediction. Princeton University Press.\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting:\nPrinciples and practice (3rd ed.). OTexts. https://otexts.com/fpp3\n\n\nJohnson, J. E. V., & Bruce, A. C. (2001). Calibration of subjective\nprobability judgments in a naturalistic setting. Organizational\nBehavior and Human Decision Processes, 85(2), 265–290. https://doi.org/10.1006/obhd.2000.2949\n\n\nKeren, G. (1987). Facing uncertainty in the game of bridge: A\ncalibration study. Organizational Behavior and Human Decision\nProcesses, 39(1), 98–114. https://doi.org/10.1016/0749-5978(87)90047-1\n\n\nKessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., &\nZubizarreta, J. R. (2020). Suicide prediction models: A critical review\nof recent research with recommendations for the way forward.\nMolecular Psychiatry, 25(1), 168–179. https://doi.org/10.1038/s41380-019-0531-0\n\n\nKievit, R., Frankenhuis, W., Waldorp, L., & Borsboom, D. (2013).\nSimpson’s paradox in psychological science: A practical guide.\nFrontiers in Psychology, 4(513). https://doi.org/10.3389/fpsyg.2013.00513\n\n\nKoehler, D. J., Brenner, L., & Griffin, D. (2002). The calibration\nof expert judgment: Heuristics and biases beyond the laboratory. In T.\nGilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics and\nbiases: The psychology of intuitive judgment. Cambridge University\nPress.\n\n\nKoriat, A., Lichtenstein, S., & Fischhoff, B. (1980). Reasons for\nconfidence. Journal of Experimental Psychology: Human Learning and\nMemory, 6(2), 107–118. https://doi.org/10.1037/0278-7393.6.2.107\n\n\nKotrba, V. (2020). Heuristics in fantasy sports: Is it profitable to\nstrategize based on favourite of the match? Mind & Society,\n19(1), 195–206. https://doi.org/10.1007/s11299-020-00231-7\n\n\nLederer, D. J., Bell, S. C., Branson, R. D., Chalmers, J. D., Marshall,\nR., Maslove, D. M., Ost, D. E., Punjabi, N. M., Schatz, M., Smyth, A.\nR., Stewart, P. W., Suissa, S., Adjei, A. A., Akdis, C. A., Azoulay, É.,\nBakker, J., Ballas, Z. K., Bardin, P. G., Barreiro, E., … Vincent, J.-L.\n(2019). Control of confounding and reporting of results in causal\ninference studies. Guidance for authors from editors of respiratory,\nsleep, and critical care journals. Annals of the American Thoracic\nSociety, 16(1), 22–28. https://doi.org/10.1513/AnnalsATS.201808-564PS\n\n\nLee, M. D., & Liu, S. (2022). Drafting strategies in fantasy\nfootball: A study of competitive sequential human decision making.\nJudgment and Decision Making, 17(4), 691–719. https://doi.org/10.1017/S1930297500008901\n\n\nLilienfeld, S. O. (2007). Psychological treatments that cause harm.\nPerspectives on Psychological Science, 2(1), 53–70. https://doi.org/10.1111/j.1745-6916.2007.00029.x\n\n\nLindhiem, O., Petersen, I. T., Mentch, L. K., & Youngstrom, E. A.\n(2020). The importance of calibration in clinical psychology.\nAssessment, 27(4), 840–854. https://doi.org/10.1177/1073191117752055\n\n\nLyons, B. D., Hoffman, B. J., Michel, J. W., & Williams, K. J.\n(2011). On the predictive efficiency of past performance and physical\nability: The case of the national football league. Human\nPerformance, 24(2), 158–172. https://doi.org/10.1080/08959285.2011.555218\n\n\nMakridakis, S., Hogarth, R. M., & Gaba, A. (2009). Forecasting and\nuncertainty in the economic and business world. International\nJournal of Forecasting, 25(4), 794–812. https://doi.org/10.1016/j.ijforecast.2009.05.012\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree:\nThe case of r and d. Psychological Methods,\n11(4), 386–401. https://doi.org/10.1037/1082-989X.11.4.386\n\n\nMeehl, P. E. (1957). When shall we use our heads instead of the formula?\nJournal of Counseling Psychology, 4(4), 268–273. https://doi.org/10.1037/h0047554\n\n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks:\nSir Karl, Sir\nRonald, and the slow progress of soft psychology.\nJournal of Consulting and Clinical Psychology, 46(4),\n806–834. https://doi.org/10.1037/0022-006x.46.4.806\n\n\nMeehl, P. E. (1986). Causes and effects of my disturbing little book.\nJournal of Personality Assessment, 50(3), 370–375. https://doi.org/10.1207/s15327752jpa5003_6\n\n\nMeehl, P. E., & Rosen, A. (1955). Antecedent probability and the\nefficiency of psychometric signs, patterns, or cutting scores.\nPsychological Bulletin, 52(3), 194–216. https://doi.org/10.1037/h0048070\n\n\nMiller, J. B., & Sanjurjo, A. (2014). A cold shower for the hot hand\nfallacy. Innocenzo Gasparini Institute for Economic Research.\nhttps://repec.unibocconi.it/igier/igi/wp/2014/518.pdf\n\n\nMoore, D. A., & Healy, P. J. (2008). The trouble with\noverconfidence. Psychological Review, 115(2), 502–517.\nhttps://doi.org/10.1037/0033-295X.115.2.502\n\n\nMorley, S. K., Brito, T. V., & Welling, D. T. (2018). Measures of\nmodel performance based on the log accuracy ratio. Space\nWeather, 16(1), 69–88. https://doi.org/10.1002/2017SW001669\n\n\nMotz, B. (2013). Fantasy football: A touchdown for undergraduate\nstatistics education. Proceedings of the Games, Learning, and\nSociety Conference, 9.0, 222–228. https://doi.org/10.1184/R1/6686804.v1\n\n\nMurphy, A. H., & Winkler, R. L. (1984). Probability forecasting in\nmeterology. Journal of the American Statistical Association,\n79(387), 489–500. https://doi.org/10.2307/2288395\n\n\nOskamp, S. (1965). Overconfidence in case-study judgments. Journal\nof Consulting Psychology, 29(3), 261–265. https://doi.org/10.1037/h0022125\n\n\nPelechrinis, K., & Winston, W. (2022). The hot hand in the wild.\nPLOS ONE, 17(1), e0261890. https://doi.org/10.1371/journal.pone.0261890\n\n\nPetersen, I. T. (2024a). petersenlab: A\ncollection of R functions by the Petersen\nLab. https://github.com/DevPsyLab/petersenlab\n\n\nPetersen, I. T. (2024c). Principles of psychological assessment:\nWith applied examples in R. University of Iowa\nLibraries. https://doi.org/10.25820/work.007199\n\n\nPetersen, I. T. (2024b). Principles of psychological assessment:\nWith applied examples in R. Chapman and\nHall/CRC. https://doi.org/10.1201/9781003357421\n\n\nRice, M. E., Harris, G. T., & Lang, C. (2013). Validation of and\nrevision to the VRAG and SORAG: The\nViolence Risk Appraisal Guide—Revised\n(VRAG-R). Psychological Assessment,\n25(3), 951–965. https://doi.org/10.1037/a0032878\n\n\nRobin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez,\nJ.-C., & Müller, M. (2023). pROC:\nDisplay and analyze ROC curves. https://xrobin.github.io/pROC/\n\n\nRusso, J. E., & Schoemaker, P. J. (1992). Managing overconfidence.\nSloan Management Review, 33(2), 7.\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002).\nExperimental and quasi-experimental designs for generalized causal\ninference. Houghton Mifflin.\n\n\nSilver, N. (2012). The signal and the noise: Why so many predictions\nfail–but some don’t. Penguin.\n\n\nSkala, D. (2008). Overconfidence in psychology and finance–an\ninterdisciplinary literature review. Bank i Kredyt, 4,\n33–50.\n\n\nStevens, R. J., & Poppe, K. K. (2020). Validation of clinical\nprediction models: What does the “calibration slope” really\nmeasure? Journal of Clinical Epidemiology, 118, 93–99.\nhttps://doi.org/10.1016/j.jclinepi.2019.09.016\n\n\nSteyerberg, E. W., & Vergouwe, Y. (2014). Towards better clinical\nprediction models: Seven steps for development and an ABCD for\nvalidation. European Heart Journal, 35(29), 1925–1931.\nhttps://doi.org/10.1093/eurheartj/ehu207\n\n\nTetlock, P. E. (2017). Expert political judgment: How good is it?\nHow can we know? - New edition. Princeton University\nPress.\n\n\nTextor, J., Zander, B. van der, Gilthorpe, M. S., Liśkiewicz, M., &\nEllison, G. T. (2017). Robust causal inference using directed acyclic\ngraphs: The R package “dagitty”.\nInternational Journal of Epidemiology, 45(6),\n1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nTofallis, C. (2015). A better measure of relative prediction accuracy\nfor model selection and model estimation. Journal of the Operational\nResearch Society, 66(8), 1352–1362. https://doi.org/10.1057/jors.2014.103\n\n\nTreat, T. A., & Viken, R. J. (2023). Measuring test performance with\nsignal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M.\nMcMullen, A. T. Panter, D. Rindskopf, & K. J. Sher (Eds.), APA\nhandbook of research methods in psychology: Foundations, planning,\nmeasures, and psychometrics (2nd ed., Vol. 1, pp. 837–858).\nAmerican Psychological Association.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty:\nHeuristics and biases. Science, 185(4157), 1124–1131.\nhttps://doi.org/10.1126/science.185.4157.1124\n\n\nUrsenbach, J., O’Connell, M. E., Neiser, J., Tierney, M. C., Morgan, D.,\nKosteniuk, J., & Spiteri, R. J. (2019). Scoring algorithms for a\ncomputer-based cognitive screening tool: An illustrative example of\noverfitting machine learning approaches and the impact on estimates of\nclassification accuracy. Psychological Assessment,\n31(11), 1377–1382. https://doi.org/10.1037/pas0000764\n\n\nWilliams, A. J., Botanov, Y., Kilshaw, R. E., Wong, R. E., &\nSakaluk, J. K. (2021). Potentially harmful therapies: A meta-scientific\nreview of evidential value. Clinical Psychology: Science and\nPractice, 28(1), 5–18. https://doi.org/10.1111/cpsp.12331\n\n\nWoodland, L. M., & Woodland, B. M. (2015). The National\nFootball League season wins total betting market: The impact of\nheuristics on behavior. Southern Economic Journal,\n82(1), 38–54. https://doi.org/10.4284/0038-4038-2013.145",
    "crumbs": [
      "References"
    ]
  }
]