<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Isaac T. Petersen">
<meta name="dcterms.date" content="today">
<title>17&nbsp; Evaluation of Prediction/Forecasting Accuracy – Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./mythbusters.html" rel="next">
<link href="./base-rates.html" rel="prev">
<link href="./images/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-1fa1d8026083a9bddc52b4aafc7f818e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-8e00f2f28a1266a720913de837032221.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-ddd431e9f75ca955af8accee01469505.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-9f2e48d73e00c997ec17b18daa718bc5.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-C3SNF7Y0PJ"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-C3SNF7Y0PJ', { 'anonymize_ip': true});
</script><script type="application/json" class="js-hypothesis-config">
{
  "theme": "classic",
  "openSidebar": false,
  "showHighlights": "whenSidebarOpen"
}
</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<meta property="og:title" content="17&nbsp; Evaluation of Prediction/Forecasting Accuracy – Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R">
<meta property="og:description" content="">
<meta property="og:image" content="https://isaactpetersen.github.io/Fantasy-Football-Analytics-Textbook/evaluating-prediction-accuracy_files/figure-html/fig-classificationDistributions-1.png">
<meta property="og:site_name" content="Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R">
<meta property="og:image:alt" content="Distribution of Test Scores by Berry Type.">
<meta property="og:image:height" content="960">
<meta property="og:image:width" content="1344">
<meta name="twitter:title" content="17&nbsp; Evaluation of Prediction/Forecasting Accuracy – Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://isaactpetersen.github.io/Fantasy-Football-Analytics-Textbook/evaluating-prediction-accuracy_files/figure-html/fig-classificationDistributions-1.png">
<meta name="twitter:image:alt" content="Distribution of Test Scores by Berry Type.">
<meta name="twitter:image-height" content="960">
<meta name="twitter:image-width" content="1344">
<meta name="twitter:card" content="summary_large_image">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./evaluating-prediction-accuracy.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Evaluation of Prediction/Forecasting Accuracy</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fantasy-football.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Intro to Football and Fantasy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Getting Started with <code>R</code> for Data Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./download-football-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Download and Process NFL Football Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./player-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Player Evaluation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./draft.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Fantasy Draft</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./research-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Research Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basic-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Basic Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Correlation Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multiple-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multiple Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Mixed Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Causal Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cognitive-bias.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Heuristics and Cognitive Biases in Prediction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./actuarial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Judgment Versus Actuarial Approaches to Prediction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./base-rates.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Base Rates</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./evaluating-prediction-accuracy.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Evaluation of Prediction/Forecasting Accuracy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mythbusters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modern-portfolio-theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Modern Portfolio Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cluster-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Cluster Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./factor-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Factor Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Data Reduction: Principal Component Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time-series-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Time Series Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision-making.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Decision Making in the Context of Uncertainty</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sports-cognitive-psychology.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Sports and Cognitive Psychology</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-predictionAccuracyGettingStarted" id="toc-sec-predictionAccuracyGettingStarted" class="nav-link active" data-scroll-target="#sec-predictionAccuracyGettingStarted"><span class="header-section-number">17.1</span> Getting Started</a>
  <ul>
<li><a href="#sec-predictionAccuracyLoadPackages" id="toc-sec-predictionAccuracyLoadPackages" class="nav-link" data-scroll-target="#sec-predictionAccuracyLoadPackages"><span class="header-section-number">17.1.1</span> Load Packages</a></li>
  </ul>
</li>
  <li><a href="#sec-predictionAccuracyOverview" id="toc-sec-predictionAccuracyOverview" class="nav-link" data-scroll-target="#sec-predictionAccuracyOverview"><span class="header-section-number">17.2</span> Overview</a></li>
  <li>
<a href="#sec-accuracyTypes" id="toc-sec-accuracyTypes" class="nav-link" data-scroll-target="#sec-accuracyTypes"><span class="header-section-number">17.3</span> Types of Accuracy</a>
  <ul>
<li><a href="#sec-discrimination" id="toc-sec-discrimination" class="nav-link" data-scroll-target="#sec-discrimination"><span class="header-section-number">17.3.1</span> Discrimination</a></li>
  <li><a href="#sec-calibration" id="toc-sec-calibration" class="nav-link" data-scroll-target="#sec-calibration"><span class="header-section-number">17.3.2</span> Calibration</a></li>
  <li><a href="#sec-generalAccuracy" id="toc-sec-generalAccuracy" class="nav-link" data-scroll-target="#sec-generalAccuracy"><span class="header-section-number">17.3.3</span> General Accuracy</a></li>
  </ul>
</li>
  <li><a href="#sec-predictionCategorical" id="toc-sec-predictionCategorical" class="nav-link" data-scroll-target="#sec-predictionCategorical"><span class="header-section-number">17.4</span> Prediction of Categorical Outcomes</a></li>
  <li><a href="#sec-predictionContinuous" id="toc-sec-predictionContinuous" class="nav-link" data-scroll-target="#sec-predictionContinuous"><span class="header-section-number">17.5</span> Prediction of Continuous Outcomes</a></li>
  <li>
<a href="#sec-thresholdDependentAccuracy" id="toc-sec-thresholdDependentAccuracy" class="nav-link" data-scroll-target="#sec-thresholdDependentAccuracy"><span class="header-section-number">17.6</span> Threshold-Dependent Accuracy Indices</a>
  <ul>
<li><a href="#sec-decisionOutcomes" id="toc-sec-decisionOutcomes" class="nav-link" data-scroll-target="#sec-decisionOutcomes"><span class="header-section-number">17.6.1</span> Decision Outcomes</a></li>
  <li><a href="#sec-percentAccuracyOverview" id="toc-sec-percentAccuracyOverview" class="nav-link" data-scroll-target="#sec-percentAccuracyOverview"><span class="header-section-number">17.6.2</span> Percent Accuracy</a></li>
  <li><a href="#sec-accuracyByChance" id="toc-sec-accuracyByChance" class="nav-link" data-scroll-target="#sec-accuracyByChance"><span class="header-section-number">17.6.3</span> Percent Accuracy by Chance</a></li>
  <li><a href="#sec-predictingFromBaseRate" id="toc-sec-predictingFromBaseRate" class="nav-link" data-scroll-target="#sec-predictingFromBaseRate"><span class="header-section-number">17.6.4</span> Predicting from the Base Rate</a></li>
  <li><a href="#sec-differentErrorsDifferentCosts" id="toc-sec-differentErrorsDifferentCosts" class="nav-link" data-scroll-target="#sec-differentErrorsDifferentCosts"><span class="header-section-number">17.6.5</span> Different Kinds of Errors Have Different Costs</a></li>
  <li>
<a href="#sec-sensitivitySpecificityPPVnpv" id="toc-sec-sensitivitySpecificityPPVnpv" class="nav-link" data-scroll-target="#sec-sensitivitySpecificityPPVnpv"><span class="header-section-number">17.6.6</span> Sensitivity, Specificity, PPV, and NPV</a>
  <ul class="collapse">
<li><a href="#sec-accuracyCutoff" id="toc-sec-accuracyCutoff" class="nav-link" data-scroll-target="#sec-accuracyCutoff"><span class="header-section-number">17.6.6.1</span> Some Accuracy Estimates Depend on the Cutoff</a></li>
  </ul>
</li>
  <li>
<a href="#sec-sdt" id="toc-sec-sdt" class="nav-link" data-scroll-target="#sec-sdt"><span class="header-section-number">17.6.7</span> Signal Detection Theory</a>
  <ul class="collapse">
<li><a href="#sec-roc" id="toc-sec-roc" class="nav-link" data-scroll-target="#sec-roc"><span class="header-section-number">17.6.7.1</span> Receiver Operating Characteristic (ROC) Curve</a></li>
  </ul>
</li>
  <li>
<a href="#sec-accuracyIndicesCategorical" id="toc-sec-accuracyIndicesCategorical" class="nav-link" data-scroll-target="#sec-accuracyIndicesCategorical"><span class="header-section-number">17.6.8</span> Accuracy Indices</a>
  <ul class="collapse">
<li><a href="#sec-confusionMatrix" id="toc-sec-confusionMatrix" class="nav-link" data-scroll-target="#sec-confusionMatrix"><span class="header-section-number">17.6.8.1</span> Confusion Matrix aka 2x2 Accuracy Table aka Cross-Tabulation aka Contingency Table</a></li>
  <li><a href="#sec-truePositive" id="toc-sec-truePositive" class="nav-link" data-scroll-target="#sec-truePositive"><span class="header-section-number">17.6.8.2</span> True Positives (TP)</a></li>
  <li><a href="#sec-trueNegative" id="toc-sec-trueNegative" class="nav-link" data-scroll-target="#sec-trueNegative"><span class="header-section-number">17.6.8.3</span> True Negatives (TN)</a></li>
  <li><a href="#sec-falsePositive" id="toc-sec-falsePositive" class="nav-link" data-scroll-target="#sec-falsePositive"><span class="header-section-number">17.6.8.4</span> False Positives (FP)</a></li>
  <li><a href="#sec-falseNegative" id="toc-sec-falseNegative" class="nav-link" data-scroll-target="#sec-falseNegative"><span class="header-section-number">17.6.8.5</span> False Negatives (FN)</a></li>
  <li><a href="#sec-selectionRatio" id="toc-sec-selectionRatio" class="nav-link" data-scroll-target="#sec-selectionRatio"><span class="header-section-number">17.6.8.6</span> Selection Ratio (SR)</a></li>
  <li><a href="#sec-pretestProbability" id="toc-sec-pretestProbability" class="nav-link" data-scroll-target="#sec-pretestProbability"><span class="header-section-number">17.6.8.7</span> Base Rate (BR)</a></li>
  <li><a href="#sec-pretestOdds" id="toc-sec-pretestOdds" class="nav-link" data-scroll-target="#sec-pretestOdds"><span class="header-section-number">17.6.8.8</span> Pretest Odds</a></li>
  <li><a href="#sec-percentAccuracy" id="toc-sec-percentAccuracy" class="nav-link" data-scroll-target="#sec-percentAccuracy"><span class="header-section-number">17.6.8.9</span> Percent Accuracy</a></li>
  <li><a href="#sec-percentAccuracyByChance" id="toc-sec-percentAccuracyByChance" class="nav-link" data-scroll-target="#sec-percentAccuracyByChance"><span class="header-section-number">17.6.8.10</span> Percent Accuracy by Chance</a></li>
  <li><a href="#sec-percentAccuracyPredictingFromBaseRate" id="toc-sec-percentAccuracyPredictingFromBaseRate" class="nav-link" data-scroll-target="#sec-percentAccuracyPredictingFromBaseRate"><span class="header-section-number">17.6.8.11</span> Percent Accuracy Predicting from the Base Rate</a></li>
  <li><a href="#sec-relativeImprovementOverChance" id="toc-sec-relativeImprovementOverChance" class="nav-link" data-scroll-target="#sec-relativeImprovementOverChance"><span class="header-section-number">17.6.8.12</span> Relative Improvement Over Chance (RIOC)</a></li>
  <li><a href="#sec-relativeImprovementOverPredictingFromBaseRate" id="toc-sec-relativeImprovementOverPredictingFromBaseRate" class="nav-link" data-scroll-target="#sec-relativeImprovementOverPredictingFromBaseRate"><span class="header-section-number">17.6.8.13</span> Relative Improvement Over Predicting from the Base Rate</a></li>
  <li><a href="#sec-sensitivity" id="toc-sec-sensitivity" class="nav-link" data-scroll-target="#sec-sensitivity"><span class="header-section-number">17.6.8.14</span> Sensitivity (SN)</a></li>
  <li><a href="#sec-specificity" id="toc-sec-specificity" class="nav-link" data-scroll-target="#sec-specificity"><span class="header-section-number">17.6.8.15</span> Specificity (SP)</a></li>
  <li><a href="#sec-falseNegativeRate" id="toc-sec-falseNegativeRate" class="nav-link" data-scroll-target="#sec-falseNegativeRate"><span class="header-section-number">17.6.8.16</span> False Negative Rate (FNR)</a></li>
  <li><a href="#sec-falsePositiveRate" id="toc-sec-falsePositiveRate" class="nav-link" data-scroll-target="#sec-falsePositiveRate"><span class="header-section-number">17.6.8.17</span> False Positive Rate (FPR)</a></li>
  <li><a href="#sec-ppv" id="toc-sec-ppv" class="nav-link" data-scroll-target="#sec-ppv"><span class="header-section-number">17.6.8.18</span> Positive Predictive Value (PPV)</a></li>
  <li><a href="#sec-npv" id="toc-sec-npv" class="nav-link" data-scroll-target="#sec-npv"><span class="header-section-number">17.6.8.19</span> Negative Predictive Value (NPV)</a></li>
  <li><a href="#sec-falseDiscoveryRate" id="toc-sec-falseDiscoveryRate" class="nav-link" data-scroll-target="#sec-falseDiscoveryRate"><span class="header-section-number">17.6.8.20</span> False Discovery Rate (FDR)</a></li>
  <li><a href="#sec-falseOmissionRate" id="toc-sec-falseOmissionRate" class="nav-link" data-scroll-target="#sec-falseOmissionRate"><span class="header-section-number">17.6.8.21</span> False Omission Rate (FOR)</a></li>
  <li><a href="#sec-youdenJ-example" id="toc-sec-youdenJ-example" class="nav-link" data-scroll-target="#sec-youdenJ-example"><span class="header-section-number">17.6.8.22</span> Youden’s J Statistic</a></li>
  <li><a href="#sec-balancedAccuracy" id="toc-sec-balancedAccuracy" class="nav-link" data-scroll-target="#sec-balancedAccuracy"><span class="header-section-number">17.6.8.23</span> Balanced Accuracy</a></li>
  <li><a href="#sec-fScore" id="toc-sec-fScore" class="nav-link" data-scroll-target="#sec-fScore"><span class="header-section-number">17.6.8.24</span> F-Score</a></li>
  <li><a href="#sec-matthewsCorrelationCoefficient" id="toc-sec-matthewsCorrelationCoefficient" class="nav-link" data-scroll-target="#sec-matthewsCorrelationCoefficient"><span class="header-section-number">17.6.8.25</span> Matthews Correlation Coefficient (MCC)</a></li>
  <li><a href="#sec-diagnosticOddsRatio" id="toc-sec-diagnosticOddsRatio" class="nav-link" data-scroll-target="#sec-diagnosticOddsRatio"><span class="header-section-number">17.6.8.26</span> Diagnostic Odds Ratio</a></li>
  <li><a href="#sec-diagnosticLikelihoodRatio2" id="toc-sec-diagnosticLikelihoodRatio2" class="nav-link" data-scroll-target="#sec-diagnosticLikelihoodRatio2"><span class="header-section-number">17.6.8.27</span> Diagnostic Likelihood Ratio</a></li>
  <li><a href="#sec-posttestOdds" id="toc-sec-posttestOdds" class="nav-link" data-scroll-target="#sec-posttestOdds"><span class="header-section-number">17.6.8.28</span> Posttest Odds</a></li>
  <li><a href="#sec-posttestProbability" id="toc-sec-posttestProbability" class="nav-link" data-scroll-target="#sec-posttestProbability"><span class="header-section-number">17.6.8.29</span> Posttest Probability</a></li>
  <li><a href="#sec-miscalibration" id="toc-sec-miscalibration" class="nav-link" data-scroll-target="#sec-miscalibration"><span class="header-section-number">17.6.8.30</span> Mean Difference Between Predicted and Observed Values</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#sec-thresholdIndependentAccuracy" id="toc-sec-thresholdIndependentAccuracy" class="nav-link" data-scroll-target="#sec-thresholdIndependentAccuracy"><span class="header-section-number">17.7</span> Threshold-Independent Accuracy Indices</a>
  <ul>
<li>
<a href="#sec-generalPredictionAccuracy" id="toc-sec-generalPredictionAccuracy" class="nav-link" data-scroll-target="#sec-generalPredictionAccuracy"><span class="header-section-number">17.7.1</span> General Prediction Accuracy</a>
  <ul class="collapse">
<li><a href="#sec-scaleDependentAccuracy" id="toc-sec-scaleDependentAccuracy" class="nav-link" data-scroll-target="#sec-scaleDependentAccuracy"><span class="header-section-number">17.7.1.1</span> Scale-Dependent Accuracy Estimates</a></li>
  <li><a href="#sec-scaleIndependentAccuracy" id="toc-sec-scaleIndependentAccuracy" class="nav-link" data-scroll-target="#sec-scaleIndependentAccuracy"><span class="header-section-number">17.7.1.2</span> Scale-Independent Accuracy Estimates</a></li>
  </ul>
</li>
  <li>
<a href="#sec-discriminationIndices" id="toc-sec-discriminationIndices" class="nav-link" data-scroll-target="#sec-discriminationIndices"><span class="header-section-number">17.7.2</span> Discrimination</a>
  <ul class="collapse">
<li><a href="#sec-aucROC" id="toc-sec-aucROC" class="nav-link" data-scroll-target="#sec-aucROC"><span class="header-section-number">17.7.2.1</span> Area under the ROC curve (AUC)</a></li>
  <li><a href="#sec-standardizedRegressionCoefficient" id="toc-sec-standardizedRegressionCoefficient" class="nav-link" data-scroll-target="#sec-standardizedRegressionCoefficient"><span class="header-section-number">17.7.2.2</span> Effect Size (<span class="math inline">\(\beta\)</span>) of Regression</a></li>
  </ul>
</li>
  <li>
<a href="#sec-calibrationIndices" id="toc-sec-calibrationIndices" class="nav-link" data-scroll-target="#sec-calibrationIndices"><span class="header-section-number">17.7.3</span> Calibration</a>
  <ul class="collapse">
<li><a href="#sec-calibrationPlot" id="toc-sec-calibrationPlot" class="nav-link" data-scroll-target="#sec-calibrationPlot"><span class="header-section-number">17.7.3.1</span> Calibration Plot</a></li>
  <li><a href="#sec-spiegelhalterZ" id="toc-sec-spiegelhalterZ" class="nav-link" data-scroll-target="#sec-spiegelhalterZ"><span class="header-section-number">17.7.3.2</span> Spiegelhalter’s <em>z</em></a></li>
  <li><a href="#sec-calibrationContinuous" id="toc-sec-calibrationContinuous" class="nav-link" data-scroll-target="#sec-calibrationContinuous"><span class="header-section-number">17.7.3.3</span> Calibration for predicting a continuous outcome</a></li>
  </ul>
</li>
  </ul>
</li>
  <li><a href="#sec-integratingAccuracy" id="toc-sec-integratingAccuracy" class="nav-link" data-scroll-target="#sec-integratingAccuracy"><span class="header-section-number">17.8</span> Integrating the Accuracy Indices</a></li>
  <li><a href="#sec-theoryVsEmpiricism" id="toc-sec-theoryVsEmpiricism" class="nav-link" data-scroll-target="#sec-theoryVsEmpiricism"><span class="header-section-number">17.9</span> Theory Versus Empiricism</a></li>
  <li><a href="#sec-testBias" id="toc-sec-testBias" class="nav-link" data-scroll-target="#sec-testBias"><span class="header-section-number">17.10</span> Test Bias</a></li>
  <li><a href="#sec-waysToImprovePredictionAccuracy" id="toc-sec-waysToImprovePredictionAccuracy" class="nav-link" data-scroll-target="#sec-waysToImprovePredictionAccuracy"><span class="header-section-number">17.11</span> Ways to Improve Prediction Accuracy</a></li>
  <li><a href="#sec-predictionAccuracyConclusion" id="toc-sec-predictionAccuracyConclusion" class="nav-link" data-scroll-target="#sec-predictionAccuracyConclusion"><span class="header-section-number">17.12</span> Conclusion</a></li>
  <li><a href="#sec-predictionAccuracySessionInfo" id="toc-sec-predictionAccuracySessionInfo" class="nav-link" data-scroll-target="#sec-predictionAccuracySessionInfo"><span class="header-section-number">17.13</span> Session Info</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/edit/main/evaluating-prediction-accuracy.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/blob/main/evaluating-prediction-accuracy.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><div class="alert alert-info hints-alert">
  <div class="hints-icon">
    <i class="fa fa-comments-o"></i>
  </div>
  <div class="hints-container">
    <b>I need your help!</b>
    <p>
    I want your feedback to make the book better for you and other readers. If you find typos, errors, or places where the text may be improved, please let me know.
    The best ways to provide feedback are by <a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook">GitHub</a> or <a href="https://hypothes.is">hypothes.is</a> annotations.
    </p>

    <p>
      <i class="fa fa-github"></i>
      <a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/issues">Opening an issue</a> or <a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/pulls">submitting a pull request</a> on GitHub: <a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook">https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook</a>
    </p>
    <p>
      <img style="display: inline;" src="https://hypothes.is/organizations/__default__/logo" alt="Hypothesis">
      Adding an annotation using <a href="https://hypothes.is">hypothes.is</a>.
      To add an annotation, <span style="background-color: #3297FD; color: white">select some text</span> and then click the
      <span class="svg-icon--inline"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewbox="0 0 16 16" class="annotator-adder-actions__icon"><path fill="currentColor" fill-rule="nonzero" d="M15 0c.27 0 .505.099.703.297A.961.961 0 0116 1v15l-4-3H1a.974.974 0 01-.703-.29A.953.953 0 010 12V1C0 .719.096.482.29.29A.966.966 0 011 0h14zM7 3l-.469.063c-.312.041-.656.187-1.031.437-.375.25-.719.646-1.031 1.188C4.156 5.229 4 6 4 7l.002.063.006.062a.896.896 0 01.008.11l-.002.074-.006.066a1.447 1.447 0 00.43 1.188C4.729 8.854 5.082 9 5.5 9c.417 0 .77-.146 1.063-.438C6.854 8.271 7 7.918 7 7.5c0-.417-.146-.77-.438-1.063A1.447 1.447 0 005.5 6c-.073 0-.146.005-.219.016-.073.01-.14.026-.203.046.177-1.03.542-1.632 1.094-1.804L7 4V3zm5 0l-.469.063c-.312.041-.656.187-1.031.437-.375.25-.719.646-1.031 1.188C9.156 5.229 9 6 9 7l.002.063.006.062a.896.896 0 01.008.11l-.002.074-.006.066a1.447 1.447 0 00.43 1.188c.291.291.645.437 1.062.437.417 0 .77-.146 1.063-.438.291-.291.437-.645.437-1.062 0-.417-.146-.77-.438-1.063A1.447 1.447 0 0010.5 6c-.073 0-.146.005-.219.016-.073.01-.14.026-.203.046.177-1.03.542-1.632 1.094-1.804L12 4V3z"></path></svg></span>
      symbol on the pop-up menu.
      To see the annotations of others, click the
      <span class="svg-icon--inline"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewbox="0 0 16 16" class=""><g fill-rule="evenodd"><rect fill="none" stroke="none" x="0" y="0" width="16" height="16"></rect><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 12L6 8l4-4"></path></g></svg></span>
      symbol in the upper right-hand corner of the page.
    </p>
  </div>
</div>

<header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-predictionAccuracy" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Evaluation of Prediction/Forecasting Accuracy</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><blockquote class="blockquote">
<p>“Nothing ruins fantasy more than reality.” – <a href="https://www.youtube.com/watch?v=gmpLFWs5ae0&amp;t=1430s">Renee Miller</a> <span class="citation" data-cites="Yahoo2024">(<a href="references.html#ref-Yahoo2024" role="doc-biblioref">Yahoo! Sports, 2024</a>)</span></p>
</blockquote>
<section id="sec-predictionAccuracyGettingStarted" class="level2" data-number="17.1"><h2 data-number="17.1" class="anchored" data-anchor-id="sec-predictionAccuracyGettingStarted">
<span class="header-section-number">17.1</span> Getting Started</h2>
<section id="sec-predictionAccuracyLoadPackages" class="level3" data-number="17.1.1"><h3 data-number="17.1.1" class="anchored" data-anchor-id="sec-predictionAccuracyLoadPackages">
<span class="header-section-number">17.1.1</span> Load Packages</h3>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">library</span>(<span class="st">"petersenlab"</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(<span class="st">"tidyverse"</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">library</span>(<span class="st">"pROC"</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="fu">library</span>(<span class="st">"magrittr"</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="fu">library</span>(<span class="st">"viridis"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section></section><section id="sec-predictionAccuracyOverview" class="level2" data-number="17.2"><h2 data-number="17.2" class="anchored" data-anchor-id="sec-predictionAccuracyOverview">
<span class="header-section-number">17.2</span> Overview</h2>
<p>Predictions can come in different types. Some predictions involve categorical data, whereas other predictions involve continuous data. When dealing with a dichotomous (<a href="research-methods.html#sec-nominal">nominal data</a> that are binary) <a href="research-methods.html#sec-correlationalStudy">predictor</a> and <a href="research-methods.html#sec-correlationalStudy">outcome</a> variable (or continuous data that have been dichotomized using a cutoff), we can evaluate predictions using a 2x2 table known as a confusion matrix (see INSERT), or with logistic regression models. When dealing with a continuous <a href="research-methods.html#sec-correlationalStudy">outcome variable</a> (e.g., <a href="research-methods.html#sec-ordinal">ordinal</a>, <a href="research-methods.html#sec-interval">interval</a>, or <a href="research-methods.html#sec-ratio">ratio</a> data), we can evaluate predictions using <a href="multiple-regression.html">multiple regression</a> or similar variants such as structural equation modeling and mixed models.</p>
<p>In fantasy football, we most commonly predict continuous <a href="research-methods.html#sec-correlationalStudy">outcome variables</a> (e.g., fantasy points, rushing yards). Nevertheless, it is also important to understand principles in the prediction of categorical <a href="research-methods.html#sec-correlationalStudy">outcomes variables</a>.</p>
<p>In any domain, it is important to evaluate the accuracy of predictions, so we can know how (in)accurate we are, and we can strive to continually improve our predictions. Fantasy performance—and human behavior more general—is incredibly challenging to predict. Indeed, many things in the world, in particular long-term trends, are unpredictable <span class="citation" data-cites="Kahneman2011">(<a href="references.html#ref-Kahneman2011" role="doc-biblioref">Kahneman, 2011</a>)</span>. In fantasy football, there is considerable luck/chance/randomness. There are relatively few (i.e.&nbsp;17) games, and there is a sizeable injury risk for each player in a given game. These and other factors combine to render fantasy football predictions not highly accurate. Domains with high uncertainty and unpredictability are considered “low-validity environments” <span class="citation" data-cites="Kahneman2011">(<a href="references.html#ref-Kahneman2011" role="doc-biblioref">Kahneman, 2011, p. 223</a>)</span>. But, first, let’s learn about the various ways we can evaluate the accuracy of predictions.</p>
</section><section id="sec-accuracyTypes" class="level2" data-number="17.3"><h2 data-number="17.3" class="anchored" data-anchor-id="sec-accuracyTypes">
<span class="header-section-number">17.3</span> Types of Accuracy</h2>
<p>There are two primary dimensions of accuracy: (1) <a href="#sec-discrimination">discrimination</a> and (2) <a href="#sec-calibration">calibration</a>. <a href="#sec-discrimination">Discrimination</a> and <a href="#sec-calibration">calibration</a> are distinct forms of accuracy. Just because predictions are high in one form of accuracy does not mean that they will be high in the other form of accuracy. As described by <span class="citation" data-cites="Lindhiem2020">Lindhiem et al. (<a href="references.html#ref-Lindhiem2020" role="doc-biblioref">2020</a>)</span>, predictions can follow any of the following configurations (and anywhere in between):</p>
<ul>
<li>high <a href="#sec-discrimination">discrimination</a>, high <a href="#sec-calibration">calibration</a>
</li>
<li>high <a href="#sec-discrimination">discrimination</a>, low <a href="#sec-calibration">calibration</a>
</li>
<li>low <a href="#sec-discrimination">discrimination</a>, high <a href="#sec-calibration">calibration</a>
</li>
<li>low <a href="#sec-discrimination">discrimination</a>, low <a href="#sec-calibration">calibration</a>
</li>
</ul>
<p>Some general indexes of accuracy combine discrimination and calibration, as described in <a href="#sec-generalAccuracy" class="quarto-xref"><span>Section 17.3.3</span></a>.</p>
<p>In addition, accuracy indices can be <a href="#sec-thresholdDependentAccuracy">threshold-dependent</a> or <a href="#sec-thresholdIndependentAccuracy">-independent</a> and can be scale-dependent or -independent. <a href="#sec-thresholdDependentAccuracy">Threshold-dependent accuracy indices</a> differ based on the cutoff (i.e., threshold), whereas <a href="#sec-thresholdIndependentAccuracy">threshold-independent accuracy indices</a> do not. Thus, raising or lowering the cutoff will change <a href="#sec-thresholdDependentAccuracy">threshold-dependent</a> accuracy indices. Scale-dependent accuracy indices depend on the metric/scale of the data, whereas scale-independent accuracy indices do not. Thus, scale-dependent accuracy indices cannot be directly compared when using measures of differing scales, whereas scale-independent accuracy indices can be compared across data of differing scales.</p>
<section id="sec-discrimination" class="level3" data-number="17.3.1"><h3 data-number="17.3.1" class="anchored" data-anchor-id="sec-discrimination">
<span class="header-section-number">17.3.1</span> Discrimination</h3>
<p>When dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. Aspects of discrimination at a particular cutoff (e.g., sensitivity, specificity, area under the ROC curve) are described in INSERT.</p>
</section><section id="sec-calibration" class="level3" data-number="17.3.2"><h3 data-number="17.3.2" class="anchored" data-anchor-id="sec-calibration">
<span class="header-section-number">17.3.2</span> Calibration</h3>
<p>When dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to discrimination) is described by <span class="citation" data-cites="Lindhiem2020">Lindhiem et al. (<a href="references.html#ref-Lindhiem2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>Calibration is relevant to all kinds of predictions, including weather forecasts. For instance, on the days that the meteorologist says there is a 60% chance of rain, it should rain about 60% of the time. Calibration is also important for fantasy football predictions. When projections state that a group of players is each expected to score 200 points, their projections would be miscalibrated if those players scored only 150 points on average.</p>
<p>There are four general patterns of miscalibration: overextremity, underextremity, overprediction, and underprediction (see <a href="#fig-miscalibration" class="quarto-xref">Figure&nbsp;<span>17.7</span></a>). <em>Overextremity</em> exists when the predicted probabilites are too close to the extremes (zero or one). <em>Underextremity</em> exists when the predicted probabilities are too far away from the extremes. <em>Overprediction</em> exists when the predicted probabilities are consistently greater than the observed probabilities. <em>Underprediction</em> exists when the predicted probabilities are consistently less than the observed probabilities. For a more thorough description of these types of miscalibration, see <span class="citation" data-cites="Lindhiem2020">Lindhiem et al. (<a href="references.html#ref-Lindhiem2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>Indices for evaluating calibration are described in <a href="#sec-calibrationIndices" class="quarto-xref"><span>Section 17.7.3</span></a>.</p>
</section><section id="sec-generalAccuracy" class="level3" data-number="17.3.3"><h3 data-number="17.3.3" class="anchored" data-anchor-id="sec-generalAccuracy">
<span class="header-section-number">17.3.3</span> General Accuracy</h3>
<p>General accuracy indices combine estimates of <a href="#sec-discrimination">discrimination</a> and <a href="#sec-calibration">calibration</a>.</p>
</section></section><section id="sec-predictionCategorical" class="level2" data-number="17.4"><h2 data-number="17.4" class="anchored" data-anchor-id="sec-predictionCategorical">
<span class="header-section-number">17.4</span> Prediction of Categorical Outcomes</h2>
<p>To evaluate the accuracy of our predictions for categorical outcome variables (e.g., binary, dichotomous, or <a href="research-methods.html#sec-nominal">nominal</a> data), we can use either <a href="#sec-thresholdDependentAccuracy">threshold-dependent</a> or <a href="#sec-thresholdIndependentAccuracy">threshold-independent</a> accuracy indices.</p>
</section><section id="sec-predictionContinuous" class="level2" data-number="17.5"><h2 data-number="17.5" class="anchored" data-anchor-id="sec-predictionContinuous">
<span class="header-section-number">17.5</span> Prediction of Continuous Outcomes</h2>
<p>To evaluate the accuracy of our predictions for continuous outcome variables (e.g., <a href="research-methods.html#sec-ordinal">ordinal</a>, <a href="research-methods.html#sec-interval">interval</a>, or <a href="research-methods.html#sec-ratio">ratio</a> data), the outcome variable does not have cutoffs, so we would use <a href="#sec-thresholdIndependentAccuracy">threshold-independent accuracy indices</a>.</p>
</section><section id="sec-thresholdDependentAccuracy" class="level2" data-number="17.6"><h2 data-number="17.6" class="anchored" data-anchor-id="sec-thresholdDependentAccuracy">
<span class="header-section-number">17.6</span> Threshold-Dependent Accuracy Indices</h2>
<section id="sec-decisionOutcomes" class="level3" data-number="17.6.1"><h3 data-number="17.6.1" class="anchored" data-anchor-id="sec-decisionOutcomes">
<span class="header-section-number">17.6.1</span> Decision Outcomes</h3>
<p>To consider how we can evaluate the accuracy of predictions for a categorical outcome, consider an example adapted from <span class="citation" data-cites="Meehl1955">Meehl &amp; Rosen (<a href="references.html#ref-Meehl1955" role="doc-biblioref">1955</a>)</span>. The military conducts a test of its prospective members to screen out applicants who would likely fail basic training. To evaluate the accuracy of our predictions using the test, we can examine a confusion matrix. A confusion matrix is a matrix that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (decision outcomes): true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).</p>
<p>When discussing the four decision outcomes, “true” means an accurate judgment, whereas “false” means an inaccurate judgment. “Positive” means that the judgment was that the person has the characteristic of interest, whereas “negative” means that the judgment was that the person does not have the characteristic of interest. A <em>true positive</em> is a correct judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually have (or will have) the characteristic. A <em>true negative</em> is a correct judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A <em>false positive</em> is an incorrect judgment (or prediction) where the judgment was that the person has (or will have) the characteristic of interest, and, in truth, they actually do not have (or will not have) the characteristic. A <em>false negative</em> is an incorrect judgment (or prediction) where the judgment was that the person does not have (or will not have) the characteristic of interest, and, in truth, they actually do have (or will have) the characteristic.</p>
<p>An example of a confusion matrix is in INSERT.</p>
<p>With the information in the confusion matrix, we can calculate the marginal sums and the proportion of people in each cell (in parentheses), as depicted in INSERT.</p>
<p>That is, we can sum across the rows and columns to identify how many people actually showed poor adjustment (<span class="math inline">\(n = 100\)</span>) versus good adjustment (<span class="math inline">\(n = 1,900\)</span>), and how many people were selected to reject (<span class="math inline">\(n = 508\)</span>) versus retain (<span class="math inline">\(n = 1,492\)</span>). If we sum the column of predicted marginal sums (<span class="math inline">\(508 + 1,492\)</span>) or the row of actual marginal sums (<span class="math inline">\(100 + 1,900\)</span>), we get the total number of people (<span class="math inline">\(N = 2,000\)</span>).</p>
<p>Based on the marginal sums, we can compute the <a href="base-rates.html#sec-baseRate">marginal probabilities</a>, as depicted in INSERT.</p>
<p>The <a href="base-rates.html#sec-baseRate">marginal probability</a> of the person having the characteristic of interest (i.e., showing poor adjustment) is called the <a href="#baseRate"><em>base rate</em></a> (BR). That is, the <a href="#baseRate">base rate</a> is the proportion of people who have the characteristic. It is calculated by dividing the number of people with poor adjustment (<span class="math inline">\(n = 100\)</span>) by the total number of people (<span class="math inline">\(N = 2,000\)</span>): <span class="math inline">\(BR = \frac{FN + TP}{N}\)</span>. Here, the <a href="#baseRate">base rate</a> reflects the prevalence of poor adjustment. In this case, the <a href="#baseRate">base rate</a> is .05, so there is a 5% chance that an applicant will be poorly adjusted. The <a href="#baseRate">marginal probability</a> of good adjustment is equal to 1 minus the <a href="#baseRate">base rate</a> of poor adjustment.</p>
<p>The <a href="#baseRate">marginal probability</a> of predicting that a person has the characteristic (i.e., rejecting a person) is called the <em>selection ratio</em> (SR). The selection ratio is the proportion of people who will be selected (in this case, rejected rather than retained); i.e., the proportion of people who are identified as having the characteristic. The selection ratio is calculated by dividing the number of people selected to reject (<span class="math inline">\(n = 508\)</span>) by the total number of people (<span class="math inline">\(N = 2,000\)</span>): <span class="math inline">\(SR = \frac{TP + FP}{N}\)</span>. In this case, the selection ratio is .25, so 25% of people are rejected. The <a href="#baseRate">marginal probability</a> of not selecting someone to reject (i.e., the <a href="#baseRate">marginal probability</a> of retaining) is equal to 1 minus the selection ratio.</p>
<p>The selection ratio might be something that the test dictates according to its cutoff score. Or, the selection ratio might be imposed by external factors that place limits on how many people you can assign a positive test value. For instance, when deciding whether to treat a client, the selection ratio may depend on how many therapists are available and how many cases can be treated.</p>
</section><section id="sec-percentAccuracyOverview" class="level3" data-number="17.6.2"><h3 data-number="17.6.2" class="anchored" data-anchor-id="sec-percentAccuracyOverview">
<span class="header-section-number">17.6.2</span> Percent Accuracy</h3>
<p>Based on the confusion matrix, we can calculate the prediction accuracy based on the percent accuracy of the predictions. The percent accuracy is the number of correct predictions divided by the total number of predictions, and multiplied by 100. In the context of a confusion matrix, this is calculated as: <span class="math inline">\(100\% \times \frac{\text{TP} + \text{TN}}{N}\)</span>. In this case, our percent accuracy was 78%—that is, 78% of our predictions were accurate, and 22% of our predictions were inaccurate.</p>
</section><section id="sec-accuracyByChance" class="level3" data-number="17.6.3"><h3 data-number="17.6.3" class="anchored" data-anchor-id="sec-accuracyByChance">
<span class="header-section-number">17.6.3</span> Percent Accuracy by Chance</h3>
<p>78% sounds pretty accurate. And it is much higher than 50%, so we are doing a pretty good job, right? Well, it is important to compare our accuracy to what accuracy we would expect to get by chance alone, if predictions were made by a random process rather than using a test’s scores. Our selection ratio was 25.4%. How accurate would we be if we randomly selected 25.4% of people to reject? To determine what accuracy we could get by chance alone given the selection ratio and the base rate, we can calculate the chance probability of true positives and the chance probability of true negatives. The probability of a given cell in the confusion matrix is a <a href="base-rates.html#sec-jointProbability">joint probability</a>—the probability of two events occurring simultaneously. To calculate a <a href="base-rates.html#sec-jointProbability">joint probability</a>, we multiply the probability of each event.</p>
<p>So, to get the chance expectancies of true positives, we would multiply the respective <a href="#baseRate">marginal probabilities</a>, as in <a href="#eq-truePositivesByChanceExample" class="quarto-xref">Equation&nbsp;<span>17.1</span></a>:</p>
<p><span id="eq-truePositivesByChanceExample"><span class="math display">\[
\begin{aligned}
  P(TP) &amp;= P(\text{Poor adjustment}) \times P(\text{Reject})\\
   &amp;= BR \times SR \\
  &amp;= .05 \times .254 \\
  &amp;= .0127
\end{aligned}
\tag{17.1}\]</span></span></p>
<p>To get the chance expectancies of true negatives, we would multiply the respective <a href="base-rates.html#sec-baseRate">marginal probabilities</a>, as in <a href="#eq-trueNegativesByChanceExample" class="quarto-xref">Equation&nbsp;<span>17.2</span></a>:</p>
<p><span id="eq-trueNegativesByChanceExample"><span class="math display">\[
\begin{aligned}
  P(TN) &amp;= P(\text{Good adjustment}) \times P(\text{Retain})\\
   &amp;= (1 - BR) \times (1 - SR) \\
  &amp;= .95 \times .746 \\
  &amp;= .7087
\end{aligned}
\tag{17.2}\]</span></span></p>
<p>To get the percent accuracy by chance, we sum the chance expectancies for the correct predictions (TP and TN): <span class="math inline">\(.0127 + .7087 = .7214\)</span>. Thus, the percent accuracy you can get by chance alone is 72%. This is because most of our predictions are to retain people, and the <a href="base-rates.html#sec-baseRate">base rate</a> of poor adjustment is quite low (.05). Our measure with 78% accuracy provides only a 6% increment in correct predictions. Thus, you cannot judge how good your judgment or prediction is until you know how you would do by random chance.</p>
<p>The chance expectancies for each cell of the confusion matrix are in INSERT</p>
</section><section id="sec-predictingFromBaseRate" class="level3" data-number="17.6.4"><h3 data-number="17.6.4" class="anchored" data-anchor-id="sec-predictingFromBaseRate">
<span class="header-section-number">17.6.4</span> Predicting from the Base Rate</h3>
<p>Now, let us consider how well you would do if you were to predict from the <a href="base-rates.html#sec-baseRate">base rate</a>. Predicting from the <a href="base-rates.html#sec-baseRate">base rate</a> is also called “betting from the <a href="base-rates.html#sec-baseRate">base rate</a>”, and it involves setting the selection ratio by taking advantage of the <a href="base-rates.html#sec-baseRate">base rate</a> so that you go with the most likely outcome in every prediction. Because the <a href="base-rates.html#sec-baseRate">base rate</a> is quite low (.05), we could predict from the <a href="base-rates.html#sec-baseRate">base rate</a> by selecting no one to reject (i.e., setting the selection ratio at zero). Our percent accuracy by chance if we predict from the <a href="base-rates.html#sec-baseRate">base rate</a> would be calculated by multiplying the <a href="base-rates.html#sec-baseRate">marginal probabilities</a>, as we did above, but with a new selection ratio, as in <a href="#eq-predictingFromBaseRateExample" class="quarto-xref">Equation&nbsp;<span>17.3</span></a>:</p>
<p><span id="eq-predictingFromBaseRateExample"><span class="math display">\[
\begin{aligned}
  P(TP) &amp;= P(\text{Poor adjustment}) \times P(\text{Reject})\\
   &amp;= BR \times SR \\
  &amp;= .05 \times 0 \\
  &amp;= 0 \\ \\
  P(TN) &amp;= P(\text{Good adjustment}) \times P(\text{Retain})\\
   &amp;= (1 - BR) \times (1 - SR) \\
  &amp;= .95 \times 1 \\
  &amp;= .95
\end{aligned}
\tag{17.3}\]</span></span></p>
<p>We sum the chance expectancies for the correct predictions (TP and TN): <span class="math inline">\(0 + .95 = .95\)</span>. Thus, our percent accuracy by predicting from the <a href="base-rates.html#sec-baseRate">base rate</a> is 95%. This is damning to our measure because it is a much higher accuracy than the accuracy of our measure. That is, we can be much more accurate than our measure simply by predicting from the <a href="base-rates.html#sec-baseRate">base rate</a> and selecting no one to reject.</p>
<p>Going with the most likely outcome in every prediction (predicting from the <a href="base-rates.html#sec-baseRate">base rate</a>) can be highly accurate (in terms of percent accuracy) as noted by <span class="citation" data-cites="Meehl1955">Meehl &amp; Rosen (<a href="references.html#ref-Meehl1955" role="doc-biblioref">1955</a>)</span>, especially when the <a href="base-rates.html#sec-baseRate">base rate</a> is very low or very high. This should serve as an important reminder that we need to compare the accuracy of our measures to the accuracy by (1) random chance and (2) predicting from the <a href="base-rates.html#sec-baseRate">base rate</a>. There are several important implications of the impact of <a href="base-rates.html#sec-baseRate">base rates</a> on prediction accuracy. One implication is that using the same test in different settings with different <a href="base-rates.html#sec-baseRate">base rates</a> will markedly change the accuracy of the test. Oftentimes, using a test will actually <em>decrease</em> the predictive accuracy when the <a href="base-rates.html#sec-baseRate">base rate</a> deviates greatly from .50. But percent accuracy is not everything. Percent accuracy treats different kinds of errors as if they are equally important. However, the value we place on different kinds of errors may be different, as described next.</p>
</section><section id="sec-differentErrorsDifferentCosts" class="level3" data-number="17.6.5"><h3 data-number="17.6.5" class="anchored" data-anchor-id="sec-differentErrorsDifferentCosts">
<span class="header-section-number">17.6.5</span> Different Kinds of Errors Have Different Costs</h3>
<p>Some errors have a high cost, and some errors have a low cost. Among the four decision outcomes, there are two types of errors: false positives and false negatives. The extent to which false positives and false negatives are costly depends on the prediction problem. So, even though you can often be most accurate by going with the <a href="base-rates.html#sec-baseRate">base rate</a>, it may be advantageous to use a screening instrument despite lower overall accuracy because of the huge difference in costs of false positives versus false negatives in some cases.</p>
<p>Consider the example of a screening instrument for HIV. False positives would be cases where we said that someone is at high risk of HIV when they are not, whereas false negatives are cases where we said that someone is not at high risk when they actually are. The costs of false positives include a shortage of blood, some follow-up testing, and potentially some anxiety, but that is about it. The costs of false negatives may be people getting HIV. In this case, the costs of false negatives greatly outweigh the costs of false positives, so we use a screening instrument to try to identify the cases at high risk for HIV because of the important consequences of failing to do so, even though using the screening instrument will lower our overall accuracy level.</p>
<p>Another example is when the Central Intelligence Agency (CIA) used a screen for protective typists during wartime to try to detect spies. False positives would be cases where the CIA believes that a person is a spy when they are not, and the CIA does not hire them. False negatives would be cases where the CIA believes that a person is not a spy when they actually are, and the CIA hires them. In this case, a false positive would be fine, but a false negative would be really bad.</p>
<p>How you weigh the costs of different errors depends considerably on the domain and context. Possible costs of false positives to society include: unnecessary and costly treatment with side effects and sending an innocent person to jail (despite our presumption of innocence in the United States criminal justice system that a person is innocent until proven guilty). Possible costs of false negatives to society include: setting a guilty person free, failing to detect a bomb or tumor, and preventing someone from getting treatment who needs it.</p>
<p>The differential costs of different errors also depend on how much flexibility you have in the selection ratio in being able to set a stringent versus loose selection ratio. Consider if there is a high cost of getting rid of people during the selection process. For example, if you must hire 100 people and only 100 people apply for the position, you cannot lose people, so you need to hire even high-risk people. However, if you do not need to hire many people, then you can hire more conservatively.</p>
<p>Any time the selection ratio differs from the <a href="base-rates.html#sec-baseRate">base rate</a>, you will make errors. For example, if you reject 25% of applicants, and the <a href="base-rates.html#sec-baseRate">base rate</a> of poor adjustment is 5%, then you are making errors of over-rejecting (false positives). By contrast, if you reject 1% of applicants and the <a href="base-rates.html#sec-baseRate">base rate</a> of poor adjustment is 5%, then you are making errors of under-rejecting or over-accepting (false negatives).</p>
<p>A low <a href="base-rates.html#sec-baseRate">base rate</a> makes it harder to make predictions, and tends to lead to less accurate predictions. For instance, it is very challenging to predict low <a href="base-rates.html#sec-baseRate">base rate</a> behaviors, including suicide <span class="citation" data-cites="Kessler2020">(<a href="references.html#ref-Kessler2020" role="doc-biblioref">Kessler et al., 2020</a>)</span>. For this reason, it is likely much more challenging to predict touchdowns—which happen relatively less often—than it is to predict passing/rushing/receiving yards—which are more frequent and continuously distributed.</p>
<p>[EVALUATE EMPIRICALLY]</p>
</section><section id="sec-sensitivitySpecificityPPVnpv" class="level3" data-number="17.6.6"><h3 data-number="17.6.6" class="anchored" data-anchor-id="sec-sensitivitySpecificityPPVnpv">
<span class="header-section-number">17.6.6</span> Sensitivity, Specificity, PPV, and NPV</h3>
<p>As described earlier, percent accuracy is not the only important aspect of accuracy. Percent accuracy can be misleading because it is highly influenced by <a href="base-rates.html#sec-baseRate">base rates</a>. You can have a high percent accuracy by predicting from the base rate and saying that no one has the condition (if the <a href="base-rates.html#sec-baseRate">base rate</a> is low) or that everyone has the condition (if the <a href="base-rates.html#sec-baseRate">base rate</a> is high). Thus, it is also important to consider other aspects of accuracy, including sensitivity (SN), specificity (SP), positive predictive value (PPV), and negative predictive value (NPV). We want our predictions to be sensitive to be able to detect the characteristic but also to be specific so that we classify only people actually with the characteristic as having the characteristic.</p>
<p>Let us return to the confusion matrix in INSERT. If we know the frequency of each of the four predicted-actual combinations of the confusion matrix (TP, TN, FP, FN), we can calculate sensitivity, specificity, PPV, and NPV.</p>
<p>Sensitivity is the proportion of those with the characteristic (<span class="math inline">\(\text{TP} + \text{FN}\)</span>) that we identified with our measure (<span class="math inline">\(\text{TP}\)</span>): <span class="math inline">\(\frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{86}{86 + 14} = .86\)</span>. Specificity is the proportion of those who do not have the characteristic (<span class="math inline">\(\text{TN} + \text{FP}\)</span>) that we correctly classify as not having the characteristic (<span class="math inline">\(\text{TN}\)</span>): <span class="math inline">\(\frac{\text{TN}}{\text{TN} + \text{FP}} = \frac{1,478}{1,478 + 422} = .78\)</span>. PPV is the proportion of those who we classify as having the characteristic (<span class="math inline">\(\text{TP} + \text{FP}\)</span>) who actually have the characteristic (<span class="math inline">\(\text{TP}\)</span>): <span class="math inline">\(\frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{86}{86 + 422} = .17\)</span>. NPV is the proportion of those we classify as not having the characteristic (<span class="math inline">\(\text{TN} + \text{FN}\)</span>) who actually do not have the characteristic (<span class="math inline">\(\text{TN}\)</span>): <span class="math inline">\(\frac{\text{TN}}{\text{TN} + \text{FN}} = \frac{1,478}{1,478 + 14} = .99\)</span>.</p>
<p>Sensitivity, specificity, PPV, and NPV are proportions, and their values therefore range from 0 to 1, where higher values reflect greater accuracy. With sensitivity, specificity, PPV, and NPV, we have a good snapshot of how accurate the measure is at a given cutoff. In our case, our measure is good at finding whom to reject (high sensitivity), but it is rejecting too many people who do not need to be rejected (lower PPV due to many FPs). Most people whom we classify as having the characteristic do not actually have the characteristic. However, the fact that we are over-rejecting could be okay depending on our goals, for instance, if we do not care about over-dropping (i.e., the PPV being low).</p>
<section id="sec-accuracyCutoff" class="level4" data-number="17.6.6.1"><h4 data-number="17.6.6.1" class="anchored" data-anchor-id="sec-accuracyCutoff">
<span class="header-section-number">17.6.6.1</span> Some Accuracy Estimates Depend on the Cutoff</h4>
<p>Sensitivity, specificity, PPV, and NPV differ based on the cutoff (i.e., threshold) for classification. Consider the following example. Aliens visit Earth, and they develop a test to determine whether a berry is edible or inedible.</p>
<p><a href="#fig-classificationDistributions" class="quarto-xref">Figure&nbsp;<span>17.1</span></a> depicts the distributions of scores by berry type. Note how there are clearly two distinct distributions. However, the distributions overlap to some degree. Thus, any cutoff will have at least some inaccurate classifications. The extent of overlap of the distributions reflects the amount of measurement error of the measure with respect to the characteristic of interest.</p>
<div class="cell">
<details class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co">#No Cutoff</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>sampleSize <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>edibleScores <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(sampleSize, <span class="dv">50</span>, <span class="dv">15</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a>inedibleScores <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(sampleSize, <span class="dv">100</span>, <span class="dv">15</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a>edibleData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb2-8"><a href="#cb2-8"></a>  <span class="at">score =</span> <span class="fu">c</span>(</span>
<span id="cb2-9"><a href="#cb2-9"></a>    edibleScores,</span>
<span id="cb2-10"><a href="#cb2-10"></a>    inedibleScores),</span>
<span id="cb2-11"><a href="#cb2-11"></a>  <span class="at">type =</span> <span class="fu">c</span>(</span>
<span id="cb2-12"><a href="#cb2-12"></a>    <span class="fu">rep</span>(<span class="st">"edible"</span>, sampleSize),</span>
<span id="cb2-13"><a href="#cb2-13"></a>    <span class="fu">rep</span>(<span class="st">"inedible"</span>, sampleSize)))</span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a>cutoff <span class="ot">&lt;-</span> <span class="dv">75</span></span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a>hist_edible <span class="ot">&lt;-</span> <span class="fu">density</span>(</span>
<span id="cb2-18"><a href="#cb2-18"></a>  edibleScores,</span>
<span id="cb2-19"><a href="#cb2-19"></a>  <span class="at">from =</span> <span class="dv">0</span>,</span>
<span id="cb2-20"><a href="#cb2-20"></a>  <span class="at">to =</span> <span class="dv">150</span>) <span class="sc">%$%</span> <span class="co"># exposition pipe magrittr::`%$%`</span></span>
<span id="cb2-21"><a href="#cb2-21"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="at">x =</span> x,</span>
<span id="cb2-23"><a href="#cb2-23"></a>    <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>  <span class="fu">mutate</span>(<span class="at">area =</span> x <span class="sc">&gt;=</span> cutoff)</span>
<span id="cb2-25"><a href="#cb2-25"></a></span>
<span id="cb2-26"><a href="#cb2-26"></a>hist_edible<span class="sc">$</span>type[hist_edible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">TRUE</span>] <span class="ot">&lt;-</span> <span class="st">"edible_FP"</span></span>
<span id="cb2-27"><a href="#cb2-27"></a>hist_edible<span class="sc">$</span>type[hist_edible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">FALSE</span>] <span class="ot">&lt;-</span> <span class="st">"edible_TN"</span></span>
<span id="cb2-28"><a href="#cb2-28"></a></span>
<span id="cb2-29"><a href="#cb2-29"></a>hist_inedible <span class="ot">&lt;-</span> <span class="fu">density</span>(</span>
<span id="cb2-30"><a href="#cb2-30"></a>  inedibleScores,</span>
<span id="cb2-31"><a href="#cb2-31"></a>  <span class="at">from =</span> <span class="dv">0</span>,</span>
<span id="cb2-32"><a href="#cb2-32"></a>  <span class="at">to =</span> <span class="dv">150</span>) <span class="sc">%$%</span> <span class="co"># exposition pipe magrittr::`%$%`</span></span>
<span id="cb2-33"><a href="#cb2-33"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb2-34"><a href="#cb2-34"></a>    <span class="at">x =</span> x,</span>
<span id="cb2-35"><a href="#cb2-35"></a>    <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb2-36"><a href="#cb2-36"></a>  <span class="fu">mutate</span>(<span class="at">area =</span> x <span class="sc">&lt;</span> cutoff)</span>
<span id="cb2-37"><a href="#cb2-37"></a></span>
<span id="cb2-38"><a href="#cb2-38"></a>hist_inedible<span class="sc">$</span>type[hist_inedible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">TRUE</span>] <span class="ot">&lt;-</span> <span class="st">"inedible_FN"</span></span>
<span id="cb2-39"><a href="#cb2-39"></a>hist_inedible<span class="sc">$</span>type[hist_inedible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">FALSE</span>] <span class="ot">&lt;-</span> <span class="st">"inedible_TP"</span></span>
<span id="cb2-40"><a href="#cb2-40"></a></span>
<span id="cb2-41"><a href="#cb2-41"></a>density_data <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb2-42"><a href="#cb2-42"></a>  hist_edible,</span>
<span id="cb2-43"><a href="#cb2-43"></a>  hist_inedible)</span>
<span id="cb2-44"><a href="#cb2-44"></a></span>
<span id="cb2-45"><a href="#cb2-45"></a>density_data<span class="sc">$</span>type <span class="ot">&lt;-</span> <span class="fu">factor</span>(</span>
<span id="cb2-46"><a href="#cb2-46"></a>  density_data<span class="sc">$</span>type,</span>
<span id="cb2-47"><a href="#cb2-47"></a>  <span class="at">levels =</span> <span class="fu">c</span>(</span>
<span id="cb2-48"><a href="#cb2-48"></a>    <span class="st">"edible_TN"</span>,</span>
<span id="cb2-49"><a href="#cb2-49"></a>    <span class="st">"inedible_TP"</span>,</span>
<span id="cb2-50"><a href="#cb2-50"></a>    <span class="st">"edible_FP"</span>,</span>
<span id="cb2-51"><a href="#cb2-51"></a>    <span class="st">"inedible_FN"</span>))</span>
<span id="cb2-52"><a href="#cb2-52"></a></span>
<span id="cb2-53"><a href="#cb2-53"></a><span class="fu">ggplot</span>(</span>
<span id="cb2-54"><a href="#cb2-54"></a>  <span class="at">data =</span> edibleData,</span>
<span id="cb2-55"><a href="#cb2-55"></a>  <span class="fu">aes</span>(</span>
<span id="cb2-56"><a href="#cb2-56"></a>    <span class="at">x =</span> score,</span>
<span id="cb2-57"><a href="#cb2-57"></a>    <span class="at">ymin =</span> <span class="dv">0</span>,</span>
<span id="cb2-58"><a href="#cb2-58"></a>    <span class="at">fill =</span> type)) <span class="sc">+</span></span>
<span id="cb2-59"><a href="#cb2-59"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb2-60"><a href="#cb2-60"></a>  <span class="fu">scale_fill_manual</span>(</span>
<span id="cb2-61"><a href="#cb2-61"></a>    <span class="at">name =</span> <span class="st">"Berry Type"</span>,</span>
<span id="cb2-62"><a href="#cb2-62"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb2-63"><a href="#cb2-63"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">2</span>)[<span class="dv">1</span>],</span>
<span id="cb2-64"><a href="#cb2-64"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">2</span>)[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb2-65"><a href="#cb2-65"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">name =</span> <span class="st">"Frequency"</span>) <span class="sc">+</span></span>
<span id="cb2-66"><a href="#cb2-66"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb2-67"><a href="#cb2-67"></a>  <span class="fu">theme</span>(</span>
<span id="cb2-68"><a href="#cb2-68"></a>    <span class="at">axis.text.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb2-69"><a href="#cb2-69"></a>    <span class="at">axis.ticks.y =</span> <span class="fu">element_blank</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-classificationDistributions" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Distribution of Test Scores by Berry Type.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-classificationDistributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="evaluating-prediction-accuracy_files/figure-html/fig-classificationDistributions-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;17.1: Distribution of Test Scores by Berry Type."><img src="evaluating-prediction-accuracy_files/figure-html/fig-classificationDistributions-1.png" class="img-fluid figure-img" alt="Distribution of Test Scores by Berry Type." width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classificationDistributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.1: Distribution of Test Scores by Berry Type.
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-classificationStandardCutoff" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> depicts the distributions of scores by berry type with a cutoff. The red line indicates the cutoff—the level above which berries are classified by the test as inedible. There are errors on each side of the cutoff. Below the cutoff, there are some false negatives (blue): inedible berries that are inaccurately classified as edible. Above the cutoff, there are some false positives (green): edible berries that are inaccurately classified as inedible. Costs of false negatives could include sickness or death from eating the inedible berries. Costs of false positives could include taking longer to find food, finding insufficient food, and starvation.</p>
<div class="cell">
<details class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co">#Standard Cutoff</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">ggplot</span>(</span>
<span id="cb3-3"><a href="#cb3-3"></a>  <span class="at">data =</span> density_data,</span>
<span id="cb3-4"><a href="#cb3-4"></a>  <span class="fu">aes</span>(</span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="at">x =</span> x,</span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="at">ymin =</span> <span class="dv">0</span>,</span>
<span id="cb3-7"><a href="#cb3-7"></a>    <span class="at">ymax =</span> y,</span>
<span id="cb3-8"><a href="#cb3-8"></a>    <span class="at">fill =</span> type)) <span class="sc">+</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>  <span class="fu">geom_ribbon</span>(<span class="at">alpha =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>  <span class="fu">scale_fill_manual</span>(</span>
<span id="cb3-11"><a href="#cb3-11"></a>    <span class="at">name =</span> <span class="st">"Berry Type"</span>,</span>
<span id="cb3-12"><a href="#cb3-12"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb3-13"><a href="#cb3-13"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">4</span>],</span>
<span id="cb3-14"><a href="#cb3-14"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">1</span>],</span>
<span id="cb3-15"><a href="#cb3-15"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">3</span>],</span>
<span id="cb3-16"><a href="#cb3-16"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">2</span>]),</span>
<span id="cb3-17"><a href="#cb3-17"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="st">"edible_TN"</span>,<span class="st">"inedible_TP"</span>,<span class="st">"edible_FP"</span>,<span class="st">"inedible_FN"</span>),</span>
<span id="cb3-18"><a href="#cb3-18"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Edible: TN"</span>,<span class="st">"Inedible: TP"</span>,<span class="st">"Edible: FP"</span>,<span class="st">"Inedible: FN"</span>)) <span class="sc">+</span></span>
<span id="cb3-19"><a href="#cb3-19"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb3-20"><a href="#cb3-20"></a>  <span class="fu">geom_vline</span>(</span>
<span id="cb3-21"><a href="#cb3-21"></a>    <span class="at">xintercept =</span> cutoff,</span>
<span id="cb3-22"><a href="#cb3-22"></a>    <span class="at">color =</span> <span class="st">"red"</span>,</span>
<span id="cb3-23"><a href="#cb3-23"></a>    <span class="at">linewidth =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb3-24"><a href="#cb3-24"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">name =</span> <span class="st">"score"</span>) <span class="sc">+</span></span>
<span id="cb3-25"><a href="#cb3-25"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">name =</span> <span class="st">"Frequency"</span>) <span class="sc">+</span></span>
<span id="cb3-26"><a href="#cb3-26"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb3-27"><a href="#cb3-27"></a>  <span class="fu">theme</span>(</span>
<span id="cb3-28"><a href="#cb3-28"></a>    <span class="at">axis.text.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb3-29"><a href="#cb3-29"></a>    <span class="at">axis.ticks.y =</span> <span class="fu">element_blank</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-classificationStandardCutoff" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-classificationStandardCutoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="evaluating-prediction-accuracy_files/figure-html/fig-classificationStandardCutoff-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;17.2: Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives."><img src="evaluating-prediction-accuracy_files/figure-html/fig-classificationStandardCutoff-1.png" class="img-fluid figure-img" alt="Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives." width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classificationStandardCutoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.2: Classifications Based on a Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.
</figcaption></figure>
</div>
</div>
</div>
<p>Based on our assessment goals, we might use a different selection ratio by changing the cutoff. <a href="#fig-classificationRaiseCutoff" class="quarto-xref">Figure&nbsp;<span>17.3</span></a> depicts the distributions of scores by berry type when we raise the cutoff. There are now more false negatives (blue) and fewer false positives (green). If we raise the cutoff (to be more conservative), the number of false negatives increases and the number of false positives decreases. Consequently, as the cutoff increases, sensitivity and NPV decrease (because we have more false negatives), whereas specificity and PPV increase (because we have fewer false positives). A higher cutoff could be optimal if the costs of false positives are considered greater than the costs of false negatives. For instance, if the aliens cannot risk eating the inedible berries because the berries are fatal, and there are sufficient edible berries that can be found to feed the alien colony.</p>
<div class="cell">
<details class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">#Raise the cutoff</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>cutoff <span class="ot">&lt;-</span> <span class="dv">85</span></span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a>hist_edible <span class="ot">&lt;-</span> <span class="fu">density</span>(</span>
<span id="cb4-5"><a href="#cb4-5"></a>  edibleScores,</span>
<span id="cb4-6"><a href="#cb4-6"></a>  <span class="at">from =</span> <span class="dv">0</span>,</span>
<span id="cb4-7"><a href="#cb4-7"></a>  <span class="at">to =</span> <span class="dv">150</span>) <span class="sc">%$%</span> <span class="co"># exposition pipe magrittr::`%$%`</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb4-9"><a href="#cb4-9"></a>    <span class="at">x =</span> x,</span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb4-11"><a href="#cb4-11"></a>  <span class="fu">mutate</span>(<span class="at">area =</span> x <span class="sc">&gt;=</span> cutoff)</span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a>hist_edible<span class="sc">$</span>type[hist_edible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">TRUE</span>] <span class="ot">&lt;-</span> <span class="st">"edible_FP"</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>hist_edible<span class="sc">$</span>type[hist_edible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">FALSE</span>] <span class="ot">&lt;-</span> <span class="st">"edible_TN"</span></span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a>hist_inedible <span class="ot">&lt;-</span> <span class="fu">density</span>(</span>
<span id="cb4-17"><a href="#cb4-17"></a>  inedibleScores,</span>
<span id="cb4-18"><a href="#cb4-18"></a>  <span class="at">from =</span> <span class="dv">0</span>,</span>
<span id="cb4-19"><a href="#cb4-19"></a>  <span class="at">to =</span> <span class="dv">150</span>) <span class="sc">%$%</span> <span class="co"># exposition pipe magrittr::`%$%`</span></span>
<span id="cb4-20"><a href="#cb4-20"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb4-21"><a href="#cb4-21"></a>    <span class="at">x =</span> x,</span>
<span id="cb4-22"><a href="#cb4-22"></a>    <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>  <span class="fu">mutate</span>(<span class="at">area =</span> x <span class="sc">&lt;</span> cutoff)</span>
<span id="cb4-24"><a href="#cb4-24"></a></span>
<span id="cb4-25"><a href="#cb4-25"></a>hist_inedible<span class="sc">$</span>type[hist_inedible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">TRUE</span>] <span class="ot">&lt;-</span> <span class="st">"inedible_FN"</span></span>
<span id="cb4-26"><a href="#cb4-26"></a>hist_inedible<span class="sc">$</span>type[hist_inedible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">FALSE</span>] <span class="ot">&lt;-</span> <span class="st">"inedible_TP"</span></span>
<span id="cb4-27"><a href="#cb4-27"></a></span>
<span id="cb4-28"><a href="#cb4-28"></a>density_data <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb4-29"><a href="#cb4-29"></a>  hist_edible,</span>
<span id="cb4-30"><a href="#cb4-30"></a>  hist_inedible)</span>
<span id="cb4-31"><a href="#cb4-31"></a></span>
<span id="cb4-32"><a href="#cb4-32"></a>density_data<span class="sc">$</span>type <span class="ot">&lt;-</span> <span class="fu">factor</span>(</span>
<span id="cb4-33"><a href="#cb4-33"></a>  density_data<span class="sc">$</span>type,</span>
<span id="cb4-34"><a href="#cb4-34"></a>  <span class="at">levels =</span> <span class="fu">c</span>(</span>
<span id="cb4-35"><a href="#cb4-35"></a>    <span class="st">"edible_TN"</span>,</span>
<span id="cb4-36"><a href="#cb4-36"></a>    <span class="st">"inedible_TP"</span>,</span>
<span id="cb4-37"><a href="#cb4-37"></a>    <span class="st">"edible_FP"</span>,</span>
<span id="cb4-38"><a href="#cb4-38"></a>    <span class="st">"inedible_FN"</span>))</span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a><span class="fu">ggplot</span>(</span>
<span id="cb4-41"><a href="#cb4-41"></a>  <span class="at">data =</span> density_data,</span>
<span id="cb4-42"><a href="#cb4-42"></a>  <span class="fu">aes</span>(</span>
<span id="cb4-43"><a href="#cb4-43"></a>    <span class="at">x =</span> x,</span>
<span id="cb4-44"><a href="#cb4-44"></a>    <span class="at">ymin =</span> <span class="dv">0</span>,</span>
<span id="cb4-45"><a href="#cb4-45"></a>    <span class="at">ymax =</span> y,</span>
<span id="cb4-46"><a href="#cb4-46"></a>    <span class="at">fill =</span> type)) <span class="sc">+</span></span>
<span id="cb4-47"><a href="#cb4-47"></a>  <span class="fu">geom_ribbon</span>(<span class="at">alpha =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb4-48"><a href="#cb4-48"></a>  <span class="fu">scale_fill_manual</span>(</span>
<span id="cb4-49"><a href="#cb4-49"></a>    <span class="at">name =</span> <span class="st">"Berry Type"</span>,</span>
<span id="cb4-50"><a href="#cb4-50"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb4-51"><a href="#cb4-51"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">4</span>],</span>
<span id="cb4-52"><a href="#cb4-52"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">1</span>],</span>
<span id="cb4-53"><a href="#cb4-53"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">3</span>],</span>
<span id="cb4-54"><a href="#cb4-54"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">2</span>]),</span>
<span id="cb4-55"><a href="#cb4-55"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="st">"edible_TN"</span>,<span class="st">"inedible_TP"</span>,<span class="st">"edible_FP"</span>,<span class="st">"inedible_FN"</span>),</span>
<span id="cb4-56"><a href="#cb4-56"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Edible: TN"</span>,<span class="st">"Inedible: TP"</span>,<span class="st">"Edible: FP"</span>,<span class="st">"Inedible: FN"</span>)) <span class="sc">+</span></span>
<span id="cb4-57"><a href="#cb4-57"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb4-58"><a href="#cb4-58"></a>  <span class="fu">geom_vline</span>(</span>
<span id="cb4-59"><a href="#cb4-59"></a>    <span class="at">xintercept =</span> cutoff,</span>
<span id="cb4-60"><a href="#cb4-60"></a>    <span class="at">color =</span> <span class="st">"red"</span>,</span>
<span id="cb4-61"><a href="#cb4-61"></a>    <span class="at">linewidth =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb4-62"><a href="#cb4-62"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">name =</span> <span class="st">"score"</span>) <span class="sc">+</span></span>
<span id="cb4-63"><a href="#cb4-63"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">name =</span> <span class="st">"Frequency"</span>) <span class="sc">+</span></span>
<span id="cb4-64"><a href="#cb4-64"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb4-65"><a href="#cb4-65"></a>  <span class="fu">theme</span>(</span>
<span id="cb4-66"><a href="#cb4-66"></a>    <span class="at">axis.text.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb4-67"><a href="#cb4-67"></a>    <span class="at">axis.ticks.y =</span> <span class="fu">element_blank</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-classificationRaiseCutoff" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-classificationRaiseCutoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="evaluating-prediction-accuracy_files/figure-html/fig-classificationRaiseCutoff-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;17.3: Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives."><img src="evaluating-prediction-accuracy_files/figure-html/fig-classificationRaiseCutoff-1.png" class="img-fluid figure-img" alt="Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives." width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classificationRaiseCutoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.3: Classifications Based on Raising the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.
</figcaption></figure>
</div>
</div>
</div>
<p><a href="#fig-classificationLowerCutoff" class="quarto-xref">Figure&nbsp;<span>17.4</span></a> depicts the distributions of scores by berry type when we lower the cutoff. There are now fewer false negatives (blue) and more false positives (green). If we lower the cutoff (to be more liberal), the number of false negatives decreases and the number of false positives increases. Consequently, as the cutoff decreases, sensitivity and NPV increase (because we have fewer false negatives), whereas specificity and PPV decrease (because we have more false positives). A lower cutoff could be optimal if the costs of false negatives are considered greater than the costs of false positives. For instance, if the aliens cannot risk missing edible berries because they are in short supply relative to the size of the alien colony, and eating the inedible berries would, at worst, lead to minor, temporary discomfort.</p>
<div class="cell">
<details class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">#Lower the cutoff</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>cutoff <span class="ot">&lt;-</span> <span class="dv">65</span></span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a>hist_edible <span class="ot">&lt;-</span> <span class="fu">density</span>(</span>
<span id="cb5-5"><a href="#cb5-5"></a>  edibleScores,</span>
<span id="cb5-6"><a href="#cb5-6"></a>  <span class="at">from =</span> <span class="dv">0</span>,</span>
<span id="cb5-7"><a href="#cb5-7"></a>  <span class="at">to =</span> <span class="dv">150</span>) <span class="sc">%$%</span> <span class="co"># exposition pipe magrittr::`%$%`</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb5-9"><a href="#cb5-9"></a>    <span class="at">x =</span> x,</span>
<span id="cb5-10"><a href="#cb5-10"></a>    <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>  <span class="fu">mutate</span>(<span class="at">area =</span> x <span class="sc">&gt;=</span> cutoff)</span>
<span id="cb5-12"><a href="#cb5-12"></a></span>
<span id="cb5-13"><a href="#cb5-13"></a>hist_edible<span class="sc">$</span>type[hist_edible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">TRUE</span>] <span class="ot">&lt;-</span> <span class="st">"edible_FP"</span></span>
<span id="cb5-14"><a href="#cb5-14"></a>hist_edible<span class="sc">$</span>type[hist_edible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">FALSE</span>] <span class="ot">&lt;-</span> <span class="st">"edible_TN"</span></span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a>hist_inedible <span class="ot">&lt;-</span> <span class="fu">density</span>(</span>
<span id="cb5-17"><a href="#cb5-17"></a>  inedibleScores,</span>
<span id="cb5-18"><a href="#cb5-18"></a>  <span class="at">from =</span> <span class="dv">0</span>,</span>
<span id="cb5-19"><a href="#cb5-19"></a>  <span class="at">to =</span> <span class="dv">150</span>) <span class="sc">%$%</span> <span class="co"># exposition pipe magrittr::`%$%`</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb5-21"><a href="#cb5-21"></a>    <span class="at">x =</span> x,</span>
<span id="cb5-22"><a href="#cb5-22"></a>    <span class="at">y =</span> y) <span class="sc">%&gt;%</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>  <span class="fu">mutate</span>(<span class="at">area =</span> x <span class="sc">&lt;</span> cutoff)</span>
<span id="cb5-24"><a href="#cb5-24"></a></span>
<span id="cb5-25"><a href="#cb5-25"></a>hist_inedible<span class="sc">$</span>type[hist_inedible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">TRUE</span>] <span class="ot">&lt;-</span> <span class="st">"inedible_FN"</span></span>
<span id="cb5-26"><a href="#cb5-26"></a>hist_inedible<span class="sc">$</span>type[hist_inedible<span class="sc">$</span>area <span class="sc">==</span> <span class="cn">FALSE</span>] <span class="ot">&lt;-</span> <span class="st">"inedible_TP"</span></span>
<span id="cb5-27"><a href="#cb5-27"></a></span>
<span id="cb5-28"><a href="#cb5-28"></a>density_data <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb5-29"><a href="#cb5-29"></a>  hist_edible,</span>
<span id="cb5-30"><a href="#cb5-30"></a>  hist_inedible)</span>
<span id="cb5-31"><a href="#cb5-31"></a></span>
<span id="cb5-32"><a href="#cb5-32"></a>density_data<span class="sc">$</span>type <span class="ot">&lt;-</span> <span class="fu">factor</span>(</span>
<span id="cb5-33"><a href="#cb5-33"></a>  density_data<span class="sc">$</span>type,</span>
<span id="cb5-34"><a href="#cb5-34"></a>  <span class="at">levels =</span> <span class="fu">c</span>(</span>
<span id="cb5-35"><a href="#cb5-35"></a>    <span class="st">"edible_TN"</span>,</span>
<span id="cb5-36"><a href="#cb5-36"></a>    <span class="st">"inedible_TP"</span>,</span>
<span id="cb5-37"><a href="#cb5-37"></a>    <span class="st">"edible_FP"</span>,</span>
<span id="cb5-38"><a href="#cb5-38"></a>    <span class="st">"inedible_FN"</span>))</span>
<span id="cb5-39"><a href="#cb5-39"></a></span>
<span id="cb5-40"><a href="#cb5-40"></a><span class="fu">ggplot</span>(</span>
<span id="cb5-41"><a href="#cb5-41"></a>  <span class="at">data =</span> density_data,</span>
<span id="cb5-42"><a href="#cb5-42"></a>  <span class="fu">aes</span>(</span>
<span id="cb5-43"><a href="#cb5-43"></a>    <span class="at">x =</span> x,</span>
<span id="cb5-44"><a href="#cb5-44"></a>    <span class="at">ymin =</span> <span class="dv">0</span>,</span>
<span id="cb5-45"><a href="#cb5-45"></a>    <span class="at">ymax =</span> y,</span>
<span id="cb5-46"><a href="#cb5-46"></a>    <span class="at">fill =</span> type)) <span class="sc">+</span></span>
<span id="cb5-47"><a href="#cb5-47"></a>  <span class="fu">geom_ribbon</span>(<span class="at">alpha =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb5-48"><a href="#cb5-48"></a>  <span class="fu">scale_fill_manual</span>(</span>
<span id="cb5-49"><a href="#cb5-49"></a>    <span class="at">name =</span> <span class="st">"Berry Type"</span>,</span>
<span id="cb5-50"><a href="#cb5-50"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb5-51"><a href="#cb5-51"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">4</span>],</span>
<span id="cb5-52"><a href="#cb5-52"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">1</span>],</span>
<span id="cb5-53"><a href="#cb5-53"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">3</span>],</span>
<span id="cb5-54"><a href="#cb5-54"></a>      viridis<span class="sc">::</span><span class="fu">viridis</span>(<span class="dv">4</span>)[<span class="dv">2</span>]),</span>
<span id="cb5-55"><a href="#cb5-55"></a>    <span class="at">breaks =</span> <span class="fu">c</span>(<span class="st">"edible_TN"</span>,<span class="st">"inedible_TP"</span>,<span class="st">"edible_FP"</span>,<span class="st">"inedible_FN"</span>),</span>
<span id="cb5-56"><a href="#cb5-56"></a>    <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">"Edible: TN"</span>,<span class="st">"Inedible: TP"</span>,<span class="st">"Edible: FP"</span>,<span class="st">"Inedible: FN"</span>)) <span class="sc">+</span></span>
<span id="cb5-57"><a href="#cb5-57"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb5-58"><a href="#cb5-58"></a>  <span class="fu">geom_vline</span>(</span>
<span id="cb5-59"><a href="#cb5-59"></a>    <span class="at">xintercept =</span> cutoff,</span>
<span id="cb5-60"><a href="#cb5-60"></a>    <span class="at">color =</span> <span class="st">"red"</span>,</span>
<span id="cb5-61"><a href="#cb5-61"></a>    <span class="at">linewidth =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb5-62"><a href="#cb5-62"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">name =</span> <span class="st">"score"</span>) <span class="sc">+</span></span>
<span id="cb5-63"><a href="#cb5-63"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">name =</span> <span class="st">"Frequency"</span>) <span class="sc">+</span></span>
<span id="cb5-64"><a href="#cb5-64"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb5-65"><a href="#cb5-65"></a>  <span class="fu">theme</span>(</span>
<span id="cb5-66"><a href="#cb5-66"></a>    <span class="at">axis.text.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb5-67"><a href="#cb5-67"></a>    <span class="at">axis.ticks.y =</span> <span class="fu">element_blank</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-classificationLowerCutoff" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-classificationLowerCutoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="evaluating-prediction-accuracy_files/figure-html/fig-classificationLowerCutoff-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;17.4: Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives."><img src="evaluating-prediction-accuracy_files/figure-html/fig-classificationLowerCutoff-1.png" class="img-fluid figure-img" alt="Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives." width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classificationLowerCutoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.4: Classifications Based on Lowering the Cutoff. Note that some true negatives and true positives are hidden behind the false positives and false negatives.
</figcaption></figure>
</div>
</div>
</div>
<p>In sum, sensitivity and specificity differ based on the cutoff for classification. If we raise the cutoff, sensitivity and PPV increase (due to fewer false positives), whereas sensitivity and NPV decrease (due to more false negatives). If we lower the cutoff, sensitivity and NPV increase (due to fewer false negatives), whereas specificity and PPV decrease (due to more false positives). Thus, the optimal cutoff depends on how costly each type of error is: false negatives and false positives. If false negatives are more costly than false positives, we would set a low cutoff. If false positives are more costly than false negatives, we would set a high cutoff.</p>
</section></section><section id="sec-sdt" class="level3" data-number="17.6.7"><h3 data-number="17.6.7" class="anchored" data-anchor-id="sec-sdt">
<span class="header-section-number">17.6.7</span> Signal Detection Theory</h3>
<p>Signal detection theory (SDT) is a probability-based theory for the detection of a given stimulus (signal) from a stimulus set that includes non-target stimuli (noise). SDT arose through the development of radar (<strong>RA</strong>dio <strong>D</strong>etection <strong>A</strong>nd <strong>R</strong>anging) and sonar (<strong>SO</strong>und <strong>N</strong>avigation <strong>A</strong>nd <strong>R</strong>anging) in World War II based on research on sensory-perception research. The military wanted to determine which objects on radar/sonar were enemy aircraft/submarines, and which were noise (e.g., different object in the environment or even just the weather itself). SDT allowed determining how many errors operators made (how accurate they were) and decomposing errors into different kinds of errors. SDT distinguishes between sensitivity and bias. In SDT, <em>sensitivity</em> (or discriminability) is how well an assessment distinguishes between a target stimulus and non-target stimuli (i.e., how well the assessment detects the target stimulus amid non-target stimuli). <em>Bias</em> is the extent to which the probability of a selection decision from the assessment is higher or lower than the true rate of the target stimulus.</p>
<p>Some radar/sonar operators were not as sensitive to the differences between signal and noise, due to factors such as age, ability to distinguish gradations of a signal, etc. People who showed low sensitivity (i.e., who were not as successful at distinguishing between signal and noise) were screened out because the military perceived sensitivity as a skill that was not easily taught. By contrast, other operators could distinguish signal from noise, but their threshold was too low or high—they could take in information, but their decisions tended to be wrong due to systematic bias or poor calibration. That is, they systematically over-rejected or under-rejected stimuli. Over-rejecting leads to many false negatives (i.e., saying that a stimulus is safe when it is not). Under-rejecting leads to many false positives (i.e., saying that a stimulus is harmful when it is not). A person who showed good sensitivity but systematic bias was considered more teach-able than a person who showed low sensitivity. Thus, radar and sonar operators were selected based on their sensitivity to distinguish signal from noise, and then were trained to improve the calibration so they reduce their systematic bias and do not systematically over- or under-reject.</p>
<p>Although SDT was originally developed for use in World War II, it now plays an important role in many areas of science and medicine. A medical application of SDT is tumor detection in radiology. Another application of SDT in society is using x-ray to detect bombs or other weapons. An example of applying SDT to fantasy football could be in the prediction (and evaluation) of whether or not a player scores a touchdown in a game.</p>
<p>SDT metrics of sensitivity include <span class="math inline">\(d'\)</span> (“<span class="math inline">\(d\)</span>-prime”), <span class="math inline">\(A\)</span> (or <span class="math inline">\(A'\)</span>), and the area under the receiver operating characteristic (ROC) curve. SDT metrics of bias include <span class="math inline">\(\beta\)</span> (beta), <span class="math inline">\(c\)</span>, and <span class="math inline">\(b\)</span>.</p>
<section id="sec-roc" class="level4" data-number="17.6.7.1"><h4 data-number="17.6.7.1" class="anchored" data-anchor-id="sec-roc">
<span class="header-section-number">17.6.7.1</span> Receiver Operating Characteristic (ROC) Curve</h4>
<p>The x-axis of the ROC curve is the false alarm rate or false positive rate (<span class="math inline">\(1 -\)</span> specificity). The y-axis is the hit rate or true positive rate (sensitivity). We can trace the ROC curve as the combination between sensitivity and specificity at every possible cutoff. At a cutoff of zero (top right of ROC curve), we calculate sensitivity (1.0) and specificity (0) and plot it. At a cutoff of zero, the assessment tells us to make an action for every stimulus (i.e., it is the most liberal). We then gradually increase the cutoff, and plot sensitivity and specificity at each cutoff. As the cutoff increases, sensitivity decreases and specificity increases. We end at the highest possible cutoff, where the sensitivity is 0 and the specificity is 1.0 (i.e., we never make an action; i.e., it is the most conservative). Each point on the ROC curve corresponds to a pair of hit and false alarm rates (sensitivity and specificity) resulting from a specific cutoff value. Then, we can draw lines or a curve to connect the points.</p>
<p>INSERT depicts an empirical ROC plot where lines are drawn to connect the hit and false alarm rates.</p>
<p>INSERT depicts an ROC curve where a smoothed and fitted curve is drawn to connect the hit and false alarm rates.</p>
<section id="sec-auc" class="level5" data-number="17.6.7.1.1"><h5 data-number="17.6.7.1.1" class="anchored" data-anchor-id="sec-auc">
<span class="header-section-number">17.6.7.1.1</span> Area Under the ROC Curve</h5>
<p>ROC methods can be used to compare and compute the discriminative power of measurement devices free from the influence of selection ratios, base rates, and costs and benefits. An ROC analysis yields a quantitative index of how well an index predicts a signal of interest or can discriminate between different signals. ROC analysis can help tell us how often our assessment would be correct. If we randomly pick two observations, and we were right once and wrong once, we were 50% accurate. But this would be a useless measure because it reflects chance responding.</p>
<p>The geometrical area under the ROC curve reflects the discriminative accuracy of the measure. The index is called the <strong>a</strong>rea <strong>u</strong>nder the <strong>c</strong>urve (AUC) of an ROC curve. AUC quantifies the discriminative power of an assessment. AUC is the probability that a randomly selected target and a randomly selected non-target is ranked correctly by the assessment method. AUC values range from 0.0 to 1.0, where chance accuracy is 0.5 as indicated by diagonal line in the ROC curve. That is, a measure can be useful to the extent that its ROC curve is above the diagonal line (i.e., its discriminative accuracy is above chance).</p>
<p>AUC is a <a href="#sec-thresholdIndependentAccuracy">threshold-independent accuracy index</a> that applies across all possible cutoff values.</p>
<p><a href="#fig-aucRange" class="quarto-xref">Figure&nbsp;<span>17.5</span></a> depicts ROC curves with a range of AUC values.</p>
<div class="cell">
<details class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu">set.seed</span>(<span class="dv">52242</span>)</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a>auc60 <span class="ot">&lt;-</span> petersenlab<span class="sc">::</span><span class="fu">simulateAUC</span>(.<span class="dv">60</span>, <span class="dv">50000</span>)</span>
<span id="cb6-4"><a href="#cb6-4"></a>auc70 <span class="ot">&lt;-</span> petersenlab<span class="sc">::</span><span class="fu">simulateAUC</span>(.<span class="dv">70</span>, <span class="dv">50000</span>)</span>
<span id="cb6-5"><a href="#cb6-5"></a>auc80 <span class="ot">&lt;-</span> petersenlab<span class="sc">::</span><span class="fu">simulateAUC</span>(.<span class="dv">80</span>, <span class="dv">50000</span>)</span>
<span id="cb6-6"><a href="#cb6-6"></a>auc90 <span class="ot">&lt;-</span> petersenlab<span class="sc">::</span><span class="fu">simulateAUC</span>(.<span class="dv">90</span>, <span class="dv">50000</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a>auc95 <span class="ot">&lt;-</span> petersenlab<span class="sc">::</span><span class="fu">simulateAUC</span>(.<span class="dv">95</span>, <span class="dv">50000</span>)</span>
<span id="cb6-8"><a href="#cb6-8"></a>auc99 <span class="ot">&lt;-</span> petersenlab<span class="sc">::</span><span class="fu">simulateAUC</span>(.<span class="dv">99</span>, <span class="dv">50000</span>)</span>
<span id="cb6-9"><a href="#cb6-9"></a></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="fu">plot</span>(</span>
<span id="cb6-11"><a href="#cb6-11"></a>  pROC<span class="sc">::</span><span class="fu">roc</span>(</span>
<span id="cb6-12"><a href="#cb6-12"></a>    y <span class="sc">~</span> x,</span>
<span id="cb6-13"><a href="#cb6-13"></a>    auc60,</span>
<span id="cb6-14"><a href="#cb6-14"></a>    <span class="at">smooth =</span> <span class="cn">TRUE</span>),</span>
<span id="cb6-15"><a href="#cb6-15"></a>  <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-16"><a href="#cb6-16"></a>  <span class="at">print.auc =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-17"><a href="#cb6-17"></a>  <span class="at">print.auc.x =</span> .<span class="dv">52</span>,</span>
<span id="cb6-18"><a href="#cb6-18"></a>  <span class="at">print.auc.y =</span> .<span class="dv">61</span>,</span>
<span id="cb6-19"><a href="#cb6-19"></a>  <span class="at">print.auc.pattern =</span> <span class="st">"%.2f"</span>)</span>
<span id="cb6-20"><a href="#cb6-20"></a></span>
<span id="cb6-21"><a href="#cb6-21"></a><span class="fu">plot</span>(</span>
<span id="cb6-22"><a href="#cb6-22"></a>  pROC<span class="sc">::</span><span class="fu">roc</span>(</span>
<span id="cb6-23"><a href="#cb6-23"></a>    y <span class="sc">~</span> x,</span>
<span id="cb6-24"><a href="#cb6-24"></a>    auc70,</span>
<span id="cb6-25"><a href="#cb6-25"></a>    <span class="at">smooth =</span> <span class="cn">TRUE</span>),</span>
<span id="cb6-26"><a href="#cb6-26"></a>  <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-27"><a href="#cb6-27"></a>  <span class="at">print.auc =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-28"><a href="#cb6-28"></a>  <span class="at">print.auc.x =</span> .<span class="dv">6</span>,</span>
<span id="cb6-29"><a href="#cb6-29"></a>  <span class="at">print.auc.y =</span> .<span class="dv">67</span>,</span>
<span id="cb6-30"><a href="#cb6-30"></a>  <span class="at">print.auc.pattern =</span> <span class="st">"%.2f"</span>,</span>
<span id="cb6-31"><a href="#cb6-31"></a>  <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-32"><a href="#cb6-32"></a></span>
<span id="cb6-33"><a href="#cb6-33"></a><span class="fu">plot</span>(</span>
<span id="cb6-34"><a href="#cb6-34"></a>  pROC<span class="sc">::</span><span class="fu">roc</span>(</span>
<span id="cb6-35"><a href="#cb6-35"></a>    y <span class="sc">~</span> x,</span>
<span id="cb6-36"><a href="#cb6-36"></a>    auc80,</span>
<span id="cb6-37"><a href="#cb6-37"></a>    <span class="at">smooth =</span> <span class="cn">TRUE</span>),</span>
<span id="cb6-38"><a href="#cb6-38"></a>  <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-39"><a href="#cb6-39"></a>  <span class="at">print.auc =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-40"><a href="#cb6-40"></a>  <span class="at">print.auc.x =</span> .<span class="dv">695</span>,</span>
<span id="cb6-41"><a href="#cb6-41"></a>  <span class="at">print.auc.y =</span> .<span class="dv">735</span>,</span>
<span id="cb6-42"><a href="#cb6-42"></a>  <span class="at">print.auc.pattern =</span> <span class="st">"%.2f"</span>,</span>
<span id="cb6-43"><a href="#cb6-43"></a>  <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-44"><a href="#cb6-44"></a></span>
<span id="cb6-45"><a href="#cb6-45"></a><span class="fu">plot</span>(</span>
<span id="cb6-46"><a href="#cb6-46"></a>  pROC<span class="sc">::</span><span class="fu">roc</span>(</span>
<span id="cb6-47"><a href="#cb6-47"></a>    y <span class="sc">~</span> x,</span>
<span id="cb6-48"><a href="#cb6-48"></a>    auc90,</span>
<span id="cb6-49"><a href="#cb6-49"></a>    <span class="at">smooth =</span> <span class="cn">TRUE</span>),</span>
<span id="cb6-50"><a href="#cb6-50"></a>  <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-51"><a href="#cb6-51"></a>  <span class="at">print.auc =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-52"><a href="#cb6-52"></a>  <span class="at">print.auc.x =</span> .<span class="dv">805</span>,</span>
<span id="cb6-53"><a href="#cb6-53"></a>  <span class="at">print.auc.y =</span> .<span class="dv">815</span>,</span>
<span id="cb6-54"><a href="#cb6-54"></a>  <span class="at">print.auc.pattern =</span> <span class="st">"%.2f"</span>,</span>
<span id="cb6-55"><a href="#cb6-55"></a>  <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-56"><a href="#cb6-56"></a></span>
<span id="cb6-57"><a href="#cb6-57"></a><span class="fu">plot</span>(</span>
<span id="cb6-58"><a href="#cb6-58"></a>  pROC<span class="sc">::</span><span class="fu">roc</span>(</span>
<span id="cb6-59"><a href="#cb6-59"></a>    y <span class="sc">~</span> x,</span>
<span id="cb6-60"><a href="#cb6-60"></a>    auc95,</span>
<span id="cb6-61"><a href="#cb6-61"></a>    <span class="at">smooth =</span> <span class="cn">TRUE</span>),</span>
<span id="cb6-62"><a href="#cb6-62"></a>  <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-63"><a href="#cb6-63"></a>  <span class="at">print.auc =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-64"><a href="#cb6-64"></a>  <span class="at">print.auc.x =</span> .<span class="dv">875</span>,</span>
<span id="cb6-65"><a href="#cb6-65"></a>  <span class="at">print.auc.y =</span> .<span class="dv">865</span>,</span>
<span id="cb6-66"><a href="#cb6-66"></a>  <span class="at">print.auc.pattern =</span> <span class="st">"%.2f"</span>,</span>
<span id="cb6-67"><a href="#cb6-67"></a>  <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-68"><a href="#cb6-68"></a></span>
<span id="cb6-69"><a href="#cb6-69"></a><span class="fu">plot</span>(</span>
<span id="cb6-70"><a href="#cb6-70"></a>  pROC<span class="sc">::</span><span class="fu">roc</span>(</span>
<span id="cb6-71"><a href="#cb6-71"></a>    y <span class="sc">~</span> x,</span>
<span id="cb6-72"><a href="#cb6-72"></a>    auc99,</span>
<span id="cb6-73"><a href="#cb6-73"></a>    <span class="at">smooth =</span> <span class="cn">TRUE</span>),</span>
<span id="cb6-74"><a href="#cb6-74"></a>  <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-75"><a href="#cb6-75"></a>  <span class="at">print.auc =</span> <span class="cn">TRUE</span>,</span>
<span id="cb6-76"><a href="#cb6-76"></a>  <span class="at">print.auc.x =</span> .<span class="dv">94</span>,</span>
<span id="cb6-77"><a href="#cb6-77"></a>  <span class="at">print.auc.y =</span> .<span class="dv">94</span>,</span>
<span id="cb6-78"><a href="#cb6-78"></a>  <span class="at">print.auc.pattern =</span> <span class="st">"%.2f"</span>,</span>
<span id="cb6-79"><a href="#cb6-79"></a>  <span class="at">add =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-aucRange" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-aucRange-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="evaluating-prediction-accuracy_files/figure-html/fig-aucRange-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;17.5: Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures."><img src="evaluating-prediction-accuracy_files/figure-html/fig-aucRange-1.png" class="img-fluid figure-img" alt="Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures." width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-aucRange-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.5: Receiver Operating Characteristic (ROC) Curves for Various Levels of Area Under The ROC Curve (AUC) for Various Measures.
</figcaption></figure>
</div>
</div>
</div>
<p>As an example, given an AUC of .75, this says that the overall score of an individual who has the characteristic in question will be higher 75% of the time than the overall score of an individual who does not have the characteristic. In lay terms, AUC provides the probability that we will classify correctly based on our instrument if we were to randomly pick one good and one bad outcome. AUC is a stronger index of accuracy than percent accuracy, because you can have high percent accuracy just by going with the base rate. AUC tells us how much better than chance a measure is at discriminating outcomes. AUC is useful as a measure of general discriminative accuracy, and it tells us how accurate a measure is at all possible cutoffs. Knowing the accuracy of a measure at all possible cutoffs can be helpful for selecting the optimal cutoff, given the goals of the assessment. In reality, however, we may not be interested in all cutoffs because not all errors are equal in their costs.</p>
<p>If we lower the base rate, we would need a larger sample to get enough people to classify into each group. SDT/ROC methods are traditionally about dichotomous decisions (yes/no), not graded judgments. SDT/ROC methods can get messy with ordinal data that are more graded because you would have an AUC curve for each ordinal grouping.</p>
</section></section></section><section id="sec-accuracyIndicesCategorical" class="level3" data-number="17.6.8"><h3 data-number="17.6.8" class="anchored" data-anchor-id="sec-accuracyIndicesCategorical">
<span class="header-section-number">17.6.8</span> Accuracy Indices</h3>
<p>There are various accuracy indices we can use to evaluate the accuracy of predictions for categorical outcome variables. We have already described several accuracy indices, including percent accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and area under the ROC curve. We describe these and other indices in greater detail below.</p>
<p>The <a href="https://github.com/DevPsyLab/petersenlab"><code>petersenlab</code></a> package <span class="citation" data-cites="R-petersenlab">(<a href="references.html#ref-R-petersenlab" role="doc-biblioref">Petersen, 2025a</a>)</span> contains the <code>accuracyAtCutoff()</code> function that computes many accuracy indices for the prediction of categorical outcome variables.</p>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="co">#petersenlab::accuracyAtCutoff()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The <a href="https://github.com/DevPsyLab/petersenlab"><code>petersenlab</code></a> package <span class="citation" data-cites="R-petersenlab">(<a href="references.html#ref-R-petersenlab" role="doc-biblioref">Petersen, 2025a</a>)</span> contains the <code>accuracyAtEachCutoff()</code> function that computes many accuracy indices for the prediction of categorical outcome variables at each possible cutoff.</p>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co">#petersenlab::accuracyAtEachCutoff()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>There are also test calculators available online:</p>
<ul>
<li>
<a href="http://araw.mede.uic.edu/cgi-bin/testcalc.pl" class="uri">http://araw.mede.uic.edu/cgi-bin/testcalc.pl</a> [<span class="citation" data-cites="Schwartz2006">Schwartz (<a href="references.html#ref-Schwartz2006" role="doc-biblioref">2006</a>)</span>; archived at <a href="https://perma.cc/X8TF-7YBX" class="uri">https://perma.cc/X8TF-7YBX</a>]</li>
<li>
<a href="https://dlrs.shinyapps.io/shinyDLRs" class="uri">https://dlrs.shinyapps.io/shinyDLRs</a> <span class="citation" data-cites="Goodman2022">(<a href="references.html#ref-Goodman2022" role="doc-biblioref">Goodman et al., 2022</a>)</span>
</li>
</ul>
<section id="sec-confusionMatrix" class="level4" data-number="17.6.8.1"><h4 data-number="17.6.8.1" class="anchored" data-anchor-id="sec-confusionMatrix">
<span class="header-section-number">17.6.8.1</span> Confusion Matrix aka 2x2 Accuracy Table aka Cross-Tabulation aka Contingency Table</h4>
<p>A confusion matrix (aka 2x2 accuracy table, cross-tabulation table, or contigency table) is a matrix for categorical data that presents the predicted outcome on one dimension and the actual outcome (truth) on the other dimension. If the predictions and outcomes are dichotomous, the confusion matrix is a 2x2 matrix with two rows and two columns that represent four possible predicted-actual combinations (<a href="#sec-decisionOutcomes">decision outcomes</a>). In such a case, the confusion matrix provides a tabular count of each type of accurate cases (<a href="#sec-truePositive">true positives</a> and <a href="#sec-trueNegative">true negatives</a>) versus the number of each type of error (<a href="#sec-falsePositive">false positives</a> and <a href="#sec-falseNegative">false negatives</a>), as shown in INSERT. An example of a confusion matrix is in INSERT.</p>
<section id="sec-confusionMatrix-number" class="level5" data-number="17.6.8.1.1"><h5 data-number="17.6.8.1.1" class="anchored" data-anchor-id="sec-confusionMatrix-number">
<span class="header-section-number">17.6.8.1.1</span> Number</h5>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co">#table(mydata$diagnosisFactor, mydata$diseaseFactor)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section><section id="sec-confusionMatrix-numberMargins" class="level5" data-number="17.6.8.1.2"><h5 data-number="17.6.8.1.2" class="anchored" data-anchor-id="sec-confusionMatrix-numberMargins">
<span class="header-section-number">17.6.8.1.2</span> Number with margins added</h5>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co">#addmargins(table(mydata$diagnosisFactor, mydata$diseaseFactor))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section><section id="sec-confusionMatrix-numberMarginsProportions" class="level5" data-number="17.6.8.1.3"><h5 data-number="17.6.8.1.3" class="anchored" data-anchor-id="sec-confusionMatrix-numberMarginsProportions">
<span class="header-section-number">17.6.8.1.3</span> Proportions</h5>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co">#prop.table(table(mydata$diagnosisFactor, mydata$diseaseFactor))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section><section id="sec-confusionMatrix-numberMarginsProportionsMargins" class="level5" data-number="17.6.8.1.4"><h5 data-number="17.6.8.1.4" class="anchored" data-anchor-id="sec-confusionMatrix-numberMarginsProportionsMargins">
<span class="header-section-number">17.6.8.1.4</span> Proportions with margins added</h5>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co">#addmargins(prop.table(table(mydata$diagnosisFactor, mydata$diseaseFactor)))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section></section><section id="sec-truePositive" class="level4" data-number="17.6.8.2"><h4 data-number="17.6.8.2" class="anchored" data-anchor-id="sec-truePositive">
<span class="header-section-number">17.6.8.2</span> True Positives (TP)</h4>
<p>True positives (TPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is correct—that is, the test says that a classification is present, and the classification is present. True positives are also called valid positives (VPs) or hits. Higher values reflect greater accuracy. The formula for true positives is in <a href="#eq-truePositive" class="quarto-xref">Equation&nbsp;<span>17.4</span></a>:</p>
<p><span id="eq-truePositive"><span class="math display">\[
\begin{aligned}
  \text{TP} &amp;= \text{BR} \times \text{SR} \times N
\end{aligned}
\tag{17.4}\]</span></span></p>
</section><section id="sec-trueNegative" class="level4" data-number="17.6.8.3"><h4 data-number="17.6.8.3" class="anchored" data-anchor-id="sec-trueNegative">
<span class="header-section-number">17.6.8.3</span> True Negatives (TN)</h4>
<p>True negatives (TNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is correct—that is, the test says that a classification is not present, and the classification is actually not present. True negatives are also called valid negatives (VNs) or correct rejections. Higher values reflect greater accuracy. The formula for true negatives is in <a href="#eq-trueNegative" class="quarto-xref">Equation&nbsp;<span>17.5</span></a>:</p>
<p><span id="eq-trueNegative"><span class="math display">\[
\begin{aligned}
  \text{TN} &amp;= (1 - \text{BR}) \times (1 - \text{SR}) \times N
\end{aligned}
\tag{17.5}\]</span></span></p>
</section><section id="sec-falsePositive" class="level4" data-number="17.6.8.4"><h4 data-number="17.6.8.4" class="anchored" data-anchor-id="sec-falsePositive">
<span class="header-section-number">17.6.8.4</span> False Positives (FP)</h4>
<p>False positives (FPs) are instances in which a positive classification (e.g., stating that a disease is present for a person) is incorrect—that is, the test says that a classification is present, and the classification is not present. False positives are also called false alarms (FAs). Lower values reflect greater accuracy. The formula for false positives is in Equation <a href="#eq-falsePositive" class="quarto-xref">Equation&nbsp;<span>17.6</span></a>:</p>
<p><span id="eq-falsePositive"><span class="math display">\[
\begin{aligned}
  \text{FP} &amp;= (1 - \text{BR}) \times \text{SR} \times N
\end{aligned}
\tag{17.6}\]</span></span></p>
</section><section id="sec-falseNegative" class="level4" data-number="17.6.8.5"><h4 data-number="17.6.8.5" class="anchored" data-anchor-id="sec-falseNegative">
<span class="header-section-number">17.6.8.5</span> False Negatives (FN)</h4>
<p>False negatives (FNs) are instances in which a negative classification (e.g., stating that a disease is absent for a person) is incorrect—that is, the test says that a classification is not present, and the classification is present. False negatives are also called misses. Lower values reflect greater accuracy. The formula for false negatives is in <a href="#eq-falseNegative" class="quarto-xref">Equation&nbsp;<span>17.7</span></a>:</p>
<p><span id="eq-falseNegative"><span class="math display">\[
\begin{aligned}
  \text{FN} &amp;= \text{BR} \times (1 - \text{SR}) \times N
\end{aligned}
\tag{17.7}\]</span></span></p>
</section><section id="sec-selectionRatio" class="level4" data-number="17.6.8.6"><h4 data-number="17.6.8.6" class="anchored" data-anchor-id="sec-selectionRatio">
<span class="header-section-number">17.6.8.6</span> Selection Ratio (SR)</h4>
<p>The selection ratio (SR) is the marginal probability of selection, independent of other things: <span class="math inline">\(P(R_i)\)</span>. It is not an index of accuracy, per se. In medicine, the selection ratio is the proportion of people who test positive for the disease. In fantasy football, the selection ratio is the proportion of players who you predict will show a given outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the selection ratio is the proportion of players who you predict will score a touchdown. The formula for calculating the selection ratio is in <a href="#eq-selectionRatio" class="quarto-xref">Equation&nbsp;<span>17.8</span></a>.</p>
<p><span id="eq-selectionRatio"><span class="math display">\[
\begin{aligned}
  \text{SR} &amp;= P(R_i) \\
  &amp;= \frac{\text{TP} + \text{FP}}{N}
\end{aligned}
\tag{17.8}\]</span></span></p>
</section><section id="sec-pretestProbability" class="level4" data-number="17.6.8.7"><h4 data-number="17.6.8.7" class="anchored" data-anchor-id="sec-pretestProbability">
<span class="header-section-number">17.6.8.7</span> Base Rate (BR)</h4>
<p>The <a href="base-rates.html#sec-baseRate">base rate</a> (BR) of a classification is its <a href="base-rates.html#sec-baseRate">marginal probability</a>, independent of other things: <span class="math inline">\(P(C_i)\)</span>. It is not an index of accuracy, per se. In medicine, the base rate of a disease is its prevalence in the population, as in <a href="#eq-baseRate" class="quarto-xref">Equation&nbsp;<span>17.9</span></a>. Without additional information, the <a href="base-rates.html#sec-baseRate">base rate</a> is used as the initial <em>pretest probability</em>. In fantasy football, the <a href="base-rates.html#sec-baseRate">base rate</a> is the proportion of players who actually show the particular outcome. For example, if you are trying to predict the players who will score a touchdown in a game, the <a href="base-rates.html#sec-baseRate">base rate</a> is the proportion of players who actually score a touchdown in the game. The formula for calculating the selection ratio is in <a href="#eq-baseRate" class="quarto-xref">Equation&nbsp;<span>17.9</span></a>.</p>
<p><span id="eq-baseRate"><span class="math display">\[
\begin{aligned}
  \text{BR} &amp;= P(C_i) \\
  &amp;= \frac{\text{TP} + \text{FN}}{N}
\end{aligned}
\tag{17.9}\]</span></span></p>
</section><section id="sec-pretestOdds" class="level4" data-number="17.6.8.8"><h4 data-number="17.6.8.8" class="anchored" data-anchor-id="sec-pretestOdds">
<span class="header-section-number">17.6.8.8</span> Pretest Odds</h4>
<p>The pretest odds of a classification can be estimated using the pretest probability (i.e., <a href="base-rates.html#sec-baseRate">base rate</a>). To convert a probability to odds, divide the probability by one minus that probability, as in <a href="#eq-pretestOdds" class="quarto-xref">Equation&nbsp;<span>17.10</span></a>.</p>
<p><span id="eq-pretestOdds"><span class="math display">\[
\begin{aligned}
  \text{pretest odds} &amp;= \frac{\text{pretest probability}}{1 - \text{pretest probability}} \\
\end{aligned}
\tag{17.10}\]</span></span></p>
</section><section id="sec-percentAccuracy" class="level4" data-number="17.6.8.9"><h4 data-number="17.6.8.9" class="anchored" data-anchor-id="sec-percentAccuracy">
<span class="header-section-number">17.6.8.9</span> Percent Accuracy</h4>
<p>Percent Accuracy is also called overall accuracy. Higher values reflect greater accuracy. The formula for percent accuracy is in <a href="#eq-percentAccuracy" class="quarto-xref">Equation&nbsp;<span>17.11</span></a>. Percent accuracy has several problems. First, it treats all errors (<a href="#sec-falsePositive">FP</a> and <a href="#sec-falseNegative">FN</a>) as equally important. However, in practice, it is rarely the case that <a href="#sec-falsePositive">false positives</a> and <a href="#sec-falseNegative">false negatives</a> are equally important. Second, percent accuracy can be misleading because it is highly influenced by <a href="base-rates.html#sec-baseRate">base rates</a>. You can have a high percent accuracy by predicting from the <a href="base-rates.html#sec-baseRate">base rate</a> and saying that no one has the characteristic (if the <a href="base-rates.html#sec-baseRate">base rate</a> is low) or that everyone has the characteristic (if the <a href="base-rates.html#sec-baseRate">base rate</a> is high). Thus, it is also important to consider other aspects of accuracy.</p>
<p><span id="eq-percentAccuracy"><span class="math display">\[
\text{Percent Accuracy} = 100\% \times \frac{\text{TP} + \text{TN}}{N}
\tag{17.11}\]</span></span></p>
</section><section id="sec-percentAccuracyByChance" class="level4" data-number="17.6.8.10"><h4 data-number="17.6.8.10" class="anchored" data-anchor-id="sec-percentAccuracyByChance">
<span class="header-section-number">17.6.8.10</span> Percent Accuracy by Chance</h4>
<p>The formula for calculating percent accuracy by chance is in <a href="#eq-percentAccuracyByChance" class="quarto-xref">Equation&nbsp;<span>17.12</span></a>.</p>
<p><span id="eq-percentAccuracyByChance"><span class="math display">\[
\begin{aligned}
  \text{Percent Accuracy by Chance} &amp;= 100\% \times [P(\text{TP}) + P(\text{TN})] \\
  &amp;= 100\% \times \{(\text{BR} \times {\text{SR}}) + [(1 - \text{BR}) \times (1 - \text{SR})]\}
\end{aligned}
\tag{17.12}\]</span></span></p>
</section><section id="sec-percentAccuracyPredictingFromBaseRate" class="level4" data-number="17.6.8.11"><h4 data-number="17.6.8.11" class="anchored" data-anchor-id="sec-percentAccuracyPredictingFromBaseRate">
<span class="header-section-number">17.6.8.11</span> Percent Accuracy Predicting from the Base Rate</h4>
<p><a href="#sec-predictingFromBaseRate"><em>Predicting from the base rate</em></a> is going with the most likely outcome in every prediction. If the <a href="base-rates.html#sec-baseRate">base rate</a> is less than .50, it would involve predicting that the condition is absent for every case. If the <a href="base-rates.html#sec-baseRate">base rate</a> is .50 or above, it would involve predicting that the condition is present for every case. <a href="#sec-predictingFromBaseRate">Predicting from the base rate</a> is a special case of <a href="#sec-percentAccuracyByChance">percent accuracy by chance</a> when the <a href="#sec-selectionRatio">selection ratio</a> is set to either one (if the <a href="base-rates.html#sec-baseRate">base rate</a> <span class="math inline">\(\geq\)</span> .5) or zero (if the <a href="base-rates.html#sec-baseRate">base rate</a> &lt; .5).</p>
</section><section id="sec-relativeImprovementOverChance" class="level4" data-number="17.6.8.12"><h4 data-number="17.6.8.12" class="anchored" data-anchor-id="sec-relativeImprovementOverChance">
<span class="header-section-number">17.6.8.12</span> Relative Improvement Over Chance (RIOC)</h4>
<p>Relative improvement over chance (RIOC) is a prediction’s improvement over chance as a proportion of the maximum possible improvement over chance, as described by <span class="citation" data-cites="Farrington1989">Farrington &amp; Loeber (<a href="references.html#ref-Farrington1989" role="doc-biblioref">1989</a>)</span>. Higher values reflect greater accuracy. The formula for calculating RIOC is in <a href="#eq-relativeImprovementOverChance" class="quarto-xref">Equation&nbsp;<span>17.13</span></a>.</p>
<p><span id="eq-relativeImprovementOverChance"><span class="math display">\[
\begin{aligned}
  \text{relative improvement over chance (RIOC)} &amp;= \frac{\text{total correct} - \text{chance correct}}{\text{maximum correct} - \text{chance correct}} \\
\end{aligned}
\tag{17.13}\]</span></span></p>
</section><section id="sec-relativeImprovementOverPredictingFromBaseRate" class="level4" data-number="17.6.8.13"><h4 data-number="17.6.8.13" class="anchored" data-anchor-id="sec-relativeImprovementOverPredictingFromBaseRate">
<span class="header-section-number">17.6.8.13</span> Relative Improvement Over Predicting from the Base Rate</h4>
<p>Relative improvement over <a href="#sec-predictingFromBaseRate">predicting from the base rate</a> is a prediction’s improvement over <a href="#sec-predictingFromBaseRate">predicting from the base rate</a> as a proportion of the maximum possible improvement over <a href="#sec-predictingFromBaseRate">predicting from the base rate</a>. Higher values reflect greater accuracy. The formula for calculating relative improvement over predicting from the base rate is in <a href="#eq-relativeImprovementOverPredictingFromBaseRate" class="quarto-xref">Equation&nbsp;<span>17.14</span></a>.</p>
<p><span id="eq-relativeImprovementOverPredictingFromBaseRate"><span class="math display">\[
\scriptsize
\begin{aligned}
  \text{relative improvement over predicting from base rate} &amp;= \frac{\text{total correct} - \text{correct by predicting from base rate}}{\text{maximum correct} - \text{correct by predicting from base rate}} \\
\end{aligned}
\tag{17.14}\]</span></span></p>
</section><section id="sec-sensitivity" class="level4" data-number="17.6.8.14"><h4 data-number="17.6.8.14" class="anchored" data-anchor-id="sec-sensitivity">
<span class="header-section-number">17.6.8.14</span> Sensitivity (SN)</h4>
<p>Sensitivity (SN) is also called true positive rate (TPR), hit rate (HR), or recall. Sensitivity is the <a href="base-rates.html#sec-conditionalProbability">conditional probability</a> of a positive test given that the person has the condition: <span class="math inline">\(P(R|C)\)</span>. Higher values reflect greater accuracy. The formula for calculating sensitivity is in <a href="#eq-sensitivity" class="quarto-xref">Equation&nbsp;<span>17.15</span></a>. As described in Section <a href="#sec-accuracyCutoff" class="quarto-xref"><span>Section 17.6.6.1</span></a>, as the cutoff increases (becomes more conservative), sensitivity decreases. As the cutoff decreases, sensitivity increases.</p>
<p><span id="eq-sensitivity"><span class="math display">\[
\begin{aligned}
  \text{sensitivity (SN)} &amp;= P(R|C) \\
  &amp;= \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{\text{TP}}{N \times \text{BR}} = 1 - \text{FNR}
\end{aligned}
\tag{17.15}\]</span></span></p>
</section><section id="sec-specificity" class="level4" data-number="17.6.8.15"><h4 data-number="17.6.8.15" class="anchored" data-anchor-id="sec-specificity">
<span class="header-section-number">17.6.8.15</span> Specificity (SP)</h4>
<p>Specificity (SP) is also called true negative rate (TNR) or selectivity. Specificity is the <a href="base-rates.html#sec-conditionalProbability">conditional probability</a> of a negative test given that the person does not have the condition: <span class="math inline">\(P(\text{not } R|\text{not } C)\)</span>. Higher values reflect greater accuracy. The formula for calculating specificity is in <a href="#eq-specificity" class="quarto-xref">Equation&nbsp;<span>17.16</span></a>. As described in Section <a href="#sec-accuracyCutoff" class="quarto-xref"><span>Section 17.6.6.1</span></a>, as the cutoff increases (becomes more conservative), specificity increases. As the cutoff decreases, specificity decreases.</p>
<p><span id="eq-specificity"><span class="math display">\[
\begin{aligned}
  \text{specificity (SP)} &amp;= P(\text{not } R|\text{not } C) \\
  &amp;= \frac{\text{TN}}{\text{TN} + \text{FP}} = \frac{\text{TN}}{N (1 - \text{BR})} = 1 - \text{FPR}
\end{aligned}
\tag{17.16}\]</span></span></p>
</section><section id="sec-falseNegativeRate" class="level4" data-number="17.6.8.16"><h4 data-number="17.6.8.16" class="anchored" data-anchor-id="sec-falseNegativeRate">
<span class="header-section-number">17.6.8.16</span> False Negative Rate (FNR)</h4>
<p>The false negative rate (FNR) is also called the miss rate. The false negative rate is the <a href="base-rates.html#sec-conditionalProbability">conditional probability</a> of a negative test given that the person has the condition: <span class="math inline">\(P(\text{not } R|C)\)</span>. Lower values reflect greater accuracy. The formula for calculating false negative rate is in <a href="#eq-falseNegativeRate" class="quarto-xref">Equation&nbsp;<span>17.17</span></a>.</p>
<p><span id="eq-falseNegativeRate"><span class="math display">\[
\begin{aligned}
  \text{false negative rate (FNR)} &amp;= P(\text{not } R|C) \\
  &amp;= \frac{\text{FN}}{\text{FN} + \text{TP}} = \frac{\text{FN}}{N \times \text{BR}} = 1 - \text{TPR}
\end{aligned}
\tag{17.17}\]</span></span></p>
</section><section id="sec-falsePositiveRate" class="level4" data-number="17.6.8.17"><h4 data-number="17.6.8.17" class="anchored" data-anchor-id="sec-falsePositiveRate">
<span class="header-section-number">17.6.8.17</span> False Positive Rate (FPR)</h4>
<p>The false positive rate (FPR) is also called the false alarm rate (FAR) or fall-out. The false positive rate is the <a href="base-rates.html#sec-conditionalProbability">conditional probability</a> of a positive test given that the person does not have the condition: <span class="math inline">\(P(R|\text{not } C)\)</span>. Lower values reflect greater accuracy. The formula for calculating false positive rate is in <a href="#eq-falsePositiveRate" class="quarto-xref">Equation&nbsp;<span>17.18</span></a>:</p>
<p><span id="eq-falsePositiveRate"><span class="math display">\[
\begin{aligned}
  \text{false positive rate (FPR)} &amp;= P(R|\text{not } C) \\
  &amp;= \frac{\text{FP}}{\text{FP} + \text{TN}} = \frac{\text{FP}}{N (1 - \text{BR})} = 1 - \text{TNR}
\end{aligned}
\tag{17.18}\]</span></span></p>
</section><section id="sec-ppv" class="level4" data-number="17.6.8.18"><h4 data-number="17.6.8.18" class="anchored" data-anchor-id="sec-ppv">
<span class="header-section-number">17.6.8.18</span> Positive Predictive Value (PPV)</h4>
<p>The positive predictive value (PPV) is also called the positive predictive power (PPP) or precision. Many people confuse <a href="#sec-sensitivity">sensitivity</a> (<span class="math inline">\(P(R|C)\)</span>) with its inverse <a href="base-rates.html#sec-conditionalProbability">conditional probability</a>, PPV (<span class="math inline">\(P(C|R)\)</span>). PPV is the <a href="base-rates.html#sec-conditionalProbability">conditional probability</a> of having the condition given a positive test: <span class="math inline">\(P(C|R)\)</span>. Higher values reflect greater accuracy. The formula for calculating positive predictive value is in <a href="#eq-positivePredictiveValue" class="quarto-xref">Equation&nbsp;<span>17.19</span></a>.</p>
<p>PPV can be low even when <a href="#sec-sensitivity">sensitivity</a> is high because it depends not only on <a href="#sec-sensitivity">sensitivity</a>, but also on <a href="#sec-specificity">specificity</a> and the <a href="base-rates.html#sec-baseRate">base rate</a>. Because PPV depends on the <a href="base-rates.html#sec-baseRate">base rate</a>, PPV is not an intrinsic property of a measure. The same measure will have a different PPV in different contexts with different <a href="base-rates.html#sec-baseRate">base rates</a> <span class="citation" data-cites="Treat2023">(<a href="references.html#ref-Treat2023" role="doc-biblioref">Treat &amp; Viken, 2023</a>)</span>. As described in Section <a href="#sec-accuracyCutoff" class="quarto-xref"><span>Section 17.6.6.1</span></a>, as the <a href="base-rates.html#sec-baseRate">base rate</a> increases, PPV increases. As the <a href="base-rates.html#sec-baseRate">base rate</a> decreases, PPV decreases. PPV also differs as a function of the cutoff. As described in Section <a href="#sec-accuracyCutoff" class="quarto-xref"><span>Section 17.6.6.1</span></a>, as the cutoff increases (becomes more conservative), PPV increases. As the cutoff decreases (becomes more liberal), PPV decreases.</p>
<p><span id="eq-positivePredictiveValue"><span class="math display">\[
\small
\begin{aligned}
  \text{positive predictive value (PPV)} &amp;= P(C|R) \\
  &amp;= \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{\text{TP}}{N \times \text{SR}}\\
  &amp;= \frac{\text{sensitivity} \times {\text{BR}}}{\text{sensitivity} \times {\text{BR}} + [(1 - \text{specificity}) \times (1 - \text{BR})]}
\end{aligned}
\tag{17.19}\]</span></span></p>
</section><section id="sec-npv" class="level4" data-number="17.6.8.19"><h4 data-number="17.6.8.19" class="anchored" data-anchor-id="sec-npv">
<span class="header-section-number">17.6.8.19</span> Negative Predictive Value (NPV)</h4>
<p>The negative predictive value (NPV) is also called the negative predictive power (NPP). Many people confuse <a href="#sec-specificity">specificity</a> (<span class="math inline">\(P(\text{not } R|\text{not } C)\)</span>) with its inverse <a href="base-rates.html#sec-conditionalProbability">conditional probability</a>, NPV (<span class="math inline">\(P(\text{not } C| \text{not } R)\)</span>). NPV is the <a href="base-rates.html#sec-conditionalProbability">conditional probability</a> of not having the condition given a negative test: <span class="math inline">\(P(\text{not } C| \text{not } R)\)</span>. Higher values reflect greater accuracy. The formula for calculating negative predictive value is in <a href="#eq-negativePredictiveValue" class="quarto-xref">Equation&nbsp;<span>17.20</span></a>.</p>
<p>NPV can be low even when <a href="#sec-specificity">specificity</a> is high because it depends not only on <a href="#sec-specificity">specificity</a>, but also on <a href="#sec-sensitivity">sensitivity</a> and the <a href="base-rates.html#sec-baseRate">base rate</a>. Because NPV depends on the <a href="base-rates.html#sec-baseRate">base rate</a>, NPV is not an intrinsic property of a measure. The same measure will have a different NPV in different contexts with different <a href="base-rates.html#sec-baseRate">base rates</a> <span class="citation" data-cites="Treat2023">(<a href="references.html#ref-Treat2023" role="doc-biblioref">Treat &amp; Viken, 2023</a>)</span>. As described in Section <a href="#sec-accuracyCutoff" class="quarto-xref"><span>Section 17.6.6.1</span></a>, as the <a href="base-rates.html#sec-baseRate">base rate</a> increases, NPV decreases. As the <a href="base-rates.html#sec-baseRate">base rate</a> decreases, NPV increases. NPV also differs as a function of the cutoff. As described in Section <a href="#sec-accuracyCutoff" class="quarto-xref"><span>Section 17.6.6.1</span></a>, as the cutoff increases (becomes more conservative), NPV decreases. As the cutoff decreases (becomes more liberal), NPV decreases.</p>
<p><span id="eq-negativePredictiveValue"><span class="math display">\[
\small
\begin{aligned}
  \text{negative predictive value (NPV)} &amp;= P(\text{not } C|\text{not } R) \\
  &amp;= \frac{\text{TN}}{\text{TN} + \text{FN}} = \frac{\text{TN}}{N(\text{1 - SR})}\\
  &amp;= \frac{\text{specificity} \times (1-{\text{BR}})}{\text{specificity} \times (1-{\text{BR}}) + [(1 - \text{sensitivity}) \times \text{BR})]}
\end{aligned}
\tag{17.20}\]</span></span></p>
</section><section id="sec-falseDiscoveryRate" class="level4" data-number="17.6.8.20"><h4 data-number="17.6.8.20" class="anchored" data-anchor-id="sec-falseDiscoveryRate">
<span class="header-section-number">17.6.8.20</span> False Discovery Rate (FDR)</h4>
<p>Many people confuse the false positive rate (<span class="math inline">\(P(R|\text{not } C)\)</span>) with its inverse <a href="base-rates.html#sec-conditionalProbability">conditional probability</a>, the false discovery rate (<span class="math inline">\(P(\text{not } C| R)\)</span>). The false discovery rate (FDR) is the <a href="base-rates.html#sec-conditionalProbability">conditional probability</a> of not having the condition given a positive test: <span class="math inline">\(P(\text{not } C| R)\)</span>. Lower values reflect greater accuracy. The formula for calculating false discovery rate is in <a href="#eq-falseDiscoveryRate" class="quarto-xref">Equation&nbsp;<span>17.21</span></a>.</p>
<p><span id="eq-falseDiscoveryRate"><span class="math display">\[
\begin{aligned}
  \text{false discovery rate (FDR)} &amp;= P(\text{not } C|R) \\
  &amp;= \frac{\text{FP}}{\text{FP} + \text{TP}} = 1 - \text{PPV}
\end{aligned}
\tag{17.21}\]</span></span></p>
</section><section id="sec-falseOmissionRate" class="level4" data-number="17.6.8.21"><h4 data-number="17.6.8.21" class="anchored" data-anchor-id="sec-falseOmissionRate">
<span class="header-section-number">17.6.8.21</span> False Omission Rate (FOR)</h4>
<p>Many people confuse the false negative rate (<span class="math inline">\(P(\text{not } R|C)\)</span>) with its inverse <a href="base-rates.html#sec-conditionalProbability">conditional probability</a>, the false omission rate (<span class="math inline">\(P(C|\text{not } R)\)</span>). The false omission rate (FOR) is the conditional probability of having the condition given a negative test: <span class="math inline">\(P(C|\text{not } R)\)</span>. Lower values reflect greater accuracy. The formula for calculating false omission rate is in <a href="#sec-falseOmissionRate" class="quarto-xref"><span>Section 17.6.8.21</span></a>.</p>
<p><span id="eq-falseOmissionRate"><span class="math display">\[
\begin{aligned}
  \text{false omission rate (FOR)} &amp;= P(C|\text{not } R) \\
  &amp;= \frac{\text{FN}}{\text{FN} + \text{TN}} = 1 - \text{NPV}
\end{aligned}
\tag{17.22}\]</span></span></p>
</section><section id="sec-youdenJ-example" class="level4" data-number="17.6.8.22"><h4 data-number="17.6.8.22" class="anchored" data-anchor-id="sec-youdenJ-example">
<span class="header-section-number">17.6.8.22</span> Youden’s J Statistic</h4>
<p>Youden’s J statistic is also called Youden’s Index or informedness. Youden’s J statistic is the sum of <a href="#sec-sensitivity">sensitivity</a> and <a href="#sec-specificity">specificity</a> (and subtracting one). Higher values reflect greater accuracy. The formula for calculating Youden’s J statistic is in <a href="#eq-youdenIndex" class="quarto-xref">Equation&nbsp;<span>17.23</span></a>.</p>
<p><span id="eq-youdenIndex"><span class="math display">\[
\begin{aligned}
  \text{Youden's J statistic} &amp;= \text{sensitivity} + \text{specificity} - 1
\end{aligned}
\tag{17.23}\]</span></span></p>
</section><section id="sec-balancedAccuracy" class="level4" data-number="17.6.8.23"><h4 data-number="17.6.8.23" class="anchored" data-anchor-id="sec-balancedAccuracy">
<span class="header-section-number">17.6.8.23</span> Balanced Accuracy</h4>
<p>Balanced accuracy is the average of <a href="#sec-sensitivity">sensitivity</a> and <a href="#sec-specificity">specificity</a>. Higher values reflect greater accuracy. The formula for calculating balanced accuracy is in <a href="#eq-balancedAccuracy" class="quarto-xref">Equation&nbsp;<span>17.24</span></a>.</p>
<p><span id="eq-balancedAccuracy"><span class="math display">\[
\begin{aligned}
  \text{balanced accuracy} &amp;= \frac{\text{sensitivity} + \text{specificity}}{2}
\end{aligned}
\tag{17.24}\]</span></span></p>
</section><section id="sec-fScore" class="level4" data-number="17.6.8.24"><h4 data-number="17.6.8.24" class="anchored" data-anchor-id="sec-fScore">
<span class="header-section-number">17.6.8.24</span> F-Score</h4>
<p>The F-score combines <a href="#sec-ppv">precision</a> (<a href="#sec-ppv">positive predictive value</a>) and <a href="#sec-sensitivity">recall</a> (<a href="#sec-sensitivity">sensitivity</a>), where <span class="math inline">\(\beta\)</span> indicates how many times more important <a href="#sec-sensitivity">sensitivity</a> is than the <a href="#sec-ppv">positive predictive value</a>. If <a href="#sec-sensitivity">sensitivity</a> and the <a href="#sec-ppv">positive predictive value</a> are equally important, <span class="math inline">\(\beta = 1\)</span>, and the F-score is called the <span class="math inline">\(F_1\)</span> score. Higher values reflect greater accuracy. The formula for calculating the F-score is in <a href="#eq-FScore" class="quarto-xref">Equation&nbsp;<span>17.25</span></a>.</p>
<p><span id="eq-FScore"><span class="math display">\[
\begin{aligned}
  F_\beta &amp;= (1 + \beta^2) \cdot \frac{\text{positive predictive value} \cdot \text{sensitivity}}{(\beta^2 \cdot \text{positive predictive value}) + \text{sensitivity}} \\
  &amp;= \frac{(1 + \beta^2) \cdot \text{TP}}{(1 + \beta^2) \cdot \text{TP} + \beta^2 \cdot \text{FN} + \text{FP}}
\end{aligned}
\tag{17.25}\]</span></span></p>
<p>The formula for calculating the <span class="math inline">\(F_1\)</span> score is in <a href="#eq-F1Score" class="quarto-xref">Equation&nbsp;<span>17.26</span></a>.</p>
<p><span id="eq-F1Score"><span class="math display">\[
\begin{aligned}
  F_1 &amp;= \frac{2 \cdot \text{positive predictive value} \cdot \text{sensitivity}}{(\text{positive predictive value}) + \text{sensitivity}} \\
  &amp;= \frac{2 \cdot \text{TP}}{2 \cdot \text{TP} + \text{FN} + \text{FP}}
\end{aligned}
\tag{17.26}\]</span></span></p>
</section><section id="sec-matthewsCorrelationCoefficient" class="level4" data-number="17.6.8.25"><h4 data-number="17.6.8.25" class="anchored" data-anchor-id="sec-matthewsCorrelationCoefficient">
<span class="header-section-number">17.6.8.25</span> Matthews Correlation Coefficient (MCC)</h4>
<p>The Matthews correlation coefficient (MCC) is also called the phi coefficient. It is a correlation coefficient between predicted and observed values from a binary classification. Higher values reflect greater accuracy. The formula for calculating the MCC is in <a href="#eq-matthewsCorrelationCoefficient" class="quarto-xref">Equation&nbsp;<span>17.27</span></a>.</p>
<p><span id="eq-matthewsCorrelationCoefficient"><span class="math display">\[
\begin{aligned}
  \text{MCC} &amp;= \frac{\text{TP} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP} + \text{FP})(\text{TP} + \text{FN})(\text{TN} + \text{FP})(\text{TN} + \text{FN})}}
\end{aligned}
\tag{17.27}\]</span></span></p>
</section><section id="sec-diagnosticOddsRatio" class="level4" data-number="17.6.8.26"><h4 data-number="17.6.8.26" class="anchored" data-anchor-id="sec-diagnosticOddsRatio">
<span class="header-section-number">17.6.8.26</span> Diagnostic Odds Ratio</h4>
<p>The diagnostic odds ratio is the odds of a positive test among people with the condition relative to the odds of a positive test among people without the condition. Higher values reflect greater accuracy. The formula for calculating the diagnostic odds ratio is in <a href="#eq-diagnosticOddsRatio" class="quarto-xref">Equation&nbsp;<span>17.28</span></a>. If the predictor is bad, the diagnostic odds ratio could be less than one, and values can go up from there. If the diagnostic odds ratio is greater than 2, we take the odds ratio seriously because we are twice as likely to predict accurately than inaccurately. However, the diagnostic odds ratio ignores/hides <a href="base-rates.html#sec-baseRate">base rates</a>. When interpreting the diagnostic odds ratio, it is important to keep in mind the <a href="basic-statistics.html#sec-practicalSignificance">practical significance</a>, because otherwise it is not very meaningful. Consider a risk factor that has a diagnostic odds ratio of 3 for tuberculosis, i.e., it puts you at 3 times as likely to develop tuberculosis. The prevalence of tuberculosis is relatively low. Assuming the prevalence of tuberculosis is less than 1/10th of 1%, your risk of developing tuberculosis is still very low even if the risk factor (with a diagnostic odds ratio of 3) is present.</p>
<p><span id="eq-diagnosticOddsRatio"><span class="math display">\[
\begin{aligned}
  \text{diagnostic odds ratio} &amp;= \frac{\text{TP} \times \text{TN}}{\text{FP} \times \text{FN}} \\
  &amp;= \frac{\text{sensitivity} \times \text{specificity}}{(1 - \text{sensitivity}) \times (1 - \text{specificity})} \\
  &amp;= \frac{\text{PPV} \times \text{NPV}}{(1 - \text{PPV}) \times (1 - \text{NPV})} \\
  &amp;= \frac{\text{LR+}}{\text{LR}-}
\end{aligned}
\tag{17.28}\]</span></span></p>
</section><section id="sec-diagnosticLikelihoodRatio2" class="level4" data-number="17.6.8.27"><h4 data-number="17.6.8.27" class="anchored" data-anchor-id="sec-diagnosticLikelihoodRatio2">
<span class="header-section-number">17.6.8.27</span> Diagnostic Likelihood Ratio</h4>
<p>The diagnostic likelihood ratio is described in <a href="base-rates.html#sec-diagnosticLikelihoodRatio" class="quarto-xref"><span>Section 16.8.2.1</span></a>. There are two types of diagnostic likelihood ratios: the <a href="base-rates.html#sec-positiveLikelihoodRatio">positive likelihood ratio</a> and the <a href="base-rates.html#sec-negativeLikelihoodRatio">negative likelihood ratio</a>.</p>
<section id="sec-positiveLikelihoodRatio2" class="level5" data-number="17.6.8.27.1"><h5 data-number="17.6.8.27.1" class="anchored" data-anchor-id="sec-positiveLikelihoodRatio2">
<span class="header-section-number">17.6.8.27.1</span> Positive Likelihood Ratio (LR+)</h5>
<p>The positive likelihood ratio (LR+) is described in <a href="base-rates.html#sec-positiveLikelihoodRatio" class="quarto-xref"><span>Section 16.8.2.1.1</span></a>. The formula for calculating the positive likelihood ratio is in <a href="base-rates.html#eq-positiveLikelihoodRatio" class="quarto-xref">Equation&nbsp;<span>16.22</span></a>.</p>
</section><section id="sec-negativeLikelihoodRatio2" class="level5" data-number="17.6.8.27.2"><h5 data-number="17.6.8.27.2" class="anchored" data-anchor-id="sec-negativeLikelihoodRatio2">
<span class="header-section-number">17.6.8.27.2</span> Negative Likelihood Ratio (LR−)</h5>
<p>The negative likelihood ratio (LR−) is described in <a href="base-rates.html#sec-negativeLikelihoodRatio" class="quarto-xref"><span>Section 16.8.2.1.2</span></a>. The formula for calculating the negative likelihood ratio is in <a href="base-rates.html#eq-positiveLikelihoodRatio" class="quarto-xref">Equation&nbsp;<span>16.22</span></a>.</p>
</section></section><section id="sec-posttestOdds" class="level4" data-number="17.6.8.28"><h4 data-number="17.6.8.28" class="anchored" data-anchor-id="sec-posttestOdds">
<span class="header-section-number">17.6.8.28</span> Posttest Odds</h4>
<p>As presented in <a href="base-rates.html#eq-bayes5" class="quarto-xref">Equation&nbsp;<span>16.21</span></a>, the posttest (or posterior) odds are equal to the <a href="#sec-pretestOdds">pretest odds</a> multiplied by the <a href="base-rates.html#sec-diagnosticLikelihoodRatio">likelihood ratio</a>. The posttest odds and <a href="#sec-posttestProbability">posttest probability</a> can be useful to calculate when the <a href="base-rates.html#sec-baseRate">pretest probability</a> is different from the <a href="base-rates.html#sec-baseRate">pretest probability</a> (or prevalence) of the classification. For instance, you might use a different <a href="base-rates.html#sec-baseRate">pretest probability</a> if a test result is already known and you want to know the updated <a href="#sec-posttestProbability">posttest probability</a> after conducting a second test. The formula for calculating posttest odds is in <a href="#eq-posttestOdds" class="quarto-xref">Equation&nbsp;<span>17.29</span></a>.</p>
<p><span id="eq-posttestOdds"><span class="math display">\[
\begin{aligned}
  \text{posttest odds} &amp;= \text{pretest odds} \times \text{likelihood ratio} \\
\end{aligned}
\tag{17.29}\]</span></span></p>
<p>For calculating the posttest odds of a <a href="#sec-truePositive">true positive</a> compared to a <a href="#sec-falsePositive">false positive</a>, we use the <a href="base-rates.html#sec-positiveLikelihoodRatio">positive likelihood ratio</a> below. We would use the <a href="base-rates.html#sec-negativeLikelihoodRatio">negative likelihood ratio</a> if we wanted to calculate the posttest odds of a <a href="#sec-falseNegative">false negative</a> compared to a <a href="#sec-trueNegative">true negative</a>.</p>
</section><section id="sec-posttestProbability" class="level4" data-number="17.6.8.29"><h4 data-number="17.6.8.29" class="anchored" data-anchor-id="sec-posttestProbability">
<span class="header-section-number">17.6.8.29</span> Posttest Probability</h4>
<p>The posttest probability is the probability of having the characteristic given a test result. When the <a href="base-rates.html#sec-baseRate">base rate</a> is used as the <a href="base-rates.html#sec-baseRate">pretest probability</a>, the posttest probability given a positive test is equal to <a href="#sec-ppv">positive predictive value</a>. To convert odds to a probability, divide the odds by one plus the odds, as is in <a href="#eq-posttestProbability" class="quarto-xref">Equation&nbsp;<span>17.30</span></a>.</p>
<p><span id="eq-posttestProbability"><span class="math display">\[
\begin{aligned}
  \text{posttest probability} &amp;= \frac{\text{posttest odds}}{1 + \text{posttest odds}}
\end{aligned}
\tag{17.30}\]</span></span></p>
</section><section id="sec-miscalibration" class="level4" data-number="17.6.8.30"><h4 data-number="17.6.8.30" class="anchored" data-anchor-id="sec-miscalibration">
<span class="header-section-number">17.6.8.30</span> Mean Difference Between Predicted and Observed Values</h4>
<p>The mean difference between predicted values versus observed values at a given cutoff is an index of <a href="#sec-calibration">miscalibration</a> of predictions at that cutoff. It is called “calibration-in-the-small” (as opposed to calibration-in-the-large, which spans all cutoffs). Values closer to zero reflect greater accuracy. Values above zero indicate that the predicted values are, on average, greater than the observed values. Values below zero indicate that the observed values are, on average, greater than the predicted values.</p>
</section></section></section><section id="sec-thresholdIndependentAccuracy" class="level2" data-number="17.7"><h2 data-number="17.7" class="anchored" data-anchor-id="sec-thresholdIndependentAccuracy">
<span class="header-section-number">17.7</span> Threshold-Independent Accuracy Indices</h2>
<p>This section describes threshold-independent indexes of accuracy. That is, each index of accuracy described in this section provides a single numerical index of accuracy that aggregates the accuracy across all possible cutoffs. The <a href="https://github.com/DevPsyLab/petersenlab"><code>petersenlab</code></a> package <span class="citation" data-cites="R-petersenlab">(<a href="references.html#ref-R-petersenlab" role="doc-biblioref">Petersen, 2025a</a>)</span> contains the <code>accuracyOverall()</code> function that computes many threshold-independent accuracy indices.</p>
<section id="sec-generalPredictionAccuracy" class="level3" data-number="17.7.1"><h3 data-number="17.7.1" class="anchored" data-anchor-id="sec-generalPredictionAccuracy">
<span class="header-section-number">17.7.1</span> General Prediction Accuracy</h3>
<p>There are many metrics of general prediction accuracy. When thinking about which metric(s) may be best for a given problem, it is important to consider the purpose of the assessment. The estimates of general prediction accuracy are separated below into <a href="#sec-scaleDependentAccuracy">scale-dependent</a> and <a href="#sec-scaleIndependentAccuracy">scale-independent</a> accuracy estimates.</p>
<section id="sec-scaleDependentAccuracy" class="level4" data-number="17.7.1.1"><h4 data-number="17.7.1.1" class="anchored" data-anchor-id="sec-scaleDependentAccuracy">
<span class="header-section-number">17.7.1.1</span> Scale-Dependent Accuracy Estimates</h4>
<p>The estimates of prediction accuracy described in this section are scale-dependent. These accuracy estimates depend on the unit of measurement and therefore cannot be compared across measures with different scales or across data sets.</p>
<section id="sec-meanError" class="level5" data-number="17.7.1.1.1"><h5 data-number="17.7.1.1.1" class="anchored" data-anchor-id="sec-meanError">
<span class="header-section-number">17.7.1.1.1</span> Mean Error</h5>
<p>Here, “error” (<span class="math inline">\(e\)</span>) is the difference between the predicted and observed value for a given individual (<span class="math inline">\(i\)</span>). Mean error (ME; also known as bias) is the mean difference between the predicted and observed values across individuals (<span class="math inline">\(i\)</span>), that is, the mean of the errors across individuals (<span class="math inline">\(e_i\)</span>). Values closer to zero reflect greater accuracy. If mean error is above zero, it indicates that predicted values are, on average, greater than observed values (i.e., overestimating errors). If mean error is below zero, it indicates that predicted values are, on average, less than observed values (i.e., underestimating errors). If both over-estimating and under-estimating errors are present, however, they can cancel each other out. As a result, even with a mean error of zero, there can still be considerable error present. Thus, although mean error can be helpful for examining whether predictions systematically under- or over-estimate the actual scores, other forms of accuracy are necessary to examine the <em>extent</em> of error. The formula for mean error is in <a href="#eq-meanError" class="quarto-xref">Equation&nbsp;<span>17.31</span></a>:</p>
<p><span id="eq-meanError"><span class="math display">\[
\begin{aligned}
  \text{mean error} &amp;= \frac{\sum\limits_{i = 1}^n(\text{predicted}_i - \text{observed}_i)}{n} \\
  &amp;= \text{mean}(e_i)
\end{aligned}
\tag{17.31}\]</span></span></p>
</section><section id="sec-meanAbsoluteError" class="level5" data-number="17.7.1.1.2"><h5 data-number="17.7.1.1.2" class="anchored" data-anchor-id="sec-meanAbsoluteError">
<span class="header-section-number">17.7.1.1.2</span> Mean Absolute Error (MAE)</h5>
<p>Mean absolute error (MAE) is the mean of the absolute value of differences between the predicted and observed values across individuals, that is, the mean of the absolute value of errors. Smaller MAE values (closer to zero) reflect greater accuracy. MAE is preferred over <a href="#sec-rootMeanSquaredError">root mean squared error</a> (RMSE) when you want to give equal weight to all errors and when the outliers have considerable impact. The formula for MAE is in <a href="#eq-meanAbsoluteError" class="quarto-xref">Equation&nbsp;<span>17.32</span></a>:</p>
<p><span id="eq-meanAbsoluteError"><span class="math display">\[
\begin{aligned}
  \text{mean absolute error (MAE)} &amp;= \frac{\sum\limits_{i = 1}^n|\text{predicted}_i - \text{observed}_i|}{n} \\
  &amp;= \text{mean}(|e_i|)
\end{aligned}
\tag{17.32}\]</span></span></p>
</section><section id="sec-meanSquaredError" class="level5" data-number="17.7.1.1.3"><h5 data-number="17.7.1.1.3" class="anchored" data-anchor-id="sec-meanSquaredError">
<span class="header-section-number">17.7.1.1.3</span> Mean Squared Error (MSE)</h5>
<p>Mean squared error (MSE) is the mean of the square of the differences between the predicted and observed values across individuals, that is, the mean of the squared value of errors. Smaller MSE values (closer to zero) reflect greater accuracy. MSE penalizes larger errors more heavily than smaller errors (unlike <a href="#sec-meanAbsoluteError">MAE</a>). However, MSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for MSE is in <a href="#eq-meanSquaredError" class="quarto-xref">Equation&nbsp;<span>17.33</span></a>:</p>
<p><span id="eq-meanSquaredError"><span class="math display">\[
\begin{aligned}
  \text{mean squared error (MSE)} &amp;= \frac{\sum\limits_{i = 1}^n(\text{predicted}_i - \text{observed}_i)^2}{n} \\
  &amp;= \text{mean}(e_i^2)
\end{aligned}
\tag{17.33}\]</span></span></p>
</section><section id="sec-rootMeanSquaredError" class="level5" data-number="17.7.1.1.4"><h5 data-number="17.7.1.1.4" class="anchored" data-anchor-id="sec-rootMeanSquaredError">
<span class="header-section-number">17.7.1.1.4</span> Root Mean Squared Error (RMSE)</h5>
<p>Root mean squared error (RMSE) is the square root of the mean of the square of the differences between the predicted and observed values across individuals, that is, the root mean squared value of errors. Smaller RMSE values (closer to zero) reflect greater accuracy. RMSE penalizes larger errors more heavily than smaller errors (unlike <a href="#sec-meanAbsoluteError">MAE</a>). However, RMSE is sensitive to outliers and can be impacted if the errors are skewed. The formula for RMSE is in <a href="#eq-rootMeanSquaredError" class="quarto-xref">Equation&nbsp;<span>17.34</span></a>:</p>
<p><span id="eq-rootMeanSquaredError"><span class="math display">\[
\begin{aligned}
  \text{root mean squared error (RMSE)} &amp;= \sqrt{\frac{\sum\limits_{i = 1}^n(\text{predicted}_i - \text{observed}_i)^2}{n}} \\
  &amp;= \sqrt{\text{mean}(e_i^2)}
\end{aligned}
\tag{17.34}\]</span></span></p>
</section></section><section id="sec-scaleIndependentAccuracy" class="level4" data-number="17.7.1.2"><h4 data-number="17.7.1.2" class="anchored" data-anchor-id="sec-scaleIndependentAccuracy">
<span class="header-section-number">17.7.1.2</span> Scale-Independent Accuracy Estimates</h4>
<p>The estimates of prediction accuracy described in this section are intended to be scale-<em>independent</em> (unit-free) so the accuracy estimates can be compared across measures with different scales or across data sets <span class="citation" data-cites="Hyndman2021">(<a href="references.html#ref-Hyndman2021" role="doc-biblioref">Hyndman &amp; Athanasopoulos, 2021</a>)</span>.</p>
<section id="sec-meanPercentageError" class="level5" data-number="17.7.1.2.1"><h5 data-number="17.7.1.2.1" class="anchored" data-anchor-id="sec-meanPercentageError">
<span class="header-section-number">17.7.1.2.1</span> Mean Percentage Error (MPE)</h5>
<p>Mean percentage error (MPE) values closer to zero reflect greater accuracy. The formula for percentage error is in <a href="#eq-percentageError" class="quarto-xref">Equation&nbsp;<span>17.35</span></a>:</p>
<p><span id="eq-percentageError"><span class="math display">\[
\begin{aligned}
  \text{percentage error }(p_i) = \frac{100\% \times (\text{observed}_i - \text{predicted}_i)}{\text{observed}_i}
\end{aligned}
\tag{17.35}\]</span></span></p>
<p>We then take the mean of the percentage errors to get MPE. The formula for MPE is in <a href="#eq-meanPercentageError" class="quarto-xref">Equation&nbsp;<span>17.36</span></a>:</p>
<p><span id="eq-meanPercentageError"><span class="math display">\[
\begin{aligned}
  \text{mean percentage error (MPE)} &amp;= \frac{100\%}{n} \sum\limits_{i = 1}^n \frac{\text{observed}_i - \text{predicted}_i}{\text{observed}_i} \\
  &amp;= \text{mean(percentage error)} \\
  &amp;= \text{mean}(p_i)
\end{aligned}
\tag{17.36}\]</span></span></p>
<p>Note: MPE is undefined when one or more of the observed values equals zero, due to division by zero. The <code>accuracyOverall()</code> function of the <a href="https://github.com/DevPsyLab/petersenlab"><code>petersenlab</code></a> package <span class="citation" data-cites="R-petersenlab">(<a href="references.html#ref-R-petersenlab" role="doc-biblioref">Petersen, 2025a</a>)</span> provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.</p>
</section><section id="sec-meanAbsolutePercentageError" class="level5" data-number="17.7.1.2.2"><h5 data-number="17.7.1.2.2" class="anchored" data-anchor-id="sec-meanAbsolutePercentageError">
<span class="header-section-number">17.7.1.2.2</span> Mean Absolute Percentage Error (MAPE)</h5>
<p>Smaller mean absolute percentage error (MAPE) values (closer to zero) reflect greater accuracy. The formula for MAPE is in <a href="#eq-meanAbsolutePercentageError" class="quarto-xref">Equation&nbsp;<span>17.37</span></a>. MAPE is asymmetric because it overweights underestimates and underweights overestimates. MAPE can be preferable to <a href="#sec-symmetricMeanAbsolutePercentageError">symmetric mean absolute percentage error</a> (sMAPE) if there are no observed values of zero and if you want to emphasize the importance of underestimates (relative to overestimates).</p>
<p><span id="eq-meanAbsolutePercentageError"><span class="math display">\[
\begin{aligned}
  \text{mean absolute percentage error (MAPE)} &amp;= \frac{100\%}{n} \sum\limits_{i = 1}^n \Bigg|\frac{\text{observed}_i - \text{predicted}_i}{\text{observed}_i}\Bigg| \\
  &amp;= \text{mean(|percentage error|)} \\
  &amp;= \text{mean}(|p_i|)
\end{aligned}
\tag{17.37}\]</span></span></p>
<p>Note: MAPE is undefined when one or more of the observed values equals zero, due to division by zero. The <code>accuracyOverall()</code> function of the <a href="https://github.com/DevPsyLab/petersenlab"><code>petersenlab</code></a> package <span class="citation" data-cites="R-petersenlab">(<a href="references.html#ref-R-petersenlab" role="doc-biblioref">Petersen, 2025a</a>)</span> provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.</p>
</section><section id="sec-symmetricMeanAbsolutePercentageError" class="level5" data-number="17.7.1.2.3"><h5 data-number="17.7.1.2.3" class="anchored" data-anchor-id="sec-symmetricMeanAbsolutePercentageError">
<span class="header-section-number">17.7.1.2.3</span> Symmetric Mean Absolute Percentage Error (sMAPE)</h5>
<p>Unlike MAPE, symmetric mean absolute percentage error (sMAPE) is symmetric because it equally weights underestimates and overestimates. Smaller sMAPE values (closer to zero) reflect greater accuracy. The formula for sMAPE is in <a href="#eq-symmetricMeanAbsolutePercentageError" class="quarto-xref">Equation&nbsp;<span>17.38</span></a>:</p>
<p><span id="eq-symmetricMeanAbsolutePercentageError"><span class="math display">\[
\small
\begin{aligned}
  \text{symmetric mean absolute percentage error (sMAPE)} = \frac{100\%}{n} \sum\limits_{i = 1}^n \frac{|\text{predicted}_i - \text{observed}_i|}{|\text{predicted}_i| + |\text{observed}_i|}
\end{aligned}
\tag{17.38}\]</span></span></p>
<p>Note: sMAPE is undefined when one or more of the individuals has a prediction–observed combination such that the sum of the absolute value of the predicted value and the absolute value of the observed value equals zero (<span class="math inline">\(|\text{predicted}_i| + |\text{observed}_i|\)</span>), due to division by zero. The <code>accuracyOverall()</code> function of the <a href="https://github.com/DevPsyLab/petersenlab"><code>petersenlab</code></a> package <span class="citation" data-cites="R-petersenlab">(<a href="references.html#ref-R-petersenlab" role="doc-biblioref">Petersen, 2025a</a>)</span> provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.</p>
</section><section id="sec-meanAbsoluteScaledError" class="level5" data-number="17.7.1.2.4"><h5 data-number="17.7.1.2.4" class="anchored" data-anchor-id="sec-meanAbsoluteScaledError">
<span class="header-section-number">17.7.1.2.4</span> Mean Absolute Scaled Error (MASE)</h5>
<p>Mean absolute scaled error (MASE) is described by <span class="citation" data-cites="Hyndman2021">(<a href="references.html#ref-Hyndman2021" role="doc-biblioref">Hyndman &amp; Athanasopoulos, 2021</a>)</span>. Values closer to zero reflect greater accuracy.</p>
<p>The adapted formula for MASE with non-time series data is described by <span class="citation" data-cites="Hyndman2014">Hyndman (<a href="references.html#ref-Hyndman2014" role="doc-biblioref">2014</a>)</span> at the following link: <a href="https://stats.stackexchange.com/a/108963/20338" class="uri">https://stats.stackexchange.com/a/108963/20338</a> (archived at <a href="https://perma.cc/G469-8NAJ" class="uri">https://perma.cc/G469-8NAJ</a>). Scaled errors are calculated using <a href="#eq-scaledError" class="quarto-xref">Equation&nbsp;<span>17.39</span></a>:</p>
<p><span id="eq-scaledError"><span class="math display">\[
\begin{aligned}
  \text{scaled error}(q_i) &amp;= \frac{\text{observed}_i - \text{predicted}_i}{\text{scaling factor}} \\
  &amp;= \frac{\text{observed}_i - \text{predicted}_i}{\frac{1}{n} \sum\limits_{i = 1}^n |\text{observed}_i - \overline{\text{observed}}|}
\end{aligned}
\tag{17.39}\]</span></span></p>
<p>Then, we calculate the mean of the absolute value of the scaled errors to get MASE, as in <a href="#eq-meanAbsoluteScaledError" class="quarto-xref">Equation&nbsp;<span>17.40</span></a>:</p>
<p><span id="eq-meanAbsoluteScaledError"><span class="math display">\[
\begin{aligned}
  \text{mean absolute scaled error (MASE)} &amp;= \frac{1}{n} \sum\limits_{i = 1}^n |q_i| \\
  &amp;= \text{mean(|scaled error|)} \\
  &amp;= \text{mean}(|q_i|)
\end{aligned}
\tag{17.40}\]</span></span></p>
<p>Note: MASE is undefined when the scaling factor is zero, due to division by zero. With non-time series data, the scaling factor is the average of the absolute value of individuals’ observed scores minus the average observed score (<span class="math inline">\(\frac{1}{n} \sum\limits_{i = 1}^n |\text{observed}_i - \overline{\text{observed}}|\)</span>).</p>
</section><section id="sec-rootMeanSquaredLogError" class="level5" data-number="17.7.1.2.5"><h5 data-number="17.7.1.2.5" class="anchored" data-anchor-id="sec-rootMeanSquaredLogError">
<span class="header-section-number">17.7.1.2.5</span> Root Mean Squared Log Error (RMSLE)</h5>
<p>The squared log of the accuracy ratio is described by <span class="citation" data-cites="Tofallis2015">Tofallis (<a href="references.html#ref-Tofallis2015" role="doc-biblioref">2015</a>)</span>. The accuracy ratio is in <a href="#eq-accuracyRatio1" class="quarto-xref">Equation&nbsp;<span>17.41</span></a>:</p>
<p><span id="eq-accuracyRatio1"><span class="math display">\[
\begin{aligned}
  \text{accuracy ratio} &amp;= \frac{\text{predicted}_i}{\text{observed}_i}
\end{aligned}
\tag{17.41}\]</span></span></p>
<p>However, the accuracy ratio is undefined with observed or predicted values of zero, so it is common to modify it by adding 1 to the predictor and denominator, as in <a href="#eq-accuracyRatio2" class="quarto-xref">Equation&nbsp;<span>17.42</span></a>:</p>
<p><span id="eq-accuracyRatio2"><span class="math display">\[
\begin{aligned}
  \text{accuracy ratio} &amp;= \frac{\text{predicted}_i + 1}{\text{observed}_i + 1}
\end{aligned}
\tag{17.42}\]</span></span></p>
<p>Squaring the log values keeps the values positive, such that smaller values (values closer to zero) reflect greater accuracy. Then we take the mean of the squared log values, which keeps the values positive, and calculate the square root of the mean squared log values to put them back on the (pre-squared) log metric. This is known as the root mean squared log error (RMSLE). Division inside the log is equal to subtraction outside the log. So, the formula can be reformulated with the subtraction of two logs, as in <a href="#eq-rootMeanSquaredLogError" class="quarto-xref">Equation&nbsp;<span>17.43</span></a>:</p>
<p><span id="eq-rootMeanSquaredLogError"><span class="math display">\[
\scriptsize
\begin{aligned}
  \text{root mean squared log error (RMSLE)} &amp;= \sqrt{\sum\limits_{i = 1}^n log\bigg(\frac{\text{predicted}_i + 1}{\text{observed}_i + 1}\bigg)^2} \\
  &amp;= \sqrt{\text{mean}\Bigg[log\bigg(\frac{\text{predicted}_i + 1}{\text{observed}_i + 1}\bigg)^2\Bigg]} \\
  &amp;= \sqrt{\text{mean}\big[log(\text{accuracy ratio})^2\big]} = \sqrt{\text{mean}\Big\{\big[log(\text{predicted}_i + 1) - log(\text{actual}_i + 1)\big]^2\Big\}}
\end{aligned}
\tag{17.43}\]</span></span></p>
<p>RMSLE can be preferable when the scores have a wide range of values and are skewed. RMSLE can help to reduce the impact of outliers. RMSLE gives more weight to smaller errors in the prediction of small observed values, while also penalizing larger errors in the prediction of larger observed values. It overweights underestimates and underweights overestimates.</p>
<p>There are other variations of prediction accuracy metrics that use the log of the accuracy ratio. One variation makes it similar to median symmetric percentage error <span class="citation" data-cites="Morley2018">(<a href="references.html#ref-Morley2018" role="doc-biblioref">Morley et al., 2018</a>)</span>.</p>
<p>Note: Root mean squared log error is undefined when one or more predicted values or actual values equals −1. When predicted or actual values are -1, this leads to <span class="math inline">\(log(0)\)</span>, which is undefined. The <code>accuracyOverall()</code> function of the <a href="https://github.com/DevPsyLab/petersenlab"><code>petersenlab</code></a> package <span class="citation" data-cites="R-petersenlab">(<a href="references.html#ref-R-petersenlab" role="doc-biblioref">Petersen, 2025a</a>)</span> provides the option in the function to drop undefined values so you can still generate an estimate of accuracy despite undefined values.</p>
</section><section id="sec-coefficientOfDetermination" class="level5" data-number="17.7.1.2.6"><h5 data-number="17.7.1.2.6" class="anchored" data-anchor-id="sec-coefficientOfDetermination">
<span class="header-section-number">17.7.1.2.6</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</h5>
<p>The <a href="multiple-regression.html#sec-multipleRegressionRSquared">coefficient of determination</a> (<span class="math inline">\(R^2\)</span>) reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions: <span class="math inline">\(R^2 = \frac{\text{variance explained in }Y}{\text{total variance in }Y}\)</span>. Larger values indicate greater accuracy.</p>
<p><span class="math inline">\(R^2\)</span> is commonly estimated in <a href="multiple-regression.html">multiple regression</a>, in which multiple predictors are allowed to predict one outcome.</p>
<section id="sec-adjustedRsquared" class="level6" data-number="17.7.1.2.6.1"><h6 data-number="17.7.1.2.6.1" class="anchored" data-anchor-id="sec-adjustedRsquared">
<span class="header-section-number">17.7.1.2.6.1</span> Adjusted <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2_{adj}\)</span>)</h6>
<p>Adjusted <span class="math inline">\(R^2\)</span> is similar to the <a href="#sec-coefficientOfDetermination">coefficient of determination</a>, but it accounts for the number of predictors included in the regression model to penalize <a href="multiple-regression.html#sec-overfitting">overfitting</a>. Adjusted <span class="math inline">\(R^2\)</span> reflects the proportion of variance in the outcome (dependent) variable that is explained by the model predictions over and above what would be expected to be accounted for by chance, given the number of predictors in the model. Larger values indicate greater accuracy. The formula for adjusted <span class="math inline">\(R^2\)</span> is in <a href="multiple-regression.html#eq-adjustedRSquared" class="quarto-xref">Equation&nbsp;<span>11.4</span></a>. Adjusted <span class="math inline">\(R^2\)</span> is described further in <a href="multiple-regression.html#sec-multipleRegressionRSquared" class="quarto-xref"><span>Section 11.5</span></a>.</p>
</section><section id="sec-predictiveRsquared" class="level6" data-number="17.7.1.2.6.2"><h6 data-number="17.7.1.2.6.2" class="anchored" data-anchor-id="sec-predictiveRsquared">
<span class="header-section-number">17.7.1.2.6.2</span> Predictive <span class="math inline">\(R^2\)</span>
</h6>
<p>Predictive <span class="math inline">\(R^2\)</span> is described by <span class="citation" data-cites="Hopper2014">Hopper (<a href="references.html#ref-Hopper2014" role="doc-biblioref">2014</a>)</span> here: <a href="https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/" class="uri">https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared/</a> (archived at <a href="https://perma.cc/BK8J-HFUK" class="uri">https://perma.cc/BK8J-HFUK</a>). Predictive <span class="math inline">\(R^2\)</span> penalizes <a href="multiple-regression.html#sec-overfitting">overfitting</a>, unlike traditional <span class="math inline">\(R^2\)</span>. Larger values indicate greater accuracy.</p>
</section></section></section></section><section id="sec-discriminationIndices" class="level3" data-number="17.7.2"><h3 data-number="17.7.2" class="anchored" data-anchor-id="sec-discriminationIndices">
<span class="header-section-number">17.7.2</span> Discrimination</h3>
<p>When dealing with a categorical outcome, discrimination is the ability to separate events from non-events. When dealing with a continuous outcome, discrimination is the strength of the association between the predictor and the outcome. <a href="#sec-thresholdDependentAccuracy">Threshold-dependent</a> aspects of <a href="#sec-discrimination">discrimination</a> at a particular cutoff (e.g., <a href="#sec-sensitivity">sensitivity</a>, <a href="#sec-specificity">specificity</a>) are described in <a href="#sec-thresholdDependentAccuracy" class="quarto-xref"><span>Section 17.6</span></a>.</p>
<section id="sec-aucROC" class="level4" data-number="17.7.2.1"><h4 data-number="17.7.2.1" class="anchored" data-anchor-id="sec-aucROC">
<span class="header-section-number">17.7.2.1</span> Area under the ROC curve (AUC)</h4>
<p>The <a href="#sec-auc">area under the ROC curve (AUC)</a> is a general index of discrimination accuracy for a categorical outcome. It is also called the concordance (<span class="math inline">\(c\)</span>) statistic. Larger values reflect greater <a href="#sec-discrimination">discrimination accuracy</a>. <a href="#auc">AUC</a> was estimated using the <code>pROC</code> package <span class="citation" data-cites="R-pROC Robin2011_packages">(<a href="references.html#ref-Robin2011_packages" role="doc-biblioref">Robin et al., 2011</a>, <a href="references.html#ref-R-pROC" role="doc-biblioref">2023</a>)</span>.</p>
</section><section id="sec-standardizedRegressionCoefficient" class="level4" data-number="17.7.2.2"><h4 data-number="17.7.2.2" class="anchored" data-anchor-id="sec-standardizedRegressionCoefficient">
<span class="header-section-number">17.7.2.2</span> Effect Size (<span class="math inline">\(\beta\)</span>) of Regression</h4>
<p>The effect size of a predictor, i.e., the standardized regression coefficient is called a beta (<span class="math inline">\(\beta\)</span>) coefficient, is a general index of <a href="#sec-discrimination">discrimination accuracy</a> for a continuous outcome. Larger values reflect greater accuracy. We can obtain standardized regression coefficients by standardizing the predictors and outcome using the <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> function in <code>R</code>.</p>
</section></section><section id="sec-calibrationIndices" class="level3" data-number="17.7.3"><h3 data-number="17.7.3" class="anchored" data-anchor-id="sec-calibrationIndices">
<span class="header-section-number">17.7.3</span> Calibration</h3>
<p>When dealing with a categorical outcome, calibration is the degree to which a probabilistic estimate of an event reflects the true underlying probability of the event. When dealing with a continuous outcome, calibration is the degree to which the predicted values are close in value to the outcome values. The importance of examining calibration (in addition to <a href="#sec-discrimination">discrimination</a>) is described by <span class="citation" data-cites="Lindhiem2020">Lindhiem et al. (<a href="references.html#ref-Lindhiem2020" role="doc-biblioref">2020</a>)</span>. Calibration can be examined in several ways, including <a href="#sec-spiegelhalterZ">Spiegelhalter’s <span class="math inline">\(z\)</span></a> (see <a href="#sec-spiegelhalterZ" class="quarto-xref"><span>Section 17.7.3.2</span></a>), and the <a href="#sec-miscalibration">mean difference between predicted and observed values</a> at different binned thresholds as depicted graphically with a <a href="#sec-calibrationPlot">calibration plot</a> (see <a href="#fig-miscalibration" class="quarto-xref">Figure&nbsp;<span>17.7</span></a>).</p>
<section id="sec-calibrationPlot" class="level4" data-number="17.7.3.1"><h4 data-number="17.7.3.1" class="anchored" data-anchor-id="sec-calibrationPlot">
<span class="header-section-number">17.7.3.1</span> Calibration Plot</h4>
<p>Calibration plots can be helpful for identifying miscalibration. A calibration plot depicts the predicted probability of an event on the x-axis, and the actual (observed) probability of the event on the y-axis. The predictions are binned into a certain number of groups (commonly 10). The diagonal line reflects predictions that are perfectly calibrated. To the extent that predictions deviate from the diagonal line, the predictions are miscalibrated.</p>
<p>Well-calibrated predictions are depicted in <a href="#fig-calibrationPlot" class="quarto-xref">Figure&nbsp;<span>17.6</span></a>:</p>
<div class="cell">
<details class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Specify data</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>examplePredictionsWellCalibrated <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> .<span class="dv">1</span>)</span>
<span id="cb13-3"><a href="#cb13-3"></a>exampleOutcomesWellCalibrated <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> .<span class="dv">1</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="co"># Plot</span></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="fu">plot</span>(</span>
<span id="cb13-7"><a href="#cb13-7"></a>  examplePredictionsWellCalibrated,</span>
<span id="cb13-8"><a href="#cb13-8"></a>  exampleOutcomesWellCalibrated,</span>
<span id="cb13-9"><a href="#cb13-9"></a>  <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb13-10"><a href="#cb13-10"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb13-11"><a href="#cb13-11"></a>  <span class="at">xlab =</span> <span class="st">"Predicted Probability"</span>,</span>
<span id="cb13-12"><a href="#cb13-12"></a>  <span class="at">ylab =</span> <span class="st">"Observed Proportion"</span>,</span>
<span id="cb13-13"><a href="#cb13-13"></a>  <span class="at">bty =</span> <span class="st">"l"</span>,</span>
<span id="cb13-14"><a href="#cb13-14"></a>  <span class="at">type =</span> <span class="st">"n"</span>)</span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a><span class="fu">lines</span>(</span>
<span id="cb13-17"><a href="#cb13-17"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb13-18"><a href="#cb13-18"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb13-19"><a href="#cb13-19"></a>  <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb13-20"><a href="#cb13-20"></a>  <span class="at">col =</span> <span class="st">"#377eb8"</span>)</span>
<span id="cb13-21"><a href="#cb13-21"></a></span>
<span id="cb13-22"><a href="#cb13-22"></a><span class="fu">points</span>(</span>
<span id="cb13-23"><a href="#cb13-23"></a>  examplePredictionsWellCalibrated,</span>
<span id="cb13-24"><a href="#cb13-24"></a>  exampleOutcomesWellCalibrated,</span>
<span id="cb13-25"><a href="#cb13-25"></a>  <span class="at">cex =</span> <span class="fl">1.5</span>,</span>
<span id="cb13-26"><a href="#cb13-26"></a>  <span class="at">col =</span> <span class="st">"#e41a1c"</span>,</span>
<span id="cb13-27"><a href="#cb13-27"></a>  <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb13-28"><a href="#cb13-28"></a>  <span class="at">type =</span> <span class="st">"p"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-calibrationPlot" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Predictions that are Well-Calibrated. That is, the predicted values are close to the observed values.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-calibrationPlot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="evaluating-prediction-accuracy_files/figure-html/fig-calibrationPlot-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;17.6: Predictions that are Well-Calibrated. That is, the predicted values are close to the observed values."><img src="evaluating-prediction-accuracy_files/figure-html/fig-calibrationPlot-1.png" class="img-fluid figure-img" alt="Predictions that are Well-Calibrated. That is, the predicted values are close to the observed values." width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-calibrationPlot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.6: Predictions that are Well-Calibrated. That is, the predicted values are close to the observed values.
</figcaption></figure>
</div>
</div>
</div>
<p>The various types of general miscalibration are depicted in <a href="#fig-miscalibration" class="quarto-xref">Figure&nbsp;<span>17.7</span></a>:</p>
<div class="cell">
<details class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># Specify data</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>examplePredictions <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> .<span class="dv">1</span>)</span>
<span id="cb14-3"><a href="#cb14-3"></a>exampleOutcomes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">15</span>, .<span class="dv">3</span>, .<span class="dv">4</span>, .<span class="dv">45</span>, .<span class="dv">5</span>, .<span class="dv">55</span>, .<span class="dv">6</span>, .<span class="dv">7</span>, .<span class="dv">85</span>, <span class="dv">1</span>)</span>
<span id="cb14-4"><a href="#cb14-4"></a></span>
<span id="cb14-5"><a href="#cb14-5"></a>overPrediction <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">02</span>, .<span class="dv">05</span>, .<span class="dv">1</span>, .<span class="dv">15</span>, .<span class="dv">2</span>, .<span class="dv">3</span>, .<span class="dv">4</span>, .<span class="dv">5</span>, .<span class="dv">7</span>, <span class="dv">1</span>)</span>
<span id="cb14-6"><a href="#cb14-6"></a>underPrediction <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">3</span>, .<span class="dv">5</span>, .<span class="dv">6</span>, .<span class="dv">7</span>, .<span class="dv">8</span>, .<span class="dv">85</span>, .<span class="dv">9</span>, .<span class="dv">95</span>, .<span class="dv">98</span>, <span class="dv">1</span>)</span>
<span id="cb14-7"><a href="#cb14-7"></a>overExtremity <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">3</span>, .<span class="dv">38</span>, .<span class="dv">42</span>, .<span class="dv">47</span>, .<span class="dv">5</span>, .<span class="dv">53</span>, .<span class="dv">58</span>, .<span class="dv">62</span>, .<span class="dv">7</span>, <span class="dv">1</span>)</span>
<span id="cb14-8"><a href="#cb14-8"></a>underExtremity <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">05</span>, .<span class="dv">08</span>, .<span class="dv">11</span>, .<span class="dv">2</span>, .<span class="dv">5</span>, .<span class="dv">8</span>, .<span class="dv">89</span>, .<span class="dv">92</span>, .<span class="dv">95</span>, <span class="dv">1</span>)</span>
<span id="cb14-9"><a href="#cb14-9"></a></span>
<span id="cb14-10"><a href="#cb14-10"></a><span class="co"># Plot</span></span>
<span id="cb14-11"><a href="#cb14-11"></a><span class="fu">par</span>(</span>
<span id="cb14-12"><a href="#cb14-12"></a>  <span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb14-13"><a href="#cb14-13"></a>  <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>) <span class="sc">+</span> <span class="fl">0.1</span>) <span class="co">#margins: bottom, left, top, right</span></span>
<span id="cb14-14"><a href="#cb14-14"></a></span>
<span id="cb14-15"><a href="#cb14-15"></a><span class="fu">plot</span>(</span>
<span id="cb14-16"><a href="#cb14-16"></a>  examplePredictions,</span>
<span id="cb14-17"><a href="#cb14-17"></a>  overExtremity,</span>
<span id="cb14-18"><a href="#cb14-18"></a>  <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-19"><a href="#cb14-19"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-20"><a href="#cb14-20"></a>  <span class="at">main =</span> <span class="st">"Overextremity"</span>,</span>
<span id="cb14-21"><a href="#cb14-21"></a>  <span class="at">xlab =</span> <span class="st">"Predicted Probability"</span>,</span>
<span id="cb14-22"><a href="#cb14-22"></a>  <span class="at">ylab =</span> <span class="st">"Observed Proportion"</span>,</span>
<span id="cb14-23"><a href="#cb14-23"></a>  <span class="at">bty =</span> <span class="st">"l"</span>,</span>
<span id="cb14-24"><a href="#cb14-24"></a>  <span class="at">cex =</span> <span class="fl">1.5</span>,</span>
<span id="cb14-25"><a href="#cb14-25"></a>  <span class="at">col =</span> <span class="st">"#e41a1c"</span>,</span>
<span id="cb14-26"><a href="#cb14-26"></a>  <span class="at">type =</span> <span class="st">"o"</span>)</span>
<span id="cb14-27"><a href="#cb14-27"></a></span>
<span id="cb14-28"><a href="#cb14-28"></a><span class="fu">lines</span>(</span>
<span id="cb14-29"><a href="#cb14-29"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-30"><a href="#cb14-30"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-31"><a href="#cb14-31"></a>  <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb14-32"><a href="#cb14-32"></a>  <span class="at">col =</span> <span class="st">"#377eb8"</span>)</span>
<span id="cb14-33"><a href="#cb14-33"></a></span>
<span id="cb14-34"><a href="#cb14-34"></a><span class="fu">plot</span>(</span>
<span id="cb14-35"><a href="#cb14-35"></a>  examplePredictions,</span>
<span id="cb14-36"><a href="#cb14-36"></a>  underExtremity,</span>
<span id="cb14-37"><a href="#cb14-37"></a>  <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-38"><a href="#cb14-38"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-39"><a href="#cb14-39"></a>  <span class="at">main =</span> <span class="st">"Underextremity"</span>,</span>
<span id="cb14-40"><a href="#cb14-40"></a>  <span class="at">xlab =</span> <span class="st">"Predicted Probability"</span>,</span>
<span id="cb14-41"><a href="#cb14-41"></a>  <span class="at">ylab =</span> <span class="st">"Observed Proportion"</span>,</span>
<span id="cb14-42"><a href="#cb14-42"></a>  <span class="at">bty =</span> <span class="st">"l"</span>,</span>
<span id="cb14-43"><a href="#cb14-43"></a>  <span class="at">cex =</span> <span class="fl">1.5</span>,</span>
<span id="cb14-44"><a href="#cb14-44"></a>  <span class="at">col =</span> <span class="st">"#e41a1c"</span>,</span>
<span id="cb14-45"><a href="#cb14-45"></a>  <span class="at">type =</span> <span class="st">"o"</span>)</span>
<span id="cb14-46"><a href="#cb14-46"></a></span>
<span id="cb14-47"><a href="#cb14-47"></a><span class="fu">lines</span>(</span>
<span id="cb14-48"><a href="#cb14-48"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-49"><a href="#cb14-49"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-50"><a href="#cb14-50"></a>  <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb14-51"><a href="#cb14-51"></a>  <span class="at">col =</span> <span class="st">"#377eb8"</span>)</span>
<span id="cb14-52"><a href="#cb14-52"></a></span>
<span id="cb14-53"><a href="#cb14-53"></a><span class="fu">plot</span>(</span>
<span id="cb14-54"><a href="#cb14-54"></a>  examplePredictions,</span>
<span id="cb14-55"><a href="#cb14-55"></a>  overPrediction,</span>
<span id="cb14-56"><a href="#cb14-56"></a>  <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-57"><a href="#cb14-57"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-58"><a href="#cb14-58"></a>  <span class="at">main =</span> <span class="st">"Overprediction"</span>,</span>
<span id="cb14-59"><a href="#cb14-59"></a>  <span class="at">xlab =</span> <span class="st">"Predicted Probability"</span>,</span>
<span id="cb14-60"><a href="#cb14-60"></a>  <span class="at">ylab =</span> <span class="st">"Observed Proportion"</span>,</span>
<span id="cb14-61"><a href="#cb14-61"></a>  <span class="at">bty =</span> <span class="st">"l"</span>,</span>
<span id="cb14-62"><a href="#cb14-62"></a>  <span class="at">cex =</span> <span class="fl">1.5</span>,</span>
<span id="cb14-63"><a href="#cb14-63"></a>  <span class="at">col =</span> <span class="st">"#e41a1c"</span>,</span>
<span id="cb14-64"><a href="#cb14-64"></a>  <span class="at">type =</span> <span class="st">"o"</span>)</span>
<span id="cb14-65"><a href="#cb14-65"></a></span>
<span id="cb14-66"><a href="#cb14-66"></a><span class="fu">lines</span>(</span>
<span id="cb14-67"><a href="#cb14-67"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-68"><a href="#cb14-68"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-69"><a href="#cb14-69"></a>  <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb14-70"><a href="#cb14-70"></a>  <span class="at">col =</span> <span class="st">"#377eb8"</span>)</span>
<span id="cb14-71"><a href="#cb14-71"></a></span>
<span id="cb14-72"><a href="#cb14-72"></a><span class="fu">plot</span>(</span>
<span id="cb14-73"><a href="#cb14-73"></a>  examplePredictions,</span>
<span id="cb14-74"><a href="#cb14-74"></a>  underPrediction,</span>
<span id="cb14-75"><a href="#cb14-75"></a>  <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-76"><a href="#cb14-76"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-77"><a href="#cb14-77"></a>  <span class="at">main =</span> <span class="st">"Underprediction"</span>,</span>
<span id="cb14-78"><a href="#cb14-78"></a>  <span class="at">xlab =</span> <span class="st">"Predicted Probability"</span>,</span>
<span id="cb14-79"><a href="#cb14-79"></a>  <span class="at">ylab =</span> <span class="st">"Observed Proportion"</span>,</span>
<span id="cb14-80"><a href="#cb14-80"></a>  <span class="at">bty =</span> <span class="st">"l"</span>,</span>
<span id="cb14-81"><a href="#cb14-81"></a>  <span class="at">cex =</span> <span class="fl">1.5</span>,</span>
<span id="cb14-82"><a href="#cb14-82"></a>  <span class="at">col =</span> <span class="st">"#e41a1c"</span>,</span>
<span id="cb14-83"><a href="#cb14-83"></a>  <span class="at">type =</span> <span class="st">"o"</span>)</span>
<span id="cb14-84"><a href="#cb14-84"></a></span>
<span id="cb14-85"><a href="#cb14-85"></a><span class="fu">lines</span>(</span>
<span id="cb14-86"><a href="#cb14-86"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-87"><a href="#cb14-87"></a>  <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb14-88"><a href="#cb14-88"></a>  <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb14-89"><a href="#cb14-89"></a>  <span class="at">col =</span> <span class="st">"#377eb8"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output-display">
<div id="fig-miscalibration" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Types of Miscalibration. From @Petersen2024a and @PetersenPrinciplesPsychAssessment.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-miscalibration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="evaluating-prediction-accuracy_files/figure-html/fig-miscalibration-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;17.7: Types of Miscalibration. From @Petersen2024a and @PetersenPrinciplesPsychAssessment."><img src="evaluating-prediction-accuracy_files/figure-html/fig-miscalibration-1.png" class="img-fluid figure-img" alt="Types of Miscalibration. From @Petersen2024a and @PetersenPrinciplesPsychAssessment." width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-miscalibration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.7: Types of Miscalibration. From <span class="citation" data-cites="Petersen2024a">Petersen (<a href="references.html#ref-Petersen2024a" role="doc-biblioref">2024</a>)</span> and <span class="citation" data-cites="PetersenPrinciplesPsychAssessment">Petersen (<a href="references.html#ref-PetersenPrinciplesPsychAssessment" role="doc-biblioref">2025b</a>)</span>.
</figcaption></figure>
</div>
</div>
</div>
<p>However, predictions could also be miscalibrated in more specific ways. For instance, predictions could be well-calibrated at all predicted probabilities except for a given predicted probability (e.g., 20%). Or, the predictions could be miscalibrated but not systematically over- or underpredicted. Thus, it is important to evaluate a calibration plot to evaluate the extent to which the predictions are miscalibrated and the pattern of that miscalibration.</p>
</section><section id="sec-spiegelhalterZ" class="level4" data-number="17.7.3.2"><h4 data-number="17.7.3.2" class="anchored" data-anchor-id="sec-spiegelhalterZ">
<span class="header-section-number">17.7.3.2</span> Spiegelhalter’s <em>z</em>
</h4>
<p>Spiegelhalter’s <em>z</em> was calculated using the <code>rms</code> package <span class="citation" data-cites="R-rms">(<a href="references.html#ref-R-rms" role="doc-biblioref">Harrell, Jr., 2024</a>)</span>. Smaller <em>z</em> values (and larger associated <em>p</em>-values) reflect greater <a href="#calibration">calibration accuracy</a>. A statistically significant Spiegelhalter’s <em>z</em> (<em>p</em> &lt; .05) indicates a significant degree of miscalibration.</p>
</section><section id="sec-calibrationContinuous" class="level4" data-number="17.7.3.3"><h4 data-number="17.7.3.3" class="anchored" data-anchor-id="sec-calibrationContinuous">
<span class="header-section-number">17.7.3.3</span> Calibration for predicting a continuous outcome</h4>
<p>When predicting a continuous outcome, <a href="#sec-calibration">calibration</a> of the predicted values in relation to the outcome values can be examined in multiple ways including:</p>
<ul>
<li>in a <a href="#sec-calibrationPlot">calibration plot</a>, the extent to which the intercept is near zero and the slope is near one</li>
<li>in a <a href="#sec-calibrationPlot">calibration plot</a>, the extent to which the 95% confidence interval of the observed value, across all values of the predicted values, includes the diagonal reference line with an intercept of zero and a slope of one</li>
<li><a href="#sec-meanError">mean error</a></li>
<li><a href="#sec-meanAbsoluteError">mean absolute error</a></li>
<li><a href="#sec-meanSquaredError">mean squared error</a></li>
<li><a href="#sec-rootMeanSquaredError">root mean squared error</a></li>
</ul>
<p>With a plot of the predictions on the x-axis, and the outcomes on the y-axis (i.e., a <a href="#sec-calibrationPlot">calibration plot</a>), <a href="#sec-calibration">calibration</a> can be examined graphically as the extent to which the best-fit regression line has an intercept (alpha) close to zero and a slope (beta) close to one <span class="citation" data-cites="Steyerberg2014 Stevens2020">(<a href="references.html#ref-Stevens2020" role="doc-biblioref">Stevens &amp; Poppe, 2020</a>; <a href="references.html#ref-Steyerberg2014" role="doc-biblioref">Steyerberg &amp; Vergouwe, 2014</a>)</span>. The intercept is also called “calibration-in-the-large”, whereas “calibration-in-the-small” refers to the extent to which the predicted values match the observed values at a specific predicted value (e.g., when the weather forecaster says that there is a 10% chance of rain, does it actually rain 10% of the time?). For predictions to be well <a href="#sec-calibration">calibrated</a>, the intercept should be close to zero and the slope should be close to one. If the slope is close to one but the intercept is not close to zero (or the intercept is close to zero but the slope is not close to one), the predictions would not be considered well <a href="#sec-calibration">calibrated</a>. The 95% confidence interval of the observed value, across all values of the predicted values, should include the diagonal reference line whose intercept is zero and whose slope is one.</p>
<p>For instance, based on the intercept and slope of the <a href="#sec-calibrationPlot">calibration plot</a> in Figure INSERT, the predictions are not well calibrated, despite having a slope near one, because the 95% confidence interval of the intercept does not include zero. The best-fit line is the yellow line. The intercept from the best-fit line is positive, as shown in the regression equation. This is a case of underprediction, where the predicted values are consistently less than the observed values. The confidence interval of the observed value (i.e., the purple band) is the interval within which we have 95% confidence that the true observed value would lie for a given predicted value, based on the model The 95% prediction interval of the observed value (i.e., the dashed red lines) is the interval within which we would expect that 95% of future observations would lie for a given predicted value. The black diagonal line indicates the reference line with an intercept of zero and a slope of one. The predictions would be significantly <a href="#sec-calibration">miscalibrated</a> at a given level of the predicted values if the 95% confidence interval of the observed value does not include the reference line at that level of the predicted value. In this case, the 95% confidence interval of the observed value does not include the reference line (i.e., the actual observed value) at lower levels of the predicted values, so the predictions are <a href="#sec-calibration">miscalibrated</a> lower levels of the predicted values.</p>
<p>Gold-standard recommendations include examining the predicted values in relation to the observed values using locally estimated scatterplot smoothing (LOESS) <span class="citation" data-cites="Austin2014">(<a href="references.html#ref-Austin2014" role="doc-biblioref">Austin &amp; Steyerberg, 2014</a>)</span>, such as in Figure INSERT. We can examine whether the LOESS-based 95% confidence interval of the observed value at every level of the predicted values includes the diagonal reference line (i.e., the actual observed value). In this case, the 95% confidence interval of the observed value does not include the reference line at lower levels of the predicted values, so the predictions are <a href="#sec-calibration">miscalibrated</a> at lower levels of the predicted values.</p>
</section></section></section><section id="sec-integratingAccuracy" class="level2" data-number="17.8"><h2 data-number="17.8" class="anchored" data-anchor-id="sec-integratingAccuracy">
<span class="header-section-number">17.8</span> Integrating the Accuracy Indices</h2>
<p>After computing the accuracy indices of <a href="#sec-discrimination">discrimination</a> and (2) <a href="#sec-calibration">calibration</a>, it is then the task to integrate the indices to determine (a) which are the most accurate predictions for the given goals, and (b) whether additional improvements and refinements to the predictions need to be made. Each of the accuracy indices is computed differently and thus reward (and penalize) predictive (in)accuracy differently. Sometimes, the the accuracy indices will paint a consistent picture regarding which predictions are the most accurate. Other times, the accuracy indices may disagree about which predictions are most accurate.</p>
<p>In fantasy football, when evaluating the accuracy of seasonal projections, we care most about accurately distinguishing between higher levels of points (e.g., 200 vs 150) as opposed to lower levels of points (e.g., 0 vs 10). Thus, it can be helpful to punish larger errors more heavily than smaller errors, as <a href="#sec-rootMeanSquaredError">RMSE</a> (unlike <a href="#sec-meanAbsoluteError">MAE</a>).</p>
<p>Thus, we would emphasize the following metrics:</p>
<ul>
<li>
<a href="#sec-discrimination">discrimination</a>:
<ul>
<li><a href="#sec-adjustedRsquared">adjusted <span class="math inline">\(R^2\)</span></a></li>
</ul>
</li>
<li>
<a href="#sec-calibration">calibration</a>:
<ul>
<li><a href="#sec-calibrationPlot">calibration plot</a></li>
</ul>
</li>
<li>
<a href="#sec-calibration">general accuracy</a>:
<ul>
<li><a href="#sec-meanAbsoluteError">MAE</a></li>
<li><a href="#sec-rootMeanSquaredError">RMSE</a></li>
</ul>
</li>
</ul>
<p>If you focus on only one accuracy index, <a href="#sec-meanAbsoluteError">MAE</a> or <a href="#sec-rootMeanSquaredError">RMSE</a> would be a good choice. However, I would also examine a <a href="#sec-calibrationPlot">calibration plot</a> to evaluate whether predictions are poorly calibrated at higher levels of points. I would also examine <a href="#sec-meanError">ME</a>—not to compare the accuracy of various predictions per se—but to determine whether predictions are systematically under- or overestimating actual points. If so, predictions may be able to be refined by adding or subtracting a constant to the predictions (or to a subset of the predictions); however, this could worsen other accuracy indices, so it is important to conduct an iterative process of modifying then evaluating, then further modifying and evaluating, etc. It may also be valuable to evaluate the accuracy of various subsets of the predictions. For instance, you might examine the predictive accuracy of players whose projected points are greater than 100, to evaluate the accuracy of predictions specifically to distinguish between players at higher levels of points, which is one of the key goals when selecting which players to draft.</p>
<p>If we are making predictions about a categorical variable, we would emphasize the following metrics:</p>
<ul>
<li>
<a href="#sec-discrimination">discrimination</a>:
<ul>
<li><a href="#sec-auc">area under the receiver operating curve</a></li>
<li>and, secondarily—depending on the particulary cutoff and the relative costs of <a href="#sec-falsePositive">false positives</a> versus <a href="#sec-falseNegative">false negatives</a>:
<ul>
<li><a href="#sec-sensitivity">sensitivity</a></li>
<li><a href="#sec-specificity">specificicity</a></li>
<li><a href="#sec-ppv">positive predictive value</a></li>
<li><a href="#sec-npv">negative predictive value</a></li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#sec-calibration">calibration</a>:
<ul>
<li><a href="#sec-calibrationPlot">calibration plot</a></li>
<li><a href="#sec-spiegelhalterZ">Spiegelhalter’s <em>z</em></a></li>
<li><a href="#sec-miscalibration">Mean difference between observed and predicted values</a></li>
</ul>
</li>
</ul></section><section id="sec-theoryVsEmpiricism" class="level2" data-number="17.9"><h2 data-number="17.9" class="anchored" data-anchor-id="sec-theoryVsEmpiricism">
<span class="header-section-number">17.9</span> Theory Versus Empiricism</h2>
<p>One question that inevitably arises when making predictions is the extent to which one should leverage theory versus empiricism. Theory involves conceptual claims of understanding how the causal system works (i.e., what influences what). For example, use of theory in prediction might involve specification of the causal system that influences player performance, measurement of those factors, and the integration of that information to make a prediction. Empiricism involves “letting the data speak for themselves” and is an atheoretical approach. For example, empiricism might involve examining how thousands of variables are associated with the criterion of interest (e.g., fantasy points) and developing the best-fitting model based on those thousands of predictor variables.</p>
<p>Although the atheoretical approach can perform reasonably well, it can be improved by making better use of theory. An empirical result (e.g., a correlation) might not necessarily have a lot of meaning associated with it. As the maxim goes, <a href="causal-inference.html#sec-correlationCausality">correlation does not imply causation</a>. Moreover, empiricism can lead to <a href="multiple-regression.html#sec-overfitting">overfitting</a>. So, empiricism is often not enough.</p>
<p>As <span class="citation" data-cites="Silver2012">Silver (<a href="references.html#ref-Silver2012" role="doc-biblioref">2012</a>)</span> notes, “The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.” (p.&nbsp;9). If we <em>understand</em> the variables in the system and how they influence each other, we can predict things more accurately than predicting for the sake of predicting. For instance, <a href="https://www.npr.org/sections/money/2023/07/11/1186458991/should-we-invest-more-in-weather-forecasting-it-may-save-your-life">we have made great strides in the last decades when it comes to more accurate weather forecasts</a> [<span class="citation" data-cites="Rosalsky2023">Rosalsky (<a href="references.html#ref-Rosalsky2023" role="doc-biblioref">2023</a>)</span>; archived at <a href="https://perma.cc/PF8P-BT3D" class="uri">https://perma.cc/PF8P-BT3D</a>], including extreme weather events like hurricanes. These great strides have more to do with a better causal understanding of the weather system and the ability to conduct simulations of the atmosphere than merely because of big data <span class="citation" data-cites="Silver2012">(<a href="references.html#ref-Silver2012" role="doc-biblioref">Silver, 2012</a>)</span>. By contrast, other events are still incredibly difficult to predict, including earthquakes, in large part because we do not have a strong understanding of the system (and because we do not have ways of precisely measuring those causes because they occur at a depth below which we are realistically able to drill) <span class="citation" data-cites="Silver2012">(<a href="references.html#ref-Silver2012" role="doc-biblioref">Silver, 2012</a>)</span>.</p>
<p>At the same time, in the social and behavioral sciences, our theories of the causal processes that influence outcomes are not yet very strong. Indeed, I have misgivings calling them theories because they do not meet the traditional scientific standard for a theory. A scientific theory is an explanation of the natural world that is testable and falsifiable, and that has withstood rigorous scientific testing and scrutiny. In psychology (and other areas of social and behavioral sciences), our “theories” are more like conceptual frameworks. And these conceptual frameworks are often vague, do not make specific predictions of effects <em>and</em> noneffects, and do not hold up consistently when rigorously tested. As described by <span class="citation" data-cites="Meehl1978">Meehl (<a href="references.html#ref-Meehl1978" role="doc-biblioref">1978</a>)</span>:</p>
<blockquote class="blockquote">
<p>I consider it unnecessary to persuade you that most so-called “theories” in the soft areas of psychology (clinical, counseling, social, personality, community, and school psychology) are scientifically unimpressive and technologically worthless … Perhaps the easiest way to convince yourself is by scanning the literature of soft psychology over the last 30 years and noticing what happens to theories. Most of them suffer the fate that General MacArthur ascribed to old generals—They never die, they just slowly fade away. In the developed sciences, theories tend either to become widely accepted and built into the larger edifice of well-tested human knowledge or else they suffer destruction in the face of recalcitrant facts and are abandoned, perhaps regretfully as a “nice try.” But in fields like personology and social psychology, this seems not to happen. There is a period of enthusiasm about a new theory, a period of attempted application to several fact domains, a period of disillusionment as the negative data come in, a growing bafflement about inconsistent and unreplicable empirical results, multiple resort to ad hoc excuses, and then finally people just sort of lose interest in the thing and pursue other endeavors.</p>
<p>Meehl <span class="citation" data-cites="Meehl1978">(<a href="references.html#ref-Meehl1978" role="doc-biblioref">1978, pp. 806–807</a>)</span></p>
</blockquote>
<p>Even if we had strong theoretical understanding of the causal system that influences behavior, we would likely still have difficulty making accurate predictions because the field has largely relied on relatively crude instruments. According to one philosophical perspective known as LaPlace’s demon, if we were able to know the exact conditions of everything in the universe, we would be able to know how the conditions would be in the future. This is an example of scientific determinism, where if you know the initial conditions, you also know the future. Other perspectives, such as quantum mechanics and chaos theory, would say that, even if we knew the initial conditions with 100% certainty, there would still be uncertainty in our understanding of the future. But assume, for a moment, that LaPlace’s demon is true. A challenge in the social and behavioral sciences is that we have a relatively poor understanding of the initial conditions of the universe. Thus, our predictions would necessarily be probabilistic, similar to weather forecasts. Despite having a strong understanding of how weather systems behave, we have imperfect understanding of the initial conditions (e.g., the position and movement of all molecules) <span class="citation" data-cites="Silver2012">(<a href="references.html#ref-Silver2012" role="doc-biblioref">Silver, 2012</a>)</span>.</p>
<p>Theories tend to make grand conceptual claims that one observed variable influences another observed variable through a complex chain of intervening processes that are unobservable. Empiricism provides rich lower-level information, but lacks the broader picture. So, it seems, that we need both theory <em>and</em> empiricism. Theory and empiricism can—and should—inform each other.</p>
</section><section id="sec-testBias" class="level2" data-number="17.10"><h2 data-number="17.10" class="anchored" data-anchor-id="sec-testBias">
<span class="header-section-number">17.10</span> Test Bias</h2>
<p>Test bias refers to systematic error (in measurement, prediction, etc.) as a function of group membership that leads the same score to have different meaning for different groups. For instance, if the Wonderlic Contemporary Cognitive Ability Test is a strong predictor of performance for Quarterbacks but not for Running Backs, the test is biased. Test bias, including how to identify and address it, is described in <a href="https://isaactpetersen.github.io/Principles-Psychological-Assessment/bias.html"><span class="citation" data-cites="PetersenPrinciplesPsychAssessment">Petersen (<span>2025b</span>)</span></a>.</p>
</section><section id="sec-waysToImprovePredictionAccuracy" class="level2" data-number="17.11"><h2 data-number="17.11" class="anchored" data-anchor-id="sec-waysToImprovePredictionAccuracy">
<span class="header-section-number">17.11</span> Ways to Improve Prediction Accuracy</h2>
<p>On the whole, experts’ predictions are inaccurate. Experts’ predictions from many different domains tend to be inaccurate, including political scientists <span class="citation" data-cites="Tetlock2017">(<a href="references.html#ref-Tetlock2017" role="doc-biblioref">Tetlock, 2017</a>)</span>, physicians <span class="citation" data-cites="Koehler2002">(<a href="references.html#ref-Koehler2002" role="doc-biblioref">Koehler et al., 2002</a>)</span>, clinical psychologists <span class="citation" data-cites="Oskamp1965">(<a href="references.html#ref-Oskamp1965" role="doc-biblioref">Oskamp, 1965</a>)</span>, stock market traders and corporate financial officers <span class="citation" data-cites="Skala2008">(<a href="references.html#ref-Skala2008" role="doc-biblioref">Skala, 2008</a>)</span>, seismologists’ predictions of earthquakes <span class="citation" data-cites="Hough2016">(<a href="references.html#ref-Hough2016" role="doc-biblioref">Hough, 2016</a>)</span>, economists’ predictions about the economy <span class="citation" data-cites="Makridakis2009">(<a href="references.html#ref-Makridakis2009" role="doc-biblioref">Makridakis et al., 2009</a>)</span>, lawyers <span class="citation" data-cites="Koehler2002">(<a href="references.html#ref-Koehler2002" role="doc-biblioref">Koehler et al., 2002</a>)</span>, and business managers <span class="citation" data-cites="Russo1992">(<a href="references.html#ref-Russo1992" role="doc-biblioref">Russo &amp; Schoemaker, 1992</a>)</span>. Thus, I would not put much confidence in the predictions by fantasy football fundits. The most common pattern of experts’ predictions is that they show overextremity, that is, their predictions have probability judgments that tend to be too extreme, as described in Section <a href="#sec-calibration" class="quarto-xref"><span>Section 17.3.2</span></a>. Overextremity of experts’ predictions reflects the <a href="cognitive-bias.html#sec-cognitiveBiasesOverconfidence">overprecision</a> type of <a href="cognitive-bias.html#sec-cognitiveBiasesOverconfidence">overconfidence bias</a>. The degree of confidence of a person’s predictions is often not a good indicator of the accuracy of their predictions [and confidence and prediction accuracy are sometimes inversely associated; <span class="citation" data-cites="Silver2012">Silver (<a href="references.html#ref-Silver2012" role="doc-biblioref">2012</a>)</span>]. <a href="cognitive-bias.html#sec-heuristics">Heuristics</a> such as the <a href="cognitive-bias.html#sec-heuristicsAnchoringAdjustment">anchoring and adjustment heuristic</a>, <a href="cognitive-bias.html#sec-cognitiveBiases">cognitive biases</a> such as <a href="cognitive-bias.html#sec-cognitiveBiasesConfirmation">confirmation bias</a> <span class="citation" data-cites="Hoch1985 Koriat1980">(<a href="references.html#ref-Hoch1985" role="doc-biblioref">Hoch, 1985</a>; <a href="references.html#ref-Koriat1980" role="doc-biblioref">Koriat et al., 1980</a>)</span>, <a href="cognitive-bias.html#sec-fallacies">fallacies</a> such as the <a href="cognitive-bias.html#sec-fallaciesBaseRate">base rate fallacy</a> <span class="citation" data-cites="Eddy1982 Koehler2002">(<a href="references.html#ref-Eddy1982" role="doc-biblioref">Eddy, 1982</a>; <a href="references.html#ref-Koehler2002" role="doc-biblioref">Koehler et al., 2002</a>)</span> could contribute to overconfidence of predictions. <a href="#sec-calibration">Poorly calibrated</a> predictions are especially likely when the <a href="base-rates.html#sec-baseRate">base rate</a> is very low (e.g., suicide) or when the <a href="base-rates.html#sec-baseRate">base rate</a> is very high <span class="citation" data-cites="Koehler2002">(<a href="references.html#ref-Koehler2002" role="doc-biblioref">Koehler et al., 2002</a>)</span>.</p>
<p>Nevertheless, there are some domains that have shown greater predictive accuracy, from which we may learn what practices may lead to greater accuracy. For instance, experts have shown stronger predictive accuracy in weather forecasting <span class="citation" data-cites="Murphy1984">(<a href="references.html#ref-Murphy1984" role="doc-biblioref">Murphy &amp; Winkler, 1984</a>)</span>, horse race betting <span class="citation" data-cites="Johnson2001">(<a href="references.html#ref-Johnson2001" role="doc-biblioref">Johnson &amp; Bruce, 2001</a>)</span>, and playing the card game of bridge <span class="citation" data-cites="Keren1987">(<a href="references.html#ref-Keren1987" role="doc-biblioref">Keren, 1987</a>)</span>, but see <span class="citation" data-cites="Koehler2002">Koehler et al. (<a href="references.html#ref-Koehler2002" role="doc-biblioref">2002</a>)</span> for exceptions.</p>
<p>Here are some potential ways to improve the accuracy (and honesty) of predictions and judgments:</p>
<ul>
<li>Provide appropriate <a href="base-rates.html#sec-accountForBaseRates">anchoring of your predictions to the base rate</a> of the phenomenon you are predicting. To the extent that the <a href="base-rates.html#sec-baseRate">base rate</a> of the event you are predicting is low, more extreme evidence should be necessary to consistently and accurately predict that the event will occur. Applying <a href="actuarial.html#sec-actuarialPrediction">actuarial formulas</a> and <a href="base-rates.html#sec-bayesTheorem">Bayes’ theorem</a> can help you appropriately weigh the <a href="base-rates.html#sec-baseRate">base rate</a> and evidence.</li>
<li>Include multiple predictors, ideally from different measures and measurement methods. Include the predictors with the strongest validity based on theory of the causal process and based on <a href="research-methods.html#sec-criterionValidity">criterion-related validity</a>.</li>
<li>When possible, aggregate multiple perspectives of predictions, especially predictions made independently (from different people/methods/etc.). The “wisdom of the crowd” is often more accurate than individuals’ predictions, including predictions by so-called “experts” <span class="citation" data-cites="Silver2012">(<a href="references.html#ref-Silver2012" role="doc-biblioref">Silver, 2012</a>)</span>.</li>
<li>A goal of prediction is to capture as much signal as possible and as little noise (error) as possible <span class="citation" data-cites="Silver2012">(<a href="references.html#ref-Silver2012" role="doc-biblioref">Silver, 2012</a>)</span>. Parsimony (i.e., not having too many predictors) can help reduce the amount of error variance captured by the prediction model. However, to accurately model complex systems like human behavior, complex models may be necessary. However, strong theory of the causal processes and dynamics may be necessary to develop accurate complex models.</li>
<li>Although incorporating theory can be helpful, provide more weight to empiricism than to theory, until our theories and measures are stronger. Ideally, we would use theory to design a model that mirrors the causal system, with accurate measures of each process in the system, so we could make accurate predictions. However, as described in <a href="#sec-theoryVsEmpiricism" class="quarto-xref"><span>Section 17.9</span></a>, our psychological theories of the causal processes that influence behavior are not yet very strong. Until we have stronger theories that specify the causal process for a given outcome, and until we have accurate measures of those causal processes, <a href="actuarial.html#sec-actuarialPrediction">actuarial approaches</a> are likely to be most accurate, as discussed in <a href="actuarial.html" class="quarto-xref"><span>Chapter 15</span></a>. At the same time, keep in mind that measures involving human behavior, and their resulting data, are often noisy. As a result, theoretically (conceptually) informed empirical approaches may lead to more accuracy than empiricism alone.</li>
<li>Use an empirically validated and cross-validated <a href="actuarial.html#sec-actuarialPrediction">statistical algorithm</a> to combine information from the predictors in a formalized way. Give each predictor appropriate weight in the statistical algorithm, according to its strength of association with the outcome. Use measures with strong <a href="research-methods.html#sec-reliability">reliability</a> and <a href="research-methods.html#sec-validity">validity</a> for assessing these processes to be used in the algorithm. Cross-validation will help reduce the likelihood that your model is fitting to noise and will maximize the likelihood that the model predicts accurately when applied to new data (i.e., the model’s predictions accurately generalize), as described in <a href="actuarial.html#sec-bestActuarialApproaches" class="quarto-xref"><span>Section 15.8</span></a>.</li>
<li>When presenting your predictions, acknowledge what you do not know.</li>
<li>Express your predictions in terms of probabilistic estimates and present the uncertainty in your predictions with confidence intervals [even though bolder, more extreme predictions tend to receive stronger television ratings; <span class="citation" data-cites="Silver2012">Silver (<a href="references.html#ref-Silver2012" role="doc-biblioref">2012</a>)</span>].</li>
<li>Qualify your predictions by identifying and noting counter-examples that would not be well fit by your prediction model, such as extreme cases, edge cases, and “broken leg” <span class="citation" data-cites="Meehl1957">(<a href="references.html#ref-Meehl1957" role="doc-biblioref">Meehl, 1957</a>)</span> cases.</li>
<li>Provide clear, consistent, and timely feedback on the outcomes of the predictions to the people making the predictions <span class="citation" data-cites="Bolger2004">(<a href="references.html#ref-Bolger2004" role="doc-biblioref">Bolger &amp; Önkal-Atay, 2004</a>)</span>.</li>
<li>Be self-critical about your predictions. Update your judgments based on their accuracy, rather than trying to confirm your beliefs <span class="citation" data-cites="Atanasov2020">(<a href="references.html#ref-Atanasov2020" role="doc-biblioref">Atanasov et al., 2020</a>)</span>.</li>
<li>In addition to considering the accuracy of the prediction, consider the quality of the prediction <em>process</em>, especially when random chance is involved to a degree, such as in poker and fantasy football <span class="citation" data-cites="Silver2012">(<a href="references.html#ref-Silver2012" role="doc-biblioref">Silver, 2012</a>)</span>.</li>
<li>Work to identify and mitigate potential blindspots; be aware of <a href="cognitive-bias.html#sec-cognitiveBiases">cognitive biases</a> and <a href="cognitive-bias.html#sec-fallacies">fallacies</a>, such as <a href="cognitive-bias.html#sec-cognitiveBiasesConfirmation">confirmation bias</a> and the <a href="cognitive-bias.html#sec-fallaciesBaseRate">base rate fallacy</a>.</li>
<li>Evaluate for the possibility of <a href="#sec-testBias">test bias</a>. Correct for any <a href="#sec-testBias">test bias</a>.</li>
</ul></section><section id="sec-predictionAccuracyConclusion" class="level2" data-number="17.12"><h2 data-number="17.12" class="anchored" data-anchor-id="sec-predictionAccuracyConclusion">
<span class="header-section-number">17.12</span> Conclusion</h2>
<p>When the <a href="#baseRate">base rate</a> of a behavior is very low or very high, you can be highly accurate in predicting the behavior by <a href="#sec-predictingFromBaseRate">predicting from the base rate</a>. Thus, you cannot judge how accurate your prediction is until you know how accurate your predictions would be by <a href="#sec-accuracyByChance">random chance</a>. Moreover, maximizing <a href="#sec-percentAccuracy">percent accuracy</a> may not be the ultimate goal because <a href="#sec-differentErrorsDifferentCosts">different errors have different costs</a>. Though there are many indices of accuracy, there are two general types of accuracy: <a href="#sec-discrimination">discrimination</a> and <a href="#sec-calibration">calibration</a>. <a href="#sec-discrimination">Discrimination</a> accuracy is frequently evaluated with the <a href="#sec-auc">area under the receiver operating characteristic curve</a>, or with <a href="#sec-sensitivity">sensitivity</a> and <a href="#sec-specificity">specificity</a>, or with <a href="#sec-standardizedRegressionCoefficient">standardized regression coefficients</a> or the <a href="multiple-regression.html#sec-multipleRegressionRSquared">coefficient of determination</a>. <a href="#sec-calibration">Calibration</a> accuracy is frequently evaluated graphically and with various indices. <a href="#sec-sensitivity">Sensitivity</a> and <a href="#sec-specificity">specificity</a> <a href="#sec-accuracyCutoff">depend on the cutoff</a>. It is important to evaluate both <a href="#sec-discrimination">discrimination</a> and <a href="#sec-calibration">calibration</a> when evaluating prediction accuracy.</p>
</section><section id="sec-predictionAccuracySessionInfo" class="level2" data-number="17.13"><h2 data-number="17.13" class="anchored" data-anchor-id="sec-predictionAccuracySessionInfo">
<span class="header-section-number">17.13</span> Session Info</h2>
<div class="cell">
<details open="" class="code-fold"><summary>Code</summary><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="fu">sessionInfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details><div class="cell-output cell-output-stdout">
<pre><code>R version 4.4.3 (2025-02-28)
Platform: x86_64-pc-linux-gnu
Running under: Ubuntu 24.04.2 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0

locale:
 [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       
 [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   
 [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          
[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   

time zone: UTC
tzcode source: system (glibc)

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] viridis_0.6.5     viridisLite_0.4.2 magrittr_2.0.3    pROC_1.18.5      
 [5] lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1     dplyr_1.1.4      
 [9] purrr_1.0.4       readr_2.1.5       tidyr_1.3.1       tibble_3.2.1     
[13] ggplot2_3.5.1     tidyverse_2.0.0   petersenlab_1.1.1

loaded via a namespace (and not attached):
 [1] gtable_0.3.6       xfun_0.51          htmlwidgets_1.6.4  psych_2.4.12      
 [5] lattice_0.22-6     tzdb_0.4.0         quadprog_1.5-8     vctrs_0.6.5       
 [9] tools_4.4.3        generics_0.1.3     stats4_4.4.3       parallel_4.4.3    
[13] cluster_2.1.8      pkgconfig_2.0.3    data.table_1.17.0  checkmate_2.3.2   
[17] RColorBrewer_1.1-3 lifecycle_1.0.4    farver_2.1.2       compiler_4.4.3    
[21] munsell_0.5.1      mnormt_2.1.1       mitools_2.4        htmltools_0.5.8.1 
[25] yaml_2.3.10        htmlTable_2.4.3    Formula_1.2-5      pillar_1.10.1     
[29] Hmisc_5.2-2        rpart_4.1.24       nlme_3.1-167       lavaan_0.6-19     
[33] tidyselect_1.2.1   digest_0.6.37      mvtnorm_1.3-3      stringi_1.8.4     
[37] reshape2_1.4.4     labeling_0.4.3     fastmap_1.2.0      grid_4.4.3        
[41] colorspace_2.1-1   cli_3.6.4          base64enc_0.1-3    pbivnorm_0.6.0    
[45] foreign_0.8-88     withr_3.0.2        scales_1.3.0       backports_1.5.0   
[49] timechange_0.3.0   rmarkdown_2.29     nnet_7.3-20        gridExtra_2.3     
[53] hms_1.1.3          evaluate_1.0.3     knitr_1.49         mix_1.0-13        
[57] rlang_1.1.5        Rcpp_1.0.14        xtable_1.8-4       glue_1.8.0        
[61] DBI_1.2.3          rstudioapi_0.17.1  jsonlite_1.9.1     R6_2.6.1          
[65] plyr_1.8.9        </code></pre>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Atanasov2020" class="csl-entry" role="listitem">
Atanasov, P., Witkowski, J., Ungar, L., Mellers, B., &amp; Tetlock, P. (2020). Small steps to accuracy: Incremental belief updaters are better forecasters. <em>Organizational Behavior and Human Decision Processes</em>, <em>160</em>, 19–35. <a href="https://doi.org/10.1016/j.obhdp.2020.02.001">https://doi.org/10.1016/j.obhdp.2020.02.001</a>
</div>
<div id="ref-Austin2014" class="csl-entry" role="listitem">
Austin, P. C., &amp; Steyerberg, E. W. (2014). Graphical assessment of internal and external calibration of logistic regression models by using loess smoothers. <em>Statistics in Medicine</em>, <em>33</em>(3), 517–535. <a href="https://doi.org/10.1002/sim.5941">https://doi.org/10.1002/sim.5941</a>
</div>
<div id="ref-Bolger2004" class="csl-entry" role="listitem">
Bolger, F., &amp; Önkal-Atay, D. (2004). The effects of feedback on judgmental interval predictions. <em>International Journal of Forecasting</em>, <em>20</em>(1), 29–39. <a href="https://doi.org/10.1016/S0169-2070(03)00009-8">https://doi.org/10.1016/S0169-2070(03)00009-8</a>
</div>
<div id="ref-Corston2000" class="csl-entry" role="listitem">
Corston, R., &amp; Colman, A. M. (2000). <em>A crash course in SPSS for <span>Windows</span></em>. Wiley-Blackwell.
</div>
<div id="ref-Eddy1982" class="csl-entry" role="listitem">
Eddy, D. M. (1982). Probabilistic reasoning in clinical medicine: Problems and opportunities. In D. Kahneman, P. Slovic, &amp; A. Tversky (Eds.), <em>Judgment under uncertainty: Heuristics and biases</em> (pp. 249–267). Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511809477.019">https://doi.org/10.1017/CBO9780511809477.019</a>
</div>
<div id="ref-Farrington1989" class="csl-entry" role="listitem">
Farrington, D. P., &amp; Loeber, R. (1989). Relative improvement over chance (<span>RIOC</span>) and phi as measures of predictive efficiency and strength of association in 2×2 tables. <em>Journal of Quantitative Criminology</em>, <em>5</em>(3), 201–213. <a href="https://doi.org/10.1007/BF01062737">https://doi.org/10.1007/BF01062737</a>
</div>
<div id="ref-Goodman2022" class="csl-entry" role="listitem">
Goodman, Z. T., Casline, E., Jensen-Doss, A., Ehrenreich-May, J., &amp; Bainter, S. A. (2022). <span class="nocase">shinyDLRs</span>: A dashboard to facilitate derivation of diagnostic likelihood ratios. <em>Psychological Assessment</em>, <em>34</em>(6), 558–569. <a href="https://doi.org/10.1037/pas0001114">https://doi.org/10.1037/pas0001114</a>
</div>
<div id="ref-R-rms" class="csl-entry" role="listitem">
Harrell, Jr., F. E. (2024). <em><span class="nocase">rms</span>: Regression modeling strategies</em>. <a href="https://hbiostat.org/R/rms/">https://hbiostat.org/R/rms/</a>
</div>
<div id="ref-Hoch1985" class="csl-entry" role="listitem">
Hoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting personal events. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, <em>11</em>(4), 719–731. <a href="https://doi.org/10.1037/0278-7393.11.1-4.719">https://doi.org/10.1037/0278-7393.11.1-4.719</a>
</div>
<div id="ref-Hopper2014" class="csl-entry" role="listitem">
Hopper, T. (2014). <em>Can we do better than r-squared?</em> <a href="https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared">https://tomhopper.me/2014/05/16/can-we-do-better-than-r-squared</a>
</div>
<div id="ref-Hough2016" class="csl-entry" role="listitem">
Hough, S. E. (2016). <em>Predicting the unpredictable: The tumultuous science of earthquake prediction</em>. Princeton University Press.
</div>
<div id="ref-Hyndman2014" class="csl-entry" role="listitem">
Hyndman, R. J. (2014). <em>Alternative to <span>MAPE</span> when the data is not a time series</em>. <a href="https://stats.stackexchange.com/a/108963/20338">https://stats.stackexchange.com/a/108963/20338</a>
</div>
<div id="ref-Hyndman2021" class="csl-entry" role="listitem">
Hyndman, R. J., &amp; Athanasopoulos, G. (2021). <em>Forecasting: Principles and practice</em> (3rd ed.). OTexts. <a href="https://otexts.com/fpp3">https://otexts.com/fpp3</a>
</div>
<div id="ref-Johnson2001" class="csl-entry" role="listitem">
Johnson, J. E. V., &amp; Bruce, A. C. (2001). Calibration of subjective probability judgments in a naturalistic setting. <em>Organizational Behavior and Human Decision Processes</em>, <em>85</em>(2), 265–290. <a href="https://doi.org/10.1006/obhd.2000.2949">https://doi.org/10.1006/obhd.2000.2949</a>
</div>
<div id="ref-Kahneman2011" class="csl-entry" role="listitem">
Kahneman, D. (2011). <em>Thinking, fast and slow</em>. <span>Farrar, Straus, and Giroux</span>.
</div>
<div id="ref-Keren1987" class="csl-entry" role="listitem">
Keren, G. (1987). Facing uncertainty in the game of bridge: A calibration study. <em>Organizational Behavior and Human Decision Processes</em>, <em>39</em>(1), 98–114. <a href="https://doi.org/10.1016/0749-5978(87)90047-1">https://doi.org/10.1016/0749-5978(87)90047-1</a>
</div>
<div id="ref-Kessler2020" class="csl-entry" role="listitem">
Kessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M., &amp; Zubizarreta, J. R. (2020). Suicide prediction models: A critical review of recent research with recommendations for the way forward. <em>Molecular Psychiatry</em>, <em>25</em>(1), 168–179. <a href="https://doi.org/10.1038/s41380-019-0531-0">https://doi.org/10.1038/s41380-019-0531-0</a>
</div>
<div id="ref-Koehler2002" class="csl-entry" role="listitem">
Koehler, D. J., Brenner, L., &amp; Griffin, D. (2002). The calibration of expert judgment: Heuristics and biases beyond the laboratory. In T. Gilovich, D. Griffin, &amp; D. Kahneman (Eds.), <em>Heuristics and biases: The psychology of intuitive judgment</em>. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511808098.041">https://doi.org/10.1017/CBO9780511808098.041</a>
</div>
<div id="ref-Koriat1980" class="csl-entry" role="listitem">
Koriat, A., Lichtenstein, S., &amp; Fischhoff, B. (1980). Reasons for confidence. <em>Journal of Experimental Psychology: Human Learning and Memory</em>, <em>6</em>(2), 107–118. <a href="https://doi.org/10.1037/0278-7393.6.2.107">https://doi.org/10.1037/0278-7393.6.2.107</a>
</div>
<div id="ref-Lindhiem2020" class="csl-entry" role="listitem">
Lindhiem, O., Petersen, I. T., Mentch, L. K., &amp; Youngstrom, E. A. (2020). The importance of calibration in clinical psychology. <em>Assessment</em>, <em>27</em>(4), 840–854. <a href="https://doi.org/10.1177/1073191117752055">https://doi.org/10.1177/1073191117752055</a>
</div>
<div id="ref-Makridakis2009" class="csl-entry" role="listitem">
Makridakis, S., Hogarth, R. M., &amp; Gaba, A. (2009). Forecasting and uncertainty in the economic and business world. <em>International Journal of Forecasting</em>, <em>25</em>(4), 794–812. <a href="https://doi.org/10.1016/j.ijforecast.2009.05.012">https://doi.org/10.1016/j.ijforecast.2009.05.012</a>
</div>
<div id="ref-Meehl1957" class="csl-entry" role="listitem">
Meehl, P. E. (1957). When shall we use our heads instead of the formula? <em>Journal of Counseling Psychology</em>, <em>4</em>(4), 268–273. <a href="https://doi.org/10.1037/h0047554">https://doi.org/10.1037/h0047554</a>
</div>
<div id="ref-Meehl1978" class="csl-entry" role="listitem">
Meehl, P. E. (1978). Theoretical risks and tabular asterisks: <span>S</span>ir <span>K</span>arl, <span>S</span>ir <span>R</span>onald, and the slow progress of soft psychology. <em>Journal of Consulting and Clinical Psychology</em>, <em>46</em>(4), 806–834. <a href="https://doi.org/10.1037/0022-006x.46.4.806">https://doi.org/10.1037/0022-006x.46.4.806</a>
</div>
<div id="ref-Meehl1955" class="csl-entry" role="listitem">
Meehl, P. E., &amp; Rosen, A. (1955). Antecedent probability and the efficiency of psychometric signs, patterns, or cutting scores. <em>Psychological Bulletin</em>, <em>52</em>(3), 194–216. <a href="https://doi.org/10.1037/h0048070">https://doi.org/10.1037/h0048070</a>
</div>
<div id="ref-Morley2018" class="csl-entry" role="listitem">
Morley, S. K., Brito, T. V., &amp; Welling, D. T. (2018). Measures of model performance based on the log accuracy ratio. <em>Space Weather</em>, <em>16</em>(1), 69–88. <a href="https://doi.org/10.1002/2017SW001669">https://doi.org/10.1002/2017SW001669</a>
</div>
<div id="ref-Murphy1984" class="csl-entry" role="listitem">
Murphy, A. H., &amp; Winkler, R. L. (1984). Probability forecasting in meterology. <em>Journal of the American Statistical Association</em>, <em>79</em>(387), 489–500. <a href="https://doi.org/10.2307/2288395">https://doi.org/10.2307/2288395</a>
</div>
<div id="ref-Oskamp1965" class="csl-entry" role="listitem">
Oskamp, S. (1965). Overconfidence in case-study judgments. <em>Journal of Consulting Psychology</em>, <em>29</em>(3), 261–265. <a href="https://doi.org/10.1037/h0022125">https://doi.org/10.1037/h0022125</a>
</div>
<div id="ref-Petersen2024a" class="csl-entry" role="listitem">
Petersen, I. T. (2024). <em>Principles of psychological assessment: With applied examples in <span>R</span></em>. <span>Chapman and Hall/CRC</span>. <a href="https://doi.org/10.1201/9781003357421">https://doi.org/10.1201/9781003357421</a>
</div>
<div id="ref-R-petersenlab" class="csl-entry" role="listitem">
Petersen, I. T. (2025a). <em><span class="nocase">petersenlab</span>: A collection of <span>R</span> functions by the <span>Petersen</span> <span>Lab</span></em>. <a href="https://doi.org/10.32614/CRAN.package.petersenlab">https://doi.org/10.32614/CRAN.package.petersenlab</a>
</div>
<div id="ref-PetersenPrinciplesPsychAssessment" class="csl-entry" role="listitem">
Petersen, I. T. (2025b). <em>Principles of psychological assessment: With applied examples in <span>R</span></em>. University of Iowa Libraries. <a href="https://doi.org/10.25820/work.007199">https://doi.org/10.25820/work.007199</a>
</div>
<div id="ref-Robin2011_packages" class="csl-entry" role="listitem">
Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2011). <span class="nocase">pROC</span>: An open-source package for <span>R</span> and <span>S+</span> to analyze and compare <span>ROC</span> curves. <em>BMC Bioinformatics</em>, <em>12</em>, 77. <a href="https://doi.org/10.1186/1471-2105-12-77">https://doi.org/10.1186/1471-2105-12-77</a>
</div>
<div id="ref-R-pROC" class="csl-entry" role="listitem">
Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2023). <em><span class="nocase">pROC</span>: Display and analyze <span>ROC</span> curves</em>. <a href="https://xrobin.github.io/pROC/">https://xrobin.github.io/pROC/</a>
</div>
<div id="ref-Rosalsky2023" class="csl-entry" role="listitem">
Rosalsky, G. (2023). <em>Should we invest more in weather forecasting? It may save your life</em>. <a href="https://www.npr.org/sections/money/2023/07/11/1186458991/should-we-invest-more-in-weather-forecasting-it-may-save-your-life">https://www.npr.org/sections/money/2023/07/11/1186458991/should-we-invest-more-in-weather-forecasting-it-may-save-your-life</a>
</div>
<div id="ref-Russo1992" class="csl-entry" role="listitem">
Russo, J. E., &amp; Schoemaker, P. J. (1992). Managing overconfidence. <em>Sloan Management Review</em>, <em>33</em>(2), 7.
</div>
<div id="ref-Schwartz2006" class="csl-entry" role="listitem">
Schwartz, A. (2006). <em>Diagnostic test calculator</em>. <a href="http://araw.mede.uic.edu/cgi-bin/testcalc.pl">http://araw.mede.uic.edu/cgi-bin/testcalc.pl</a>
</div>
<div id="ref-Silver2012" class="csl-entry" role="listitem">
Silver, N. (2012). <em>The signal and the noise: Why so many predictions fail–but some don’t</em>. Penguin.
</div>
<div id="ref-Skala2008" class="csl-entry" role="listitem">
Skala, D. (2008). Overconfidence in psychology and finance–an interdisciplinary literature review. <em>Bank i Kredyt</em>, <em>4</em>, 33–50.
</div>
<div id="ref-Stevens2020" class="csl-entry" role="listitem">
Stevens, R. J., &amp; Poppe, K. K. (2020). Validation of clinical prediction models: What does the <span>“calibration slope”</span> really measure? <em>Journal of Clinical Epidemiology</em>, <em>118</em>, 93–99. <a href="https://doi.org/10.1016/j.jclinepi.2019.09.016">https://doi.org/10.1016/j.jclinepi.2019.09.016</a>
</div>
<div id="ref-Steyerberg2014" class="csl-entry" role="listitem">
Steyerberg, E. W., &amp; Vergouwe, Y. (2014). Towards better clinical prediction models: Seven steps for development and an ABCD for validation. <em>European Heart Journal</em>, <em>35</em>(29), 1925–1931. <a href="https://doi.org/10.1093/eurheartj/ehu207">https://doi.org/10.1093/eurheartj/ehu207</a>
</div>
<div id="ref-Tetlock2017" class="csl-entry" role="listitem">
Tetlock, P. E. (2017). <em>Expert political judgment: How good is it? How can we know? - <span>N</span>ew edition</em>. Princeton University Press.
</div>
<div id="ref-Tofallis2015" class="csl-entry" role="listitem">
Tofallis, C. (2015). A better measure of relative prediction accuracy for model selection and model estimation. <em>Journal of the Operational Research Society</em>, <em>66</em>(8), 1352–1362. <a href="https://doi.org/10.1057/jors.2014.103">https://doi.org/10.1057/jors.2014.103</a>
</div>
<div id="ref-Treat2023" class="csl-entry" role="listitem">
Treat, T. A., &amp; Viken, R. J. (2023). Measuring test performance with signal detection theory techniques. In H. Cooper, M. N. Coutanche, L. M. McMullen, A. T. Panter, D. Rindskopf, &amp; K. J. Sher (Eds.), <em>APA handbook of research methods in psychology: Foundations, planning, measures, and psychometrics</em> (2nd ed., Vol. 1, pp. 837–858). American Psychological Association. <a href="https://doi.org/10.1037/0000318-038">https://doi.org/10.1037/0000318-038</a>
</div>
<div id="ref-Yahoo2024" class="csl-entry" role="listitem">
Yahoo! Sports. (2024). <em>How cognitive bias affects your fantasy draft strategy with neuroscience professor <span>Dr. Renee Miller</span></em>. <a href="https://www.youtube.com/watch?v=gmpLFWs5ae0">https://www.youtube.com/watch?v=gmpLFWs5ae0</a>
</div>
</div>
</section></main><!-- /main --><h3>Feedback</h3>

<div class="feedback">
    Please consider providing feedback about this textbook, so that I can make it as helpful as possible. You can provide feedback at the following link:
    
    <a href="https://forms.gle/LsnVKwqmS1VuxWD18">https://forms.gle/LsnVKwqmS1VuxWD18</a>
</div>

<h3>Email Notification</h3>

<div class="rmdsale">
The online version of this book will remain open access. If you want to know when the print version of the book is for sale, enter your email below so I can let you know.
<br><iframe src="https://docs.google.com/forms/d/e/1FAIpQLSccMKaTBXzMBmcy4myCNQnL8mCgRczh0Eg_Pw3c15bork7-xQ/viewform?embedded=true" width="640" height="750" frameborder="0" marginheight="0" marginwidth="0">Loading...</iframe>

</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/isaactpetersen\.github\.io\/Fantasy-Football-Analytics-Textbook");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://giscus.app/client.js" data-repo="isaactpetersen/Fantasy-Football-Analytics-Textbook" data-repo-id="R_kgDOL6_f_A" data-category="Chapter Comments" data-category-id="DIC_kwDOL6_f_M4Cnvl7" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark"><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./base-rates.html" class="pagination-link" aria-label="Base Rates">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Base Rates</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mythbusters.html" class="pagination-link" aria-label="Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Mythbusters: Putting Fantasy Football Beliefs/Anecdotes to the Test</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>“Fantasy Football Analytics: Statistics, Prediction, and Empiricism Using R” was written by <a href="https://psychology.uiowa.edu/people/isaac-petersen">Isaac T. Petersen</a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/edit/main/evaluating-prediction-accuracy.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/blob/main/evaluating-prediction-accuracy.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li><li><a href="https://github.com/isaactpetersen/Fantasy-Football-Analytics-Textbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>


</body></html>