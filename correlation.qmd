# Correlation Analysis {#sec-correlation}

## Getting Started {#sec-correlationGettingStarted}

### Load Packages {#sec-correlationLoadPackages}

```{r}
library("petersenlab")
library("XICOR")
library("psych")
library("DescTools")
library("psych")
library("corrplot")
library("correlation")
library("ggridges")
library("tidyverse")
```

### Load Data {#sec-correlationLoadData}

```{r}
#| eval: false
#| include: false

load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/player_stats_weekly.RData", fsep = ""))
load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/player_stats_seasonal.RData", fsep = ""))
```

```{r}
load(file = "./data/player_stats_weekly.RData")
load(file = "./data/player_stats_seasonal.RData")
```

We created the `player_stats_weekly.RData` and `player_stats_seasonal.RData` objects in @sec-calculatePlayerAge.

## Overview of Correlation {#sec-correlationOverview}

Correlation is an index of the association between variables.
Covariance is the association between variables and in an unstandardized metric that differs for variables with different scales.
By contrast, correlation is in a standardized metric that does not differ for variables with different scales.
When examining the association between variables that are [interval](#sec-interval) or [ratio](#sec-ratio) levels of measurement, [Pearson correlation](#sec-correlationExamplesPearson) is used.
When examining the association between variables that are [ordinal](#sec-ordinal) in level of measurement, [Spearman correlation](#sec-correlationExamplesSpearman) is used.
[Pearson correlation](#sec-correlationExamplesPearson) is an index of the *linear* association between variables.
If a nonlinear association is present, other indices like xi [$\xi$; @Chatterjee2021] and distance correlation coefficients are better suited to detect the association.

## The Correlation Coefficient ($r$)

The formula for the ([Pearson](#sec-correlationExamplesPearson)) correlation coefficient\index{correlation} is in @eq-pearsonCorrelation.
As noted in @sec-statisticalTestsPartitionedVariance, the correlation coefficient can be thought of as the ratio of shared variance (i.e., covariance) to total variance, as in @eq-correlationPartitionedVariance.

The correlation coefficient ranges from âˆ’1.0 to +1.0.\index{correlation}
The correlation coefficient ($r$) tells you two things: (1) the direction (sign) of the association (positive or negative) and (2) the magnitude of the association.\index{correlation}
If the correlation coefficient is positive, the association is positive.\index{correlation}
If the correlation coefficient is negative, the association is negative.\index{correlation}
If the association is positive, as `X` increases, `Y` increases (or conversely, as `X` decreases, `Y` decreases).\index{correlation}
If the association is negative, as `X` increases, `Y` decreases (or conversely, as `X` decreases, `Y` increases).\index{correlation}
The smaller the absolute value of the correlation coefficient (i.e., the closer the $r$ value is to zero), the weaker the association and the flatter the slope of the best-fit line in a scatterplot.\index{correlation}
The larger the absolute value of the correlation coefficient (i.e., the closer the absolute value of the $r$ value is to one), the stronger the association and the steeper the slope of the best-fit line in a scatterplot.\index{correlation}
See @fig-rangeOfCorrelations for a range of different correlation coefficients and what some example data may look like for each direction and strength of association.\index{correlation}

```{r}
#| label: fig-rangeOfCorrelations
#| fig-cap: "Correlation Coefficients."
#| fig-alt: "Correlation Coefficients."
#| fig-height: 12
#| code-fold: true

set.seed(52242)
correlations <- data.frame(criterion = rnorm(1000))

correlations$v1 <- complement(correlations$criterion, -1)
correlations$v2 <- complement(correlations$criterion, -.9)
correlations$v3 <- complement(correlations$criterion, -.8)
correlations$v4 <- complement(correlations$criterion, -.7)
correlations$v5 <- complement(correlations$criterion, -.6)
correlations$v6 <- complement(correlations$criterion, -.5)
correlations$v7 <- complement(correlations$criterion, -.4)
correlations$v8 <- complement(correlations$criterion, -.3)
correlations$v9 <- complement(correlations$criterion, -.2)
correlations$v10 <-complement(correlations$criterion, -.1)
correlations$v11 <-complement(correlations$criterion, 0)
correlations$v12 <-complement(correlations$criterion, .1)
correlations$v13 <-complement(correlations$criterion, .2)
correlations$v14 <-complement(correlations$criterion, .3)
correlations$v15 <-complement(correlations$criterion, .4)
correlations$v16 <-complement(correlations$criterion, .5)
correlations$v17 <-complement(correlations$criterion, .6)
correlations$v18 <-complement(correlations$criterion, .7)
correlations$v19 <-complement(correlations$criterion, .8)
correlations$v20 <-complement(correlations$criterion, .9)
correlations$v21 <-complement(correlations$criterion, 1)

par(mfrow = c(7,3), mar = c(1, 0, 1, 0))

# -1.0
plot(correlations$criterion, correlations$v1, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v1)$estimate, 2))))
abline(lm(v1 ~ criterion, data = correlations), col = "black")

# -.9
plot(correlations$criterion, correlations$v2, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v2)$estimate, 2))))
abline(lm(v2 ~ criterion, data = correlations), col = "black")

# -.8
plot(correlations$criterion, correlations$v3, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v3)$estimate, 2))))
abline(lm(v3 ~ criterion, data = correlations), col = "black")

# -.7
plot(correlations$criterion, correlations$v4, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v4)$estimate, 2))))
abline(lm(v4 ~ criterion, data = correlations), col = "black")

# -.6
plot(correlations$criterion, correlations$v5, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v5)$estimate, 2))))
abline(lm(v5 ~ criterion, data = correlations), col = "black")

# -.5
plot(correlations$criterion, correlations$v6, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v6)$estimate, 2))))
abline(lm(v6 ~ criterion, data = correlations), col = "black")

# -.4
plot(correlations$criterion, correlations$v7, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v7)$estimate, 2))))
abline(lm(v7 ~ criterion, data = correlations), col = "black")

# -.3
plot(correlations$criterion, correlations$v8, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v8)$estimate, 2))))
abline(lm(v8 ~ criterion, data = correlations), col = "black")

# -.2
plot(correlations$criterion, correlations$v9, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v9)$estimate, 2))))
abline(lm(v9 ~ criterion, data = correlations), col = "black")

# -.1
plot(correlations$criterion, correlations$v10, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v10)$estimate, 2))))
abline(lm(v10 ~ criterion, data = correlations), col = "black")

# 0.0
plot(correlations$criterion, correlations$v11, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v11)$estimate, 2))))
abline(lm(v11 ~ criterion, data = correlations), col = "black")

# 0.1
plot(correlations$criterion, correlations$v12, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v12)$estimate, 2))))
abline(lm(v12 ~ criterion, data = correlations), col = "black")

# 0.2
plot(correlations$criterion, correlations$v13, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v13)$estimate, 2))))
abline(lm(v13 ~ criterion, data = correlations), col = "black")

# 0.3
plot(correlations$criterion, correlations$v14, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v14)$estimate, 2))))
abline(lm(v14 ~ criterion, data = correlations), col = "black")

# 0.4
plot(correlations$criterion, correlations$v15, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v15)$estimate, 2))))
abline(lm(v15 ~ criterion, data = correlations), col = "black")

# 0.5
plot(correlations$criterion, correlations$v16, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v16)$estimate, 2))))
abline(lm(v16 ~ criterion, data = correlations), col = "black")

# 0.6
plot(correlations$criterion, correlations$v17, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v17)$estimate, 2))))
abline(lm(v17 ~ criterion, data = correlations), col = "black")

# 0.7
plot(correlations$criterion, correlations$v18, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v18)$estimate, 2))))
abline(lm(v18 ~ criterion, data = correlations), col = "black")

# 0.8
plot(correlations$criterion, correlations$v19, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v19)$estimate, 2))))
abline(lm(v19 ~ criterion, data = correlations), col = "black")

# 0.9
plot(correlations$criterion, correlations$v20, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v20)$estimate, 2))))
abline(lm(v20 ~ criterion, data = correlations), col = "black")

# 1.0
plot(correlations$criterion, correlations$v21, xaxt = "n", yaxt = "n", xlab = "" , ylab = "",
     main = substitute(paste(italic(r), " = ", x, sep = ""), list(x = round(cor.test(x = correlations$criterion, y = correlations$v21)$estimate, 2))))
abline(lm(v21 ~ criterion, data = correlations), col = "black")

invisible(dev.off()) #par(mfrow = c(1,1))
```

See @fig-interepretingCorrelationCoefficients for the interpretation of the magnitude and direction (sign) of various correlation coefficients.

```{r}
#| label: fig-interepretingCorrelationCoefficients
#| fig-cap: "Interpretation of the Magnitude and Direction (Sign) of Correlation Coefficients."
#| fig-alt: "Interpretation of the Magnitude and Direction (Sign) of Correlation Coefficients."
#| fig-height: 9
#| fig-width: 9
#| code-fold: true

library("patchwork")

set.seed(52242)
correlations2 <- data.frame(criterion = rnorm(15))

correlations2$v1 <- complement(correlations2$criterion, -1)
correlations2$v2 <- complement(correlations2$criterion, -.9)
correlations2$v3 <- complement(correlations2$criterion, -.8)
correlations2$v4 <- complement(correlations2$criterion, -.7)
correlations2$v5 <- complement(correlations2$criterion, -.6)
correlations2$v6 <- complement(correlations2$criterion, -.5)
correlations2$v7 <- complement(correlations2$criterion, -.4)
correlations2$v8 <- complement(correlations2$criterion, -.3)
correlations2$v9 <- complement(correlations2$criterion, -.2)
correlations2$v10 <-complement(correlations2$criterion, -.1)
correlations2$v11 <-complement(correlations2$criterion, 0)
correlations2$v12 <-complement(correlations2$criterion, .1)
correlations2$v13 <-complement(correlations2$criterion, .2)
correlations2$v14 <-complement(correlations2$criterion, .3)
correlations2$v15 <-complement(correlations2$criterion, .4)
correlations2$v16 <-complement(correlations2$criterion, .5)
correlations2$v17 <-complement(correlations2$criterion, .6)
correlations2$v18 <-complement(correlations2$criterion, .7)
correlations2$v19 <-complement(correlations2$criterion, .8)
correlations2$v20 <-complement(correlations2$criterion, .9)
correlations2$v21 <-complement(correlations2$criterion, 1)

# -1.0
p1 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v1
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Perfect Negative Association",
    subtitle = expression(paste(italic("r"), " = ", "âˆ’1.0"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# -0.9
p2 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v2
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Strong Negative Association",
    subtitle = expression(paste(italic("r"), " = ", "âˆ’.9"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# -0.5
p3 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v6
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Moderate Negative Association",
    subtitle = expression(paste(italic("r"), " = ", "âˆ’.5"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# -0.2
p4 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v9
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Weak Negative Association",
    subtitle = expression(paste(italic("r"), " = ", "âˆ’.2"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# 0.0
p5 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v11
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "No Association",
    subtitle = expression(paste(italic("r"), " = ", ".0"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# 0.2
p6 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v13
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Weak Positive Association",
    subtitle = expression(paste(italic("r"), " = ", ".2"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# 0.5
p7 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v16
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Moderate Positive Association",
    subtitle = expression(paste(italic("r"), " = ", ".5"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# 0.9
p8 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v20
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Strong Positive Association",
    subtitle = expression(paste(italic("r"), " = ", ".9"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

# 1.0
p9 <- ggplot(
  data = correlations2,
  mapping = aes(
    x = criterion,
    y = v21
  )
) + 
  geom_point() +
  geom_smooth(
    method = "lm",
    se = FALSE) +
  labs(
    title = "Perfect Positive Association",
    subtitle = expression(paste(italic("r"), " = ", "1.0"))
  ) +
  theme_classic(
    base_size = 12) +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank())

p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 +
  plot_layout(
    ncol = 3,
    heights = 1,
    widths = 1)
```

An interactive visualization by @Magnusson2023 on interpreting correlations is at the following link: <https://rpsychologist.com/correlation/> (archived at <https://perma.cc/G8YR-VCM4>)

Keep in mind that the ([Pearson](#sec-correlationExamplesPearson)) correlation examines the strength of the *linear* association between two variables.\index{correlation}
If the association between two variables is nonlinear, the ([Pearson](#sec-correlationExamplesPearson)) correlation provides the strength of the linear trend and may not provide a meaningful index of the strength of the association between the variables.\index{correlation}
For instance, Anscombe's quartet includes four sets of data that have nearly identical basic descriptive statistics (see Tables [-@tbl-anscombe] and [-@tbl-anscombeStats]), including the same bivariate correlation, yet have very different distributions and whose association takes very different forms (see @fig-anscombeQuartet).\index{correlation}

| x1 | y1    | x2 | y2   | x3 | y3    | x4 | y4    |
|---:|------:|---:|-----:|---:|------:|---:|------:|
| 10 | 8.04  | 10 | 9.14 | 10 | 7.46  | 8  | 6.58  |
| 8  | 6.95  | 8  | 8.14 | 8  | 6.77  | 8  | 5.76  |
| 13 | 7.58  | 13 | 8.74 | 13 | 12.74 | 8  | 7.71  |
| 9  | 8.81  | 9  | 8.77 | 9  | 7.11  | 8  | 8.84  |
| 11 | 8.33  | 11 | 9.26 | 11 | 7.81  | 8  | 8.47  |
| 14 | 9.96  | 14 | 8.10 | 14 | 8.84  | 8  | 7.04  |
| 6  | 7.24  | 6  | 6.13 | 6  | 6.08  | 8  | 5.25  |
| 4  | 4.26  | 4  | 3.10 | 4  | 5.39  | 19 | 12.50 |
| 12 | 10.84 | 12 | 9.13 | 12 | 8.15  | 8  | 5.56  |
| 7  | 4.82  | 7  | 7.26 | 7  | 6.42  | 8  | 7.91  |
| 5  | 5.68  | 5  | 4.74 | 5  | 5.73  | 8  | 6.89  |

: Anscombe's Quartet {#tbl-anscombe}

| Property                     | Value        |
|:-----------------------------|:-------------|
| Sample size                  | 11           |
| Mean of X                    | 9.0          |
| Mean of Y                    | ~7.5         |
| Variance of X                | 11.0         |
| Variance of Y                | ~4.1         |
| Equation of regression line  | Y = 3 + 0.5X |
| Standard error of slope      | 0.118        |
| One-sample t-statistic       | 4.24         |
| Sum of squares of X          | 110.0        |
| Regression sum of squares    | 27.50        |
| Residual sum of squares of Y | 13.75        |
| Correlation coefficient      | .816         |
| Coefficient of determination | .67          |

: Descriptive Statistics of Anscombe's Quartet {#tbl-anscombeStats}

```{r}
#| output: false

anscombe
```

```{r}
#| label: fig-anscombeQuartet
#| fig-cap: "Anscombe's Quartet."
#| fig-alt: "Anscombe's Quartet."
#| fig-height: 6
#| fig-width: 6
#| code-fold: true

par(mfrow = c(2,2))

plot(
  anscombe$x1,
  anscombe$y1,
  xlab = "x",
  ylab = "y",
  xlim = c(0,20),
  ylim = c(0,15),
  main = 
    substitute(
      paste(italic(r), " = ", x, sep = ""),
      list(x = round(cor.test(x = anscombe$x1, y = anscombe$y1)$estimate, 2))))

abline(lm(
  y1 ~ x1,
  data = anscombe),
  col = "black",
  lty = 2)

plot(
  anscombe$x2,
  anscombe$y2,
  xlab = "x",
  ylab = "y",
  xlim = c(0,20),
  ylim = c(0,15),
  main = substitute(
    paste(italic(r), " = ", x, sep = ""),
    list(x = round(cor.test(x = anscombe$x2, y = anscombe$y2)$estimate, 2))))

abline(lm(
  y2 ~ x2,
  data = anscombe),
  col = "black",
  lty = 2)

plot(
  anscombe$x3,
  anscombe$y3,
  xlab = "x",
  ylab = "y",
  xlim = c(0,20),
  ylim = c(0,15),
  main = substitute(
    paste(italic(r), " = ", x, sep = ""),
    list(x = round(cor.test(x = anscombe$x3, y = anscombe$y3)$estimate, 2))))

abline(lm(
  y3 ~ x3,
  data = anscombe),
  col = "black",
  lty = 2)

plot(
  anscombe$x4,
  anscombe$y4,
  xlab = "x",
  ylab = "y",
  xlim = c(0,20),
  ylim = c(0,15),
  main = substitute(
    paste(italic(r), " = ", x, sep = ""),
    list(x = round(cor.test(x = anscombe$x4, y = anscombe$y4)$estimate, 2))))

abline(lm(
  y4 ~ x4,
  data = anscombe),
  col = "black",
  lty = 2)
```

## Examples {#sec-correlationExamples}

### Player Height and Fantasy Points {#sec-correlationHeightFantasyPoints}

Is there an association between a player's height and the number of fantasy points they score?
We can examine this possibility separately by position.

First, let's examine [descriptive statistics](#sec-descriptiveStatistics) of height and fantasy points (height is measured in inches):

```{r}
player_stats_seasonal %>% 
  dplyr::select(height, fantasyPoints) %>% 
  dplyr::summarise(across(
      everything(),
      .fns = list(
        n = ~ length(na.omit(.)),
        missingness = ~ mean(is.na(.)) * 100,
        M = ~ mean(., na.rm = TRUE),
        SD = ~ sd(., na.rm = TRUE),
        min = ~ min(., na.rm = TRUE),
        max = ~ max(., na.rm = TRUE),
        range = ~ max(., na.rm = TRUE) - min(., na.rm = TRUE),
        IQR = ~ IQR(., na.rm = TRUE),
        MAD = ~ mad(., na.rm = TRUE),
        median = ~ median(., na.rm = TRUE),
        pseudomedian = ~ DescTools::HodgesLehmann(., na.rm = TRUE),
        mode = ~ petersenlab::Mode(., multipleModes = "mean"),
        skewness = ~ psych::skew(., na.rm = TRUE),
        kurtosis = ~ psych::kurtosi(., na.rm = TRUE)),
      .names = "{.col}.{.fn}")) %>%
    tidyr::pivot_longer(
      cols = everything(),
      names_to = c("variable","index"),
      names_sep = "\\.") %>% 
    tidyr::pivot_wider(
      names_from = index,
      values_from = value)
```

#### Covariance {#sec-correlationExamplesCovariance}

The following syntax shows the variance-covariance of a set of variables, where the covariance of a variable with itself is the variable's variance.

```{r}
cov(
  player_stats_seasonal[,c("height","weight","fantasyPoints")],
  use = "pairwise.complete.obs")
```

If you just want the covariance between two variables, you can use the following syntax.

```{r}
cov(
  player_stats_seasonal$height,
  player_stats_seasonal$fantasyPoints,
  use = "pairwise.complete.obs")
```

Thus, it appears there is a negative covariance between height and fantasy points in the whole population of National Football League (NFL) players.

As shown below, the negative covariance between height and fantasy points holds when examining just Quarterbacks, Running Backs, Wide Receivers, and Tight Ends.

```{r}
cov(
  player_stats_seasonal %>% filter(position %in% c("QB","RB","WR","TE")) %>% select(height,fantasyPoints),
  use = "pairwise.complete.obs")
```

We can also examine the association separately by position.
Here is the association for Quarterbacks:

```{r}
cov(
  player_stats_seasonal %>% filter(position == "QB") %>% select(height,fantasyPoints),
  use = "pairwise.complete.obs")
```

Here is the association for Running Backs:

```{r}
cov(
  player_stats_seasonal %>% filter(position == "RB") %>% select(height,fantasyPoints),
  use = "pairwise.complete.obs")
```

Here is the association for Wide Receivers:

```{r}
cov(
  player_stats_seasonal %>% filter(position == "WR") %>% select(height,fantasyPoints),
  use = "pairwise.complete.obs")
```

Here is the association for Tight Ends:

```{r}
cov(
  player_stats_seasonal %>% filter(position == "TE") %>% select(height,fantasyPoints),
  use = "pairwise.complete.obs")
```

Interestingly, there is a *positive* covariance between height and fantasy points when examining each position separately.
This is an example of [Simpson's paradox](#sec-simpsonsParadox), where the sign of an association differs at different levels of analysis, as described in @sec-simpsonsParadox.
There is a *negative* covariance between height and fantasy points when examining Quarterbacks, Running Backs, Wide Receivers, and Tight Ends altogether, but there is a *positive* covariance between height and fantasy points when examining the association between each position separately.
That is, if you were to examine all positions together, you might assume that being taller might be disadvantageous to scoring fantasy fantasy points; however, once we examine the association within a given position, it emerges that height appears to be somewhat advantageous, if anything, for scoring fantasy points.

The covariance is an unstandardized index of the association between two variables.
However, it can be helpful to use a standardized index to examine the [effect size](#sec-practicalSignificance)â€”i.e., the strength of the association.
The [Pearson correlation coefficient](#sec-correlationExamplesPearson), $r$, is an example of a standardized index of the covariance between two variables.

#### Pearson Correlation {#sec-correlationExamplesPearson}

Pearson correlation assumes that the variables are [interval](#sec-interval) or [ratio](#sec-interval) level of measurement.
If the data are [ordinal](#sec-ordinal), [Spearman correlation](#sec-correlationExamplesSpearman) is more appropriate. 

The following syntax shows the Pearson correlation matrix of a set of variables.

```{r}
cor(
  player_stats_seasonal[,c("height","weight","fantasyPoints")],
  use = "pairwise.complete.obs")
```

Notice the diagonal values are all 1.0.
That is because a variable is perfectly correlated with itself.
Also notice that the values above the diagonal are a mirror image of (i.e., they are the same as) the respective values below the diagonal.
The association between two variables is the same regardless of the order (i.e., which is the [predictor variable](#sec-correlationalStudy) and which is the [outcome variable](#sec-correlationalStudy)); that is, the association between variables A and B is the same as the association between variables B and A.

If you just want the Pearson correlation between two variables, you can use the following syntax.

```{r}
cor.test(
  ~ height + fantasyPoints,
  data = player_stats_seasonal
)
```

```{r}
#| include: false

pearson_test <- cor.test(
  ~ height + fantasyPoints,
  data = player_stats_seasonal
)

pearson_r <- pearson_test$estimate
pearson_df <- pearson_test$parameter
pearson_p <- pearson_test$p.value
```

The $r$ value (`{r} petersenlab::apa(pearson_r, decimals = 2, leading = FALSE)`) is slightly negative.
The [effect size](#sec-practicalSignificance) of the association is smallâ€”for what is considered a small, medium, and large effect size for $r$ values, see @sec-effectSizeThresholds.
The $p$-value is less than .05, so it is a statistically significant correlation.
We could report this finding as follows: There was a statistically significant, weak negative association between height and fantasy points ($r(`r pearson_df`) = `r petersenlab::apa(pearson_r, decimals = 2, leading = FALSE)`$, $p `r petersenlab::pValue(pearson_p)`$); the taller a player was, the fewer fantasy points they tended to score.

Now let's examine the association separately by position.
Here is the association for Quarterbacks:

```{r}
cor.test(
  ~ height + fantasyPoints,
  data = player_stats_seasonal %>% 
    filter(position == "QB")
)
```

Here is the association for Running Backs:

```{r}
cor.test(
  ~ height + fantasyPoints,
  data = player_stats_seasonal %>% 
    filter(position == "RB")
)
```

Here is the association for Wide Receivers:

```{r}
cor.test(
  ~ height + fantasyPoints,
  data = player_stats_seasonal %>% 
    filter(position == "WR")
)
```

Here is the association for Tight Ends:

```{r}
cor.test(
  ~ height + fantasyPoints,
  data = player_stats_seasonal %>% 
    filter(position == "TE")
)
```

The sign of the association was positive for each position, suggesting that, for a given position (among Quarterbacks, Running Backs, Wide Receivers, and Tight Ends), taller players tend to score more fantasy points.
However, the [effect size](#sec-practicalSignificance) is small.
Moreover, as described in @sec-correlation-correlationAndCausation, just because there is an association between variables does not mean that the association reflects a causal effect.

#### Scatterplot with Best-Fit Line

A [scatterplot](#sec-scatterplot) with best-fit line is in @fig-scatterplotWithBestFitLine.

```{r}
#| label: fig-scatterplotWithBestFitLine
#| fig-cap: "Scatterplot of Fantasy Points (Season) by Player Height With Best-Fit Line. The linear best-fit line is in black. The nonlinear best-fit line is in blue."
#| fig-alt: "Scatterplot of Fantasy Points (Season) by Player Height With Best-Fit Line. The linear best-fit line is in black. The nonlinear best-fit line is in blue."

plot_scatterplot <- ggplot2::ggplot(
  data = player_stats_seasonal %>% filter(position %in% c("QB","RB","WR","TE")),
  aes(
    x = height,
    y = fantasyPoints)) +
  geom_point(
    aes(
      text = player_display_name, # add player name for mouse over tooltip
      label = season), # add season for mouse over tooltip
    alpha = 0.05) +
  geom_smooth(
    method = "lm",
    color = "black") +
  geom_smooth() +
  coord_cartesian(
    ylim = c(0,NA),
    expand = FALSE) +
  labs(
    x = "Player Height (Inches)",
    y = "Fantasy Points (Season)",
    title = "Fantasy Points (Season) by Player Height",
    subtitle = "(Among QBs, RBs, WRs, and TEs)"
  ) +
  theme_classic()

plotly::ggplotly(plot_scatterplot)
```

Examining the linear line of best fit, we would see that there is a slight negative slope, consistent with a weak negative association.
However, the nonlinear best-fit line suggests that there may be an inverted-U-shaped association, which might suggest that there may be an "optimal range of height," where being too tall or too short may be a disadvantage.
We now examine the association between height and fantasy points separately by position in @fig-scatterplotByPositionWithBestFitLine.

```{r}
#| label: fig-scatterplotByPositionWithBestFitLine
#| fig-cap: "Scatterplot of Fantasy Points (Season) by Player Height and Position With Best-Fit Line. The linear best-fit line for each position is in black. The nonlinear best-fit line for each group is represented by the color line."
#| fig-alt: "Scatterplot of Fantasy Points (Season) by Player Height and Position With Best-Fit Line. The linear best-fit line for each position is in black. The nonlinear best-fit line for each group is represented by the color line."

plot_scatterplotByPosition <- ggplot2::ggplot(
  data = player_stats_seasonal %>% filter(position %in% c("QB","RB","WR","TE")),
  aes(
    x = height,
    y = fantasyPoints,
    color = position,
    fill = position)) +
  geom_point(
    aes(
      text = player_display_name, # add player name for mouse over tooltip
      label = season), # add season for mouse over tooltip
    alpha = 0.7) +
  geom_smooth(
    method = "lm",
    color = "black") +
  geom_smooth(
    method = "loess",
    span = 0.5) +
  coord_cartesian(
    ylim = c(0,NA),
    expand = FALSE) +
  labs(
    x = "Player Height (Inches)",
    y = "Fantasy Points (Season)",
    title = "Fantasy Points (Season) by Player Height and Position"
  ) +
  guides(fill = "none") +
  theme_classic()

plotly::ggplotly(plot_scatterplotByPosition)
```

Examining the association between height and fantasy points separately by position, it appears that there is an inverted-U-shaped associations for Wide Receivers, a relatively flat association for Running Backs and Tight Ends, and a nonlinear association among Quarterbacks.
Interestingly, the best-fit lines suggest that the different positions have different height ranges, which is confirmed in @fig-ridgelinePlotPlayerHeightByPosition.

```{r}
#| label: fig-ridgelinePlotPlayerHeightByPosition
#| fig-cap: "Ridgeline Plot of Player Height by Position."
#| fig-alt: "Ridgeline Plot of Player Height by Position."

ggplot2::ggplot(
  data = player_stats_seasonal %>%
    filter(position %in% c("QB","RB","WR","TE")),
  mapping = aes(
    x = height,
    y = position,
    group = position,
    fill = position)
) +
  ggridges::geom_density_ridges(
    bandwidth = 0.6
  ) +
  labs(
    x = "Height (inches)",
    y = "Position",
    title = "Ridgeline Plot of Player Height by Position"
  ) +
  theme_classic() + 
  theme(
    legend.position = "none",
    axis.title.y = element_text(angle = 0, vjust = 0.5)) # horizontal y-axis title
```

According to the distributions of player height by position (among Quarterbacks, Running Backs, Wide Receivers, and Tight Ends), Running Backs tend to be the shortest, followed by Wide Receivers; Tight Ends tend to be the tallest, followed by Quarterbacks.
Wide Receivers showed the greatest variability; some were quite short, whereas others were quite tall.

#### Rank Correlation: Spearman's Rho ($\rho$) {#sec-correlationExamplesSpearman}

Spearman's rho ($\rho$) is a rank correlation.
Spearman's rho does not assume that the data are [interval](#sec-interval) or [ratio](#sec-ratio) level of measurement.
Unlike [Pearson correlation](sec-correlationExamplesPearson), Spearman's rho allows estimating associations among variables that are [ordinal](#sec-ordinal) level of measurement.

For instance, using Spearman's rho, we could examine the association between a player's draft round and their fantasy points, because draft round is an [ordinal](#sec-ordinal) variable:

```{r}
cor(
  player_stats_seasonal[,c("draftround","fantasyPoints")],
  use = "pairwise.complete.obs",
  method = "spearman")

cor.test(
  ~ draftround + fantasyPoints,
  data = player_stats_seasonal,
  method = "spearman"
)
```

```{r}
#| include: false

spearman_test <- cor.test(
  ~ draftround + fantasyPoints,
  data = player_stats_seasonal,
  method = "spearman"
)

spearman_r <- spearman_test$estimate
spearman_df <- cor.test(
  ~ draftround + fantasyPoints,
  data = player_stats_seasonal)$parameter
spearman_p <- spearman_test$p.value
```

Using Spearman's rho, there is a negative association between draft round and fantasy points ($r(`r spearman_df`) = `r petersenlab::apa(spearman_r, decimals = 2, leading = FALSE)`$, $p `r petersenlab::pValue(spearman_p)`$); unsurprisingly, the earlier the round a player is drafted, the more fantasy points they tend to score.

#### Partial Correlation {#sec-correlationExamplesPartial}

A partial correlation examines the association between variables after controlling for one or more variables.
Below, we examine a partial [Pearson correlation](#sec-correlationExamplesPearson) between height and fantasy points after controlling for age using the the `partial.r()` function from the `psych` package [@R-psych].

```{r}
psych::partial.r(
  data = player_stats_seasonal,
  x = c("height","fantasyPoints"),
  y = "age"
)
```

Below, we examine a partial [Spearman's rho rank correlation](#sec-correlationExamplesSpearman) between height and fantasy points after controlling for age.

```{r}
psych::partial.r(
  data = player_stats_seasonal,
  x = c("draftround","fantasyPoints"),
  y = "age",
  method = "spearman"
)
```

#### Nonlinear Correlation {#sec-correlationExamplesNonlinear}

An example of a nonlinear correlation is a distance correlation and the xi ($\xi$) coefficient.
These estimates of nonlinear correlation only range from 0â€“1 (where 0 represents no association and 1 represents a strong association), so they are interpreted differently than a traditional correlation coefficient (which can be negative).
We can estimate a distance correlation using the `correlation()` function of the correlation package [@R-correlation; @Makowski2020_packages].

```{r}
correlation(
  data = player_stats_seasonal %>% select(draftround, fantasyPoints),
  method = "distance"
)
```

A nonlinear correlation can be estimated using the xi ($\xi$) coefficient from the `XICOR` package [@R-XICOR; @Chatterjee2021]:

```{r}
set.seed(52242) # for reproducibility

XICOR::xicor(
  player_stats_seasonal$height,
  player_stats_seasonal$fantasyPoints,
  method = "permutation",
  pvalue = TRUE
)
```

#### Correlation Matrix {#sec-correlationExamplesMatrix}

The [`petersenlab`](https://cran.r-project.org/web/packages/petersenlab/index.html) package [@R-petersenlab] contains the `cor.table()` function that generates a correlation matrix of variables, including the $r$ value, number of observations ($n$), asterisks denoting statistical significance, and $p$-value for each association.
The asterisks follow the following traditional convention for statistical significance: $^\dagger p < .1$; $^* p < .05$; $^{**} p < .01$; $^{***} p < .001$.
Here is a [Pearson correlation](#sec-correlationExamplesPearson) matrix:

```{r}
petersenlab::cor.table(
  player_stats_seasonal[,c("height","weight","fantasyPoints")]
)
```

Here is a [Spearman correlation](#sec-correlationExamplesSpearman) matrix:

```{r}
petersenlab::cor.table(
  player_stats_seasonal[,c("height","weight","fantasyPoints")],
  correlation = "spearman"
)
```

Here is a correlation matrix of variables that includes just the $r$ value and asterisks denoting the statistical significance of the association, for greater concision.

```{r}
petersenlab::cor.table(
  player_stats_seasonal[,c("height","weight","fantasyPoints")],
  type = "manuscript" # include r values and asterisks
)
```

Here is a correlation matrix of variables that includes just the $r$ value for even greater concision.

```{r}
petersenlab::cor.table(
  player_stats_seasonal[,c("height","weight","fantasyPoints")],
  type = "manuscriptBig" # include just r values
)
```

The [`petersenlab`](https://cran.r-project.org/web/packages/petersenlab/index.html) package [@R-petersenlab] contains the `partialcor.table()` function that generates a [partial correlation](#sec-correlationExamplesPartial) matrix.
Here is a [partial correlation](#sec-correlationExamplesPartial) matrix, controlling for the player's age:

```{r}
petersenlab::partialcor.table(
  player_stats_seasonal[,c("height","weight","fantasyPoints")],
  z = player_stats_seasonal$age # control for the player's age
)
```

#### Correlogram {#sec-correlationExamplesCorrelogram}

We can depict a correlogram using the `corrplot()` function of the `corrplot` package [@R-corrplot], as shown in @fig-correlogram.

```{r}
#| label: fig-correlogram
#| fig-cap: "Correlogram."
#| fig-alt: "Correlogram."

corrplot(cor(
  player_stats_seasonal[,c("height","weight","fantasyPoints")],
  use = "pairwise.complete.obs"))
```

## Addressing Non-Independence of Observations {#sec-correlationNonIndependence}

Please note that the $p$-value for a correlation assumes that the observations are independentâ€”in particular, that the residuals are not correlated.
However, the observations are not independent in the `player_stats_seasonal` dataframe used above, because the same player has multiple rowsâ€”one row corresponding to each season they played.
This non-independence violates the traditional assumptions of the significance test of a correlation.
We could address this assumption by analyzing only one season from each player or by estimating the significance of the correlation coefficient using cluster-robust standard errors.
For simplicity, we present results above from the whole dataframe.
In @sec-mixedModels, we discuss [mixed model](#sec-mixedModels) approaches that handle repeated measures and other data that violate assumptions of non-independence that are shared by correlation and [multiple regression](#sec-multipleRegression); assumptions of [multiple regression](#sec-multipleRegression) are described in @sec-assumptionsRegression.
In section @sec-multipleRegressionNonIndependence, we demonstrate how to account for non-independence of observations using cluster-robust standard errors.

## Impact of Outliers {#sec-correlationOutliers}

The correlation coefficient is strongly impacted by outliersâ€”i.e., extreme (i.e., very large or very small) values relative to the other observations.
Consider the following example:

```{r}
set.seed(52242)

v1 <- rnorm(10)
v2 <- rnorm(10)

cor.test(
  v1,
  v2
)
```

The associated scatterplot and best-fit line is in @fig-plotWithoutOutliers.

```{r}
#| label: fig-plotWithoutOutliers
#| fig-cap: "Scatterplot Without Strong Outliers."
#| fig-alt: "Scatterplot Without Strong Outliers."
#| code-fold: true

plot(
  v1,
  v2
)

petersenlab::addText(
  v1,
  v2,
  ycoord = 0.7
)

abline(lm(
  v2 ~ v1),
  col = "black")
```

Now, let's add one outlier.

```{r}
v1[length(v1) + 1] <- 4
v2[length(v2) + 1] <- 4

cor.test(
  v1,
  v2
)
```

The associated scatterplot and best-fit line for the updated data are in @fig-plotWithOutlier.

```{r}
#| label: fig-plotWithOutlier
#| fig-cap: "Scatterplot With Outlier (in Red)."
#| fig-alt: "Scatterplot With Outlier (in Red)."
#| code-fold: true

plot(
  v1,
  v2
)

petersenlab::addText(
  v1,
  v2
)

points(
  v1[length(v1)],
  v2[length(v2)],
  pch = 19,
  col = "red"
)

text(
  x = 3.8,
  y = 3.8,
  labels = "outlier")

abline(lm(
  v2 ~ v1),
  col = "black")
```

Note how the association was not close to being statistically significant without the outlier, but with the outlier added, the association is statistically significant.
One way to combat this is to use methods for estimating the association between variables that are robust to (i.e., less impacted by) outliers.
In the next section, we describe robust correlation methods.

### Examining Robust Correlation {#sec-correlationRobust}

There are various approaches to estimating correlation in the presence of outliersâ€”so-called *robust correlation* methods.

One approach is the biweight midcorrelation, which is based on the median rather than the mean, and is thus less sensitive to outliers.
Another approach is the percentage bend correlation, which gives less weight to observations that are farther away from the median.
We can estimate the each using the `correlation()` function of the correlation package [@R-correlation; @Makowski2020_packages].

```{r}
correlation(
  data = data.frame(v1, v2),
  method = "biweight"
)

correlation(
  data = data.frame(v1, v2),
  method = "percentage"
)
```

## Impact of Restricted Range {#sec-correlationRestrictedRange}

In addition to being [impacted by outliers](#sec-correlationOutliers), correlations can also be greatly impacted by restricted variability or restricted range [@Petersen2024a; @PetersenPrinciplesPsychAssessment].
Correlation depends on variability; if there is no or limited variability, it can be difficult to detect an association with another variable.
Thus, if a variable has restricted rangeâ€”such as owing to a floor effect or ceiling effectâ€”that tends to artificially weaken associations.
For instance, a floor effect is one in which many of the scores are the lowest possible score.
By contrast, a ceiling effect is one in which many of the scores are the highest possible score.

Consider the following association between passing attempts and expected points added (EPA) via passing.
Expected points added from a given play is calculated as the difference between a team's expected points before the play and the team's expected points after the play.
This can be summed across all passing plays to determine a player's EPA from passing during a given season.

```{r}
cor.test(
  player_stats_seasonal$attempts,
  player_stats_seasonal$passing_epa
)
```

There is a statistically significant, moderate positive association between passing attempts and expected points added (EPA) via passing, as depicted in @fig-scatterplotWithoutRangeRestriction.

```{r}
#| label: fig-scatterplotWithoutRangeRestriction
#| fig-cap: "Scatterplot of Player Passing Expected Points Added (Season) by Passing Attempts, Without Range Restriction. The best-fit line is in blue."
#| fig-alt: "Scatterplot of Player Passing Expected Points Added (Season) by Passing Attempts, Without Range Restriction. The best-fit line is in blue."
#| code-fold: true

plot_scatterplotWithoutRangeRestriction <- ggplot2::ggplot(
  data = player_stats_seasonal,
  aes(
    x = attempts,
    y = passing_epa)) +
  geom_point(
    aes(
      text = player_display_name, # add player name for mouse over tooltip
      label = season), # add season for mouse over tooltip
    alpha = 0.3) +
  geom_smooth(
    method = "lm") +
  #geom_smooth() +
  coord_cartesian(
    expand = FALSE) +
  labs(
    x = "Player's Passing Attempts",
    y = "Player's Expected Points Added (EPA) via Passing",
    title = "Player's Passing Expected Points Added (EPA)\nby Passing Attempts (Season)",
    #subtitle = ""
  ) +
  theme_classic()

plotly::ggplotly(plot_scatterplotWithoutRangeRestriction)
```

Now, consider the same association when we restrict the range to examine only those players who had fewer than 450 pass attempts in a season (thus resulting in less variability):

```{r}
cor.test(
  player_stats_seasonal %>% 
    filter(attempts < 450) %>% 
    select(attempts) %>% 
    pull(),
  player_stats_seasonal %>% 
    filter(attempts < 450) %>% 
    select(passing_epa) %>% 
    pull()
)
```

The association is no longer positive and is weak in terms of [effect size](#sec-effectSizeThresholds), as depicted in @fig-scatterplotWithRangeRestriction.

```{r}
#| label: fig-scatterplotWithRangeRestriction
#| fig-cap: "Scatterplot of Player Passing Expected Points Added (Season) by Passing Attempts, With Range Restriction. The best-fit line is in blue."
#| fig-alt: "Scatterplot of Player Passing Expected Points Added (Season) by Passing Attempts, With Range Restriction. The best-fit line is in blue."
#| code-fold: true

plot_scatterplotWithRangeRestriction <- ggplot2::ggplot(
  data = player_stats_seasonal %>% filter(attempts < 450),
  aes(
    x = attempts,
    y = passing_epa)) +
  geom_point(
    aes(
      text = player_display_name, # add player name for mouse over tooltip
      label = season), # add season for mouse over tooltip
    alpha = 0.3) +
  geom_smooth(
    method = "lm") +
  #geom_smooth() +
  coord_cartesian(
    expand = FALSE) +
  labs(
    x = "Player's Passing Attempts",
    y = "Player's Expected Points Added (EPA) via Passing",
    title = "Player's Passing Expected Points Added (EPA)\nby Passing Attempts (Season)",
    subtitle = "(Among Players with < 450 Passing Attempts)"
  ) +
  theme_classic()

plotly::ggplotly(plot_scatterplotWithRangeRestriction)
```

## Correlation Does Not Imply Causation {#sec-correlation-correlationAndCausation}

As described in @sec-correlationCausation, correlation does not imply causation.
There are several reasons (described in @sec-correlationCausation) that, just because `X` is correlated with `Y` does not necessarily mean that `X` causes `Y`.
However, correlation can still be useful.
In order for two processes to be causally related, they must be associated.
That is, association is necessary but insufficient for causality.

## Conclusion {#sec-correlationConclusion}

Correlation is an index of the association between variables.
The correlation coefficient ($r$) ranges from âˆ’1 to +1, and indicates the sign and magnitude of the association.
Although correlation does not imply causation, identifying associations between variables can still be useful because association is a necessary (but insufficient) condition for causality.

## Session Info {#sec-correlationSessionInfo}

::: {.content-visible when-format="html"}

```{r}
sessionInfo()
```

:::
