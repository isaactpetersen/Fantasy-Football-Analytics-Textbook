# Factor Analysis {#sec-factorAnalysis}

## Getting Started {#sec-factorAnalysisGettingStarted}

### Load Packages {#sec-factorAnalysisLoadPackages}

```{r}
library("psych")
library("nFactors")
library("lavaan")
library("lavaangui")
library("tidyverse")
```

### Load Data {#sec-factorAnalysisLoadData}

```{r}
#| eval: false
#| include: false

load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/player_stats_weekly.RData", fsep = ""))
load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/nfl_nextGenStats_weekly.RData", fsep = ""))
```

```{r}
load(file = "./data/player_stats_weekly.RData")
load(file = "./data/nfl_nextGenStats_weekly.RData")
```

### Prepare Data {#sec-factorAnalysisPrepareData}

#### Merge Data {#sec-factorAnalysisPrepareDataMerge}

```{r}
dataMerged <- full_join(
  player_stats_weekly,
  nfl_nextGenStats_weekly %>% select(-any_of(c("player_display_name","completions","attempts","receptions","targets"))),
  by = c("player_id" = "player_gsis_id","season","season_type","week"),
)
```

#### Specify Variables {#sec-pcaPrepareDataSpecifyVars}

```{r}
faVars <- c(
  "completions","attempts","passing_yards","passing_tds","passing_interceptions",
  "sacks_suffered","sack_yards_lost","sack_fumbles","sack_fumbles_lost",
  "passing_air_yards","passing_yards_after_catch","passing_first_downs",
  "passing_epa","passing_cpoe","passing_2pt_conversions","pacr","pass_40_yds",
  "pass_inc","pass_comp_pct","fumbles","two_pts",
  "avg_time_to_throw","avg_completed_air_yards","avg_intended_air_yards",
  "avg_air_yards_differential","aggressiveness","max_completed_air_distance",
  "avg_air_yards_to_sticks","passer_rating", #,"completion_percentage"
  "expected_completion_percentage","completion_percentage_above_expectation",
  "avg_air_distance","max_air_distance")
```

## Overview of Factor Analysis {#sec-factorAnalysisOverview}

Factor analysis involves the estimation of latent variables.
Latent variables are ways of studying and operationalizing theoretical constructs that cannot be directly observed or quantified.
Factor analysis is a class of latent variable models that is designated to identify the structure of a measure or set of measures, and ideally, a construct or set of constructs.\index{factor analysis}
It aims to identify the optimal latent structure for a group of variables.\index{factor analysis}
The goal of factor analysis is to identify simple, parsimonious factors that underlie the "junk" (i.e., scores filled with measurement error) that we observe.\index{parsimony}\index{factor analysis!confirmatory}

Factor analysis encompasses two general types: [confirmatory factor analysis](#sec-cfa) and [exploratory factor analysis](#sec-efa).\index{factor analysis}\index{factor analysis!confirmatory}\index{factor analysis!exploratory}
[*Exploratory factor analysis*](#sec-efa) (EFA) is a latent variable modeling approach that is used when the researcher has no a priori hypotheses about how a set of variables is structured.\index{factor analysis!exploratory}
[EFA](#sec-efa) seeks to identify the empirically optimal-fitting model in ways that balance accuracy (i.e., variance accounted for) and parsimony (i.e., simplicity).\index{parsimony}\index{factor analysis!exploratory}\index{latent variable}
[*Confirmatory factor analysis*](#sec-cfa) (CFA) is a latent variable modeling approach that is used when a researcher wants to evaluate how well a hypothesized model fits, and the model can be examined in comparison to alternative models.\index{factor analysis!confirmatory}\index{latent variable}
Using a [CFA](#sec-cfa) approach, the researcher can pit models representing two theoretical frameworks against each other to see which better accounts for the observed data.\index{factor analysis!confirmatory}

Factor analysis involves observed (manifest) variables and unobserved (latent) factors.\index{factor analysis}\index{latent variable}
Factor analysis assumes that the latent factor influences the manifest variables, and the latent factor therefore reflects the common variance among the variables.\index{factor analysis}\index{latent variable}\index{latent variable}
A factor model potentially includes factor loadings, residuals (errors or disturbances), intercepts/means, covariances, and regression paths.\index{factor analysis}
When depicting a factor analysis model, rectangles represent variables we observe (i.e., manifest variables), and circles represent latent (i.e., unobserved) variables.\index{factor analysis}\index{latent variable}
A regression path indicates a hypothesis that one variable (or factor) influences another, and it is depicted using a single-headed arrow.\index{factor analysis}
The standardized regression coefficient (i.e., beta or $\beta$) represents the strength of association between the variables or factors.\index{factor analysis}\index{standardized regression coefficient}
A factor loading is a regression path from a latent factor to an observed (manifest) variable.\index{factor analysis}\index{factor analysis!factor loading}\index{latent variable}
The standardized factor loading represents the strength of association between the variable and the latent factor.\index{factor analysis}\index{factor analysis!factor loading}\index{latent variable}\index{standardized regression coefficient}
A residual is variance in a variable (or factor) that is unexplained by other variables or factors.\index{factor analysis}\index{factor analysis!residual}
A variable's intercept is the expected value of the variable when the factor(s) (onto which it loads) is equal to zero.\index{factor analysis}\index{factor analysis!intercept}
A covariance is the unstandardized index of the strength of association between between variables (or factors), and it is depicted with a double-headed arrow.\index{factor analysis}\index{factor analysis!covariance}
Because a covariance is unstandardized, its scale depends on the scale of the variables.\index{factor analysis!covariance}
A covariance path between two variables represents omitted shared cause(s) of the variables.\index{factor analysis}\index{factor analysis!covariance}
For instance, if you depict a covariance path between two variables, it means that there is a shared cause of the two variables that is omitted from the model (for instance, if the common cause is not known or was not assessed).\index{factor analysis}\index{factor analysis!covariance}

In factor analysis, the relation between an indicator ($\text{X}$) and its underlying latent factor(s) ($\text{F}$) can be represented with a regression formula as in @eq-indicatorLatentAssociation:\index{factor analysis}\index{factor analysis!factor loading}\index{factor analysis!intercept}

$$
X = \lambda \cdot \text{F} + \text{Item Intercept} + \text{Error Term}
$$ {#eq-indicatorLatentAssociation}

where:

- $\text{X}$ is the observed value of the indicator
- $\lambda$ is the factor loading, indicating the strength of the association between the indicator and the latent factor(s)
- $\text{F}$ is the person's value on the latent factor(s)
- $\text{Item Intercept}$ represents the constant term that accounts for the expected value of the indicator when the latent factor(s) are zero
- $\text{Error Term}$ is the residual, indicating the extent of variance in the indicator that is not explained by the latent factor(s)

When the latent factors are uncorrelated, the (standardized) error term for an indicator is calculated as 1 minus the sum of squared standardized factor loadings for a given item (including cross-loadings).\index{factor analysis}\index{factor analysis!residual}\index{factor analysis!factor loading}\index{cross-loading}
A cross-loadings is when a variable loads onto more than one latent factor.\index{factor analysis}\index{cross-loading}

Factor analysis is a powerful technique to help identify the factor structure that underlies a measure or construct.\index{factor analysis}
However, given the extensive method variance that influences scores on measure, factor analysis (and [principal component analysis](#sec-pca)) tends to extract method factors.\index{factor analysis}\index{principal component analysis}\index{method bias}\index{factor analysis!method factor}
Method factors are factors that are related to the methods being assessed rather than the construct of interest.\index{factor analysis}\index{method bias}\index{factor analysis!method factor}
To better estimate construct factors, it is sometimes necessary to estimate both construct and method factors.\index{factor analysis}\index{factor analysis!method factor}

## Exploratory Factor Analysis {#sec-efa}

Exploratory factor analysis (EFA) is used if you have no a priori hypotheses about the factor structure of the model, but you would like to understand the latent variables represented by your items.\index{factor analysis!exploratory}

EFA is partly induced from the data.\index{factor analysis!exploratory}
You feed in the data and let the program build the factor model.\index{factor analysis!exploratory}
You can set some parameters going in, including how to extract or rotate the factors.\index{factor analysis!exploratory}
The factors are extracted from the data without specifying the number and pattern of loadings between the items and the latent factors [@Bollen2002].\index{factor analysis!exploratory}
All cross-loadings are freely estimated.\index{cross-loading}\index{factor analysis!exploratory}

### Factor Rotation {#sec-factorAnalysisRotation}

When using EFA or [principal component analysis](#sec-pca), an important step is, possibly, to rotate the factors to make them more interpretable and simple, which is the whole goal.\index{factor analysis}\index{principal component analysis}\index{rotation}
To interpret the results of a factor analysis, we examine the factor matrix.\index{factor analysis}\index{rotation}
The columns refer to the different factors; the rows refer to the different observed variables.\index{factor analysis}\index{rotation}
The cells in the table are the factor loadings—they are basically the correlation between the variable and the factor.\index{factor analysis}\index{rotation}\index{factor analysis!factor loading}
Our goal is to achieve a model with simple structure because it is easily interpretable.\index{factor analysis}\index{simple structure}
*Simple structure* means that every variable loads perfectly on one and only one factor, as operationalized by a matrix of factor loadings with values of one and zero and nothing else.\index{factor analysis}\index{simple structure}
An example of a factor matrix that follows simple structure is depicted in @fig-simpleStructure.\index{factor analysis}\index{simple structure}

::: {#fig-simpleStructure}
![](images/simpleStructure.png){fig-alt="Example of a Factor Matrix That Follows Simple Structure."}

Example of a Factor Matrix That Follows Simple Structure.
:::

An example of a factor analysis model that follows simple structure is depicted in @fig-factorSolutionSimpleStructure.\index{factor analysis}\index{simple structure}
Each variable loads onto one and only one factor, which makes it easy to interpret the meaning of each factor, because a given factor represents the common variance among the items that load onto it.\index{factor analysis}

::: {#fig-factorSolutionSimpleStructure}
![](images/factorSolutionSimpleStructure.png){fig-alt="Example of a Factor Analysis Model That Follows Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems."}

Example of a Factor Analysis Model That Follows Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems.
:::

However, pure simple structure only occurs in simulations, not in real-life data.\index{factor analysis}\index{simple structure}
In reality, our unrotated factor analysis model might look like the model in @fig-factorSolutionUnrotatedExample.\index{factor analysis}
In this example, the factor analysis model does not show simple structure because the items have cross-loadings—that is, the items load onto more than one factor.\index{factor analysis}\index{cross-loading}\index{simple structure}
The cross-loadings make it difficult to interpret the factors, because all of the items load onto all of the factors, so the factors are not very distinct from each other, which makes it difficult to interpret what the factors mean.\index{cross-loading}\index{factor analysis}\index{cross-loading}\index{simple structure}

::: {#fig-factorSolutionUnrotatedExample}
![](images/factorSolutionUnrotatedExample.png){fig-alt="Example of a Factor Analysis Model That Does Not Follow Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems."}

Example of a Factor Analysis Model That Does Not Follow Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems.
:::

As a result of the challenges of interpretability caused by cross-loadings, factor rotations are often performed.\index{cross-loading}\index{factor analysis}\index{rotation}
An example of an unrotated factor matrix is in @fig-factorMatrix.\index{factor analysis}\index{rotation}

::: {#fig-factorMatrix}
![](images/factorMatrix.png){fig-alt="Example of a Factor Matrix."}

Example of a Factor Matrix.
:::

In the example factor matrix in @fig-factorSolutionUnrotated, the factor analysis is not very helpful—it tells us very little because it did not distinguish between the two factors.\index{factor analysis}\index{rotation}
The variables have similar loadings on Factor 1 and Factor 2.\index{factor analysis}\index{rotation}
An example of a unrotated factor solution is in @fig-factorSolutionUnrotated.\index{factor analysis}\index{rotation}
In the figure, all of the variables are in the midst of the quadrants—they are not on the factors' axes.\index{factor analysis}\index{rotation}
Thus, the factors are not very informative.\index{factor analysis}\index{rotation}

::: {#fig-factorSolutionUnrotated}
![](images/factorSolutionUnrotated.png){fig-alt="Example of an Unrotated Factor Solution."}

Example of an Unrotated Factor Solution.
:::

As a result, to improve the interpretability of the factor analysis, we can do what is called rotation.\index{factor analysis}\index{rotation}
Rotation leverages the idea that there are infinite solutions to the factor analysis model that fit equally well.\index{factor analysis}\index{rotation}
*Rotation* involves changing the orientation of the factors by changing the axes so that variables end up with very high (close to one or negative one) or very low (close to zero) loadings, so that it is clear which factors include which variables.\index{factor analysis}\index{rotation}
That is, rotation rescales the factors and tries to identify the ideal solution (factor) for each variable.\index{factor analysis}\index{rotation}
It searches for simple structure and keeps searching until it finds a minimum.\index{factor analysis}\index{rotation}\index{simple structure}
After rotation, if the rotation was successful for imposing simple structure, each factor will have loadings close to one (or negative one) for some variables and close to zero for other variables.\index{factor analysis}\index{rotation}\index{simple structure}
The goal of factor rotation is to achieve simple structure, to help make it easier to interpret the meaning of the factors.\index{factor analysis}\index{rotation}\index{simple structure}

To perform factor rotation, orthogonal rotations are often used.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
Orthogonal rotations make the rotated factors uncorrelated.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
An example of a commonly used orthogonal rotation is varimax rotation.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
Varimax rotation maximizes the sum of the variance of the squared loadings (i.e., so that items have either a very high or very low loading on a factor) and yields axes with a 90-degree angle.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

An example of a factor matrix following an orthogonal rotation is depicted in @fig-factorMatrixRotated.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
An example of a factor solution following an orthogonal rotation is depicted in @fig-factorSolutionRotated.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

::: {#fig-factorMatrixRotated}
![](images/factorMatrixRotated.png){fig-alt="Example of a Rotated Factor Matrix."}

Example of a Rotated Factor Matrix.
:::

::: {#fig-factorSolutionRotated}
![](images/factorSolutionRotated.png){fig-alt="Example of a Rotated Factor Solution."}

Example of a Rotated Factor Solution.
:::

An example of a factor matrix from SPSS following an orthogonal rotation is depicted in @fig-rotatedFactorMatrix.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

::: {#fig-rotatedFactorMatrix}
![](images/rotatedFactorMatrix.png){fig-alt="Example of a Rotated Factor Matrix From SPSS."}

Example of a Rotated Factor Matrix From SPSS.
:::

An example of a factor structure from an orthogonal rotation is in @fig-orthogonalRotation.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

::: {#fig-orthogonalRotation}
![](images/FactorAnalysis-08.png){fig-alt="Example of a Factor Structure From an Orthogonal Rotation."}

Example of a Factor Structure From an Orthogonal Rotation.
:::

Sometimes, however, the two factors and their constituent variables may be correlated.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
Examples of two correlated factors may be depression and anxiety.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
When the two factors are correlated in reality, if we make them uncorrelated, this would result in an inaccurate model.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
Oblique rotation allows for factors to be correlated and yields axes with less an angle of less than 90 degrees.\index{factor analysis}\index{rotation}\index{oblique rotation}
However, if the factors have low correlation (e.g., .2 or less), you can likely continue with orthogonal rotation.\index{factor analysis}\index{rotation}\index{oblique rotation}
Nevertheless, just because an oblique rotation allows for correlated factors does not mean that the factors will be correlated, so oblique rotation provides greater flexibility than orthogonal rotation.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
An example of a factor structure from an oblique rotation is in @fig-obliqueRotation.\index{factor analysis}\index{rotation}\index{oblique rotation}
Results from an oblique rotation are more complicated than orthogonal rotation—they provide lots of output and are more complicated to interpret.\index{factor analysis}\index{rotation}\index{oblique rotation}
In addition, oblique rotation might not yield a smooth answer if you have a relatively small sample size.\index{factor analysis}\index{rotation}\index{oblique rotation}

::: {#fig-obliqueRotation}
![](images/FactorAnalysis-09.png){fig-alt="Example of a Factor Structure From an Oblique Rotation."}

Example of a Factor Structure From an Oblique Rotation.
:::

As an example of rotation based on interpretability, consider the Five-Factor Model of Personality (the Big Five), which goes by the acronym, OCEAN: **O**penness, **C**onscientiousness, **E**xtraversion, **A**greeableness, and **N**euroticism.\index{factor analysis}\index{rotation}\index{oblique rotation}
Although the five factors of personality are somewhat correlated, we can use rotation to ensure they are maximally independent.\index{factor analysis}\index{rotation}
Upon rotation, extraversion and neuroticism are essentially uncorrelated, as depicted in @fig-factorRotation.\index{factor analysis}\index{rotation}
The other pole of extraversion is intraversion and the other pole of neuroticism might be emotional stability or calmness.

::: {#fig-factorRotation}
![](images/factorRotation.png){fig-alt="Example of a Factor Rotation of Neuroticism and Extraversion."}

Example of a Factor Rotation of Neuroticism and Extraversion.
:::

Simple structure is achieved when each variable loads highly onto as few factors as possible (i.e., each item has only one significant or primary loading).\index{simple structure}
Oftentimes this is not the case, so we choose our rotation method in order to decide if the factors can be correlated (an oblique rotation) or if the factors will be uncorrelated (an orthogonal rotation).\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
If the factors are not correlated with each other, use an orthogonal rotation.\index{factor analysis}\index{rotation}\index{orthogonal rotation}
The correlation between an item and a factor is a factor loading, which is simply a way to ask how much a variable is correlated with the underlying factor.\index{factor analysis}\index{structural equation modeling!factor loading}
However, its interpretation is more complicated if there are correlated factors!\index{factor analysis}\index{rotation}\index{oblique rotation}

An orthogonal rotation (e.g., varimax) can help with simplicity of interpretation because it seeks to yield simple structure without cross-loadings.\index{simple structure}\index{cross-loading}\index{factor analysis}\index{rotation}\index{orthogonal rotation}
Cross-loadings are instances where a variable loads onto multiple factors.\index{cross-loading}
My recommendation would always be to use an orthogonal rotation if you have reason to believe that finding simple structure in your data is possible; otherwise, the factors are extremely difficult to interpret—what exactly does a cross-loading even mean?\index{cross-loading}\index{factor analysis}\index{rotation}\index{orthogonal rotation}
However, you should always try an oblique rotation, too, to see how strongly the factors are correlated.\index{factor analysis}\index{rotation}\index{oblique rotation}
Examples of oblique rotations include oblimin and promax.\index{factor analysis}\index{rotation}\index{oblique rotation}

### Example {#sec-efaExample}

We generated the scree plot in @fig-efaScreePlot1 using the `fa.parallel()` function of the `psych` package [@R-psych].

```{r}
#| label: fig-efaScreePlot1
#| fig-cap: "Scree Plot: With Comparisons to Simulated and Resampled Data."
#| fig-alt: "Scree Plot: With Comparisons to Simulated and Resampled Data."

psych::fa.parallel(
  x = dataMerged[faVars],
  fm = "minres", # errors out when using "ml"
  fa = "fa"
)
```

We generated the scree plot in @fig-efaScreePlot2 using the `nScree()` and `plotnScree()` functions of the `nFactors` package [@R-nFactors].

```{r}
#| label: fig-efaScreePlot2
#| fig-cap: "Scree Plot with Parallel Analysis."
#| fig-alt: "Scree Plot with Parallel Analysis."

screeDataEFA <- nFactors::nScree(
  x = cor(
    dataMerged[faVars],
    use = "pairwise.complete.obs"),
  model = "factors")

nFactors::plotnScree(screeDataEFA)
```

We generated the very simple structure (VSS) plots in Figures [-@fig-efaVSSPlot1] and [-@fig-efaVSSPlot2] using the `vss()` and `nfactors()` functions of the `psych` package [@R-psych].

```{r}
#| label: fig-efaVSSPlot1
#| fig-cap: "Very Simple Structure Plot."
#| fig-alt: "Very Simple Structure Plot."

psych::vss(
  dataMerged[faVars],
  rotate = "oblimin",
  fm = "minres") # errors out when using "mle"
```

```{r}
#| label: fig-efaVSSPlot2
#| fig-cap: "Model Indices by Number of Factors."
#| fig-alt: "Model Indices by Number of Factors."

psych::nfactors(
  dataMerged[faVars],
  rotate = "oblimin",
  fm = "minres") # errors out when using "ml"
```

We fit EFA models using the `efa()` function of the `lavaan` package [@Rosseel2012_packages; @R-lavaan].

```{r}
#| eval: false

efaOblique_fit <- lavaan::efa(
  data = dataMerged,
  ov.names = faVars,
  nfactors = 1:9,
  rotation = "geomin",
  missing = "ML",
  estimator = "MLR",
  meanstructure = TRUE
)
```

A path diagram of the EFA model was created using the `plot_lavaan()` function of the `lavaangui` package [@R-lavaangui; @Karch2025_packages].

```{r}
#| eval: false

lavaangui::plot_lavaan(efaOblique_fit)
```

## Confirmatory Factor Analysis {#sec-cfa}

Confirmatory factor analysis (CFA) is used to (dis)confirm a priori hypotheses about the factor structure of the model.\index{factor analysis!confirmatory}
CFA is a test of the hypothesis.\index{factor analysis!confirmatory}
In CFA, you specify the model and ask how well this model represents the data.\index{factor analysis!confirmatory}
The researcher specifies the number, meaning, associations, and pattern of free parameters in the factor loading matrix [@Bollen2002].\index{factor analysis!confirmatory}
A key advantage of CFA is the ability to directly compare alternative models (i.e., factor structures), which is valuable for theory testing [@Strauss2009].\index{factor analysis!confirmatory}
For instance, you could use [CFA](#cfa) to test whether the variance in several measures' scores is best explained with one factor or two factors.\index{factor analysis!confirmatory}
In CFA, cross-loadings are not estimated unless the researcher specifies them.\index{cross-loading}\index{factor analysis!confirmatory}

### Example {#sec-cfaExample}

We fit CFA models using the `cfa()` function of the `lavaan` package [@Rosseel2012_packages; @R-lavaan].

```{r}
cfaModel1_syntax <- '
 #Factor loadings
 F1 =~ completions + attempts + passing_yards + passing_tds + passing_interceptions + sacks_suffered + sack_yards_lost + sack_fumbles + sack_fumbles_lost + passing_air_yards + passing_yards_after_catch + passing_first_downs + pass_inc + fumbles
 F2 =~ passing_air_yards + pacr + avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + avg_air_yards_to_sticks + expected_completion_percentage + avg_air_distance + max_air_distance
 F3 =~ passing_tds + passing_epa + passing_cpoe + pass_40_yds + pass_comp_pct + avg_air_yards_differential + max_completed_air_distance + avg_time_to_throw + avg_completed_air_yards + avg_intended_air_yards + passer_rating + completion_percentage_above_expectation
'
```

```{r}
#| eval: false

cfaModel1_fit <- lavaan::cfa(
  cfaModel1_syntax,
  data = dataMerged,
  missing = "ML",
  estimator = "MLR",
  std.lv = TRUE)
```

```{r}
#| eval: false

summary(
  cfaModel1_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)
```

```{r}
#| eval: false

lavaan::fitMeasures(
  cfaModel1_fit,
  fit.measures = c(
    "chisq", "df", "pvalue",
    "chisq.scaled", "df.scaled", "pvalue.scaled",
    "chisq.scaling.factor",
    "baseline.chisq","baseline.df","baseline.pvalue",
    "rmsea", "cfi", "tli", "srmr",
    "rmsea.robust", "cfi.robust", "tli.robust"))
```

```{r}
#| eval: false

lavaan::residuals(
  cfaModel1_fit,
  type = "cor")
```

```{r}
#| eval: false

lavaan::modificationindices(
  cfaModel1_fit,
  sort. = TRUE)
```

```{r}
#| eval: false

lavaan::lavPredict(cfaModel1_fit)
```

A path diagram of the CFA model was created using the `plot_lavaan()` function of the `lavaangui` package [@R-lavaangui; @Karch2025_packages].

```{r}
#| eval: false

lavaangui::plot_lavaan(cfaModel1_fit)
```

## Determining the Number of Factors to Retain {#sec-factorAnalysisNumFactors}

A goal of factor analysis and [principal component analysis](#sec-pca) is simplification or parsimony, while still explaining as much variance as possible.\index{parsimony}\index{factor analysis}\index{principal component analysis}
The hope is that you can have fewer factors that explain the associations between the variables than the number of observed variables.\index{factor analysis}
But how do you decide on the number of factors?\index{factor analysis}

There are a number of criteria that one can use to help determine how many factors/components to keep:\index{factor analysis}

- Kaiser-Guttman criterion: factors with eigenvalues greater than zero\index{factor analysis}\index{eigenvalue}
    - or, for [principal component analysis](#sec-pca), components with eigenvalues greater than 1\index{principal component analysis}\index{eigenvalue}
- Cattell's scree test: the "elbow" in a scree plot minus one; sometimes operationalized with optimal coordinates (OC) or the acceleration factor (AF)\index{factor analysis}\index{scree plot}
- Parallel analysis: factors that explain more variance than randomly simulated data\index{factor analysis}\index{parallel analysis}
- Very simple structure (VSS) criterion: larger is better\index{factor analysis}
- Velicer's minimum average partial (MAP) test: smaller is better\index{factor analysis}
- Akaike information criterion (AIC): smaller is better\index{factor analysis}
- Bayesian information criterion (BIC): smaller is better\index{factor analysis}
- Sample size-adjusted BIC (SABIC): smaller is better\index{factor analysis}
- Root mean square error of approximation (RMSEA): smaller is better\index{factor analysis}
- Chi-square difference test: smaller is better; a significant test indicates that the more complex model is significantly better fitting than the less complex model\index{factor analysis}
- Standardized root mean square residual (SRMR): smaller is better\index{factor analysis}
- Comparative Fit Index (CFI): larger is better\index{factor analysis}
- Tucker Lewis Index (TLI): larger is better\index{factor analysis}

There is not necessarily a "correct" criterion to use in determining how many factors to keep, so it is generally recommended that researchers use multiple criteria in combination with theory and interpretability.\index{factor analysis}

A scree plot provides lots of information.\index{factor analysis}\index{scree plot}
A scree plot has the factor number on the x-axis and the eigenvalue on the y-axis.\index{factor analysis}\index{scree plot}\index{eigenvalue}
The eigenvalue is the variance accounted for by a factor; when using a varimax (orthogonal) rotation, an eigenvalue (or factor variance) is calculated as the sum of squared standardized factor (or component) loadings on that factor.\index{factor analysis}\index{scree plot}\index{eigenvalue}
An example of a scree plot is in @fig-screePlot.\index{factor analysis}\index{scree plot}

::: {#fig-screePlot}
![](images/screePlot.png){fig-alt="Example of a Scree Plot."}

Example of a Scree Plot.
:::

The total variance is equal to the number of variables you have, so one eigenvalue is approximately one variable's worth of variance.\index{eigenvalue}\index{factor analysis}
The first factor accounts for the most variance, the second factor accounts for the second-most variance, and so on.\index{factor analysis}
The more factors you add, the less variance is explained by the additional factor.\index{factor analysis}

One criterion for how many factors to keep is the Kaiser-Guttman criterion.
According to the Kaiser-Guttman criterion, you should keep any factors whose eigenvalue is greater than 1.\index{factor analysis}
That is, for the sake of simplicity, parsimony, and data reduction, you should take any factors that explain more than a single variable would explain.\index{parsimony}\index{data!reduction}\index{factor analysis}
According to the Kaiser-Guttman criterion, we would keep three factors from @fig-screePlot that have eigenvalues greater than 1.\index{factor analysis}\index{eigenvalue}
The default in SPSS is to retain factors with eigenvalues greater than 1.\index{factor analysis}\index{eigenvalue}
However, keeping factors whose eigenvalue is greater than 1 is not the most correct rule.\index{factor analysis}\index{eigenvalue}
If you let SPSS do this, you may get many factors with eigenvalues around 1 (e.g., factors with an eigenvalue ~ 1.0001) that are not adding so much that it is worth the added complexity.\index{factor analysis}\index{eigenvalue}
The Kaiser-Guttman criterion usually results in keeping too many factors.\index{factor analysis}
Factors with small eigenvalues around 1 could reflect error shared across variables.\index{factor analysis}\index{eigenvalue}
For instance, factors with small eigenvalues could reflect method variance (i.e., method factor), such as a self-report factor that turns up as a factor in factor analysis, but that may be useless to you as a conceptual factor of a construct of interest.\index{factor analysis!method factor}\index{factor analysis}\index{eigenvalue}

Another criterion is Cattell's scree test, which involves selecting the number of factors from looking at the scree plot.\index{factor analysis}\index{scree plot}
"Scree" refers to the rubble of stones at the bottom of a mountain.\index{factor analysis}\index{scree plot}
According to Cattell's scree test, you should keep the factors before the last steep drop in eigenvalues—i.e., the factors before the rubble, where the slope approaches zero.\index{factor analysis}\index{eigenvalue}\index{scree plot}
The beginning of the scree (or rubble), where the slope approaches zero, is called the "elbow" of a scree plot.\index{factor analysis}\index{scree plot}
Using Cattell's scree test, you retain the number of factors that explain the most variance prior to the explained variance drop-off, because, ultimately, you want to include only as many factors in which you gain substantially more by the inclusion of these factors.\index{factor analysis}\index{scree plot}
That is, you would keep the number of factors at the elbow of the scree plot minus one.\index{factor analysis}\index{scree plot}
If the last steep drop occurs from Factor 4 to Factor 5 and the elbow is at Factor 5, we would keep four factors.\index{factor analysis}\index{scree plot}
In @fig-screePlot, the last steep drop in eigenvalues occurs from Factor 3 to Factor 4; the elbow of the scree plot occurs at Factor 4.\index{factor analysis}\index{scree plot}
We would keep the number of factors at the elbow minus one.\index{factor analysis}\index{scree plot}
Thus, using Cattell's scree test, we would keep three factors based on @fig-screePlot.\index{factor analysis}\index{scree plot}

There are more sophisticated ways of using a scree plot, but they usually end up at a similar decision.\index{factor analysis}\index{scree plot}
Examples of more sophisticated tests include parallel analysis and very simple structure (VSS) plots.\index{factor analysis}\index{very simple structure plot}\index{parallel analysis}
In a parallel analysis, you examine where the eigenvalues from observed data and random data converge, so you do not retain a factor that explains less variance than would be expected by random chance.\index{factor analysis}\index{parallel analysis}\index{eigenvalue}

In general, my recommendation is to use Cattell's scree test, and then test the factor solutions with plus or minus one factor.\index{factor analysis}\index{scree plot}
You should never accept factors with eigenvalues less than zero (or components from [principal component analysis](#sec-pca) with eigenvalues less than one), because they are likely to be largely composed of error.\index{factor analysis}\index{eigenvalue}
If you are using maximum likelihood factor analysis, you can compare the fit of various models with model fit criteria to see which model fits best for its parsimony.\index{parsimony}\index{factor analysis}
A model will always fit better when you add additional parameters or factors, so you examine if there is *significant* improvement in model fit when adding the additional factor—that is, we keep adding complexity until additional complexity does not buy us much.\index{parsimony}\index{factor analysis}
Always try a factor solution that is one less and one more than suggested by Cattell's scree test to buffer your final solution because the purpose of factor analysis is to explain things and to have interpretability.\index{factor analysis}\index{scree plot}
Even if all rules or indicators suggest to keep X number of factors, maybe $\pm$ one factor helps clarify things.\index{factor analysis}
Even though factor analysis is empirical, theory and interpretatability should also inform decisions.\index{factor analysis}

## Interpreting and Using Latent Factors {#sec-interpretingAndUsingLatentFactors}

The next step is interpreting the model and latent factors.\index{factor analysis}
One data matrix can lead to many different (correct) models—you must choose one based on the factor structure and theory.\index{factor analysis}
Use theory to interpret the model and label the factors.\index{factor analysis}

## Conclusion {#sec-factorAnalysisConclusion}

::: {.content-visible when-format="html"}

## Session Info {#sec-factorAnalysisSessionInfo}

```{r}
sessionInfo()
```

:::
