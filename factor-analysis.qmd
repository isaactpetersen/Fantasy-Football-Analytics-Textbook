{{< include _chunk-timing.qmd >}}

# Factor Analysis {#sec-factorAnalysis}

> "*All models are wrong, but some are useful.*"
>
> --- George Box [-@Box1979, p. 202]

## Getting Started {#sec-factorAnalysisGettingStarted}

### Load Packages {#sec-factorAnalysisLoadPackages}

```{r}
#| eval: false

library("psych")
library("nFactors")
library("lavaan")
library("lavaanPlot")
library("lavaangui")
library("tidyverse")
```

```{r}
#| include: false

# Don't load lavaangui (b/c not installed)
library("psych")
library("nFactors")
library("lavaan")
library("lavaanPlot")
library("tidyverse")
```

### Load Data {#sec-factorAnalysisLoadData}

```{r}
#| eval: false
#| include: false

load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/player_stats_seasonal.RData", fsep = ""))
load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/player_stats_weekly.RData", fsep = ""))
load(file = file.path(path, "/OneDrive - University of Iowa/Teaching/Courses/Fantasy Football/Data/nfl_nextGenStats_weekly.RData", fsep = ""))
```

```{r}
load(file = "./data/player_stats_seasonal.RData")
load(file = "./data/player_stats_weekly.RData")
load(file = "./data/nfl_nextGenStats_weekly.RData")
```

### Prepare Data {#sec-factorAnalysisPrepareData}

#### Season-Averages {#sec-factorAnalysisPrepareDataSeasonAvg}

```{r}
player_stats_seasonal_avgPerGame <- player_stats_seasonal %>% 
  mutate(
    completionsPerGame = completions / games,
    attemptsPerGame = attempts / games,
    passing_yardsPerGame = passing_yards / games,
    passing_tdsPerGame = passing_tds / games,
    passing_interceptionsPerGame = passing_interceptions / games,
    sacks_sufferedPerGame = sacks_suffered / games,
    sack_yards_lostPerGame = sack_yards_lost / games,
    sack_fumblesPerGame = sack_fumbles / games,
    sack_fumbles_lostPerGame = sack_fumbles_lost / games,
    passing_air_yardsPerGame = passing_air_yards / games,
    passing_yards_after_catchPerGame = passing_yards_after_catch / games,
    passing_first_downsPerGame = passing_first_downs / games,
    #passing_epaPerGame = passing_epa / games,
    #passing_cpoePerGame = passing_cpoe / games,
    passing_2pt_conversionsPerGame = passing_2pt_conversions / games,
    #pacrPerGame = pacr / games
    pass_40_ydsPerGame = pass_40_yds / games,
    pass_incPerGame = pass_inc / games,
    #pass_comp_pctPerGame = pass_comp_pct / games,
    fumblesPerGame = fumbles / games
  )

nfl_nextGenStats_seasonal <- nfl_nextGenStats_weekly %>% 
  filter(season_type == "REG") %>% 
  select(-any_of(c("week","season_type","player_display_name","player_position","team_abbr","player_first_name","player_last_name","player_jersey_number","player_short_name"))) %>% 
  group_by(player_gsis_id, season) %>% 
  summarise(
    across(everything(),
      ~ mean(.x, na.rm = TRUE)),
    .groups = "drop")
```

#### Merge Data {#sec-factorAnalysisPrepareDataMerge}

```{r}
dataMerged <- full_join(
  player_stats_seasonal_avgPerGame %>% select(-any_of("week")),
  nfl_nextGenStats_seasonal %>% select(-any_of(c("player_display_name","completions","attempts","receptions","targets"))),
  by = c("player_id" = "player_gsis_id","season"),
)
```

#### Specify Variables {#sec-pcaPrepareDataSpecifyVars}

```{r}
#| include: false

c(
  "completionsPerGame","attemptsPerGame","passing_yardsPerGame","passing_tdsPerGame","passing_interceptionsPerGame",
  "sacks_sufferedPerGame","sack_yards_lostPerGame","sack_fumblesPerGame","sack_fumbles_lostPerGame",
  "passing_air_yardsPerGame","passing_yards_after_catchPerGame","passing_first_downsPerGame",
  "passing_epa","passing_cpoe","passing_2pt_conversionsPerGame","pacr","pass_40_ydsPerGame",
  "pass_incPerGame","pass_comp_pct","fumblesPerGame", #"two_pts",
  "avg_time_to_throw","avg_completed_air_yards","avg_intended_air_yards",
  "avg_air_yards_differential","aggressiveness","max_completed_air_distance",
  "avg_air_yards_to_sticks","passer_rating", #,"completion_percentage"
  "expected_completion_percentage","completion_percentage_above_expectation",
  "avg_air_distance","max_air_distance")
```

```{r}
faVars <- c(
 "completionsPerGame","attemptsPerGame","passing_yardsPerGame","passing_tdsPerGame",
 "passing_air_yardsPerGame","passing_yards_after_catchPerGame","passing_first_downsPerGame",
 "avg_completed_air_yards","avg_intended_air_yards","aggressiveness","max_completed_air_distance",
 "avg_air_distance","max_air_distance","avg_air_yards_to_sticks","passing_cpoe","pass_comp_pct",
 "passer_rating","completion_percentage_above_expectation"
)
```

#### Subset Data {#sec-pcaPrepareDataSubset}

```{r}
dataMerged_subset <- dataMerged %>% 
  filter(!is.na(player_id)) %>% 
  filter(position == "QB")
```

#### Standardize Variables {#sec-pcaPrepareDataStandardize}

Standardizing variables is not strictly necessary in factor analysis; however, standardization can be helpful to prevent some variables from having considerably larger variances than others.

```{r}
dataForFA <- dataMerged_subset
dataForFA_standardized <- dataForFA

dataForFA_standardized[faVars] <- scale(dataForFA_standardized[faVars])
```

## Overview of Factor Analysis {#sec-factorAnalysisOverview}

Factor analysis involves the estimation of latent variables.
Latent variables are ways of studying and operationalizing theoretical constructs that cannot be directly observed or quantified.
Factor analysis is a class of latent variable models that is designated to identify the structure of a measure or set of measures, and ideally, a construct or set of constructs.\index{factor analysis}
It aims to identify the optimal latent structure for a group of variables.\index{factor analysis}
The goal of factor analysis is to identify simple, parsimonious factors that underlie the "junk" (i.e., scores filled with measurement error) that we observe.\index{parsimony}\index{factor analysis!confirmatory}

Factor analysis encompasses two general types: [confirmatory factor analysis](#sec-cfa) and [exploratory factor analysis](#sec-efa).\index{factor analysis}\index{factor analysis!confirmatory}\index{factor analysis!exploratory}
[*Exploratory factor analysis*](#sec-efa) (EFA) is a latent variable modeling approach that is used when the researcher has no a priori hypotheses about how a set of variables is structured.\index{factor analysis!exploratory}
[EFA](#sec-efa) seeks to identify the empirically optimal-fitting model in ways that balance accuracy (i.e., variance accounted for) and parsimony (i.e., simplicity).\index{parsimony}\index{factor analysis!exploratory}\index{latent variable}
[*Confirmatory factor analysis*](#sec-cfa) (CFA) is a latent variable modeling approach that is used when a researcher wants to evaluate how well a hypothesized model fits, and the model can be examined in comparison to alternative models.\index{factor analysis!confirmatory}\index{latent variable}
Using a [CFA](#sec-cfa) approach, the researcher can pit models representing two theoretical frameworks against each other to see which better accounts for the observed data.\index{factor analysis!confirmatory}

Factor analysis involves observed (manifest) variables and unobserved (latent) factors.\index{factor analysis}\index{latent variable}
Factor analysis assumes that the latent factor influences the manifest variables, and the latent factor therefore reflects the common variance among the variables.\index{factor analysis}\index{latent variable}\index{latent variable}
A factor model potentially includes factor loadings, residuals (errors or disturbances), intercepts/means, covariances, and regression paths.\index{factor analysis}
When depicting a factor analysis model, rectangles represent variables we observe (i.e., manifest variables), and circles represent latent (i.e., unobserved) variables.\index{factor analysis}\index{latent variable}
A regression path indicates a hypothesis that one variable (or factor) influences another, and it is depicted using a single-headed arrow.\index{factor analysis}
The standardized regression coefficient (i.e., beta or $\beta$) represents the strength of association between the variables or factors.\index{factor analysis}\index{standardized regression coefficient}
A factor loading is a regression path from a latent factor to an observed (manifest) variable.\index{factor analysis}\index{factor analysis!factor loading}\index{latent variable}
The standardized factor loading represents the strength of association between the variable and the latent factor, where conceptually, it is intended to reflect the magnitude that the latent factor influences the observed variable.\index{factor analysis}\index{factor analysis!factor loading}\index{latent variable}\index{standardized regression coefficient}
A residual is variance in a variable (or factor) that is unexplained by other variables or factors.\index{factor analysis}\index{factor analysis!residual}
A variable's intercept is the expected value of the variable when the factor(s) (onto which it loads) is equal to zero.\index{factor analysis}\index{factor analysis!intercept}
A [*covariance*](#sec-correlationExamplesCovariance) is the unstandardized index of the strength of association between between two variables (or factors), and it is depicted with a double-headed arrow.\index{factor analysis}\index{factor analysis!covariance}

Because a covariance is unstandardized, its scale depends on the scale of the variables.
The covariance between two variables is the average product of their deviations from their respective means, as in @eq-covariance.
The covariance of a variable with itself is equivalent to its variance, as in @eq-covarianceToVariance.
By contrast, a [*correlation*](#sec-correlation) is a standardized index of the strength of association between two variables.
Because a correlation is standardized (fixed between [−1,1]), its scale does not depend on the scales of the variables.
Because a covariance is unstandardized, its scale depends on the scale of the variables.\index{factor analysis!covariance}
A covariance path between two variables represents omitted shared cause(s) of the variables.\index{factor analysis}\index{factor analysis!covariance}
For instance, if you depict a covariance path between two variables, it means that there is a shared cause of the two variables that is omitted from the model (for instance, if the common cause is not known or was not assessed).\index{factor analysis}\index{factor analysis!covariance}

In factor analysis, the relation between an indicator ($\text{X}$) and its underlying latent factor(s) ($\text{F}$) can be represented with a regression formula as in @eq-indicatorLatentAssociation:\index{factor analysis}\index{factor analysis!factor loading}\index{factor analysis!intercept}

$$
X = \lambda \cdot \text{F} + \text{Item Intercept} + \text{Error Term}
$$ {#eq-indicatorLatentAssociation}

where:

- $\text{X}$ is the observed value of the indicator
- $\lambda$ is the factor loading, indicating the strength of the association between the indicator and the latent factor(s)
- $\text{F}$ is the person's value on the latent factor(s)
- $\text{Item Intercept}$ represents the constant term that accounts for the expected value of the indicator when the latent factor(s) are zero
- $\text{Error Term}$ is the residual, indicating the extent of variance in the indicator that is not explained by the latent factor(s)

When the latent factors are uncorrelated, the (standardized) error term for an indicator is calculated as 1 minus the sum of squared standardized factor loadings for a given item (including cross-loadings).\index{factor analysis}\index{factor analysis!residual}\index{factor analysis!factor loading}\index{cross-loading}
A cross-loading is when a variable loads onto more than one latent factor.\index{factor analysis}\index{cross-loading}

Factor analysis is a powerful technique to help identify the factor structure that underlies a measure or construct.\index{factor analysis}
However, given the extensive method variance that influences scores on measure, factor analysis (and [principal component analysis](#sec-pca)) tends to extract method factors.\index{factor analysis}\index{principal component analysis}\index{method bias}\index{factor analysis!method factor}
Method factors are factors that are related to the methods being assessed rather than the construct of interest.\index{factor analysis}\index{method bias}\index{factor analysis!method factor}
To better estimate construct factors, it is sometimes necessary to estimate both construct and method factors.\index{factor analysis}\index{factor analysis!method factor}

## Factor Analysis and Structural Equation Modeling

Factor analysis forms the measurement model component of a structural equation model (SEM).
The *measurement model* is what we settle on as the estimation of each construct before we add the structural component to estimate the relations among latent variables.
Basically, in a structural equation model, we add the structural component onto the measurement model.
For instance, our measurement model (i.e., based on factor analyis) might be to estimate, from a set of items, three latent factors: usage, aggressiveness, and performance.
Our structural model then may examine what processes (e.g., sport drink consumption or sleep) influence these latent factors, how the latent factors influence each other, and what the latent factors influence (e.g., fantasy points).
SEM is [confirmatory factor analysis](#sec-cfa) with regression paths that specify hypothesized causal relations between the latent variables (the structural component of the model).\index{latent variable}\index{structural equation modeling}\index{factor analysis!confirmatory}
Exploratory structural equation modeling (ESEM) is a form of SEM that allows for a combination of [exploratory factor analysis](#sec-efa) and [confirmatory factor analysis](#sec-cfa) to estimate latent variables and the relations between them.\index{latent variable}\index{structural equation modeling}\index{factor analysis!exploratory}\index{factor analysis!confirmatory}\index{structural equation modeling!exploratory}

## Path Diagrams {#sec-pathDiagrams}

A key tool when designing a factor analysis or structural equation model is a conceptual depiction of the hypothesized causal processes.
A *path diagram* depicts the hypothesized causal processes that link two or more variables.
Path diagrams are an example of a causal diagram and are similar to [directed acyclic graphs](#sec-causalDiagrams) discussed in @sec-causalDiagrams.
@Karch2025_packages provides a tool to create `lavaan` `R` syntax from a path analytic diagram: <https://lavaangui.org>.

In a path analysis diagram, rectangles represent variables we observe, and circles represent latent (i.e., unobserved) variables.
Single-headed arrows indicate regression paths, where conceptually, one variable is thought to influence another variable.
Double-headed arrows indicate covariance paths, where conceptually, two variables are associated for some unknown reason (i.e., an omitted shared cause).

## Decisions in Factor Analysis {#sec-factorAnalysisDecisions}

There are five primary decisions to make in factor analysis:

1. what variables to include in the model and how to scale them
1. method of factor extraction: whether to use exploratory or confirmatory factor analysis
1. if using exploratory factor analysis, whether and how to rotate factors
1. how many factors to retain (and what variables load onto which factors)
1. how to interpret and use the factors

The answer you get can differ highly depending on the decisions you make.
Below, we provide guidance for each of these decisions.

### 1. Variables to Include and their Scaling {#sec-variablesToInclude-factorAnalysis}

The first decision when conducting a factor analysis is which variables to include and the scaling of those variables.
What factors you extract can differ widely depending on what variables you include in the analysis.
For example, if you include many variables from the same source (e.g., self-report), it is possible that you will extract a factor that represents the common variance among the variables from that source (i.e., the self-reported variables).
This would be considered a method factor, which works against the goal of estimating latent factors that represent the constructs of interest (as opposed to the measurement methods used to estimate those constructs).

An additional consideration is the scaling of the variables: whether to use raw variables, standardized variables, or dividing some variables by a constant to make the variables' variances more similar.
Before performing a [principal component analysis](#sec-pca) (PCA), it is generally important to ensure that the variables included in the PCA are on the same scale.
PCA seeks to identify components that explain variance in the data, so if the variables are not on the same scale, some variables may contribute considerably more variance than others.
A common way of ensuring that variables are on the same scale is to standardize them using, for example,  [*z*-scores](#zScores) that have a mean of zero and standard deviation of one.
By contrast, factor analysis can better accommodate variables that are on different scales.

### 2. Method of Factor Extraction {#sec-methodOfFactorExtraction}

#### Exploratory Factor Analysis {#sec-efa}

Exploratory factor analysis (EFA) is used if you have no a priori hypotheses about the factor structure of the model, but you would like to understand the latent variables represented by your items.\index{factor analysis!exploratory}

EFA is partly induced from the data.\index{factor analysis!exploratory}
You feed in the data and let the program build the factor model.\index{factor analysis!exploratory}
You can set some parameters going in, including how to extract or rotate the factors.\index{factor analysis!exploratory}
The factors are extracted from the data without specifying the number and pattern of loadings between the items and the latent factors [@Bollen2002].\index{factor analysis!exploratory}
All cross-loadings are freely estimated.\index{cross-loading}\index{factor analysis!exploratory}

#### Confirmatory Factor Analysis {#sec-cfa}

Confirmatory factor analysis (CFA) is used to (dis)confirm a priori hypotheses about the factor structure of the model.\index{factor analysis!confirmatory}
CFA is a test of the hypothesis.\index{factor analysis!confirmatory}
In CFA, you specify the model and ask how well this model represents the data.\index{factor analysis!confirmatory}
The researcher specifies the number, meaning, associations, and pattern of free parameters in the factor loading matrix [@Bollen2002].\index{factor analysis!confirmatory}
A key advantage of CFA is the ability to directly compare alternative models (i.e., factor structures), which is valuable for theory testing [@Strauss2009].\index{factor analysis!confirmatory}
For instance, you could use [CFA](#cfa) to test whether the variance in several measures' scores is best explained with one factor or two factors.\index{factor analysis!confirmatory}
In CFA, cross-loadings are not estimated unless the researcher specifies them.\index{cross-loading}\index{factor analysis!confirmatory}

### 3. Factor Rotation {#sec-factorAnalysisRotation}

When using EFA or [principal component analysis](#sec-pca), an important step is, possibly, to rotate the factors to make them more interpretable and simple, which is the whole goal.\index{factor analysis}\index{principal component analysis}\index{rotation}
To interpret the results of a factor analysis, we examine the factor matrix.\index{factor analysis}\index{rotation}
The columns refer to the different factors; the rows refer to the different observed variables.\index{factor analysis}\index{rotation}
The cells in the table are the factor loadings—they are basically the correlation between the variable and the factor.\index{factor analysis}\index{rotation}\index{factor analysis!factor loading}
Our goal is to achieve a model with simple structure because it is easily interpretable.\index{factor analysis}\index{simple structure}
*Simple structure* means that every variable loads perfectly on one and only one factor, as operationalized by a matrix of factor loadings with values of one and zero and nothing else.\index{factor analysis}\index{simple structure}
An example of a factor matrix that follows simple structure is depicted in @fig-simpleStructure.\index{factor analysis}\index{simple structure}

::: {#fig-simpleStructure}
![](images/simpleStructure.png){fig-alt="Example of a Factor Matrix That Follows Simple Structure."}

Example of a Factor Matrix That Follows Simple Structure.
:::

An example of a factor analysis model that follows simple structure is depicted in @fig-factorSolutionSimpleStructure.\index{factor analysis}\index{simple structure}
Each variable loads onto one and only one factor, which makes it easy to interpret the meaning of each factor, because a given factor represents the common variance among the items that load onto it.\index{factor analysis}

::: {#fig-factorSolutionSimpleStructure}
![](images/factorSolutionSimpleStructure.png){fig-alt="Example of a Factor Analysis Model That Follows Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems."}

Example of a Factor Analysis Model That Follows Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems.
:::

However, pure simple structure only occurs in simulations, not in real-life data.\index{factor analysis}\index{simple structure}
In reality, our unrotated factor analysis model might look like the model in @fig-factorSolutionUnrotatedExample.\index{factor analysis}
In this example, the factor analysis model does not show simple structure because the items have cross-loadings—that is, the items load onto more than one factor.\index{factor analysis}\index{cross-loading}\index{simple structure}
The cross-loadings make it difficult to interpret the factors, because all of the items load onto all of the factors, so the factors are not very distinct from each other, which makes it difficult to interpret what the factors mean.\index{cross-loading}\index{factor analysis}\index{cross-loading}\index{simple structure}

::: {#fig-factorSolutionUnrotatedExample}
![](images/factorSolutionUnrotatedExample.png){fig-alt="Example of a Factor Analysis Model That Does Not Follow Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems."}

Example of a Factor Analysis Model That Does Not Follow Simple Structure. 'INT' = internalizing problems; 'EXT' = externalizing problems; 'TD' = thought-disordered problems.
:::

As a result of the challenges of interpretability caused by cross-loadings, factor rotations are often performed.\index{cross-loading}\index{factor analysis}\index{rotation}
An example of an unrotated factor matrix is in @fig-factorMatrix.\index{factor analysis}\index{rotation}

::: {#fig-factorMatrix}
![](images/factorMatrix.png){fig-alt="Example of a Factor Matrix."}

Example of a Factor Matrix.
:::

In the example factor matrix in @fig-factorSolutionUnrotated, the factor analysis is not very helpful—it tells us very little because it did not distinguish between the two factors.\index{factor analysis}\index{rotation}
The variables have similar loadings on Factor 1 and Factor 2.\index{factor analysis}\index{rotation}
An example of a unrotated factor solution is in @fig-factorSolutionUnrotated.\index{factor analysis}\index{rotation}
In the figure, all of the variables are in the midst of the quadrants—they are not on the factors' axes.\index{factor analysis}\index{rotation}
Thus, the factors are not very informative.\index{factor analysis}\index{rotation}

::: {#fig-factorSolutionUnrotated}
![](images/factorSolutionUnrotated.png){fig-alt="Example of an Unrotated Factor Solution."}

Example of an Unrotated Factor Solution.
:::

As a result, to improve the interpretability of the factor analysis, we can do what is called rotation.\index{factor analysis}\index{rotation}
Rotation leverages the idea that there are infinite solutions to the factor analysis model that fit equally well.\index{factor analysis}\index{rotation}
*Rotation* involves changing the orientation of the factors by changing the axes so that variables end up with very high (close to one or negative one) or very low (close to zero) loadings, so that it is clear which factors include which variables.\index{factor analysis}\index{rotation}
That is, rotation rescales the factors and tries to identify the ideal solution (factor) for each variable.\index{factor analysis}\index{rotation}
It searches for simple structure and keeps searching until it finds a minimum.\index{factor analysis}\index{rotation}\index{simple structure}
After rotation, if the rotation was successful for imposing simple structure, each factor will have loadings close to one (or negative one) for some variables and close to zero for other variables.\index{factor analysis}\index{rotation}\index{simple structure}
The goal of factor rotation is to achieve simple structure, to help make it easier to interpret the meaning of the factors.\index{factor analysis}\index{rotation}\index{simple structure}

To perform factor rotation, orthogonal rotations are often used.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
Orthogonal rotations make the rotated factors uncorrelated.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
An example of a commonly used orthogonal rotation is varimax rotation.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
Varimax rotation maximizes the sum of the variance of the squared loadings (i.e., so that items have either a very high or very low loading on a factor) and yields axes with a 90-degree angle.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

An example of a factor matrix following an orthogonal rotation is depicted in @fig-factorMatrixRotated.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}
An example of a factor solution following an orthogonal rotation is depicted in @fig-factorSolutionRotated.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

::: {#fig-factorMatrixRotated}
![](images/factorMatrixRotated.png){fig-alt="Example of a Rotated Factor Matrix."}

Example of a Rotated Factor Matrix.
:::

::: {#fig-factorSolutionRotated}
![](images/factorSolutionRotated.png){fig-alt="Example of a Rotated Factor Solution."}

Example of a Rotated Factor Solution.
:::

An example of a factor matrix from SPSS following an orthogonal rotation is depicted in @fig-rotatedFactorMatrix.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

::: {#fig-rotatedFactorMatrix}
![](images/rotatedFactorMatrix.png){fig-alt="Example of a Rotated Factor Matrix From SPSS."}

Example of a Rotated Factor Matrix From SPSS.
:::

An example of a factor structure from an orthogonal rotation is in @fig-orthogonalRotation.\index{factor analysis}\index{rotation}\index{simple structure}\index{orthogonal rotation}

::: {#fig-orthogonalRotation}
![](images/FactorAnalysis-08.png){fig-alt="Example of a Factor Structure From an Orthogonal Rotation."}

Example of a Factor Structure From an Orthogonal Rotation.
:::

Sometimes, however, the two factors and their constituent variables may be correlated.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
Examples of two correlated factors may be depression and anxiety.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
When the two factors are correlated in reality, if we make them uncorrelated, this would result in an inaccurate model.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
Oblique rotation allows for factors to be correlated and yields axes with less an angle of less than 90 degrees.\index{factor analysis}\index{rotation}\index{oblique rotation}
However, if the factors have low correlation (e.g., .2 or less), you can likely continue with orthogonal rotation.\index{factor analysis}\index{rotation}\index{oblique rotation}
Nevertheless, just because an oblique rotation allows for correlated factors does not mean that the factors will be correlated, so oblique rotation provides greater flexibility than orthogonal rotation.\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
An example of a factor structure from an oblique rotation is in @fig-obliqueRotation.\index{factor analysis}\index{rotation}\index{oblique rotation}
Results from an oblique rotation are more complicated than orthogonal rotation—they provide lots of output and are more complicated to interpret.\index{factor analysis}\index{rotation}\index{oblique rotation}
In addition, oblique rotation might not yield a smooth answer if you have a relatively small sample size.\index{factor analysis}\index{rotation}\index{oblique rotation}

::: {#fig-obliqueRotation}
![](images/FactorAnalysis-09.png){fig-alt="Example of a Factor Structure From an Oblique Rotation."}

Example of a Factor Structure From an Oblique Rotation.
:::

As an example of rotation based on interpretability, consider the Five-Factor Model of Personality (the Big Five), which goes by the acronym, OCEAN: **O**penness, **C**onscientiousness, **E**xtraversion, **A**greeableness, and **N**euroticism.\index{factor analysis}\index{rotation}\index{oblique rotation}
Although the five factors of personality are somewhat correlated, we can use rotation to ensure they are maximally independent.\index{factor analysis}\index{rotation}
Upon rotation, extraversion and neuroticism are essentially uncorrelated, as depicted in @fig-factorRotation.\index{factor analysis}\index{rotation}
The other pole of extraversion is intraversion and the other pole of neuroticism might be emotional stability or calmness.

::: {#fig-factorRotation}
![](images/factorRotation.png){fig-alt="Example of a Factor Rotation of Neuroticism and Extraversion."}

Example of a Factor Rotation of Neuroticism and Extraversion.
:::

Simple structure is achieved when each variable loads highly onto as few factors as possible (i.e., each item has only one significant or primary loading).\index{simple structure}
Oftentimes this is not the case, so we choose our rotation method in order to decide if the factors can be correlated (an oblique rotation) or if the factors will be uncorrelated (an orthogonal rotation).\index{factor analysis}\index{rotation}\index{orthogonal rotation}\index{oblique rotation}
If the factors are not correlated with each other, use an orthogonal rotation.\index{factor analysis}\index{rotation}\index{orthogonal rotation}
The correlation between an item and a factor is a factor loading, which is simply a way to ask how much a variable is correlated with the underlying factor.\index{factor analysis!factor loading}
However, its interpretation is more complicated if there are correlated factors!\index{factor analysis}\index{rotation}\index{oblique rotation}

An orthogonal rotation (e.g., varimax) can help with simplicity of interpretation because it seeks to yield simple structure without cross-loadings.\index{simple structure}\index{cross-loading}\index{factor analysis}\index{rotation}\index{orthogonal rotation}
Cross-loadings are instances where a variable loads onto multiple factors.\index{cross-loading}
My recommendation would always be to use an orthogonal rotation if you have reason to believe that finding simple structure in your data is possible; otherwise, the factors are extremely difficult to interpret—what exactly does a cross-loading even mean?\index{cross-loading}\index{factor analysis}\index{rotation}\index{orthogonal rotation}
However, you should always try an oblique rotation, too, to see how strongly the factors are correlated.\index{factor analysis}\index{rotation}\index{oblique rotation}
Examples of oblique rotations include oblimin and promax.\index{factor analysis}\index{rotation}\index{oblique rotation}

### 4. Determining the Number of Factors to Retain {#sec-factorAnalysisNumFactors}

A goal of factor analysis and [principal component analysis](#sec-pca) is simplification or parsimony, while still explaining as much variance as possible.\index{parsimony}\index{factor analysis}\index{principal component analysis}
The hope is that you can have fewer factors that explain the associations between the variables than the number of observed variables.\index{factor analysis}
It does not make sense to replace 18 variables with 18 latent factors because that would not result in any simplification.
But how do you decide on the number of factors?\index{factor analysis}

There are a number of criteria that one can use to help determine how many factors/components to keep:\index{factor analysis}

- Kaiser-Guttman criterion: factors with eigenvalues greater than zero\index{factor analysis}\index{eigenvalue}
    - or, for [principal component analysis](#sec-pca), components with eigenvalues greater than 1\index{principal component analysis}\index{eigenvalue}
- Cattell's scree test: the "elbow" in a scree plot minus one; sometimes operationalized with optimal coordinates (OC) or the acceleration factor (AF)\index{factor analysis}\index{scree plot}
- Parallel analysis: factors that explain more variance than randomly simulated data\index{factor analysis}\index{parallel analysis}
- Very simple structure (VSS) criterion: larger is better\index{factor analysis}
- Velicer's minimum average partial (MAP) test: smaller is better\index{factor analysis}
- Akaike information criterion (AIC): smaller is better\index{factor analysis}
- Bayesian information criterion (BIC): smaller is better\index{factor analysis}
- Sample size-adjusted BIC (SABIC): smaller is better\index{factor analysis}
- Root mean square error of approximation (RMSEA): smaller is better\index{factor analysis}
- Chi-square difference test: smaller is better; a significant test indicates that the more complex model is significantly better fitting than the less complex model\index{factor analysis}
- Standardized root mean square residual (SRMR): smaller is better\index{factor analysis}
- Comparative Fit Index (CFI): larger is better\index{factor analysis}
- Tucker Lewis Index (TLI): larger is better\index{factor analysis}

There is not necessarily a "correct" criterion to use in determining how many factors to keep, so it is generally recommended that researchers use multiple criteria in combination with theory and interpretability.\index{factor analysis}

A scree plot provides lots of information.\index{factor analysis}\index{scree plot}
A scree plot has the factor number on the x-axis and the eigenvalue on the y-axis.\index{factor analysis}\index{scree plot}\index{eigenvalue}
The eigenvalue is the variance accounted for by a factor; when using a varimax (orthogonal) rotation, an eigenvalue (or factor variance) is calculated as the sum of squared standardized factor (or component) loadings on that factor.\index{factor analysis}\index{scree plot}\index{eigenvalue}
An example of a scree plot is in @fig-screePlot.\index{factor analysis}\index{scree plot}

::: {#fig-screePlot}
![](images/screePlot.png){fig-alt="Example of a Scree Plot."}

Example of a Scree Plot.
:::

The total variance is equal to the number of variables you have, so one eigenvalue is approximately one variable's worth of variance.\index{eigenvalue}\index{factor analysis}
The first factor accounts for the most variance, the second factor accounts for the second-most variance, and so on.\index{factor analysis}
The more factors you add, the less variance is explained by the additional factor.\index{factor analysis}

One criterion for how many factors to keep is the Kaiser-Guttman criterion.
According to the Kaiser-Guttman criterion, you should keep any factors whose eigenvalue is greater than 1.\index{factor analysis}
That is, for the sake of simplicity, parsimony, and data reduction, you should take any factors that explain more than a single variable would explain.\index{parsimony}\index{data!reduction}\index{factor analysis}
According to the Kaiser-Guttman criterion, we would keep three factors from @fig-screePlot that have eigenvalues greater than 1.\index{factor analysis}\index{eigenvalue}
The default in SPSS is to retain factors with eigenvalues greater than 1.\index{factor analysis}\index{eigenvalue}
However, keeping factors whose eigenvalue is greater than 1 is not the most correct rule.\index{factor analysis}\index{eigenvalue}
If you let SPSS do this, you may get many factors with eigenvalues around 1 (e.g., factors with an eigenvalue ~ 1.0001) that are not adding so much that it is worth the added complexity.\index{factor analysis}\index{eigenvalue}
The Kaiser-Guttman criterion usually results in keeping too many factors.\index{factor analysis}
Factors with small eigenvalues around 1 could reflect error shared across variables.\index{factor analysis}\index{eigenvalue}
For instance, factors with small eigenvalues could reflect method variance (i.e., method factor), such as a self-report factor that turns up as a factor in factor analysis, but that may be useless to you as a conceptual factor of a construct of interest.\index{factor analysis!method factor}\index{factor analysis}\index{eigenvalue}

Another criterion is Cattell's scree test, which involves selecting the number of factors from looking at the scree plot.\index{factor analysis}\index{scree plot}
"Scree" refers to the rubble of stones at the bottom of a mountain.\index{factor analysis}\index{scree plot}
According to Cattell's scree test, you should keep the factors before the last steep drop in eigenvalues—i.e., the factors before the rubble, where the slope approaches zero.\index{factor analysis}\index{eigenvalue}\index{scree plot}
The beginning of the scree (or rubble), where the slope approaches zero, is called the "elbow" of a scree plot.\index{factor analysis}\index{scree plot}
Using Cattell's scree test, you retain the number of factors that explain the most variance prior to the explained variance drop-off, because, ultimately, you want to include only as many factors in which you gain substantially more by the inclusion of these factors.\index{factor analysis}\index{scree plot}
That is, you would keep the number of factors at the elbow of the scree plot minus one.\index{factor analysis}\index{scree plot}
If the last steep drop occurs from Factor 4 to Factor 5 and the elbow is at Factor 5, we would keep four factors.\index{factor analysis}\index{scree plot}
In @fig-screePlot, the last steep drop in eigenvalues occurs from Factor 3 to Factor 4; the elbow of the scree plot occurs at Factor 4.\index{factor analysis}\index{scree plot}
We would keep the number of factors at the elbow minus one.\index{factor analysis}\index{scree plot}
Thus, using Cattell's scree test, we would keep three factors based on @fig-screePlot.\index{factor analysis}\index{scree plot}

There are more sophisticated ways of using a scree plot, but they usually end up at a similar decision.\index{factor analysis}\index{scree plot}
Examples of more sophisticated tests include parallel analysis and very simple structure (VSS) plots.\index{factor analysis}\index{very simple structure plot}\index{parallel analysis}
In a parallel analysis, you examine where the eigenvalues from observed data and random data converge, so you do not retain a factor that explains less variance than would be expected by random chance.\index{factor analysis}\index{parallel analysis}\index{eigenvalue}
Using the VSS criterion, the optimal number of factors to retain is the number of factors that maximizes the VSS criterion [@Revelle1979].
The VSS criterion is evaluated with models in which factor loadings for a given item that are less than the maximum factor loading for that item are suppressed to zero, thus forcing simple structure (i.e., no cross-loadings).
The goal is finding a factor structure with interpretability so that factors are clearly distinguishable.
Thus, we want to identify the number of factors with the highest VSS criterion.

In general, my recommendation is to use Cattell's scree test, and then test the factor solutions with plus or minus one factor, in addition to examining [model fit](#sec-modelFitIndices).\index{factor analysis}\index{scree plot}
You should never accept factors with eigenvalues less than zero (or components from [PCA](#sec-pca) with eigenvalues less than one), because they are likely to be largely composed of error.\index{factor analysis}\index{eigenvalue}
If you are using maximum likelihood factor analysis, you can compare the fit of various models with model fit criteria to see which model fits best for its parsimony.\index{parsimony}\index{factor analysis}
A model will always fit better when you add additional parameters or factors, so you examine if there is *significant* improvement in model fit when adding the additional factor—that is, we keep adding complexity until additional complexity does not buy us much.\index{parsimony}\index{factor analysis}
Always try a factor solution that is one less and one more than suggested by Cattell's scree test to buffer your final solution because the purpose of factor analysis is to explain things and to have interpretability.\index{factor analysis}\index{scree plot}
Even if all rules or indicators suggest to keep X number of factors, maybe $\pm$ one factor helps clarify things.\index{factor analysis}
Even though factor analysis is empirical, theory and interpretatability should also inform decisions.\index{factor analysis}

#### Model Fit Indices {#sec-modelFitIndices}

In factor analysis, we fit a model to observed data, or to the variance-covariance matrix, and we evaluate the degree of model misfit.
That is, fit indices evaluate how likely it is that a given causal model gave rise to the observed data.
Various model fit indices can be used for evaluating how well a model fits the data and for comparing the fit of two competing models.
Fit indices known as absolute fit indices compare whether the model fits better than the best-possible fitting model (i.e., a saturated model).
Examples of absolute fit indices include the chi-square test, root mean square error of approximation (RMSEA), and the standardized root mean square residual (SRMR).

The chi-square test evaluates whether the model has a significant degree of misfit relative to the best-possible fitting model (a saturated model that fits as many parameters as possible; i.e., as many parameters as there are degrees of freedom); the null hypothesis of a chi-square test is that there is no difference between the predicted data (i.e., the data that would be observed if the model were true) and the observed data.
Thus, a non-significant chi-square test indicates good model fit.
However, because the null hypothesis of the chi-square test is that the model-implied covariance matrix is exactly equal to the observed covariance matrix (i.e., a model of perfect fit), this may be an unrealistic comparison.
Models are simplifications of reality, and our models are virtually never expected to be a perfect description of reality.
Thus, we would say a model is "useful" and partially validated if "it helps us to understand the relation between variables and does a 'reasonable' job of matching the data...A perfect fit may be an inappropriate standard, and a high chi-square estimate may indicate what we already know—that the hypothesized model holds approximately, not perfectly." [@Bollen1989, p. 268].
The power of the chi-square test depends on sample size, and a large sample will likely detect small differences as significantly worse than the best-possible fitting model [@Bollen1989].

RMSEA is an index of absolute fit.
Lower values indicate better fit.

SRMR is an index of absolute fit with no penalty for model complexity.
Lower values indicate better fit.

There are also various fit indices known as incremental, comparative, or relative fit indices that compare whether the model fits better than the worst-possible fitting model (i.e., a "baseline" or "null" model).
Incremental fit indices include a chi-square difference test, the comparative fit index (CFI), and the Tucker-Lewis index (TLI).
Unlike the chi-square test comparing the model to the best-possible fitting model, a significant chi-square test of the relative fit index indicates better fit—i.e., that the model fits better than the worst-possible fitting model.

CFI is another relative fit index that compares the model to the worst-possible fitting model.
Higher values indicate better fit.

TLI is another relative fit index.
Higher values indicate better fit.

Parsimony fit include fit indices that use information criteria fit indices, including the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC).\index{parsimony}
BIC penalizes model complexity more so than AIC.
Lower AIC and BIC values indicate better fit.

Chi-square difference tests and CFI can be used to compare two nested models.
AIC and BIC can be used to compare two non-nested models.

Criteria for acceptable fit and good fit of SEM models are in @tbl-semFitIndices.
In addition, dynamic fit indexes have been proposed based on simulation to identify fit index cutoffs that are tailored to the characteristics of the specific model and data [@McNeish2023]; dynamic fit indexes are available via the `dynamic` package [@R-dynamic] or with a [webapp](https://dynamicfit.app).

| SEM Fit Index | Acceptable Fit | Good Fit   |
|---------------|----------------|------------|
| RMSEA         | $\leq$ .08     | $\leq$ .05 |
| CFI           | $\geq$ .90     | $\geq$ .95 |
| TLI           | $\geq$ .90     | $\geq$ .95 |
| SRMR          | $\leq$ .10     | $\leq$ .08 |

: Criteria for Acceptable and Good Fit of Factor Analysis and Structural Equation Models Based on Fit Indices. {#tbl-semFitIndices}

However, good model fit does not necessarily indicate a true model.

In addition to global fit indices, it can also be helpful to examine evidence of local fit, such as the residual covariance matrix.
The residual covariance matrix represents the difference between the observed covariance matrix and the model-implied covariance matrix (the observed covariance matrix minus the model-implied covariance matrix).
These difference values are called *covariance residuals*.
Standardizing the covariance matrix by converting each to a correlation matrix can be helpful for interpreting the magnitude of any local misfit.
This is known as a residual correlation matrix, which is composed of *correlation residuals*.
Correlation residuals greater than |.10| are possible evidence for poor local fit [@Kline2023].
If a correlation residual is positive, it suggests that the model underpredicts the observed association between the two variables (i.e., the observed covariance is greater than the model-implied covariance).
If a correlation residual is negative, it suggests that the model overpredicts their observed association between the two variables (i.e., the observed covariance is smaller than the model-implied covariance).
If the two variables are connected by only indirect pathways, it may be helpful to respecify the model with direct pathways between the two variables, such as a direct effect (i.e., regression path) or a covariance path.
For guidance on evaluating local fit, see @Kline2024.

### 5. Interpreting and Using Latent Factors {#sec-interpretingAndUsingLatentFactors}

The next step is interpreting the model and latent factors.\index{factor analysis}
One data matrix can lead to many different (correct) models—you must choose one based on the factor structure and theory.\index{factor analysis}
Use theory to interpret the model and label the factors.\index{factor analysis}
In latent variable models, factors have meaning.
You can use them as predictors, mediators, moderators, or outcomes.
When possible, it is preferable to use the factors by examining their associations with other variables in the same model.
You can extract factor scores, if necessary, for use in other analyses; however, it is preferable to examine the associations between factors and other variables in the same model if possible.
And, using latent factors helps disattenuate associations for measurement error, to identify what the association is between variables when removing random measurement error.

## Example of Exploratory Factor Analysis {#sec-efaExample}

```{r}
#| eval: false
#| include: false

cor_mat <- cor(dataForFA[faVars], use = "pairwise.complete.obs")

cor_tidy <- cor_mat %>%
  as.data.frame() %>%
  rownames_to_column("var1") %>%
  pivot_longer(-var1, names_to = "var2", values_to = "correlation") %>%
  filter(var1 != var2) %>% # remove self-correlations
  mutate(pair = pmap_chr(list(var1, var2), ~ paste(sort(c(...)), collapse = "___"))) %>% 
  distinct(pair, .keep_all = TRUE) # remove duplicate pairs (symmetry)

high_corrs <- cor_tidy %>%
  filter(abs(correlation) > 0.85) %>%
  arrange(desc(abs(correlation)))

high_corrs

cov_mat <- cov(dataForFA[, faVars], use = "pairwise.complete.obs")
eigen_cov_mat <- eigen(cov_mat)
print(eigen_cov_mat$values)

# Index of smallest (most problematic) eigenvalue
idx <- which.min(eigen_cov_mat$values)

# Extract the associated eigenvector
problem_vector <- eigen_cov_mat$vectors[, idx]
names(problem_vector) <- faVars

# Sort by absolute magnitude to identify most influential variables
sort(abs(problem_vector), decreasing = TRUE)
```

We generated the scree plot in @fig-efaScreePlot1 using the `fa.parallel()` function of the `psych` package [@R-psych].
The optimal coordinates and the acceleration factor attempt to operationalize the Cattell scree test: i.e., the "elbow" of the scree plot [@Ruscio2012].
The optimal coordinators factor is quantified using a series of linear equations to determine whether observed eigenvalues exceed the predicted values.
The acceleration factor is quantified using the acceleration of the curve, that is, the second derivative.
The Kaiser-Guttman rule states to keep principal components whose eigenvalues are greater than 1.
However, for [exploratory factor analysis](#efa) (as opposed to [PCA](#pca)), the criterion is to keep the factors whose eigenvalues are greater than zero (i.e., not the factors whose eigenvalues are greater than 1) [@Dinno2014].

The number of factors to keep would depend on which criteria one uses.
Based on the rule to keep factors whose eigenvalues are greater than zero and based on the parallel test, we would keep five factors.
However, based on the Cattell scree test (the "elbow" of the screen plot minus one), we would keep three factors.
If using the optimal coordinates, we would keep eight factors; if using the acceleration factor, we would keep one factor.
Therefore, interpretability of the factors would be important for deciding how many factors to keep.

```{r}
#| label: fig-efaScreePlot1
#| fig-cap: "Scree Plot: With Comparisons to Simulated and Resampled Data."
#| fig-alt: "Scree Plot: With Comparisons to Simulated and Resampled Data."

psych::fa.parallel(
  x = dataForFA[faVars],
  fm = "minres", # errors out when using "ml"
  fa = "fa"
)
```

We generated the scree plot in @fig-efaScreePlot2 using the `nScree()` and `plotnScree()` functions of the `nFactors` package [@R-nFactors].

```{r}
#| label: fig-efaScreePlot2
#| fig-cap: "Scree Plot with Parallel Analysis."
#| fig-alt: "Scree Plot with Parallel Analysis."

#screeDataEFA <- nFactors::nScree(
#  x = cor( # throws error with correlation matrix, so use covariance instead (below)
#    dataForFA[faVars],
#    use = "pairwise.complete.obs"),
#  model = "factors")

screeDataEFA <- nFactors::nScree(
  x = cov(
    dataForFA[faVars],
    use = "pairwise.complete.obs"),
  cor = FALSE,
  model = "factors")

nFactors::plotnScree(screeDataEFA)
```

We generated the very simple structure (VSS) plots in Figures [-@fig-efaVSSPlot1] and [-@fig-efaVSSPlot2] using the `vss()` and `nfactors()` functions of the `psych` package [@R-psych].
In addition to VSS plots, the output also provides additional criteria by which to determine the optimal number of factors, each for which lower values are better, including the Velicer minimum average partial (MAP) test, the Bayesian information criterion (BIC), the sample size-adjusted BIC (SABIC), and the root mean square error of approximation (RMSEA).
Depending on the criterion, the optimal number of factors extracted varies between 2 and 8 factors.

```{r}
#| label: fig-efaVSSPlot1
#| fig-cap: "Very Simple Structure Plot."
#| fig-alt: "Very Simple Structure Plot."

psych::vss(
  dataForFA[faVars],
  rotate = "oblimin",
  fm = "minres") # errors out when using "mle"
```

```{r}
#| label: fig-efaVSSPlot2
#| fig-cap: "Model Indices by Number of Factors."
#| fig-alt: "Model Indices by Number of Factors."

psych::nfactors(
  dataForFA[faVars],
  rotate = "oblimin",
  fm = "minres") # errors out when using "ml"
```

We fit EFA models using the `efa()` function of the `lavaan` package [@Rosseel2012_packages; @R-lavaan].

```{r}
#| label: efa

efa_fit <- lavaan::efa(
  data = dataForFA,
  ov.names = faVars,
  nfactors = 1:7,
  rotation = "geomin",
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  meanstructure = TRUE,
  em.h1.iter.max = 2000000)
```

The model fits well according to CFI with 5 or more factors; the model fits well according to RMSEA with 6 or more factors.
However, 5+ factors does not represent much of a simplification (relative to the 18 variables included).
Moreover, in the model with four factors, only one variable had a significant loading on Factor 1.
Thus, even with four factors, one of the factors does not seem to represent the aggregation of multiple variables.
For these reasons, and because the "elbow test" for the scree plot suggested three factors, we decided to retain three factors and to see if we could achieve better fit by making additional model modifications (e.g., correlated residuals).
Correlated residuals may be necessary, for example, when variables are correlated for reasons other than the latent factors.

```{r}
#| label: efa-summary

summary(efa_fit)
```

A path diagram of the three-factor EFA model in @fig-efaPathDiagram-3factor was created using the `lavaanPlot()` function of the `lavaanPlot` package [@R-lavaanPlot].

```{r}
#| label: fig-efaPathDiagram-3factor
#| fig-cap: "Path Diagram of the Three-Factor Exploratory Factor Analysis Model."
#| fig-alt: "Path Diagram of the Three-Factor Exploratory Factor Analysis Model."

lavaanPlot::lavaanPlot(
  efa_fit$nf3,
  coefs = TRUE,
  #covs = TRUE,
  stand = TRUE)
```

Here is the syntax for estimating a three-factor EFA using exploratory structural equation modeling (ESEM).
Estimating the model in a ESEM framework allows us to make modifications to the model, such as adding correlated residuals, and adding predictors or outcomes of the latent factors. 
The syntax below represents the same model (with the same fit indices) as the three-factor EFA model above.

```{r}
#| label: efa-3factor-syntax

efa3factor_syntax <- '
 # EFA Factor Loadings
 efa("efa1")*f1 + 
 efa("efa1")*f2 + 
 efa("efa1")*f3 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + 
 passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame + 
 avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + 
 avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + 
 passer_rating + completion_percentage_above_expectation
'
```

To fit the ESEM model, we use the `sem()` function of the `lavaan` package [@Rosseel2012_packages; @R-lavaan].

```{r}
#| label: efa-3factor-fit

efa3factor_fit <- sem(
  efa3factor_syntax,
  data = dataForFA,
  information = "observed",
  missing = "ML",
  estimator = "MLR",
  rotation = "geomin",
  bounds = "standard",
  meanstructure = TRUE,
  em.h1.iter.max = 2000000)
```

The fit indices suggests that the model does not fit well to the data and that additional model modifications are necessary.
The fit indices are below.

```{r}
summary(
  efa3factor_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)
```

```{r}
lavaan::fitMeasures(
  efa3factor_fit,
  fit.measures = c(
    "chisq", "df", "pvalue",
    "chisq.scaled", "df.scaled", "pvalue.scaled",
    "chisq.scaling.factor",
    "baseline.chisq","baseline.df","baseline.pvalue",
    "rmsea", "cfi", "tli", "srmr",
    "rmsea.robust", "cfi.robust", "tli.robust"))
```

```{r}
lavaan::residuals(
  efa3factor_fit,
  type = "cor")
```

We can examine the model modification indices to identify parameters that, if estimated, would substantially improve model fit.
For instance, the modification indices below indicate additional correlated residuals that could substantially improve model fit.
However, it is generally not recommended to blindly estimate additional parameters solely based on modification indices, which can lead to data dredging and [overfitting](#sec-overfitting).
Rather, it is generally advised to consider modification indices in light of theory.
Based on the modification indices, we will add several correlated residuals to the model, to help account for why variables are associated with each other for reasons other than their underlying latent factors.

```{r}
lavaan::modificationindices(
  efa3factor_fit,
  sort. = TRUE)
```

Below are factor scores from the model for the first six players:

```{r}
efa3factor_factorScores <- lavaan::lavPredict(efa3factor_fit)

head(efa3factor_factorScores)
```

A path diagram of the three-factor ESEM model is in @fig-esemPathDiagram-3factor.

```{r}
#| label: fig-esemPathDiagram-3factor
#| fig-cap: "Path Diagram of the Three-Factor Exploratory Structural Equation Model."
#| fig-alt: "Path Diagram of the Three-Factor Exploratory Structural Equation Model."

lavaanPlot::lavaanPlot(
  efa3factor_fit,
  coefs = TRUE,
  covs = TRUE,
  stand = TRUE)
```

To make the plot interactive for editing, you can use the `plot_lavaan()` function of the `lavaangui` package [@R-lavaangui; @Karch2025_packages]:

```{r}
#| eval: false

lavaangui::plot_lavaan(efa3factor_fit)
```

Below is a modification of the three-factor model with correlated residuals.
For instance, it makes sense that passing completions and attempts are related to each other (even after accounting for their latent factor).

```{r}
#| label: efa-3factorModified-syntax

efa3factorModified_syntax <- '
 # EFA Factor Loadings
 efa("efa1")*F1 + 
 efa("efa1")*F2 + 
 efa("efa1")*F3 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + 
 passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame + 
 avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + 
 avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + 
 passer_rating + completion_percentage_above_expectation
 
 # Correlated Residuals
 completionsPerGame ~~ attemptsPerGame
 passing_yardsPerGame ~~ passing_yards_after_catchPerGame
 attemptsPerGame ~~ passing_yardsPerGame
 attemptsPerGame ~~ passing_air_yardsPerGame
 passing_air_yardsPerGame ~~ passing_yards_after_catchPerGame
 passing_yards_after_catchPerGame ~~ avg_completed_air_yards
 passing_tdsPerGame ~~ passer_rating
 completionsPerGame ~~ passing_air_yardsPerGame
 max_completed_air_distance ~~ max_air_distance
 aggressiveness ~~ completion_percentage_above_expectation
'
```

```{r}
#| label: efa-3factorModified-fit

efa3factorModified_fit <- sem(
  efa3factorModified_syntax,
  data = dataForFA,
  information = "observed",
  missing = "ML",
  estimator = "MLR",
  rotation = "geomin",
  bounds = "standard",
  meanstructure = TRUE,
  em.h1.iter.max = 2000000)
```

```{r}
summary(
  efa3factorModified_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)
```

The model fits substantially better (though not perfectly) with the additional correlated residuals.
Below are the model fit indices:

```{r}
lavaan::fitMeasures(
  efa3factorModified_fit,
  fit.measures = c(
    "chisq", "df", "pvalue",
    "chisq.scaled", "df.scaled", "pvalue.scaled",
    "chisq.scaling.factor",
    "baseline.chisq","baseline.df","baseline.pvalue",
    "rmsea", "cfi", "tli", "srmr",
    "rmsea.robust", "cfi.robust", "tli.robust"))
```

```{r}
lavaan::residuals(
  efa3factorModified_fit,
  type = "cor")
```

```{r}
lavaan::modificationindices(
  efa3factorModified_fit,
  sort. = TRUE)
```

Below are factor scores from the model for the first six players:

```{r}
efa3factorModified_factorScores <- lavaan::lavPredict(efa3factorModified_fit)

head(efa3factorModified_factorScores)

dataForFA_efa <- cbind(dataForFA, efa3factorModified_factorScores)
```

The path diagram of the modified ESEM model with correlated residuals is in @fig-esemPathDiagram-3factorModified.

```{r}
#| label: fig-esemPathDiagram-3factorModified
#| fig-cap: "Path Diagram of the Three-Factor Exploratory Structural Equation Model With Correlated Residuals."
#| fig-alt: "Path Diagram of the Three-Factor Exploratory Structural Equation Model With Correlated Residuals."

lavaanPlot::lavaanPlot(
  efa3factorModified_fit,
  coefs = TRUE,
  #covs = TRUE,
  stand = TRUE)
```

```{r}
#| eval: false

lavaangui::plot_lavaan(efa3factorModified_fit)
```

Model fit of nested models can be compared with a chi-square difference test.
This allows us to evaluate whether the more complex model fits signficantly better than the less complex model.
In this case, our more complex model is the three-factor model with correlated residuals; the less complex model is the three-factor model without correlated residuals.
The modified model with the correlated residuals and the original model are considered "nested" models.
The original model is nested within the modified model because the modified model includes all of the terms of the original model along with additional terms.

```{r}
anova(
  efa3factorModified_fit,
  efa3factor_fit
)
```

In this case, the model with the correlated residuals fit significantly better (i.e., has a significantly smaller chi-square value) than the model without the correlated residuals.

Here are the variables that had a standardized loading greater than 0.3 on each of the factors:

```{r}
factor1vars <- c(
  "completionsPerGame","attemptsPerGame","passing_yardsPerGame","passing_tdsPerGame",
  "passing_air_yardsPerGame","passing_yards_after_catchPerGame","passing_first_downsPerGame")

factor2vars <- c(
  "avg_completed_air_yards","avg_intended_air_yards","aggressiveness","max_completed_air_distance",
  "avg_air_distance","max_air_distance","avg_air_yards_to_sticks")

factor3vars <- c(
  "passing_cpoe","pass_comp_pct","passer_rating","completion_percentage_above_expectation")
```

The variables that loaded most strongly onto factor 1 appear to reflect Quarterback usage: completions per game, passing attempts per game, passing yards per game, passing touchdowns per game, passing air yards (total horizontal distance the ball travels on all pass attempts) per game, passing yards after the catch per game, and first downs gained per game by passing.
Quarterbacks who tend to throw more tend to have higher levels on those variables.
Thus, we label component 1 as "Usage", which reflects total Quarterback involvement, regardless of efficiency or outcome.

The variables that loaded most strongly onto factor 2 appear to reflect Quarterback aggressiveness: average air yards on completed passes, average air yards on all attempted passes, aggressiveness (percentage of passing attempts thrown into tight windows, where there is a defender within one yard or less of the receiver at the time of the completion or incompletion), average amount of air yards ahead of or behind the first down marker on passing attempts, average air distance (the true three-dimensional distance the ball travels in the air), maximum air distance, and maximum air distance on completed passes.
Quarterbacks who throw the ball farther and into tighter windows tend to have higher values on those variables.
Thus, we label component 2 as "Aggressiveness", which reflects throwing longer, more difficult passes with a tight window.

The variables that loaded most strongly onto factor 3 appear to reflect Quarterback performance: passing completion percentage above expectation, pass completion percentage, and passer rating.
Quarterbacks who perform better tend to have higher values on those variables.
Thus, we label component 3 as "Performance".

Here are the players and seasons that showed the highest levels of Quarterback "Usage":

```{r}
dataForFA_efa %>% 
  arrange(-F1) %>% 
  select(player_display_name, season, F1, all_of(factor1vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the lowest levels of Quarterback "Usage":

```{r}
dataForFA_efa %>% 
  arrange(F1) %>% 
  select(player_display_name, season, F1, all_of(factor1vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the highest levels of Quarterback "Aggressiveness":

```{r}
dataForFA_efa %>% 
  arrange(-F2) %>% 
  select(player_display_name, season, F2, all_of(factor2vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the lowest levels of Quarterback "Aggressiveness":

```{r}
dataForFA_efa %>% 
  arrange(F2) %>% 
  select(player_display_name, season, F2, all_of(factor2vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the highest levels of Quarterback "Performance":

```{r}
dataForFA_efa %>% 
  arrange(-F3) %>% 
  select(player_display_name, season, games, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

If we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the highest levels of Quarterback "Performance":

```{r}
dataForFA_efa %>% 
  arrange(-F3) %>% 
  filter(games >= 10) %>% 
  select(player_display_name, season, games, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the lowest levels of Quarterback "Performance":

```{r}
dataForFA_efa %>% 
  arrange(F3) %>% 
  select(player_display_name, season, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

If we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the lowest levels of Quarterback "Performance":

```{r}
dataForFA_efa %>% 
  arrange(F3) %>% 
  filter(games >= 10) %>% 
  select(player_display_name, season, games, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

## Example of Confirmatory Factor Analysis {#sec-cfaExample}

```{r}
#| eval: false
#| include: false

psych::principal(
  dataForFA_standardized[faVars],
  nfactors = 3,
  rotate = "oblimin")

'
 #Factor loadings
 F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + passing_interceptionsPerGame + passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame
 F2 =~ pacr + avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + avg_air_yards_to_sticks + expected_completion_percentage + avg_air_distance + max_air_distance
 F3 =~ passing_epa + passing_cpoe + pass_comp_pct + passer_rating + completion_percentage_above_expectation
'

factor1_syntax <- '
 # Factor loadings
 F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame
 
 # Correlated residuals
 passing_air_yardsPerGame ~~ passing_yards_after_catchPerGame
 completionsPerGame ~~ attemptsPerGame
 attemptsPerGame ~~ passing_yards_after_catchPerGame
 passing_yards_after_catchPerGame ~~ passing_first_downsPerGame
 completionsPerGame ~~ passing_first_downsPerGame
 attemptsPerGame ~~ passing_tdsPerGame
 completionsPerGame ~~ passing_tdsPerGame
 passing_tdsPerGame ~~ passing_air_yardsPerGame
'

factor1_fit <- lavaan::cfa(
  factor1_syntax,
  data = dataForFA,
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  std.lv = TRUE,
  cluster = "player_id", # account for nested data within the same player (i.e., longitudinal data)
  em.h1.iter.max = 2000000)

summary(
  factor1_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)

lavaan::modificationindices(
  factor1_fit,
  sort. = TRUE)

factor2_syntax <- '
 # Factor loadings
 F2 =~ avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + avg_air_distance + max_air_distance + avg_air_yards_to_sticks #expected_completion_percentage + pacr
 
 # Correlated residuals
 max_completed_air_distance ~~ max_air_distance
 avg_completed_air_yards ~~ max_completed_air_distance
 avg_completed_air_yards ~~ avg_air_distance
 avg_completed_air_yards ~~ max_air_distance
 avg_intended_air_yards ~~ max_completed_air_distance
'

factor2_fit <- lavaan::cfa(
  factor2_syntax,
  data = dataForFA,
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  std.lv = TRUE,
  cluster = "player_id", # account for nested data within the same player (i.e., longitudinal data)
  em.h1.iter.max = 2000000)

summary(
  factor2_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)

lavaan::modificationindices(
  factor2_fit,
  sort. = TRUE)

factor3_syntax <- '
 #Factor loadings
 F3 =~ passing_cpoe + pass_comp_pct + passer_rating + completion_percentage_above_expectation
'

factor3_fit <- lavaan::cfa(
  factor3_syntax,
  data = dataForFA,
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  std.lv = TRUE,
  cluster = "player_id", # account for nested data within the same player (i.e., longitudinal data)
  em.h1.iter.max = 2000000)

summary(
  factor3_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)

lavaan::modificationindices(
  factor3_fit,
  sort. = TRUE)
```

We fit CFA models using the `cfa()` function of the `lavaan` package [@Rosseel2012_packages; @R-lavaan].
We compare one-, two-, and three-factor CFA models.

Below is the syntax for the one-factor CFA model:

```{r}
cfa1factor_syntax <- '
 #Factor loadings
 F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + 
 passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame + 
 avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + 
 avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + 
 passer_rating + completion_percentage_above_expectation
'
```

```{r}
#| label: cfa-1factor-fit

cfa1factor_fit <- lavaan::cfa(
  cfa1factor_syntax,
  data = dataForFA,
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  std.lv = TRUE,
  cluster = "player_id", # account for nested data within the same player (i.e., longitudinal data)
  em.h1.iter.max = 2000000)
```

```{r}
summary(
  cfa1factor_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)
```

The one-factor model did not fit well according to the fit indices:

```{r}
lavaan::fitMeasures(
  cfa1factor_fit,
  fit.measures = c(
    "chisq", "df", "pvalue",
    "chisq.scaled", "df.scaled", "pvalue.scaled",
    "chisq.scaling.factor",
    "baseline.chisq","baseline.df","baseline.pvalue",
    "rmsea", "cfi", "tli", "srmr",
    "rmsea.robust", "cfi.robust", "tli.robust"))
```

```{r}
lavaan::residuals(
  cfa1factor_fit,
  type = "cor")
```

```{r}
lavaan::modificationindices(
  cfa1factor_fit,
  sort. = TRUE)
```

Below are factor scores from the model for the first six players:

```{r}
cfa1factor_factorScores <- lavaan::lavPredict(cfa1factor_fit)

head(cfa1factor_factorScores)
```

The path diagram of the one-factor CFA model is in @fig-cfaPathDiagram-1factor.

```{r}
#| label: fig-cfaPathDiagram-1factor
#| fig-cap: "Path Diagram of the One-Factor Confirmatory Factor Analysis Model."
#| fig-alt: "Path Diagram of the One-Factor Confirmatory Factor Analysis Model."

lavaanPlot::lavaanPlot(
  cfa1factor_fit,
  coefs = TRUE,
  #covs = TRUE,
  stand = TRUE)
```

```{r}
#| eval: false

lavaangui::plot_lavaan(cfa1factor_fit)
```

Below is the syntax for the two-factor CFA model:

```{r}
cfa2factor_syntax <- '
 #Factor loadings
 F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + 
 passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame
 
 F2 =~ avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + 
 avg_air_distance + max_air_distance + avg_air_yards_to_sticks + passing_cpoe + pass_comp_pct + 
 passer_rating + completion_percentage_above_expectation
'
```

```{r}
#| label: cfa-2factor-fit

cfa2factor_fit <- lavaan::cfa(
  cfa2factor_syntax,
  data = dataForFA,
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  std.lv = TRUE,
  cluster = "player_id", # account for nested data within the same player (i.e., longitudinal data)
  em.h1.iter.max = 2000000)
```

```{r}
summary(
  cfa2factor_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)
```

The two-factor model did not fit well according to the fit indices:

```{r}
lavaan::fitMeasures(
  cfa2factor_fit,
  fit.measures = c(
    "chisq", "df", "pvalue",
    "chisq.scaled", "df.scaled", "pvalue.scaled",
    "chisq.scaling.factor",
    "baseline.chisq","baseline.df","baseline.pvalue",
    "rmsea", "cfi", "tli", "srmr",
    "rmsea.robust", "cfi.robust", "tli.robust"))
```

```{r}
lavaan::residuals(
  cfa2factor_fit,
  type = "cor")
```

```{r}
lavaan::modificationindices(
  cfa2factor_fit,
  sort. = TRUE)
```

Below are factor scores from the model for the first six players:

```{r}
cfa2factor_factorScores <- lavaan::lavPredict(cfa2factor_fit)

head(cfa2factor_factorScores)
```

The path diagram of the two-factor CFA model is in @fig-cfaPathDiagram-2factor.

```{r}
#| label: fig-cfaPathDiagram-2factor
#| fig-cap: "Path Diagram of the Two-Factor Confirmatory Factor Analysis Model."
#| fig-alt: "Path Diagram of the Two-Factor Confirmatory Factor Analysis Model."

lavaanPlot::lavaanPlot(
  cfa2factor_fit,
  coefs = TRUE,
  covs = TRUE,
  stand = TRUE)
```

```{r}
#| eval: false

lavaangui::plot_lavaan(cfa2factor_fit)
```

Because the one-factor model is nested within the two-factor model, we can compare them using a chi-square difference test.
The two factor model fit considerably better than the one-factor model in terms of a lower chi-square value:

```{r}
anova(
  cfa2factor_fit,
  cfa1factor_fit
)
```

Below is the syntax for the three-factor model:

```{r}
cfa3factor_syntax <- '
 #Factor loadings
 F1 =~ completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + 
 passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame
 
 F2 =~ avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + 
 avg_air_distance + max_air_distance + avg_air_yards_to_sticks
 
 F3 =~ passing_cpoe + pass_comp_pct + passer_rating + completion_percentage_above_expectation
'
```

```{r}
#| label: cfa-3factor-fit

cfa3factor_fit <- lavaan::cfa(
  cfa3factor_syntax,
  data = dataForFA,
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  std.lv = TRUE,
  cluster = "player_id", # account for nested data within the same player (i.e., longitudinal data)
  em.h1.iter.max = 2000000)
```

```{r}
summary(
  cfa3factor_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)
```

The three-factor model did not fit well according to fit indices:
```{r}
lavaan::fitMeasures(
  cfa3factor_fit,
  fit.measures = c(
    "chisq", "df", "pvalue",
    "chisq.scaled", "df.scaled", "pvalue.scaled",
    "chisq.scaling.factor",
    "baseline.chisq","baseline.df","baseline.pvalue",
    "rmsea", "cfi", "tli", "srmr",
    "rmsea.robust", "cfi.robust", "tli.robust"))
```

However, we know that the three-factor model fit improved considerably when accounting for correlated residuals.
So, we plan to examine modification indices to see if we can account for covariances between variables that were not explained by their underlying latent factors.

```{r}
lavaan::residuals(
  cfa3factor_fit,
  type = "cor")
```

Below are the modification indices, suggesting potential model modifications that would improve model fit such as correlated residuals and cross-loadings.

```{r}
lavaan::modificationindices(
  cfa3factor_fit,
  sort. = TRUE)
```

Below are factor scores from the model for the first six players:

```{r}
cfa3factor_factorScores <- lavaan::lavPredict(cfa3factor_fit)

head(cfa3factor_factorScores)
```

The path diagram of the three-factor CFA model is in @fig-cfaPathDiagram-3factor.

```{r}
#| label: fig-cfaPathDiagram-3factor
#| fig-cap: "Path Diagram of the Three-Factor Confirmatory Factor Analysis Model."
#| fig-alt: "Path Diagram of the Three-Factor Confirmatory Factor Analysis Model."

lavaanPlot::lavaanPlot(
  cfa3factor_fit,
  coefs = TRUE,
  covs = TRUE,
  stand = TRUE)
```

```{r}
#| eval: false

lavaangui::plot_lavaan(cfa3factor_fit)
```

Because the two-factor model is nested within the three-factor model, we can compare them using a chi-square difference test.
The three factor model fit considerably better than the two-factor model in terms of a lower chi-square value:

```{r}
anova(
  cfa3factor_fit,
  cfa2factor_fit
)
```

Based on the model modifications suggested by the modification indices for the three-factor model, we modify the model to account for correlated residuals, using the syntax below:

```{r}
cfa3factorModified_syntax <- '
 # Factor loadings
 F1 =~ NA*completionsPerGame + attemptsPerGame + passing_yardsPerGame + passing_tdsPerGame + 
 passing_air_yardsPerGame + passing_yards_after_catchPerGame + passing_first_downsPerGame
 
 F2 =~ NA*avg_completed_air_yards + avg_intended_air_yards + aggressiveness + max_completed_air_distance + 
 avg_air_distance + max_air_distance + avg_air_yards_to_sticks
 
 F3 =~ NA*passing_cpoe + pass_comp_pct + passer_rating + completion_percentage_above_expectation
 
 # Cross loadings
 #F3 =~ attemptsPerGame

 # Correlated residuals
 passing_air_yardsPerGame ~~ passing_yards_after_catchPerGame
 completionsPerGame ~~ attemptsPerGame
 attemptsPerGame ~~ passing_yards_after_catchPerGame
 passing_yards_after_catchPerGame ~~ passing_first_downsPerGame
 completionsPerGame ~~ passing_first_downsPerGame
 attemptsPerGame ~~ passing_tdsPerGame
 completionsPerGame ~~ passing_tdsPerGame
 passing_tdsPerGame ~~ passing_air_yardsPerGame
 
 max_completed_air_distance ~~ max_air_distance
 avg_completed_air_yards ~~ max_completed_air_distance
 avg_completed_air_yards ~~ avg_air_distance
 avg_completed_air_yards ~~ max_air_distance
 avg_intended_air_yards ~~ max_completed_air_distance
 
 completionsPerGame ~~ pass_comp_pct
 passing_yardsPerGame ~~ avg_completed_air_yards
 passing_yardsPerGame ~~ max_completed_air_distance
 passing_tdsPerGame ~~ passer_rating
 aggressiveness ~~ completion_percentage_above_expectation
 
 # Variances
 F1 ~~ 1*F1
 F2 ~~ 1*F2
'
```

```{r}
#| label: cfa-3factorModified-fit

cfa3factorModified_fit <- lavaan::cfa(
  cfa3factorModified_syntax,
  data = dataForFA,
  missing = "ML",
  estimator = "MLR",
  bounds = "standard",
  #std.lv = TRUE,
  cluster = "player_id", # account for nested data within the same player (i.e., longitudinal data)
  em.h1.iter.max = 2000000)
```

```{r}
summary(
  cfa3factorModified_fit,
  fit.measures = TRUE,
  standardized = TRUE,
  rsquare = TRUE)
```

The three-factor model with correlated residuals fit was acceptable according to CFI and TLI.
The model fit was subpar for RMSEA and SRMR.

```{r}
lavaan::fitMeasures(
  cfa3factorModified_fit,
  fit.measures = c(
    "chisq", "df", "pvalue",
    "chisq.scaled", "df.scaled", "pvalue.scaled",
    "chisq.scaling.factor",
    "baseline.chisq","baseline.df","baseline.pvalue",
    "rmsea", "cfi", "tli", "srmr",
    "rmsea.robust", "cfi.robust", "tli.robust"))
```

```{r}
lavaan::residuals(
  cfa3factorModified_fit,
  type = "cor")
```

```{r}
lavaan::modificationindices(
  cfa3factorModified_fit,
  sort. = TRUE)
```

Below are factor scores from the model for the first six players:

```{r}
cfa3factorModified_factorScores <- lavaan::lavPredict(cfa3factorModified_fit)

head(cfa3factorModified_factorScores)

dataForFA_cfa <- cbind(dataForFA, cfa3factorModified_factorScores)
```

The path diagram of the modified three-factor CFA model with correlated residuals is in @fig-cfaPathDiagram-3factorModified.

```{r}
#| label: fig-cfaPathDiagram-3factorModified
#| fig-cap: "Path Diagram of the Modified Three-Factor Confirmatory Factor Analysis Model With Correlated Residuals."
#| fig-alt: "Path Diagram of the Modified Three-Factor Confirmatory Factor Analysis Model With Correlated Residuals."

lavaanPlot::lavaanPlot(
  cfa3factorModified_fit,
  coefs = TRUE,
  #covs = TRUE,
  stand = TRUE)
```

```{r}
#| eval: false

lavaangui::plot_lavaan(cfa3factorModified_fit)
```

```{r}
anova(
  cfa3factorModified_fit,
  cfa3factor_fit
)
```

Here are the variables that loaded onto each of the factors:

```{r}
factor1vars <- c(
  "completionsPerGame","attemptsPerGame","passing_yardsPerGame","passing_tdsPerGame",
  "passing_air_yardsPerGame","passing_yards_after_catchPerGame","passing_first_downsPerGame")

factor2vars <- c(
  "avg_completed_air_yards","avg_intended_air_yards","aggressiveness","max_completed_air_distance",
  "avg_air_distance","max_air_distance","avg_air_yards_to_sticks")

factor3vars <- c(
  "passing_cpoe","pass_comp_pct","passer_rating","completion_percentage_above_expectation")
```

The variables that loaded most strongly onto factor 1 appear to reflect Quarterback usage: completions per game, passing attempts per game, passing yards per game, passing touchdowns per game, passing air yards (total horizontal distance the ball travels on all pass attempts) per game, passing yards after the catch per game, and first downs gained per game by passing.
Quarterbacks who tend to throw more tend to have higher levels on those variables.
Thus, we label component 1 as "Usage", which reflects total Quarterback involvement, regardless of efficiency or outcome.

The variables that loaded most strongly onto factor 2 appear to reflect Quarterback aggressiveness: average air yards on completed passes, average air yards on all attempted passes, aggressiveness (percentage of passing attempts thrown into tight windows, where there is a defender within one yard or less of the receiver at the time of the completion or incompletion), average amount of air yards ahead of or behind the first down marker on passing attempts, average air distance (the true three-dimensional distance the ball travels in the air), maximum air distance, and maximum air distance on completed passes.
Quarterbacks who throw the ball farther and into tighter windows tend to have higher values on those variables.
Thus, we label component 2 as "Aggressiveness", which reflects throwing longer, more difficult passes with a tight window.

The variables that loaded most strongly onto factor 3 appear to reflect Quarterback performance: passing completion percentage above expectation, pass completion percentage, and passer rating.
Quarterbacks who perform better tend to have higher values on those variables.
Thus, we label component 3 as "Performance".

Here are the players and seasons that showed the highest levels of Quarterback "Usage":

```{r}
dataForFA_cfa %>% 
  arrange(-F1) %>% 
  select(player_display_name, season, F1, all_of(factor1vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the lowest levels of Quarterback "Usage":

```{r}
dataForFA_cfa %>% 
  arrange(F1) %>% 
  select(player_display_name, season, F1, all_of(factor1vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the highest levels of Quarterback "Aggressiveness":

```{r}
dataForFA_cfa %>% 
  arrange(-F2) %>% 
  select(player_display_name, season, F2, all_of(factor2vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the lowest levels of Quarterback "Aggressiveness":

```{r}
dataForFA_cfa %>% 
  arrange(F2) %>% 
  select(player_display_name, season, F2, all_of(factor2vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the highest levels of Quarterback "Performance":

```{r}
dataForFA_cfa %>% 
  arrange(-F3) %>% 
  select(player_display_name, season, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

If we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the highest levels of Quarterback "Performance":

```{r}
dataForFA_cfa %>% 
  arrange(-F3) %>% 
  filter(games >= 10) %>% 
  select(player_display_name, season, games, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

Here are the players and seasons that showed the lowest levels of Quarterback "Performance":

```{r}
dataForFA_cfa %>% 
  arrange(F3) %>% 
  select(player_display_name, season, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

If we restrict it to Quarterbacks who played at least 10 games in the season, here are the players and seasons that showed the lowest levels of Quarterback "Performance":

```{r}
dataForFA_cfa %>% 
  arrange(F3) %>% 
  filter(games >= 10) %>% 
  select(player_display_name, season, games, F3, all_of(factor3vars)) %>% 
  na.omit() %>% 
  head()
```

## Conclusion {#sec-factorAnalysisConclusion}

::: {.content-visible when-format="html"}

## Session Info {#sec-factorAnalysisSessionInfo}

```{r}
sessionInfo()
```

:::
